---
video_id: TUp-bu7HnKM
video_url: https://www.youtube.com/watch?v=TUp-bu7HnKM
title: AWS re:Invent 2025 - Boost performance and reduce costs in Amazon Aurora and Amazon RDS (DAT312)
author: AWS Events
published_date: 2025-12-03
length_minutes: 58.8
views: 475
description: "In this session, explore Amazon Aurora and Amazon RDS cost components and learn important best practices that can help you improve the performance of your relational database workloads while reducing spend on cost components such as compute, storage, backup, and I/O. Learn about the latest performance monitoring features, which provide insights on how to efficiently track and optimize database performance at scale and help ensure that youâ€™re maximizing efficiency while keeping costs under contro..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

OK, hello everyone and welcome. I'm excited to have you here today as we dive into the world of performance and cost optimization in Amazon Aurora and Amazon RDS. We're going to follow the journey of a fictional company named Any Company, and while the name is made up, it does represent some of the most common questions I get from customers just like you. My goal is that you will walk away from this session with actionable insights that you can directly apply to your own database environments. My name is Penny De Basque, and I'm a database solution architect here at AWS. Thank you for joining me, and let's get started. I assume many of you are already familiar with Amazon RDS, but just to level set, Amazon RDS is a fully managed relational database service. With Amazon RDS you can spend time innovating and optimizing your applications instead of focusing on operational database tasks such as upgrades, backups, provisioning, disaster recovery, etc. Amazon RDS supports both popular commercial engines and open source engines, and as part of the Amazon RDS family, we also offer Amazon Aurora, which deserves special mention as our cloud native database relational engine which is fully compatible with both MySQL and Postgress, and it was designed to provide you the enterprise grade security. Availability and reliability of commercial grade databases with the simplicity and the cost effectiveness of open source databases and due to Amazon Aurora innovations and unique capabilities, it is actually the fastest growing service in the history of AWS. We have hundreds of thousands of customers who use it today for their relational database. Amazon Aurora was actually evolved over the last decade into a family of different options. The latest addition to Amazon Aurora Family was Aura DSQL, is already sequel, which was announced last year and reinvent. How many of you have heard about this sequel? OK, so I just want to set expectations up front. We are not going to cover this sequel in this presentation. And the reason for that is because Aurora DSQL has a completely different pricing model and architecture. We are going to talk about RDS and Aurora, both Aurora provisioned, AKA instance-based, and Aurora Serless. Having said that, let's get started by talking about the RDS cost dimensions. There are costs associated to every RDS cluster, and this would be compute and storage. There are also costs associated to most of the RDS clusters, like backup, data transfer, and IOPs, and this depends on your specific cluster configuration. And there are additional cost dimensions which really depend on your specific business and technical use cases and which database features you use for your applications. And as part of today's presentation, we will talk about some of these cost dimensions by showing examples from any company. And speaking of any company, I would like to provide some context to our customer story. Any company empowers e-commerce sellers with generative AI to turn every product into a bestseller. They run Amazon EKS for the application layer and Amazon RDS for the operational database. As an early stage, fast growing startup, they faced a common challenge balancing performance needs as workload scales while doing so in a cost conscious manner. So this is a common challenge that we as solution architects help customers to solve, and today we'll see how we do that. So back to the RDS cost dimensions, I would like to focus on 3 cost dimensions today compute, storage, and backup. So let's get started with compute. And with that, I would like to introduce any company's first challenge. You probably could resonate with some of those challenges. Any company started very fast. They have a great customer base, but suddenly their CPU spiked to 100% due to poorly running SQL statements. So there are two key questions we need to ask here. First, can we identify and resolve the problematic SQL? And second, what is the right instance type to use for any company's workload? They started with R6G to Excel. However, does that mean that this is the best instance for their needs? So we need to think about that, and this is the common questions we get from customers where scaling decisions need to be both performance and cost conscious. And with our with AWS you have um a wide range of instance classes to choose from burstable for smaller and variable workloads. General purpose for CPU intensive tasks, memory optimized for memory intensive applications, and Graviton, which are based on ARM CPU architecture, offer enhanced price performance compared to its 86 counterparts, making them a great choice and cost effective option for many of the workloads running on AWS. And choosing the right instance class and instance type may seem complex, but with the right observability tools, it could become much simpler. And I would like to talk about observability for a moment. For many years, RDS and Aurora customers just like you used various different tools and solutions, so cloudWatch metrics, for instance, level metrics. Enhanced monitoring for operating system level insights, performance insights for career analysis and weight set analysis, and custom solutions built on RDS events, RDS logs, and scripts. However, all of these tools created silos, forcing you to jump between different consoles without necessarily seeing the full picture, and this is exactly why last year we introduced Cloud Watch Database Insights. Cloud Watch Database Insights is our single pane of glass for database observability. It unifies metrics, logs, and performance data into one cohesive experience, and I would like to show you how it looks like with a quick demo. So let's do that. What you can see here is the fleet level view, and essentially that means that you can see an entire fleet of instances. You can choose fleet based on tags or specific instances you would like to dive into. What we can see here is that we have a fleet of 6 instances, and one of them has a warning due to database load. OK, so what we would like to do is to understand what is the root cause for that. So when we go to top database instances, I can see there is one instance with a name which might look familiar to you, any company, and that instance is the top in terms of database load. We also can see the top SQL across my entire fleet. There is a joint between customers and payments, and we'll talk about that SQL later in this presentation. Now I can also look at the top database instances based on various metrics like CPU utilization, number of connections, I ops, network throughput, etc. And also here you can see that any company is the one that suffers from most of the bottlenecks, most of the database issues. So we would like to dive into any company's instance, and what we can see here is the database flow chart, the average active sessions, and the number of VCPUs, which is represented by the horizontal dotted line, and we can see that we have top weight events that exceed the number of VCPUs in the machine, and we can see CPU and also IO buff file weight. Please remember IO buff file right weight event because we will talk about it later in this presentation. You can see database telemetry. There are dozens of metrics you can choose from. You can see logs, slow query logs, RDS events, OS processes, all of these different capabilities which you used to have in the past in different consoles now are fully available in one unified experience, unified cohesive experience. Now I can analyze the dashboard. I can analyze the performance based on a specific time period. And as you can see here, It brings me automated documentations, so I click on view performance analysis dashboard. I can see unusual high load. So we have 21 times more load than the typical database load. Obviously there is something that we need to address here. Now, when I scroll down, I can see that it identifies that 91% of my database load is associated to CPU and IO weight events, and it also shows me what is the problematic sequel. This is the same one as before, we will see that in a second. Essentially this is a joint between customers and payments, so we do a joint between those two tables and we order by the amount. So we show the top customer information and payment information, as you can see, it's ordered by descending. So we show the top X payments. If I want to see the exact SQL statement, I can go back to Cloud Watch Database Insights, and here I can see the full text of the SQL. So we can see the standing limit 100. So essentially it means I would like to see the top 100 payments of those customers. I'm using my favorite IDE, which is Data Grip, and I'm running Explain plan. Explain, analyze. You can see the query takes 30 seconds. This is pretty slow for operational database. We're not going to dive into performance and execution plans in this session, but we are looking to see that we have parallel sequation scans on both payments and customers. So full table scans, I need to sort the entire table in order to get the top 100 because there is no index. Index by definition, by design is already ordered, but because, but because I don't have an index, it takes a lot of time. So I would create an index based on amount and customer ID. On the payments table and now when I run, explain, analyze again you can see instead of 30 seconds it's reduced to 46 milliseconds. That's a pretty good, pretty good improvement I would say. And now I can see that I'm using indexes for the join and I no longer have the issue. If I go back to Cloudwatch Database insights, I can see that any company is no longer has any warning. The sequel that we saw earlier doesn't appear as a top sequel. We actually see the create index statement instead. And if I scroll down to the uh top database load instances instead of 90+% utilization, now it's 11%. So we look at a much healthier instances instance right now. If I go to database instance, everything looks so much better. So CPU utilization dropped. You can see the IO latency also dropped. You can see the database low chart at the top. And in the past you can see that the weight events were higher than the number of VCPUs, which is always a red flag for performance, but now I no longer have this issue. So that was a quick demo to show you how useful cloud Watch database insights can be used. And as a reminder, the first issue we had with this with this any company. Is that we saw spikes for 100% and we suffered from SQL optimization from from poorly running SQL. So what we did, we optimized the SQL and following that optimization, now I don't need such a large instance. I can, I can optimize cost. So instead of using two extra large instances, I moved to extra large. It has half of the VCPUs and half of the memory, but I don't need more than that. You may also notice something interesting here as well. I moved from R6G to RAG. So R 6G is based on graviton 2 generation and RAG is based on graviton 4 generation, which provides improved price performance by combining SQL optimization and instance right sizing and leveraging the latest graviton generation and graviton innovation. I was able to reduce 46% on cost and also reduce 70% CPU utilization on average. So that was a quick example of how using the right tools we can identify problematic SQL and using right sizing we can also making them cost conscious in terms of the right instance that we're using. So now I would like to talk about the 2nd challenge, which is noisy neighbors. Any company's core API performance degraded once data analysts, who are essentially internal customers, basically employees of any company, started running reporting queries on top of this database. So what happens is that this is the same database that serves the customer facing application where the company actually makes money from. And this impacted the overall user experience and the reliability. So a common solution here or a common challenge that we need to address here is we have competing workloads that interfere with each other. So we need a solution that can separate these workloads and manage them separately without affecting each other. One traditional scaling option is to scale vertically. You all know that. You are all familiar with this option. We can add more memory and VCPUs and get more resources. However, for this use case, since we're talking about only reporting queries, reporting data, why not offloading data to read replicas? We can offload data to read replicas, and by doing so we can effectively scale read traffic without impacting the core customer facing application. So you may wonder about the cost differences. What are the differences between vertical scaling to horizontal scaling? So let's assume that we did vertical scale and we moved from RAG extra large to RAG to extra large. We will end up with doubling the compute cost. Instead, with reed replicas, we have more flexibility. We can create up to 15 reed replicas, each one in different size. So for the reporting queries we realized we don't need such a large instance. We can use large and by using RAG large that can meet the requirements for the reporting queries, we managed to reduce the cost significantly. So that's another way for us to making cost conscious decisions instead of just scanning vertically and spending money. And by the way, please note that for this example and all the remaining examples in this presentation, I will be using the on-demand cost from on-demand pricing from North Virginia region. You could obviously further reduce the cost by using longer term commitments with reserved instances, for example, but just for the simplicity's sake, I'm using the on-demand pricing for North Virginia. Another alternative to handle the noisy neighbors issue is to use caching. So for example, we can Cash frequently frequently access data like SQL result sets. One way to do that is using elastic cash. So we have one strategy which is right through, and with this strategy we update the data in the cache whenever we write data to the database. So now when we want to retrieve the information by the query. We can access the in-memory database, the Amazon Elastic cache, and that provides us microsecond latency. That's much faster than any EBS option we offer. So that's one example of scaling without necessarily increasing the instance to a larger instance type. However, this approach is typically useful when you have repetitive queries. For the any company use case, most of the queries are actually based on ad hoc, um, dynamic queries. They have dynamic filters, so they decided not to go with the approach of caching. Instead, they offloaded the queries using the read replica approach, and that enabled them to maintain consistent performance for their production application without interfering the core customer application. And an overall 50% improvement in the core API of the production application. So, read replica is key when you have reporting queries that you can offload them to the read replicas, you should be aware of that and utilize that. So we talked about compute. Now I would like to talk about storage. In Amazon RDS you have a range of storage options. GP 2 ties the number of ops to the amount of storage allocated. So as you allocate more storage to the volume, you will get more IOPs. For example, the ratio is 1 to 3. So for each 1 gigabyte you get 3 IOs. 100 gigabytes of volume equals 300 IOs. With GP 3 on the other hand, you have more flexibility. You can allocate IOPs and throughput independently of the storage size. However, both GP2 and GP 3 are best suited for development and testing environments, as well as medium sized workloads. If you have mission critical applications which require consistent low latency, I would typically recommend my customers to use IO1 or IO2 Block Express. These are the go to options for mission critical applications. With IO2 being the more recommended option because of its improved performance and durability, in fact, IO2 is the only EBS option in RDS that provides sub millisecond latency. There is no other way with RDS to provide submillisecond latency, so that's the only EBS option. And also IO2 provides 5/9 durability. This is 100 times more durable than IO1. So this is the long hanging fruit. We priceO1 the same IO 2, the same as iO1. So if you are using io1 today, you can and you should move to IO2 to achieve improved performance and durability for the same price. Low hanging fruit without downtime, without any workload impact, you can do that and achieve improved performance and durability. So back to any company for another challenge, and this time it revolves around EBS high latency as you probably guessed because we're talking about EBS now and storage. As the database scaled, they reached EBS IOS limits, so essentially they saw spike from 2 milliseconds IO latency to 500 milliseconds. That's a significant performance hit. This is a scaling issue we occasionally see where high demand. Could lead to bottlenecks. So we need to find a solution that ensures sustained performance under the load. The requirement from the company's dev ops team is two things. First, we need to support 20 ks, and second, we need to support submillisecond IO latency. Similar to what we saw earlier, Cloud Watch Database Insights was useful to identify the issue. So if we look at the database telemetry, we can clearly identify that there are two issues here. First, we see an increase in the number of EBS I ops, and second, in the IO latency, which spiked to roughly 500 milliseconds. That's, uh, that's a clear indication of a problem we need to address. So the solution here was to use. Right sizing for the storage. We talked about compute right sizing, but we can also do storage right sizing. And now the company decided to move to IO 2, and that allowed them to achieve consistent submillisecond IO latency for the workload. Challenge number 4 and the challenge here is about slow reporting queries running on the read replica. These are queries which use joins, aggregations, group by, basically it's complex queries that generate a lot of temporary objects. So what happens is that the executives in any company, they would like to see dashboards showing performance data. But the dashboard takes around 10 seconds, so that wasn't acceptable for the company executives. We need to have a requirement to reduce the dashboard load time to 5 seconds or less. So the root cause for the performance issues is. Temporary objects written to the EBS volume. Basically what happens is that we identify that these queries generate a lot of temporary objects. Database systems like Posgress try to perform operations like complex sorting in memory, and you can tune parameters like workmen in Pogress. However, there are situations where you must write to the EBS, and when we write to EBS, or in other words, spinning to disk, that causes IO latency. And impacts the performance of the queries. So there is one very elegant way to address that. But before we talk about that, I would like to show you how we identify the issue of the temp objects. So this is the cloudwatch database insights that we saw earlier, and you can see here very clearly two clear indications or evidence for the problem. First, IO buff file right, it's represented in the purple color. We talked about it earlier, and it looks familiar probably for you because we show it in the demo. What this means is that there is a weight event which occurs whenever RDS creates 1 files, OK. The second identification is database telemetry, and I mentioned that you, you can create dozens of different metrics. I chose 10 files and 10 bytes per second, and you can clearly see. That both of these metrics indicate bottlenecks from excessive temp rights caused by complex sequel. So we know we have an issue and we know what is the issue. Now, what I would like to do, I would like to show you an elegant way to address that. Instead of just increasing to a very large instance which has a lot of memory in CPU, why not using a feature called optimized reads? Anyone here heard about optimized reads? OK, not many. Uh, basically, optimized reads is a feature which allows you to use local NVME SSD to process these complex temp objects in the local NVMESSD instead of EBS. So it provides a much faster query execution time by utilizing instance types that have that have local NVME SSD. These are the instance types with D at the end. So for example, R8GD or M8GD. This instance type. Utilize the local NVME SSD. So now instead of writing the temp objects to the EBS, which adds additional high latency, we use the local NVME SSD, and that could be a much more cost effective approach to address this type of issue with slow reporting queries instead of just getting to a larger and larger instance which may cost more money. So optimized reads very, very recommended for this use case, and I highly encourage you to use it for similar challenges. So now there is a very similar comparison, right? Like every time I would like to show you some cost examples, so you could choose between larger instance and using optimized reads. If we go to a larger instance and we scale vertically from RAG large to RAG extra large, we can see that the price is now doubled. The compute cost is doubled. Instead, With the same, the same instance type with the same amount of VCPs and memory, but using the D at the end, R D at the end, you can see that the cost is significantly lower compared to the vertical scaling approach that provides us a way to improve performance without overspending. So by changing the red replica to RAGD large, now the instance comes with local NVMEs D, and that allows a significant performance boost in a much more cost effective way than scaling vertically. It actually enabled the dashboard load time to to load 2 times faster, so 50%, I would say actually 100% improvement in the database load time. And now for the last cost dimension we will cover, which is backup. In Amazon RDS we offer two types of backups automated backups and database snapshots. Automated backups include daily EPS snapshots and transaction logs being sent to Amazon S3 every 5 minutes. And the benefit of using automated backups is that it allows you for automated point in time recovery, so you can go back in time and to a specific second in the last 35 days. You have a granularity for up to 35 days. Alternatively, if you would like to have more retention for let's say one year or a couple of months, you can use database snapshots. Database snapshots can be taken at any time and they do not expire, so they give you more control over long term backup retention. So I would like to introduce the last challenge with RDS, and that's high backup costs. Any company faces this challenge because they have a backup retention policy of storing data up to one month. Now, a closer look at the customer business requirement revealed they don't really need one month of data to be restorable. They only need 1 week of data to be restorable. Any older data, let's say 2 weeks or 3 weeks older data, needs to be available only for compliance queries. Many of my customers, they say they need a retention period of 1 month or 2 months, but they only need x amount of data to be restorable, maybe a few days, and all the data for compliance requirements needs for queries only. So we can find a solution that will be much more effective, cost effective than just using the automated backups for one month. Let's see a cost example like we did earlier. So Before we do that, before we show the cost example, I would like to show you the different options to have longer term backup methods which are more cost effective. Snapshot expo to S3 allows you for full or partial parquet format. This is open source Apache parquet format. It's a columnar storage that allows you to access the data in a very fast manner with tools like Amazon Athena for those of you who are familiar with it. Alternatively, it is possible to run logical backups to Amazon S3 with native tools like PG dump in the case of Posgress or MySQL Shell dump in the case of MySQL. The benefit in both of these options is that you actually have access to the underlying S3 storage bucket. With the RDS native backups that we talked earlier, automated backups and RDS snapshots, you don't have access to the S3 storage class. It's stored in AWS own backup. Here you have control over it. So based on the access pattern, you can decide that if this is a frequently accessed data, I can use S3 standard. But if it's longer term archival data, maybe I can use Glacihare, so I have more control over storing it in the right S3 bucket. And now I would like to show you the cost example. So the original backup plan was 30 days, and we assume that the initial 500 gigabytes of full snapshot is free of charge because with RDS we provide you free backup up to 100% of the database storage size in a region. However, each daily incremental backup you are being charged for, so you're being charged for the EBS block changes that you have in your database. So we can see the calculation now if you want to go to the other approach where we store only one week of data. With automated backups and any any older data being stored using Apache Parque snapshot export, then the total cost can be reduced dramatically. And I've seen cases where we got 50% or higher reduction in backup costs just by leveraging this approach and not storing only other snapshots that we don't really need. So there are ways to optimize backup costs as well, and this strategy provided a 30% cost reduction in backup costs, demonstrating how efficient backup data retention could lead to significant savings and still maintaining access and retention requirements. So I would like to summarize the journey of RDS with any company, and then we will talk about a similar but different journey with Aurora. So we started with the challenge of increased workload and CPU spikes due to poorly running SQLs. We did SQL optimization by using indexes and instant stride sizing. We utilized the latest graviton generation RAG that provided us 46% instant cost reduction. Then we had the issue of noisy neighbors using query offloading to read replicas. We managed to have a consistent performance and reduced the API response time of the core customer facing application by 50%. Then we had the issue of high latency and latency spiked from 2 milliseconds to 500 milliseconds. We managed to reduce it to sub-millisecond high latency by using IO2 block express. Later we had the issue of slow reporting queries running on the red replica. By utilizing RDS optimized rates with with RAGD instance type with D at the end that comes with the local NVME SSD, we managed to um improve the dashboard load time by 2 2X and lastly to address the high cost of back of backups, we use the more cost effective long term backup method which reduced the backup cost by 30%. So this was a summary of the journey with RDS. Hopefully you learn some new techniques of how to optimize performance and cost in RDS. But now, I would like to follow any company's journey again, starting from a similar place, but with a different punchline, now with Aurora. As a reminder, Aurora is our cloud native database engine. It is fully compatible with MySQL and Postgress as mentioned in the beginning. It has commercial grade reliability capabilities and scalability with a cost effectiveness and simplicity of open source databases. One of Aurora's key innovations lies in the storage layer, which automatically stores your data across 3 availability zones. And for these properties and more, any company now choose to use Amazon Aurora. Last year we celebrated a decade of innovation with Amazon Aurora, and as you can see now, we have Aurora provisioned, AKA Aurora Instance-based, for predictable workloads. Aurora Servalless, which is great for spiking and unpredictable workloads. It has an auto scanning mechanism, so you actually get more resources. It can grow and shrink the amount of CPUs and memory based on the actual usage. Without any interruption to the sessions that are running or sequels that are running, without any connection. And Aurora DSQL, which is the latest addition to the Aurora family that provides virtually unlimited scale fully surveillance. It could be either in single region deployment or multi-region deployment with strong consistency, active, active characteristics, as I mentioned in the beginning of the presentation, just to set expectations up front, we will be focusing on all our provisions and our our servers just because O DSQL has a completely different architecture and pricing model. As I mentioned, also, if you would like to learn about this sequel, there are great sessions here at Reinvent for that. So, with Aurora, we have the same, basically uh cost model of the three dimensions, but they work differently. So, let's explore how. Let's learn how. Aurora is mostly different than standard databases in the storage layer. So I would like to break the sequence, and this time to start with storage. With RDS we started with compute. Now I would like to start with storage. Or this aggregates storage from the database instance to the Aurora storage fleet. This is a multi-tenant storage fleet that is spread across 3 availability zones in AWS region made up from a large number of special purpose nodes. These nodes are responsible for storing your data, balancing, repairing, and Essentially offloading many of the other operations from the database instance, Aurora makes 6 copies across 3 availability zones. In other words, 2 copies per each availability zone. But you only pay for one copy, so you don't really care about the fact that we stole 6 copies of the data. You only pay for 1 copy. This means that Aurora can handle the unavailability of storage in the entire availability zone, plus 1 small storage node, and that's one of the key features with the Aura. But how does that impact performance and cost? Let's explore this by using some challenges like we did before. So the first challenge is growth, growth challenges. We have more customers, but we also have bursty customers. Perhaps when their geography wakes up, maybe there is a new sale of Taylor Swift, and now there is a high load and burst. So we need to find a way to autopilot it and oil will do it for you. So this flexibility creates some challenges and cost considerations. We need to make sure that we have more predictability around cost. So let's see how we do that. And of course, in addition to predictability, we would like to reduce the cost. That's the most important thing here. All storage is a pay for for what you use system in two storage dimensions storage size and storage aisle. Let's start with storage size. You are billed per gigabyte month, but unlike other options like RDS, you don't need to provision storage in advance. You don't need to provision IOS or throughput. It can grow and shrink as needed. That means that. Because it automatically resizes your storage, you can have more control. You can do things like drop and use partition or old partition. You can use vacuum in progress to keep storage under control, so you have more control here, and you can monitor using volume bytes used metric in cloud watch. In terms of IO, we charge based on 1 million IO operations. So every page read, whether it's MySQL or Posgress, is counted as one aisle. And for rights, because we don't write pages in Aurora, we actually write logs. We write in units of 4 kilobytes. So we chunk it into 4 kilobyte units. But in both cases, whether it's reads or writes, you are billed per 1 million eye operations. IO is also automatically scalable. You don't need to define IOs in advance, and I will, I will spoil the surprise for later by telling you right now that the same works with other nodes. So if you add more replicas to Aurora cluster, they all work against the same shared storage layer. So from that perspective, they all operate the same, and we'll see that in a minute. So let me walk you through a cost example. Now we have 100 gigabytes of storage and we grow 100 megabytes per day. So the read and write eye operations is 600 ops per second, with 400 reads and 200 rights. Just like in the earlier RDS example, we use RAG extra large. And you can see the cost of the monthly cost for the instance. We also can see the storage cost per month. Now we don't need to think about the math here. I already did the math for how much it costs per month by taking into account the calculation of growth rate, not just the initial storage size, but also the growth rate, and you can see that with the 10 cents per gigabyte a month, that's the storage pricing based on North Virginia, we get to $300 roughly per month. Now we have regular storage IO and that leads us to around $207. And here is the challenge we have also bursty eyes, and the bursty eyes are associated with 15,000 eye operations. So every day we have two hours of 15,000 eye operations per second. That's a big challenge that we need to solve because you can see that it adds to $648. That's pretty big in terms of the cost, and it also creates some predictability challenges because we cannot predict in advance how much IO exactly we will use. One way to deal with this issue is to reduce cost by scaling up the instance, which brings more memory to the instance, which reduces the um which which improves the the buffer pool cash hit ratio. And by doing that we need to We are able to use more IO reads from the memory instead of the disks, so that can reduce the IO cost. So that's one way. And in addition to reducing the cost, it also improved read latency because now I can read from the memory because I have a bigger instance. This is a valid technique. This, this works and many customers use this technique, but what I would like to show you is what I believe would be a better technique for you. And that would be using Aurora I optimized. With Aurora I optimized, there are zero charges for eye operations, so you pay nothing for any eye operations you have. It also brings you IO predictability because IO is no longer a variable. You no longer need to estimate in advance how much you will use. That means that you can save up to 40% of the storage cost and you can move once every 30 days from Aurora storage standard to Aurora bio optimized storage and go back at any time, so that's without any interruption, without any workload impact you can do that. So I would like to show you a cost example again. By this time I'm going to use i optimize. You see that some of the data is grayed out. Some of the things is grayed out because it's the same. There are no changes in the fact that we are using 30 days, 24 hours, but you can notice that the pricing of the compute is at 30% higher rate because of I optimized. Also, the storage cost is at higher rates, so it used to be 10 cents per gigabyte a month. Now it's 2.25. It's, it's much higher, OK. So because we have um. 22.5 cents per gigabyte month, then we know that both compute cost and storage cost is higher. However, and this is where it comes to be more exciting, is that you don't pay nothing for the IO either the the regular IO and also the burst IO. These are completely free. And the outcome if you actually combine the cost together, you get 23% cost savings even though you pay more for compute and storage. You still manage to achieve 23% cost savings because you pay nothing for. This is particularly useful for bio intensive applications. By seamlessly switching to high optimized, any company managed to achieve 23% cost savings and also as a bonus point got predictable pricing. Now any company has another challenge. They have internal customers running different workloads. It could be writ heavy reporting jobs or bad jobs, and this can cause buffer pool cash overload. So this is another challenge that we need to solve. What they want is a way to reduce the impact of these back-end jobs on the front-end queries, those that are actually important for the business. They would like to improve the overall performance for these lead heavy queries and remain cost effective. So what we can do here, like we described earlier in the RDS cost section is we can add more replicas to separate workloads. Aurora supports this very similarly but with a different approach, because with Aurora. Due to its shared storage architecture, all the reed replicas access the same shared storage. We can add replicas, and they can all access the same physical storage. So what is important to say here, all of that has nothing to do with durability. Durability is fully handled by the storage layer. So even if we add replicas or remove, remove the replicas, everything about durability lay in the stor lies in the storage. There's still one writer node, and the writer updates the reed replicas with up to 100 millisecond latency. We talked about Aurora optimized reads earlier in the context of RDS, but now I would like to talk about it in the context of Aurora. It has the same name and it's kind of working the same way, but it's a bit different. Let's see how it's still using the RAGD instance. It still has the feature to store temporary objects in the local NVME SSD, but here is the big difference which is unique to Aurora. With Aurora, you could actually store. Up to 8 times, you get up to 8 times improved query performance by storing tilt cache. So what we can do with Aurora, we can scale up the amount of memory we have into the local NVME SSD. So tilt cache lets you expand this buffer pool into the local NVME SSD, and then you get more performance for read heavy queries. So remember you can do this by switching the instance to D type, like by doing a fellover operation. It's very easy, and you can do fellover operation with Aurora with up to 30 seconds RTO. So you can also reduce it with RES proxy tools to make it even seamless. Remember the D type. It could be available with. So the impact of any caching solution also when using optimized Reds is always going to be dependent on the workloads cacheability. So that depends on the size of the cache. In the case of optimized Reds, you have great control over it as you use a larger instance. So for example, if you move from RAGD 2XL to 4XL or 8 XL, you will get a larger local NVMED which improves the amount of cash you have. So that's one thing. The other thing is the working set of your application, and you can use that. You can use those metrics in CloudWatch to estimate the working set for your application. So far we've been using RAG Extra Large in the example. So I would like to switch to RAG D Extra Large, and that gives us 230 gig, 230 gigabytes of tilt cache in the instance. If you would need to get an equivalent amount of memory from. Regular instance, not optimized instance, not optimized read instance with RAG. I would need to use RAG 12 Xcel. Now, obviously that would be much more expensive compared to using RAGD. So RAG 12 XL will be roughly $6200. RAGD Xcel comes with $580. Now obviously memory is faster than local NV misses. But if local NVMD meets your performance requirements, that would be a much better solution and provide you a much more cost effective solution to address your performance challenges. Storage size and storage aisles are the same in this example. The only difference is that we compare between optimized reads and regular instant with more memory, so you can see how powerful this could be. By using Aurora optimized reeds, any company was able to reduce costs by 90%. That's huge, and this is something I can see also with my customers that are using this option, and that's, that's much better than moving to a larger instance. The transition is easy, as we said. You can use classic failover operation in Aurora, and you get the deinstance type. Now, I would like to move to challenge number 3. So any company have developers that need to test their environments. They want to test it with production-like data, but they obviously don't want to do it on production. Because that would be a bad idea. So what they can do and what they have been doing so far is custom complex ETL processes. So they have ATL pipeline and it takes time, it costs money. They would like something much better. So these ETL jobs are expensive and we would like to improve that. Basically each developer needs their own environment, so that drives up the compute and storage costs for the development. Also remember developers don't work 24 hours per day. I hope so. So most of them keep their environments idle overnight and that also leads to waste of money. So ideally what we would like to do is to make a solution that can refresh these environments very quickly to improve developer productivity. So we want to reduce storage costs, reduce compute costs, and improve creation time. So basically to improve the refresh time and develop developer productivity. What we can do is use a feature called fast clones. Anyone heard about fast clones in Aurora? OK, some of you. Fast clones is a technique where Aurora storage volume underneath, your instance, can be virtually copied. This is using a copy and write protocol. That means that we don't really actually copy the data, only a few pointers. Every day, any company, what they do is they create a golden image of the production, so they do a clone, and only when you read from the unmodified data of the clone. You don't need to actually pay any money for the storage of the clone because it uses the pointers. It follows the pointer to access the source clone, the source storage. Only the changes are built for your cost. So this is the one way to deal with this situation, and that drives lower the cost of the storage. So we can do that by creating the clones and we can create up to 15 clones in the set. Here you can see only 2. And as I mentioned, you're only being charged for the space you actually occupy. So when you change the blocks either in the clone or in the source database, then you pay for it. So if you have 1 terabyte of storage size and you only change 10 megabytes of data in the cloned environment, you pay for only 10 megabytes of storage. Obviously that would be much more cost effective than restoring the entire volume. That would be more costly. But we need to talk about compute because we talked about how we can reduce storage size and we dramatically reduced the storage size, but what we can do about the compute of the test environments. The solution for that would be all serverless. So any company test environments need good performance when in use, but when they are idle. They don't want to pay for it. So what we can do is use automatic scaling model for Aurora, which is Aurora Servalus. It's built from the ground up to instantly scale up and down. It can grow and shrink. And Servalus scales the database based on capacity units. Aurora capacity units, or in short, ACU. Each ACU comes with 2 gigabytes of memory. You can define a range of mean and max capacity units, and it will know how to grow and shrink based on your actual usage. It can grow, by the way up to 256 ACUs, which is equivalent to half a terabyte of memory, so it can actually provide a solution not only to the test environments that we've been discussing so far, but for production like use cases as well. And it's also another instance in your cluster you can mix and match, so you can have a production primary with a provision instance, and you can have serverless in a red replica. You can have a lot of flexibility here. So we'll follow the same example with severalvalescent clones, and you can see that both work with the Aurora standard storage and Aurora I optimized. There is no limitation from that perspective. So the team of 20 DBAs or 20 developers use the same 100 gigabytes of storage, and you can see that there is 10% change rate per developer, similar as we discussed before. By looking at historical patterns, we can notice that any company knows that the instance they're currently using is idle for roughly half of their shift. So they have 8 hour shifts, half of the shift is idle, and in the other half, the instance fits nicely to RAG large, which is 16 gigabytes of memory. So that is the equivalent of 8 ACUs. In all servals, if we do the math, we can see that the instance cost with the provision instance is roughly $132 per day and with servales it's roughly $76 per day, so much more cost effective. Now if we take into account the storage difference because now we are using clones instead of full volume restorations, we managed to reduce it by 90%, so that's a pretty significant improvement. The IO is the same for both. So how did we do? First, by using fast clones to create those test databases with a low rate of changes, we managed to reduce the storage cost by 90%. By using Servalless, we managed to reduce the compute cost by 38%. And the bonus point is that with Serverless we can scale down to zero. So if your database is idle overnight because you're not working in the environment, you pay nothing for it. It's completely paused and you pay $0 for the compute. So let's quickly talk about the first cost pillar, which is backup, and from that perspective it's pretty similar to RDS, but some points are quite different. Aurora provides built-in continuous incremental backup. So we have the continuous backup, and essentially what we're doing with Aurora, the Aurora storage writes data to Amazon S3 with no performance impact since that's totally handled by the storage and not by the database instance. There is no scheduled maintenance for that. Backups happen continuously and they are current within 5 minutes, so we can always restore to up to the latest 5 minutes. Incremental backups are charged based on storage needed to restore to any point in time, and you get free backup retention if you decide that you only have 1 day of retention. So if you want 1 day of retention, you get it completely free. If you want more than 35 days, then from that perspective it's very similar to RDS. You have no expiration for database snapshots. So let's see another example. We can use snapshot exports to Apache parquet format like we did earlier. You could use logical backups like just like with RDS, and you can also optimize the retention period. And because with Aurora you have more control over the storage size, you can do things like vacuuming in progress or drop unused partition. Then you can also reduce the backup storage cost by having more control on your volume size. And now I would like to consider some challenges across all the dimensions. That would be the last challenge. And that would be global, global resiliency. So basically what happens is that now things are looking great. Any company has customers all around the globe. But what they are thinking is creating a parallel application stacks in multiple regions, so they would like to have an opportunity to support low latency rates across regions. So if there is a customer in NMEA, they want to have low local latency rates. If they have customers in Australia, they want them to have low local latency reads. So that's one thing. The other thing is that they also want resiliency across global regions. So what we can do is use a feature called raw global database. And with the raw global database we are using storage-based replication, so physical replication to replicate the data. It's much more consistent and it's much lower latency compared to logical replication. And the typical RPO is up to 1 2nd, so that means that the lag or the latency between the primary region to the secondary region is up to 1 2nd. So for any company we're going to focus on how global database enables local reads while giving flexible price performance. Here is a raw global database designed to show you how we can provide a solution with minimal operation on the overhead. So we have this one region and one instance. Now we would like to create another region. We just click on with the click of a button or with CLI. We can create this replication which is based on storage, and you have zero operational overhead for that. That's where our global database comes in. So you have this solution that can provide you very consistent lag between, between primary region and secondary region. Here you see only one secondary region, but we actually support up to 10 regions. Traditionally those of you who use global database are probably familiar with the limit of 5 regions, but earlier this year we improved it to 10 regions, so that was also improved this year. And by the way, each one of those secondary regions can scale independently to the primary region, and you remember we talked about replicas earlier. You can create in each one of those regions replicas, and Aurora will take care of the replication with the aurora storage independently. So regardless of how many replicas you use or how many regions you use, you can create asymmetric clusters. So each cluster can be different than the other one. So to summarize, by switching the previous handmade solution which was based on logical replication to our global database, any company is now able to achieve in-region fast local reads with millisecond latencies instead of hundreds of milliseconds, and they are able to optimize costs by using different instance types rather than using symmetrical clusters. And for some pretty bonus points, they also got disaster recovery with RPO of 1 2nd. So now I would like to reach the end of any company's Aurora storage and do a quick recap. We solve the challenge of Uh, cost control with storage by using IO optimize that led us to 23% cost savings. We solved the issue of grid performance by using Aurora optimized reeds that gave us up to 8 times gri performance. We talked about the fact that Aurora has tilt cash in addition to the temporary objects with the local NVME SSD, and that's unique to Aurora. We had the issue of exploiting cost of test environments by using the combination of fast clones and Aurora servers. We managed to reduce 90% on storage costs and also a significant amount of money on the compute by using Aurora serverless. And with the issue of global data and customers that are working globally in any company now with global database we managed to achieve local fast reads with millisecond latencies for global reads and as a bonus point we also got the R with 1 2nd RPO. OK, so finally, for all of you, any companies out there, uh, you now have the blueprint, so you learned about features and techniques in both RDS and Aurora. So cost and performance optimization aren't competing priorities, they're actually complementary. You could use powerful tools like Cloud Watch database insights to monitor and improve your performance because you can't fix what you can't see. Observability is key, and this is why we use Cloud Watch database insights throughout this session to show you some examples of how you can use the right tools to improve performance. And what I would like to leave you here is with this one mission. When you go back home after you had a great time at Reinvent, go forth and optimize your RDS and the roll-out performance and cost. You can do both at the same time. Here are some extra recommended sessions if you're interested. You can also see this sequel at the bottom. I mentioned that we didn't cover it today, but if you would like to learn about it, you can go to other related sessions that I find interesting. Um, please do fill in the survey. We are a data-driven company, so your feedback would be greatly appreciated and hopefully you'll learn new things and new techniques about RDS and Aurora performance and cost optimization. Um, thank you for your time and enjoy reinvent.