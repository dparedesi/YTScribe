---
video_id: 5brezteoiks
video_url: https://www.youtube.com/watch?v=5brezteoiks
is_generated: False
is_translatable: True
---

we're ready Hello everyone. Hope your rain event has been great. My name is Gu Yu. I am a senior solutions architect supporting the AWS Social responsibility and Impact modernization. Hi everyone, I'm Nathan Pathak. I'm a technical account manager at AWS and part of the sustainability team for almost 2 years now. And thank you so much for joining our session on sustainable computing for climate solutions. In today's session, we're going to first of all talk about climate challenges and the complexity of solving climate challenges. We're also going to talk about why HPC or high performance computing is important in solving climate challenges. After that, we'll dive a little bit deeper into some patterns in terms of sustainable HPC workload and then followed up with a case study from the University of Oxford to see how they're applying some of these practices into their HPC workload. Then we'll summarize with some next steps. But before we get started, by show of hands, how many of you have been impacted by climate change over the past few years? Looks like everyone has been impacted. All right, Nitin, I saw that you got impacted. Can you share your experience and how it felt like and what did you have to do to adapt? Certainly, so I live in a part of India. It's called New Delhi, and every winter we have a concern that when the temperature starts dipping. There's a lot of smog in the air, so it's low visibility, dusty particles, and the AQIs get really high. The pollution gets really high, so we have to live with air purifiers all the time, and it becomes problematic. People have problems with breathing, breathing, and they're going outside. So this becomes a challenge almost every year, and we hope, we hope there's some solution for that. Yeah, and it probably doesn't help with the health equity and different types of socioeconomics it doesn't. It certainly doesn't. I'm sure some of you are experiencing some of the similar challenges as well with climate challenge. We're having more and more extreme weather events. For example, our summers are getting hotter and lasting longer, which not only reduces yields for major crops, but also but also leads to death. Our hurricane is lasting longer and intensifies faster, which makes it very difficult for fast disaster recovery efforts. In some parts of the world, increased precipitation leads to flooding. In other parts of the world, decreased precipitation leads to drought, which impacts food security. And now we have wildfire. Wildfire season is lasting longer and it's leading to damages that's not only in property but also leads to air pollution. I have the opportunity to work with many customers that are tackling some of these challenges. At AWS Social Responsibility and Impact, our vision is to improve lives every day. And we believe that cloud and AI technologies are powerful tools to help build solutions that tackle some of these challenges in health, education, and climate. But building climate challenge, building climate solutions is difficult. In fact, climate challenge is very complex because first of all, The climate system itself is complex. As we know, there are different components in the climate system. We have the atmosphere with the air. We have the hydrosphere with the water, the lake, the ocean. We have the cryosphere with the ice. We have the lithosphere, the land, and we have the biosphere, the living beings, we are part of it. And these 5 components interact with each other in very complex ways which makes it very difficult to understand the impact of any intervention. Apart from the complex climate system, we also have a very complex human society. This is not just socioeconomic, political, cultural, and also because climate change is a global issue. Communities, regions are impacted very differently. So there are also disagreements in terms of the best way to adapt and mitigate. And then the urgency makes solving climate challenges even more difficult because now we have to evaluate the long-term and short-term trade-offs. Scientists tell us that we have a limited window to make unprecedented headway in order to keep our global temperature rise to below 1.5 Â°C. And as we are taking action now, we also have to evaluate the impacts in the long term to make sure that our efforts are sustainable. The good news is that we are making progress. We're making our buildings. Greener. We're taking more into consideration of sustainable building practices from building design, the type of materials that we're using to build our buildings, the type of energy we're using and when and how we're using the energy. When it comes to energy, we're using more renewable energy, wind, solar, geothermal, hydro, and more. When it comes to food supply, we're applying more sustainable agriculture practices so that we can reduce water waste as well as improve soil health. And we're also electrifying our transportation sector so that we can reduce the emissions from harmful, reduce the harmful emissions from conventional combustion engine. In order for all this to work effectively, we need technology. Technology enables and accelerates climate solutions. Now, as solutions architect or researcher or innovator, as we are translating into climate challenge into kind of technical requirements, we really need our technology staff to do 3 things and do and do 3 things very well. First of all, in terms of data. Data is important in any system, but it's extremely important in Building a climate solution because, as we mentioned earlier, climate system is extremely complex. So in order to understand the complex system, we need data from different sources. So this could be data from supply chain data to understand carbon footprint. This could be satellite imagery to understand vegetation index. This could also be the sound of the ocean to understand ocean noise pollution, right? So these data needs to come from various sources and some of them can be very large. But the ability to acquire high value is extremely important. And secondly, we need a storage option, a storage solution and because our data is from different types of sources and can be very large, our storage needs to be able to accommodate that. And also because these data can be large, we also need to be able to retrieve the data easily and efficiently as well. So now we have the data and we store the data. Now we can actually understand the patterns behind the data. So the third component is compute. We need our technology stack to be able to. Do large scale data processing, simulation so that we can understand the patterns behind the data. And I hear that Newton has some recommendations. And we have, we have a technology stack that can help you to build solutions towards finding the solutions for your climate problem. Have you heard of high performance computing, a show of hands, and have you been using it? Certainly it is an amazing and efficient way of how technology can interact with one another in order to provide us an infrastructure and a setup that will help us conduct our research, that will give you solutions for complex problems and specifically problems related to climate research. I've been an HPC admin myself for a large part of my career. And I know this is extremely useful. However, there's one problem with it. An HPC grid usually consists of hundreds and hundreds of servers. That would mean it can consume loads of energy. Setting this up is never easy. Even though it is the right tool set for our solution, it might not be the easiest and the most sustainable way to do it in an on-premises setup. Thus, there needs to be a fine. That's where AWS comes in and sees how it can help you for the next one. When it comes to sustainability, we also follow the shared responsibility model. You might also, you might already be familiar with the shared responsibility model of security, but in terms of sustainability, AWS is responsible for sustainability of the cloud. So that means our global infrastructure, including our data center, our electricity supply, servers, cooling, building materials, and so forth. Customers, you are responsible for sustainability in the cloud, so that's your workloads, right. Region selection, how you're using your data, how you're storing your data. And if you're working on machine learning workloads, that could also mean what type of model you're choosing, how you're building your model, how you're deploying your model and writing your inferences and scaling your model. Now diving a little bit deeper into sustainability of the cloud and our AWS global infrastructure. And then AWS continues to relentlessly innovate our infrastructure to build the most secure, performing, resilient and sustainable cloud for our customers worldwide. We have 120 available zones in 245 countries and territories across 38 geographic regions. And according to an Accenture study, Workloads on AWS is up to 4.1 times more efficient than on premises. How we're able to achieve that is through our data center design. Which is enhanced for tomorrow's AI tech uh AI workloads. So one way we evaluate data center efficiency is to use this metric called power usage effectiveness or PUE. And the PUE, a lower POE indicates a more efficient, uh, data center and a POE one is perfect. So as you can see, our HBS global infrastructure have a POE of 1.15 which outperforms on-premises average as well as public cloud industry standard. We did that through optimizing our data center design with 46% reduced mechanical energy as well as 35% less embodied carbon in the concrete that we use. And with that, by just bringing your workloads to AWS you are already benefiting from the carbon reduction. And depending on the region where your workloads are running, uh, you will receive different benefits in terms of energy efficiency and by optimizing your workloads on AWS, which Nain we'll talk about in a little bit, you will be reducing carbon footprint even more. So, I want to watch. Now AWS has been playing its part in order to make things more efficient for you, the infrastructure more efficient for you by various means. Now, we also don't leave it there. We'll provide you enough material so how you can improvise your, improve your workloads and go buy it. And this thing you might have heard of your solution architects, and it's a very common thing from AWVL arched framework. It was built on 5 major principles, operational excellence, performance efficiency, reliability, security, and cost efficiency. When we added the sustainability pillar to it in 2021, our focus was to have a standard framework which our users can utilize and implement the same strategies to their workload, to the architecture, to continue to improve and reduce their overall carbon emissions. Then when we go around following these practices, reviewing our workloads against these best practices, against these pillars, we are able to identify and improve our overall workload setup. The sustainability best practices lie around these six foundational pillars which we call the cornerstone for any of the any of the architecture which we will define for you. So when we are thinking about sustainability of our workloads, we'll talk about the region selection, alignment to demand, data, hardware and services, our software and architecture, and most importantly, process and culture. That is what we do within ourselves. In addition to that, we also have a customer carbon footprint tool. We have been using our billing and billing dashboard regularly to visualize how much, how much costs are ongoing. This also helps you to get a reflection of where your carbon emissions are also going for your workloads. This will give you a breakdown based on the regions and the services and overall usage per account so that you will have clarity of the kind of things you have done for your carbon emissions and where is it going and what you probably should consider in order to go forward, have a proper measuring and reporting tool. Now with this we have this ability to visualize what we have. Now we go back to the HPC and go back to how AWS can help you get to a sustainable high performance computing setup, and the building blocks for sustainable high performance computing setups are as follows. I'm just showcasing this divided into three common layers. The first one being the cohort, the the the framework, something where the foundation lies, which is where the storage will be in terms of file systems and where. Our network will be. So we have Amazon FSX for luster, for luster-based computing, and elastic fabric adapter that will help you for high fast speed network interface for your HPC cluster. Then we'll go for the compute and our scheduling services, and we have options of AWS Pallel cluster, AWS batch, and AWS parallel computing Service. I'll dig a big deep dive. I'll do a deep dive down on this to get you where we can utilize each of these. And finally, how do we visualize, how do we interact with the system? And we have the Amazon DCV and we have the research and engineering studio on AWS that can be utilized for you to visualize and work with your workflows. Now we go back to the, the, the aspects which Guy spoke about, the key areas and avenues that we want to focus on as we build solutions on for for our climate research. The first one is data. And data is pivotal for any kind of research, specifically when we are doing climate research. We need a lot of data from various sources that can be, that can give us the most accurate results. And if that data is not good for us, it will give us, it will not give us good enough results. It will be any kind, it will be stored across coming from various sources and it will be populated in various places, and we need to utilize the same. There's one concern with this. The data which we might need for research will be in exabytes and exabytes overall, and that will cost emission. Storing all that data will cost emission. That means we will have our storage resources consuming all these data stored coming from various sources, storing our storage systems and using it over and over again. Another aspect to this is the data transfer. Most of the customers we have worked with who want to reduce their carbon emissions tend to neglect the emissions coming from data transfer from one source to source to a destination. That's a key aspect of carbon emissions which is going around, and we need to reduce the same. The key thing which will come out from there is that the data is reusable, and we need to find out ways on how to do it. Well, AWS has found one way of how to do it for you. By creating an open data on AWS. Now this platform has data coming from various sources, research facilities, various types of research, be it nonprofit organizations, scientific organizations, multiple places correlated to the data in a single place, and using AWS data exchange, you have access to them. Along with that, one good thing comes over there, it is ready to use on the fly. You don't need to download it to your or transfer it to your storage accesses, your storage devices. You don't need to move it to your SD buckets. You can keep it there, so the major chunk of data is just available. You can start to use it on the fly, use it for reset purposes, and get on the next, get on with your development part. So once we have the data part sorted, next is the storage. While we conduct our research, we will require file systems and we'll also require a lot of artifacts which will develop out or spin out from our research and how do we manage this. As those artifacts will again be of a huge volume, we need to first identify data access patterns, and we need to ensure that we are able to reduce our usage in that front as well. While we're using Amazon S3. We specify that there are life cycle policies that we can certainly utilize. Now these life cycle policies help in order to move data from used very frequently to not used at all. And by this, and even from there further deleting them when there's something that is not being used. Now understanding this aspect that considering the raw data, the source data is available for us. The computers can be readily available for us. We don't necessarily have to store unnecessary data, and we can get rid of it to ensure that we are keeping our costs in check and keeping our ambitions in check as well. Next part is the file system. Now for an on-premises high performance computing setup, we require a file system which is very fast, very efficient, and we have to replicate something similar for for our AW services as well. And that's where Amazon FSX for Luster came into the picture. It provides great benefits such as intelligent tiering, which moves data from one type of storage tier to another. It will help you to reduce your emissions. It will also help us with life cycle management, automated lifecycle management, so that you can clean up data as well. And the data archival along with the integration with S3 will help you move your artifacts in the right place, right position as well. Overall, when it combines together. Amazon Amazon FSS for Luster provides high performance file system for your high performance computing. And can reduce your storage footprint as compared to standard data center by up to 50%. That's a huge differentiation in terms of sustainability, in terms of benefiting from carbon emission efficiency. And the final component, compute. Now this is where AWS has been really Doing a lot of innovation and making strides on how we can benefit the power of cloud in order to conduct the research for high performance computing. Like I said, a normal cluster will take up to hundreds of servers at least to get started for various workloads. We have, I spoke about three solutions. We'll start with this AWS parallel cluster. Now if you're coming from an HPC background and you have your open MP and MPI-based workloads and you want to replicate the same on cloud, this is a good starting place for you. It will provide you the scheduler that will be integrating that will integrate with our auto scaling mechanism with our clusters with our with our in servers and instances that can scale up and scale down based on requirements, and we have seen that by using AWS parallel cluster. It can reduce carbon carbon footprint by up to 57% from a standard HPC workloads. So that means it is efficient. It integrates well with AW services such as Luster, EC2, EFA, and NICE DCV for visualization purposes and gives you a good platform for you to conduct a research. If you already have your workloads already built for on premises, and just bring them on the fly and start using them. The next part is the AWS batch. An AWS batch can be extremely efficient for your high performance computing scheduling system, provided if you have loosely covered workloads or your workloads are already on containers or you're building something on containers, and you want to run them on a parallel computing part. This will give you a provision of having a job schedule that will reduce, will do the scheduling part itself, and it will also help in reduction of compute time by up to 60%. And as we reduce the compute time, we will be able to see it also the resources are used for a shorter period of time. That means your efficiency increases in terms of your emissions. And the last one. Let's say you're just getting started. You're just a researcher who has not have any experience of any kind of HPC or a container workload, and you want to get started towards building a carbon intelligence orchestration system. In this scenario, AWS parallel computing system service can be really useful for you as it does all the heavy lifting work for itself. You don't have to worry about what kind of computer you want to keep in place. It will not have to worry about how you will do scheduling. You just build your job, build your research, uh, research algorithm your codes and stuff, and it will just be deployed from the best usage in the most efficient way that will help reduce your wait time. It will, it will be easily, easy to set up and prioritize. You can just focus towards your research and not worry about the infrastructure part. So all the, all of these combined will give you a core area of the compute platform that can be utilized for a research part. Next, we'll talk about the instances. AWS has been working a lot on the instances which are focused towards high performance computing and accelerated computing aspects. We have instances which are HPC optimized, that will be efficient while we are conducting research and running our jobs for HPC-based setup. And also for accelerated computing where we might require graphical intensive workloads as well where like an example which we'll probably show we'll have that showcase as well where graphical based results needs to be also managed so we have instances which are developed for the same as well. Guy initially spoke about sustainability of the cloud, and AWS has been doing a lot of work on this fronts. When it comes to efficient silicon, and that means the chip design, we come from the bottom up, we start working on the chip design aspect as well, so where we can improve on that. And our three key chips which have been utilized, one Graviton, which might have been the most common for any compute purposes, they're extremely power efficient. They're up to 60% more power efficient than a standard EC-2 of a similar, similar family. For a lot of our research we require model training, model optimization, and for that we have our inferentia and cranium, and inferentia was specifically built for being more giving you better performance per the power per watt in terms of the utilization. That means and as we grow to newer models of influentia as we have gone to infringentia 2, influential 3. It keeps improving in terms of the power efficiency for each and every time it conducts any inference and optimization, and tranium for any training purposes, it can give up to 25% better energy efficiency than any standard easy to instance of the same kind. And as we keep developing a newer version, the focus always remains how tranium can be used for training your models at a faster pace and be more efficient pace as well. Now I spoke about all these materials. Now how will it look like when you are really building it all into a play? How all of this will come together? I'll give you a sam few sample diagrams if you, in case in any of these three fronts you want to utilize AWS parallel cluster, AWS batch, or AWS uh parallel computing service. Any of, in any of these scenario. And architecture will look something like this. Uh, let's say that you are just a res you are a researcher who's conducting climate research and you want to utilize parallels, so that means you already have jobs which are from HPC based from open MP or MP based jobs. Over here you require a nice DCV setup, so that's something for your visualization perspectives, how it will interact with your head node and your queue. You will have auto scaling. You will create queues for your requirements based on, it might be a high memory queue or a GPU-based queue or a compute-based queue and set this up in your VPC and over there, FSX Fluster will be your file system interacting with your. Various compute nodes and you will be able to interact with this and have the and run run and schedule your jobs. The one benefit of pilot trust is, like I said, if you want to manage things yourself, if you want to identify which kind of computer resources you want to keep, this gives you a flexibility of choosing those resources. So if you have a more customized job which requires specific understanding which you want to do, take control in your hand, you can utilize parallel cryst in that scenario. Next we go to AW's batch, and this, like I mentioned, is going to be a similar setup. The only difference is if you have more loosely coupled or container-based workloads which will run in parallel. AWS batch will be a solution for you as you can start to run your containers, deploy them, and they will run in parallel across for these various queues and do the work for you. The job scheduling part will be managed by AWS batch, so the scheduling part is completed. Your only part is running the jobs and having this set up there again. You have an option of choosing what kind of instances you would like on the back end. So that's one more control you will have in your hand. And the third part is the AWS parallel computing service, and this is where we need the least operational overhead altogether. We are only focusing on submitting our jobs and letting them work on the back end, so all the part of managing the cluster, managing the compute resources, managing any kind of resources will be handled by AWS and Parallel Computing Service, and you will be only focused on developing your job codes, developing your solutions and building them same, so you will get the information in, get it done, and once you have the artifacts ready, you're happy to go in this. So this is what AWS HPC architectures could look like, and now we'll take a look at what it could lead, what kind of research it could facilitate, and for this we have an example of a case study where we'll you will talk about how University of Oxford has been benefiting towards this. Thank you. So, I'm going to tell you a story. The story starts in the Indo-Gadantic region, so that's in the India, Pakistan, and Bangladesh area. Air pollution is a big problem. Li Tian will tell you all about it. In fact, this is one of the most polluted areas globally, with pollution level exceeding the WHOO standard by up to 10 times. And that leads to over 2 million premature deaths annually. But because There is no precise data in terms of where the pollution sources are. The efforts to address this crisis has been hindered for decades. One of the main pollution sources is a brick kiln. Brick kilns are kilns that are used to make bricks, and in the process of making bricks, the kilns emit dangerous pollutants such as carbon dioxide and particulate matters. And because of the rapid rise of real estate industry. There has been a lot more vehicles that have been built, and a lot of them are operating. Without without regulation. Now, enters APA or the Air Pollution Asset Level detection, which is a research project from the Smith School of Enterprise and Environment at the University of Oxford. So APA believes that clean air is a fundamental human right. They, they envision communities that are free from. Air pollution and they believe that clean air should be prioritized for the current and future generations. So how they plan to do that is they want to detect all the bras in the Indoggenic region. And that covers 1.5 million square kilometers of land. And after detecting all of them, they want to create a map, an interactive dashboard so that communities can understand. Where they are and so that they can make more data-driven decisions. Now for researchers, when you start working on the research, the first step is to start looking at some of the data and then maybe use the local machine to run some analysis, do some geospatial data processing, run uh build some machine learning models. As they are working on this project, they realized that local machines couldn't handle the scale of data storage, processing, and machine learning workloads because they are working with millions of data points from different types of data sources. Now the main data that you're, they're using are satellite imageries. They choose satellite imagery because satellites offer consistent wide area and historical Earth observations so that it can help them to understand the region that they are studying. And there are different types of satellite imageries. Um, a lot of them are publicly available such as the Sentinel 2 and Lancesat projects. There are also some sala imageries that are, uh, require, uh, that are commercially available. There's also the difference between low low resolution satellite imagery and high resolution satellite imagery. So if we're looking at brake hills with low resolution versus high resolution, you see the clear difference. If we're looking at the ones that are towards your right, those are the ones with lower resolution at 10 m resolution. Brake hills look like an orange blob. You're with a bunch of pixels. You probably wouldn't be able to confidently say that this is a brick hill. This, you can just say this is might be an orange oval object, right? Versus if you're looking at the higher resolution satellite imagery on the left at the 0.5 m resolution, you can confidently say this is the Brick hill. But the higher resolution satellite imagery can be expensive. So what the researchers did is they started out with low resolution imagery using Sentinel 2. So they built a random forest classification model to identify orange oval objects and here is one object being identified. So this is the potential bra kill. s As we know, because for the low resolution satellite imageries, we cannot confidently say that this is actual a brick kiln. The result is they identified 31,000 brick kilns in Pakistan, which is a lot more than how many there actually are. So they understand that there are a lot of false positives. But what they then did is to use these identified. Objects as potential brake hills for further analysis. They then acquired the sala imageries for only those areas that are identified and then. Acquire the right high resolution imageries and then um run a high resolution workflow to understand the locations of the exact uh exact uh bra hills. So here they are annotating uh the brakis using uh with high resolution satellite imagery and then run computer vision models in order to identify where the brake kilns actually are. Now with that, they are able to create an interactive map of where all the brake hills are in the region. As well as some other pollution sources such as boiler, steel, and and primary energy generation sources such as coal, fossil fuel, and more. And if you are clicking into each of the assets, you will be able to understand also the um the the actual impact of these pollution sources and this really helps the community to understand their areas better. And they are expanding their work in Africa as well to help communities there to make data-driven decisions. So tying this back to some of the best practices and patterns that Newton has shared with us before. For the APA solution, in terms of data, they use open data as the foundation of their analysis. They use Sentinel 2 from the open data registry for uh so that they don't have to store the data. And then storage, the analyze the data access pattern and only store the data that's needed. And in terms of compute, they run their low resolution workflow and high resolution workflow on EC2, but they understand that for different types of workflows they require different resources. For example, the low resolution work. Flow will require more compute and the high high resolution workflow will require more memory, so they are optimizing, they choose their EC2 instances accordingly in order to use the right tool for the job. As a result, they were able to process 1.2 million satellite imageries and saved over 17,000 compute hours. With up to 80% reduction in infrastructure costs as well as 90% reduction in monitoring time and task run time. Solomon, the machine learning and deep learning specialist has said, we started with a social goal and AWS was a means to reach that goal. So what's next for the APEC group is that as you already saw, they are expanding their work in the Africa region to help the communities there to understand air pollution problems better as well. They also launched a volunteer program. Along with an app for the on the ground uh pollution reporting. That will help them to capture additional data points to create more comprehensive map of pollution sources. And they are also looking to leverage additional AWS services such as Bedrock uh for geospatial foundation models. So now we're going to talk about next steps, a little bit of summary, Nin. Absolutely. Sure. Now we'll talk about the things that we learned, what we have key takeaways from this. First of all, high performance computing doesn't require those huge data centers powered by thermal energy. We can get it done sustainably. We have ways and means for it and utilize. If that's a concern that how you will set up and most importantly, the speed of it. You don't need to wait to get your high performance computing clusters up and running. You can get it done very quickly in a matter of minutes, get it up for you, and start with your research. That's something very useful for you. The next part is identifying the data sources and data transfers. We don't have to keep moving data from one source to another. We don't have to worry about storing all the data which we need for our research at various places. We don't have to worry about those costs. We don't have to worry about those emissions. We have everything in a single place that we can just directly go ahead and use, utilize the scalability and the flexibility the cloud has to offer, that AWS has to offer. The Third thing is understand the data access pattern, the requirements, what kind of file system you will require for your computing cluster. It is important to have that set up in place. It might not be the case that a shared file system, a standard NFS-based file system like an EFS, might be the best solution. We need faster solutions. We have solutions for them. If you need a slower solution, there are things available which are more effective for you. Choose and identify the same. You have the ability and flexibility to manage that on your own. And the performance requirement. What kind of compute do you want to do and how much effort do you want to do in order to manage that cluster for yourself? Add these elements together and finally choosing the right tool for the job. Does your workload workload require a good stereographic setup for your 3D imagery process as well? If you that's a, that's a requirement, certainly utilize nice DCB and Research Engineering studio for AWS in that capacity. If using an HPC accelerated instance will be the right solution for you, utilize parallel cluster and add it over there. And finally, if you just want, don't want to add to any of these elements, utilize ABS parallel computing service. Understand your requirements. What is the goal for you? What's your business use case? When do you want to achieve it? What's the speed and cost consideration comes into the picture. Analyze all of them. And you can find out the solutions exist and you can easily go in and also there's an ability for you to just try out these things as well and which works fits best for you and try to start using the same as well. And for that we have a few more resources that can be helpful. For example, for example, we have. You. To deliver to you something that you can rely on and then ultimately verify, which is why people. verification. She's representing the review. This is a new initiative that we're to showcase how we're using AI to help solve humanity's most pressing problems. If you want to learn more about how our customers are doing so, check out this page. And last but not the least, level up your skills on AWS Skills Builder. So this is a great place for you to build your cloud and AI skills your way at your own pace. And if you can please complete the session survey in your application, and we'll really appreciate that and thank you so much for joining our session. And if you have any questions. If you have any questions, we'll be happy to answer you and we're helping you for the same. Thank you so much for attending. Thank you.