---
video_id: DlIHB8i6uyE
video_url: https://www.youtube.com/watch?v=DlIHB8i6uyE
title: AWS re:Invent 2025 - Scale agent tools with Amazon Bedrock AgentCore Gateway (AIM3313)
author: AWS Events
published_date: 2025-12-03
length_minutes: 56.87
views: 381
description: "Unlock AI agent potential with this advanced session on scaling tools using Amazon Bedrock AgentCore Gateway. Learn to build and manage secure, production-ready agent tool environments. We'll cover multi-tenant architecture, deployment strategies, and integration patterns for agentic tools. Explore zero-code MCP tool creation with intelligent tool discovery. Gain insights on security controls, monitoring, and performance optimization for enterprise-scale agent tool ecosystems.  Learn more: AWS r..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Good day, good people. wow, it's amazing to be here. It's such a great venue with you all in person. Thanks for joining us. Quick show of hands, how many of you are building AI agents? Almost everyone, right? Sure. How many of you have deployed agents in production? Wow, OK, perfect. Most of you guys, all right, how many of you are developing MCP tools? A lot of them, great. And then how many of you have more than 10 MCP tools? More than 10. Few 25%, OK, more than 50 MCP tools, just two folks. All right, excellent, thank you. So you're gonna be learning in this session how to scale from those first few tools to thousands of tools across hundreds of agents. Securely at enterprise grade without operational headache. My name is Daul Patel. I'm the global lead, uh, for solutions, uh, solution architecture team in, uh, generative AI. And let's dive into this topic, so. Across industry verticals, across multiple companies and industry verticals like healthcare, finance, security operations, observability, and many more, companies are transforming their customer experience using agentic AI. And we're really honored to have one of our customers, Sumo Logic, who's going to share our journey, their journey in Agent AKI in security operations space in this session. So I'm really looking forward to their story as well. But here's the thing. Your AI agents needs real data. Your data. The data could be sitting in knowledge bases, private data stores, enterprise APIs, or even the data could be sitting on one-prem. The magic happens right there agent to tool data integration layer. This is really critical for you to scale in order to scale and be able to succeed in your agent TKI success in their journey. So let's dive in. Why? So let's imagine. That you are developing a customer support agent for retail. And the users are basically able to shop. They're able to get the support, check the order status, um, and then you develop this great POC and works great with a few of your user queries. You're excited about it. Your stakeholders are impressed, and now they ask you to deploy it in production at scale. That's when the reality hits. Now you think about scaling from those first few users to thousands of users, you need to also manage the infrastructure scaling. But hey, before that you need to make sure that your AI agents can talk to real data, your data, and can integrate with your enterprise APIs so that you can accurately respond, your agents to respond to the user queries with right accuracy and without hallucinations. How do you do that? So what you do then essentially is, well, you say let me integrate with some of my. Existing enterprise APIs that could help my customer support agent like product catalog and then pricing and then customer details and then you you slowly and steadily like you try to integrate these tools with your AI agent and then why just only APIs and then you think about like let me also integrate my MCP tools or MCP servers as well or maybe talk to other AI agents as well as agents as tool pattern. So now you try to decouple the agents and tools to scale and so now you have this great architecture where you have a clear distinction between agents and tools now. You might have 2030, 50 tools to manage, still manageable, right? Well. Let's see how so now you have your other team members on boarding developing different other agents like pricing agent, shopping agent, or essentially maybe that you try to decompose that large customer support agent into manageable specialized agents or maybe you have different use case or maybe you're developing new product features and now you have these. N number of agents to manage with m number of tools to manage with that mm mn exponential growing complexity to manage and that's exactly why you need to scale this and you have the protocols like model context protocol. Which you are all aware of uh and which really takes care of uh interoperability and making sure that you have this unified communication between the agents and the tools however, this is clearly not enough because you still have this sprawling architecture where. Still have these agents talking to these number of tools and you need to handle the governance, you need to handle the fine grained access control. You need to handle the security and you need that single communication unified point where you can govern all these things and scale at enterprise level, right? And then think about it when your customer base grows. You're gonna have a lot of users, maybe thousands, maybe hundreds of thousands of users, and then you're gonna have these different personas of these users, um, accessing these agents. And then bring this multi-tenancy into the play where you're developing these agents as a service and now you have to manage these tenants like hundreds of tenants, so you have these different permutation combinations of fine-grained access control of who gets to access which tools. It's really a lot of heavy lifting for you to manage. In this diagram what I'm showing is a pricing agent is the same but being accessed by two different users. You want one of those users or some of those users to have access to promotional discounts while you don't want other users to have access to those, uh, promotional discounts. How do you do that? How do you do that dynamically as well, right? You need to scale that as well. So here's the thing. You start with business use case. You start with the user queries, you define your agent goals. You decompose your agent goals into manageable agenttic tasks, and then walk backwards and identify existing enterprise APIs or data that you can use to satisfy the agenttic tasks. And then as you continue working backwards, you identify those data custodians and the existing microservices that can satisfy those tasks and then map it out, work backwards in identifying the right MCP targeted tools for our agents and then what you end up with is lots of MCP servers, maybe hundreds of MCP servers now. And then you have these on the other end like lots of agents, maybe hundreds of agents accessing these thousands of MCP servers. It's clearly a lot of heavy lifting again, you're gonna have to manage the infrastructures, scaling, containerization of these MCP servers, a lot of heavy lift for you, right? What do we do? What's the answer? Well, before we go into the solution, here's the thing you need to scale. Maybe at hundreds of agents with thousands of tools with hundreds of MCP servers to manage with these different types of API types to manage and MCP file them and then add this multi-tenancy into the mix, probably you're gonna end up having hundreds of thousands of tenants with hundreds of thousands of permutation combinations of tools policies to manage dynamically. How do we scale? What you really need is. A single unified communication point for all of your agent to tool interactions and when I say tool, I just don't mean only the APIs or data stores. I also mean other agents as well acting as the tool as well for the other agents and what you really need to have is a very fast, quick way of MC MCPifying your existing APIs. You need to have purpose-built features like tools, semantic tools discovery, fine-grained access control, integration with authorization, maybe also in integrating with your existing IDP solutions for authorization, integration with observability, and don't forget the evaluations. This is what you need in order to scale, and that's exactly how we built Amazon Bedrock Agent Core gateway. The Amazon Bedrock Agent core gateway offers the single unified communication point. It's a purpose-built AI gateway for all of your agent to tool interactions with one click MCPification of your existing enterprise APIs. You can also MCPify your AWS lambda functions. You can. Also integrate your existing MCP servers and unify that in that unified endpoint. It does all of that heavy lifting of MCP protocols handshake for you so that you're not in the business of managing, upgrading your containers and servers to new protocol versions and then. Think about the fine grid access control. Think about all the problems that I shared before, right? You need to scale as an M cross and complexity increases. It's completely serverless, pay as you go, and you can also connect from your private VPC and have a secure communication with your MCP gateway, with your agent court gateway. All right, so I'm gonna hand it over to Nick to further dive deep into agent core gateway features. Thank you. Hello, can everyone hear me OK? OK, fantastic. Um, so my name is Nick Aldridge. I'm one of the principal engineers in the Agentic AI org at AWS. I've been at Amazon for about 7 years, worked on a bunch of AI products like Amazon Lex, Amazon Bedrock, led the team that built knowledge bases, um, and now I helped create Agent Core. Uh, I also happen to be one of the core maintainers of MCP, and I serve on the steering committee of AAA. Um, so what I wanna do is dive a little bit deeper into some of the things that Doal mentioned and talk a bit about how this works from a technical lens. Um, so what you see on the screen is a high level architecture diagram of the gateway. Right, and you can see that gateway is acting as this intermediary like Dew mentioned, right, where, uh, on the left hand side you see your agent which is in theory interacting with other agents and tools and resources, right? And the gateway is really acting, uh, as a, as an intermediary like I said and also as a translator. It's providing a standard interface for all of those tools, APIs and agents, uh, to talk to. And the cool thing is those tools, APIs, and agents can live on any cloud, right? They might be on GCP, they might be on AWS, um, they could be running on EKS or they could be running on EC2. Gateway is fronting all of those things. So on the left hand side you can see Gateway is providing a few operations list tools, search tools, invoke tool. These are standard MCP operations. It's also providing a standard authorization interface, right? So you can decide whether you want to use OAuth or IM, but Gateway will perform that standard authorization check. It will talk to your identity provider if necessary and then it will perform routing so it will figure out hey is the API I need to talk to uh running on this server or that server and it will make the appropriate call with the appropriate credential so it will also perform a credential exchange and all of those calls will have standard logging and auditing so as a business who wants to solve some specific business problems, right. I can use Gateway to not worry about the integration points. I can use Gateway just to give my developers who are building agents that standard interface so that they can just focus on the agent building and Gateway can manage the integration points. Now, uh, there are different ways that you can provide these tools, APIs and agents to the gateway. Uh, one way you can provide them is by hooking up a lambda function. So you can hook up any arbitrary lambda function and Gateway will be able to expose that lambda function as an MCP uh tool or group of MCP tools, and Gateway will be able to do the translation necessary to authorize to that lambda function by, uh, assuming this IM role all the while you're, you're getting sort of that log and auditability that I talked about. Something similar works for APIs. Now when I talk to customers about open API specs, they often think about these as existing APIs that are sort of production grade, right? This is not necessarily a spec for a production grade API. This could be a spec that defines the input to your, uh, server that's running on EKS with Kubernetes, right? It could be, uh, an API spec for something that's running on GCP. It could even be a spec for another company's API which you don't own. And all of those API specs can be hooked up to the gateway. You can choose from different credential types like API key, OA token, and then Gateway will again provide that sort of standard interface, uh, to those APIs for your agent. The third way you can provide APIs and tools is a Smithy model, Smithy specification. This may not be a familiar term to you all, but this is how we define APIs at AWS. We use this, uh, specification called Smithy. And so the amazing thing about this integration is there are 400+ AWS services that offer these Smithy specifications on GitHub. So you can actually take those specifications, plug them into Gateway. And get an interface to any of the APIs that those services use. So suddenly your agents can do things like download files from S3 buckets. They can execute jobs on Transcribe and do all sorts of other things. So this is an incredibly powerful integration and one that a lot of teams in AWS are also very excited about. Now we had those integrations and we're excited about those integrations and customers told us, hey, that's all well and good, but I have some MCP servers in my organization today plus I have MCP servers outside my organization that I want to connect with and ideally I want your fancy gateway thing to compose all of them in a single interface and so in response to that problem we launched MCP server as a target. And what MCP server as a target does is it lets you attach any MCP server, as many as you want, to a gateway. And we will fill all of their tools in the list of tools alongside the API spec ones, etc. When a customer invokes a tool, we'll invoke the corresponding tool on the MCP server, and we even support search over those tools and we'll talk about how that works. Now When you are listing tools, we do this by caching the tool schema of all of the tools that your APIs and MCP servers expose. So Gateway first caches all of those tools, your agent can ask, hey, give me the list, and we'll respond to the list even without making additional downstream calls. The agent can then decide it wants to invoke a tool and Gateway will uh facilitate that invocation while also doing the appropriate credential exchange. And as I mentioned, Gateway will offer search, so it will rely on this cached copy of tools to create an internal index which it's actually able to search um in real time in response to an agent query, and what this unlocks is that rather than giving an agent a list of a few 100 tools, let's say, you can actually give it just the appropriate list of the 10 most relevant tools. Now there are two ways that Gateway actually does this synchronization of your tool set, and the entire fact that we cache tool sets is something that is based on feedback from customers as well. Customers told us, Hey, I don't want the tools to be dynamically changing under the hood. I want those tools to be static. I want to control which tools are exposed, when they're exposed, what the interfaces are, right? I want to evaluate those interfaces on my agent and make sure I'm getting the appropriate accuracy. So we cache those tools. One way we do that is implicit synchronization. So whenever you change a target. Uh, whenever you update a target, we cache the tool set. The other way that we let you do this is something called explicit synchronization where you can explicitly invoke this API and we will look at all of the tools available on your MCP server and cache them so that they can be used in search and listing, etc. Now, to dive a little deeper on search, right, as I mentioned, a lot of customers are building MCP servers with a lot of tools. At the beginning, a bunch of you raised your hand that you had, you know, dozens of tools in your MCP servers, and that will likely increase, right? as you continue to scale up your applications, you'll have more and more tools and what we found from customers is when they had more and more tools, it was bloating their context window. So they had these agents which suddenly had like 300 tools and we're spending hundreds of thousands of tokens of context on just loading up the tools, which was not only high latency and very expensive but also impacting accuracy, right? The agents were not able to orchestrate effectively over so many tools and so we introduced this thing called search. And uh I think we we may have even been first to market with this feature where we allow you to call a search tool and get back the 10 most relevant tools. um, obviously this drastically reduces latency and cost and improves accuracy. It also is is pretty remarkable that we're able to do this across so many targets, right? We are managing all that heavy lifting of pulling in all the tools from all of the different targets so that you can search them together. Otherwise, if you hook up many servers to your agent, it will have to sort of do search across them, which is complicated. The other thing that customers told us is, hey, if I'm gonna have a unified layer that's gonna integrate with all my agents and tools and resources, then I am absolutely going to need this to connect to my VPC um and so we quickly launched private link integration so that you can actually communicate with this gateway over the secure AWS network backbone so that traffic is never going out to the public Internet to go talk to this gateway. As important as secure inbound connectivity is, secure outbound connectivity is just as important, right? I want to be able to access APIs that may live inside of EPC. And so with our lambda integration you can actually call any API in any VPC and have that call again go over the secure AWS network backbone so you can have this entire life cycle of requests that never actually go over the public internet. Now, uh The reason I think Gateway is so amazing is I really think that the protocols are going to become like TCP UDP. In other words, our developers are not going to be diving deep into protocols like MCP in the future, right? And I think that I will want my stack to be continuously cross cloud, cross framework, etc. And I will want an infrastructure product, a basic product that can solve all of that integration stuff. And so in this picture you can see just how powerful Gateway is. We've hooked up, uh, AWS services, right? A private MCP server which is running in a VPC, a private agent which is running in a VPC. Another agent core gateway that another team in our organization has built, public MCP servers which we may have discovered uh online. And then APIs, some of which may be running on agent core runtime or maybe running on EC2, etc. and we're able to expose all of that in a single interface. Um, diving a little bit deeper right on how we do this authorization thing because this is part of the magic, we offer two types of ingress authorization like I mentioned. One is OAF, and the awesome thing about the OO integration is it uses your proprietary IDP, your proprietary identity provider, to do authorization. So when a caller tries to access a gateway, gateway will say, hey, you need to go talk to authorization server X, which you all own, and then the client can go talk to that authorization server, get an access token, and then make calls through the gateway, which the gateway will authorize against your, uh, your identity provider. So you sort of have this complete loop which can be using your proprietary identity provider instead of IM. The other approach for those of you that are deep in the IM ecosystem is to use IM. So Gateway supports IM and you can uh call with IM and we really did this in response to customers who were saying, hey, like I love IM today. I use IM for a bunch of my machine to machine communication and so we said that's fine. We know MCP doesn't support IM today. We added IM support to the gateway and we actually launched a secure proxy that you can run on the client side to make your MCP client IM or SIGV4 compatible, and you can discover that on GitHub as well. The other thing that customers started telling us was, hey, I wanna expose gateway uh in front of my real end users, and those end users may not be off. I might wanna make this thing public. And so we actually ended up launching a mode of gateway that does not have off. In fact, uh, many of you may have seen that AWS launched an MCP server for our documentation. It uses no off gateways to make that public documentation available. We'll also see later how you can integrate this with a custom lambda authorizer to run your custom auth only. On the egress side, right, I mentioned that we do secure credential exchange. So on the egress side, what Gateway is essentially doing is picking the correct token to call the downstream. By calling our agent core identity service and that service is responsible for, uh, securely storing your credentials whether those are API keys, tokens, etc. and also caching the actual access tokens so that when you make those downstream calls you don't have the off fatigue of continuously offing your users uh again and again for accessing the same downstream API. Now that was all sort of well and good, uh, but when we talked to most of our customers they said, hey, we have fine grain access control logic we wanna run we have, uh, multi-tenant scenarios that require us to implement some custom logic we wanna prefill some parameters and not prefill other parameters, right? And so in response to this overwhelming demand we launched last week something called Interceptors. And interceptors are groundbreaking because they allow you or your central teams to run some standard logic in between the agent invocation and the tool invocation, so that allows you to do things like fine grain access control. It allows you to do things like filtering of of parameters or schema translation or things like this, so it's an incredibly powerful feature and it's powered by lambda today. So this is kind of the architecture diagram of how this looks essentially, um, a request comes in and we first pass it through this lambda which gets the MCP request and outputs another MCP request which can be totally mutated or the lambda can even say hey this caller is not authorized to access this tool, sorry. If the request is authorized, we'll actually invoke the target process, get back the result, and send the result through the lambda as well, and you can choose whether to do this or not, but then on the result side you can also do filtering or inject additional parameters or things like that, so it's an incredibly powerful feature. And this is what it would look like if you just did it for fine-grain access control, right? So in this case, I'm gonna run that lambda. I will look at the caller OO information which we provide. I'll look at the tool name and I'll say, OK, this caller is in finance group and they're trying to access a non-finance tool denied, right? Or this caller is allowed to access this tool. OK, fine, and I'll let it through. Um They can also kind of do this uh in conjunction with list tools. So when they, when they use this feature in conjunction with list tools they can decide independently whether they want to run the lambda on list tools or not. In this case, what we're showing is they're not running the lambda on list tools, so the list of tools is sort of public and uh the, the lambda is running only on invocation, essentially only preventing you from incorrectly invoking a tool which you don't have access to. But alternatively, you could run this lambda on list tools as well. So you could actually filter out tools proactively that the caller will not have access to, or you could proactively remove parameters which you don't want the agent to fill. So it gives you just a totally newfound control over the interface you are providing to the agent, and you can make it totally custom. This also works for search, so you can also filter that search list so that it's only the tools which the caller has access to. And uh many customers are interested in using this for things like token propagation. They wanna actually inject their own downstream token and take over some of that secure credential exchange. You can do that too, right? Your lambda can add this thing in pink, this authorization header with a new token so that they can do federated calls downstream. So this is, uh, I think, a really a game changer and really helping customers to actually take this thing live in production. And the last thing I wanna quickly mention is um we are continuously hearing from customers that setting up your tool set to expose it to an agent is actually pretty hard, right? Nobody wants to expose their APIs as is. Everybody wants to write some code on top of them and then you have to, you know, edit your descriptions and figure out the right format and all this stuff so we actually built a custom agent which helps you do that. This custom agent which you can find on GitHub walks you through the process of getting a schema which is useful to an agent, uh, which is valid right from whatever internal schema you may be using and which can actually be integrated in gateway so this really really simplifies the onboarding process. And so I know that was a lot of information and uh with that I wanna turn it over to Kui to talk a little bit about how Suma Logic has deployed this in production and how they're solving some of these problems for real. Thanks. Thank you, Nick. Great presentation to kick off the use case of Agent Core Gateway as well as dive into the nuts and bolts of how Agent Core Gateway works. We like Agent Core and for the next 15 minutes or so I'm going to take you guys through our journey to Agente AI using Amazon Bedrock Agent Core. So, uh, how many, can I take a quick show of hands, how many people heard or used sumo logic before? OK. How many of you have uh solved the reliability and observability issues with your, with your workload? Show of hands. How many of you have worked with the security investigation, things like a triage or investigation for vulnerabilities? OK, cool. That's a good, good ratios here. Uh, I like all the agentic AI builders. This is our experience like to share how we did that. Uh, and, uh, actually, before I show the demos, uh, I want to give a little highlight about the Suma logic. We are, uh, leading cloud scale intelligent operation platforms and uh we were born in AWS cloud 10 years ago and uh right now we are running on 10 AWS regions worldwide. And uh just to give you a sense of our scales, we ingest net new telemetry data, multi petabytes of data ingestion every day, and multi-exabyte of data scan every day. That gives you a sense of the scale Sumo Logic can handle. We have 100, we have thousands of enterprise grade customers. And the demo I'm going to show you is how our agent works. I'm going to show you two parts of the experience. The first part is there are several autonomous investigation agents work simultaneously, and, uh, and secondly, I want to show you beyond ABox, beyond going to summa logic consoles, how to leverage your everyday apps, how the agent works seamlessly with your day to day uh apps, and thanks for Agent Core Gateway. All right, let's, uh, let's show the demos. Let's see. Can everyone hear the voice? Good. OK, thank you. Our security operation with Sumo Logic Dojo AI running on Amazon Baro agent Core. Here we have the Suma Logic Clausian console where a of inside has fire alerts. Before performing any manual investigation, the sock analyst agent has automatically triaged each insight with a verdict assigned such as benign, suspicious, or malicious. Jumping into an insight. AI investigation details are immediately surfaced, provide concise context to help understand this event. All helps to the sock analyst agent. Along with the AI verdicts, the agent recommends adjusted severity level using evidence-backed reasoning from inside data, signals and enrichments. By synthesizing the related signals, a concise narrative style summary is generated. Allow analysts to quickly understand what happened, who was involved, and why it matters. Specific detailed key findings are also presented, providing analysts and immediately sense of threat impact. With deeper analysis is required, the stock analyst agent support hypothesis-based investigation, using your log data and the help of mobile launching directly from the inside. We can begin to use natural language to investigate further. Notice that the context of our incidents is already inherited. To dive a little deeper, we will go ahead and ask mobile to run a quick investigation. Leveraging our security data, the agent will begin to automatically execute his own analysis by running several search queries and summarizing its findings. With the help of stock analyst agent running our agent core, we can bring agentic AI reasoning directly into your security operation workflows and enable analysts to effectively capture the information they need. Here we have another cloudsin insight that has notified us in Slack. Before requesting more details about this event, we will take a look at what suma logic tools are available for our MCP server, prompting it directly from our Slack thread. In the background, our Slack application is leveraging an orchestrator agent hosted on. Agent core runtime, which then communicate with agent core gateway via standard MCP protocol. The agent core gateway is fully managed and the security integrated with Sumo Logic platform APIs and Dojo AI agents. Several actions are ready to use, like capturing more details related to insights and running queries directly in Sumo Logic. As multi-tool calls are supported, we will go ahead and ask mobile to provide any triage details that the sock agent generated and update the status of this insight to in progress. We quickly see our triage details summarized and response that our change has been reflected. To capture more details, we will request a list of related entities tied to this event. Here we can see any user account, IP device information, and other entities associated with this insight. We can even run a search directly into our raw logs in Sumo Logic and see if other users may have experienced similar activity in the last 3 days. Upon retrieve of the results from our log search, we see that other users have in fact been impacted. To wrap this up, we will add a comments to the insight saying that other users appear to be affected. Hopefully this gave you a clearer picture of how you can leverage Dojo AI either directly in Sumo Logic console or from within your favorite apps and tools, all thanks to the help of Amazon Brock Agent Corp. Thank you. Uh, pretty interesting demo, isn't it? Yeah. So for the next couple of slides, I'm gonna show you how we build this. Let me start with uh uh Sima logic platform. From the left hand side you can see we ingest our customers' data, telemetry data for the main part, regardless where they sit, could be in Amazon Cloud, Google Cloud, whatever clouds you're living in, seamlessly ingest this at a super high scale into our platform, and then we have a single pane of glass to operate and monitor and analyze your data for your applications. And then in the middle, as you can see, we have a rich set of analytics tool sets. Uh, so we automatically convert the raw telemetry data into meaningful insights and recommendations so the team can make a decision. And then on the right hand side, we have hundreds of auto box integration for actions, for example, reporting, ticketing, and remediation or, or containment. So, and uh last but not least, this. The top tier which I highlighted is the Dojo AI. It's the multi AI agent systems that we're building and are launching this year. So what is suma logic dojo AI? Dojio is a Japanese word to demonstrate a collection of, of, of intelligence, uh, inte intelligent personas are living. And working together and learning their skill as they go. So I think that perfect fit to our, our agentic AI visions. So we are Dojo AI in a nutshell is a multi-AI agent systems for proactively doing the security and incident analysis and response. So there's a 4 important principles when we build Dojio AI. First, we treat the Dojo AI agent as the digital teammates for the existing security operation teams. They can work seamlessly with the humans and other agents using natural language, multi-turn conversational natural language interface, and very, very important why we decide to use agentic AI versus just the vanilla generate AI. Because we want each of our agents to be fully contextual aware they can seamlessly share the context, update the context between the agents, as well as automatically make decisions about which tool should I use and, and the agent also being smart to discover tools as we go and the tool also be able to access very important domain knowledge in our in in our use cases. So can dynamic access domain knowledge and grounded with domain knowledge. Last but not least, all our agents are secured by design. So let me show you the architecture about how this whole thing looks like. Let me start with the assume logic platform tier. There are 3 pillars of resources exposing as an MCP based resource. The first one is what we call the platform APIs. We have hundreds of open APIs, Public APIs expose our functionality of the core data platforms. And then in the middle, this is the new family of suma logic platforms we call the Dojo AI. Uh, foundaries and as you see at this reinvent we announced 3 agents are G8. They are our query agents can seamlessly translate the natural language into highly scalable DSLL. We call the summa logic query language, and this is where we are able to run natural language, ask questions from Slack or from our console, and then directly compile into natural language queries, run and bring the result back, and summarize in natural language too. Summarize summarize summarization agent is another one where are G88 and then the interesting part about the more autonomous and uh stock analyst agent is where you see in our triage agent as well as our investigation agent. You can fire up a pretty higher level question to say run a quick investigation for me or run a deep investigation and then the agent behind the thing leveraging the reasoning models, able to create a plan, investigating plan. And then generate actionable steps. Each step can automatically run by our platform and bring the result back. So human is also in the loop about this, which makes these things very interesting. The third pillar is our MCP servers. So why we need MCP server, we see there's more and more new, uh, new contexts during the investigation. We want to be able to expose them as a resource to the other tiers. And uh so this is pretty much our summa logic platform tier. And then let's look at our client tiers. The client tier is quite interesting as well. In our default personas, we have a developers or the so-called SREs. They are looking at the code and then they want to be able to, without leaving their own IDEs, whether it would be a VS code or cursors or be a cloud code. Uh, so they want to be able to directly leverage our platform APIs, uh, or using our AI agent remotely through MCP protocol, which is pretty interesting. The other part of the Presonus is our so-called the, the, the IT analysts, security analysts, and they want to stay within their favorite Slack tool, for example, they want to be able also do the same thing. How can I reach to the powerful summa logic platform to build. Uh, to, to build the integration use cases and each of those are customers, type of customers, they are also building their own, uh, customer AI agent and uh. Last but not least, look at our summa logical platform tiers, or the partner tiers. More and more partners are also building their own MCP servers that became publicly addressed, security managed, but then we can enable the end to end integration, uh, using the, using this, this thing. So we're missing something here. How to make this super complex multi-tier cross or kind of uh things working together? How to enable the agent to agent collaboration using MCP. And the answer is agent core gateway. And uh so there's two, at least there's two integration patterns enabled by this architecture. The first one we saw is coming from the client side. The client shall be able to reach out to agent core gateway using the standard MCP protocol to leverage either our API or our Dojo AI agents or reach out to our MCP servers. The other way also quite interesting, our agent using the same patterns shall be able to reach out to our rich ecosystem of partner tier leveraging, for example, Amazon has launched nearly 100s of MCP servers and there are some multi hundreds of Very rich kind of a tool sets and similar for our thread intel partners, similar for our action automation partners so we can our Dojo AI agent can now leverage their rich ecosystem to build a very rich integration. So next, I'm gonna take you to 11 level step down. I want to show you uh 3 patterns to how to make these things happen. The first thing, let's start with the client here. As I said, we have a Slack app, and, uh, guess what, uh, Slack is not super friendly with AI native things like a cursor or VS code, so that we build a client site AI agent just to emulate how our client, how our customers are building their own, uh, custom AI agent, and this AI agent hasn't to be hosted on agent core runtime, which is a great way to simplify a lot of things and, uh, so. Why we need this agent with the tools. The whole topic for today is about using reach that over to chains. So there's 3 important capabilities. There's a, we, we use a land graph to build this oxration agent. The first one is called the planner nodes, and then we have execution nodes. We also have a summarization nodes, so. Now let's start to jump in my first patterns is what we call the API as a tools. So typically there's two types of APIs. One is the API exposed through open API protocols, and agent core gate. We happen to natively support this using open API target. And you have another type of perhaps a more older or legacy AI endpoints, uh, for example, using AI or using graphQL. Those kind of things we can use a lambda. Translator in between so you can translate the protocols and then wire it up with agent core gateway using the lambda target. So that's the first pattern API. Expose API as a tools. The second pattern I want to mention expose AI agent as tools. This is where you see a number of AI native agentic AI agents. Uh, they can also using simply using a lambda target, you can expose the functionality through the lambda target under agent core gateway and Then expose them to the client tier agent. The client tier agent will be able to discover the new AI native functionality through this, uh, patterns. And the third patterns also I want to be interesting, we see a growing demand of MCP servers either as a third party or as a first party. So in that case, thank you for Asian Core Gateway. This half have did a fantastic job to do this piece of heavy lifting for us so we don't have to reinvent the wheel, so they have a native MCP target support as Nick and dwell leader and even better, they have a better fine ground access control. So in the case you want to provide a more custom access control policies, now the fine ground access control is your friends. So those are the three powerful patterns I want to offer with this audience. And uh to wrap up, I want to highlight we already adopt several important agent core gateway features and and the patterns on the left, and we are in the process of actually working with this wonderful service teams from AWS Brock agent core teams to evaluate and testing all the fine ground access control. We're also working super closely on the end to end OOS integrations. And uh we are also exporting, looking for, for the next year. We are also exploring the next more advanced features, how to leverage 18 core memory features because I mentioned it's very important to share memories, share context across agents, uh, uh, our dojo agent families, and also how to enable more advanced collaboration using the A2A uh protocols. So there's a lot of exciting things coming up. Just to wrap up my, my talk here, we found agent core is a great way for us as agentic application builders. We can now focus on our core value proposition. We want to build the agentic stack of intelligence for our customers and uh just to summarize the business impact by shipping the agent core, uh, shipping the Dojo I via agent core, we are able to see. About 50% of faster analysis time, up to 75% reduction into so-called MTTR mean time to resolutions, and for that we see millions of dollars savings from the incidents to happen. So with that, if you want to see a More demos. We have a live demo booth in the in the expos. Feel free to explore. Our demo team will show you live demos and you can try to ask all kinds of tricky questions. Our agent, uh, will be able to give you a surprise. Uh, OK, with that, thank you for the opportunity, turning to do well. All right, thank you, Kui. Um, so let's go through the key takeovers, uh, some of the best practices I want to share as you scale your MCP tools and agents. Always start with your user queries, start with the agent goals. Decompose your agentic goals into agentic tasks and then start working backwards from identifying the right set of enterprise APIs, data stores, and then start MCPifying them. Do not convert all of your enterprise APIs into MCP tools. The agents are autonomous, nondeterministic, and you want to make sure these agents are invoking the right tools. Based on your user queries and agenttic goals, when you create a new MCP tool, make sure you select, you create such type of tool, maybe consolidate multiple APIs into a targeted MCP tool that is aligned to your agenttic tasks. All right, now there are lots of questions about how do you improve the performance of the MCP tools and overall agents. Well, the number one thing that we must take into consideration is how effective your MCP tools are in driving the accuracy and reducing the hallucinations and overall driving the business value and the agent goals, and for that it is extremely important to. Have a continuous feedback loop how how well your agents are invoking your tools, how many times these agents are invoking your tools, what is the impact of these tools on the accuracy? That feedback loop must be closed and The way how you do it is to have those matrix of the MCP tools streamed into the agent core observability, and you have that end to end agent core observability with the traceability and a full fledged matrix of understanding how well your agents are invoking these tools. Extremely important now talking about the performance. The performance of the AI agents are dependent on various different factors, but one of the major factors is the LLM context. One of the major complaints from our customers is, oh well, my agents are latent. They're not responding on time. And then the first question I ask is how many MCP tools does your agent have, and then how much of the context size that each tool bring in and. Accumulate as an overall LLM context size. What's your MCP tool description look like? And when I see that it's like pages and pages of the MCP tool description. So be very cautious how you describe your MCP tools. Prompt engineer these tools because you know what the tool descriptions are extremely important in driving the accuracy for sure, but then it could also introduce different types of attack vectors like command injection or tools poisoning or command control or even different types of prompt injection attacks can happen. So make sure that you are very deliberate about what you put there as part of the tool's description. And then you can reduce the LLM context size by making sure that you have that effective right size tool description for all of yours, and this is not just it. There are lots of lots of other factors contributing to the overall agenttic performance. Obviously I'm just trying to just go through only the tool specific recommendations now. There are other things that you also should consider while scaling with your MCP tools. Security's job zero. And when it comes to the security of agent are extremely important because they're autonomous and again I'm mentioning it's they're nondeterministic. There are different ways of handling the security for agents, the delegation model, as well as impersonation model. The impersonation model is a very well known model where you trust an upstream service and then you call the downstream services by sharing the credentials in a typical microservices world. Well, that's not going to work out. For agentic AI because because these agents are autonomous and non administered again, so you need to have this fine-grained access control with delegation model where your agents are acting on behalf of the end user's persona. They're assuming the role dynamically from the end users, and then they are acting on behalf of the user with. Exchanging the end user's credentials or JWT tokens to the downstream systems, so make sure you have this act on behalf of the delegation model in place now in terms of the fine grain access control, use gateway interceptors that Nick introduced to have the fine grain access control of who is able to call which tool dynamically and then. Customers tell me that, hey, I have these growing number of agents and tools. How do I scale? And the first thing I ask is, do you have a single source of truth of your enterprise approved tools and agents in a single place as a single source of truth? And that's where the agent registry and tools registry comes into play whenever your developer pushes a new. Tool or a new agent have the development and deployment pipeline to your registry or agent registry that does this tools checking statically or dynamically when the tools are being executed. When I say static tools checking, check for security vulnerabilities, confused deputy problem or command injection and tools poisoning, etc. and then. Make sure you use the semantic search capability that Nick walked through, uh, which is the native feature within the agent core gateway. When your tools, number of tools grow, your list tools, MCP list tools performance is going to take a hit. You're going to have a large payloads being sent to your MCP clients. Use semantic Search to take care of that and improve the performance of your MCP tools. So having said that. There are lots of other best practices that I wanna just quickly project um the registry, the the the the importance of having a single source of truth for tools and agents registry and uh bringing this multi-tenancy into the play like use gateway interceptors to isolate these tenants if you're sharing the agent court gateway across multiple tenants. Alright, so here's, uh, uh, uh, uh, here's the getting started. You can use, um, AWS Console or Boto 3, and AWS CLI, and I really recommend you try Agent Core starter toolkit. It's a really easiest way to get started with Agent Core Gateway and integrating with other Agent Core primitives as well. And here are some of the bunch of resources like there are about 50 plus resources that you can go and check out. It's not about, it's not just about the tutorials. It's also like end to end use cases. You'll find lots of use cases out there in this repository. And then obviously what Nick introduced about the schema repair agent, do check that out. It'll help you in MCPifying your existing open API APIs and make sure that your schemas are valid. All right, thank you very much. Do, do not forget to visit Sumo Logic booth and I really appreciate the time. If you have any questions we're gonna be available outside of this of this hall, um, happy to talk, happy to answer your questions. Thank you very much.