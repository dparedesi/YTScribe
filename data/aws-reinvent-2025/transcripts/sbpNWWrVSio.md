---
video_id: sbpNWWrVSio
video_url: https://www.youtube.com/watch?v=sbpNWWrVSio
is_generated: False
is_translatable: True
---

Yeah, cool, big legs. Hey everybody, thank you for coming this afternoon. Uh, my name is Jason Levin. I am the director of partnerships for Luma AI and today we are here to talk about the path to multimodal artificial intelligence. Uh, if that is not what you're here to talk about, then you are in the wrong room. So Language models like Gemini, chat GPT, Cloud, they are incredibly capable. Most of us probably use them every single day. But they don't understand the physical world. They don't know how people move, how objects interact, how light behaves, and so if AGI is going to interact with the real world, the place where we all live and breathe, it needs to understand visual and spatial intelligence, not just linguistic intelligence. So humans, we think visually and AGI has to as well. So at Luma, our mission is to build multimodal AGI and we build video models because video is the closest representation of the real world that we can teach to an AI model. When we say multimodal AGI, what we mean is models that can generate, understand, and ultimately operate within the physical world, from a product sitting on a shelf to a character walking through a scene or even a robot navigating a home. But building these models requires enormous amounts of compute. Thanks, that's where AWS comes and helps out, uh, and it also requires world scale training data. So that's why we are proud to announce that we recently raised our 900 million Series C round, and this funding allows us to train the next generation of multimodal models, models that truly understand motion, physics, spatial reasoning, and continuity. Because AGI is multimodal and the training data, the data set, it's reality itself. When a model can keep a character consistent, understand spatial depth, simulate particle physics, follow instructions, and maintain continuity over time, that's a form of world understanding. This is the foundation of multimodal AGI. That's why we created Ray 3. That's our state of the art video generation model. Video isn't just images in motion. Video encodes physics, cause and effect, timing, behavior, camera movement, and context, and a system that can generate and reason about video is building an internal model of how the real world works. Ray 3 is our most advanced step towards that goal. To walk you through the model's capabilities text to video, image to video, dynamic motion, HDR and EXR output, key frames, reasoning, there's a ton. Uh, I'm gonna hand things over to our resident expert, so please welcome, uh, Davicho Barona who will take you through Ray 3 in depth. Thank you Jason. I appreciate that. Thank you guys for being here. Um, before I go too much further, I really wanna touch on just a couple of the things that make Ray 3 really special and that make it stand out from all the other rest of the video models out there. Uh, number 1 is the Ray 3 reasoning model, and this is, uh, I'll touch on. This a little bit deeper in a minute, but this is a model that judges and refines its own output in real time, uh, trying to match your instructions, uh, without you even necessarily, uh, looking after it, uh, and then the ability of Rai 3 to, export HDR and EXR files directly out of Dream Machine. Um, is also super special. A lot of VFX, uh, teams out there and compositing artists and postproduction teams rely on this kind of file format, and it's something that, uh, stands out and stands apart from all the other models out there, um. And I wanna touch on some of the model's strengths. These are sort of the bright spots, some of the places where we see ray 3 really shining, um, and really standing out here, uh, starting with physics simulations, particles, fluids, explosions, uh, the way that ray 3 is able to deliver really high motion fidelity and, uh, really stable stimulations, natural flow of particles, uh, coherent fluids, and really consistent and punchy, uh, explosions on screen. Then moving to world motion understanding exploration, this is the way that Ray 3 kind of travels through the world and just showing really strong spatial reasoning, maintaining true depth, perspective, seamless motion, really stable sense of scale, and just coherent across all kinds of different environments. Then character anatomy, action animation, different art styles, just the way that Ray 3 is able to preserve anatomical accuracy, expressive movement, fluid action sequences, keeping characters really consistent, believable, and just a really punchy and, uh, uh, exceptional performances on screen. Uh, to close up details, lighting effects, the way that Ray 3 is able to preserve those really fine details, clear textures, surfaces, all kinds of lighting, uh, handling reflections, glows, shadows, all will staying really consistent and true to life in that sense. Um, here's a quick reel that I wanna show you guys. This encompasses a lot of those points that I just, uh, mentioned here. Let's just watch this together. And you actually just watched a bunch of the different functions of Dream Machine here in one. You saw some image to video, text to video, some key frame animations, and even some of the sound effects that you heard there. Uh, I created directly in Dream Machine as well. Uh, now I wanna touch on some of the new features, um, that Ray 3 brings into play, uh, starting with the HDR and EXR outputs. This is a huge step. Forward and no other model uh does this natively and again the way that it exports HDR and EXR files allows a lot of different uh pipelines out there uh to have a lot more control over um the output, um, exporting anywhere from 10 to 16 bit EXR files and allowing those compositing artists postproduction folks VFX artists to control those. Layers of color unlike uh any standard video generations out there. He just a quick example here the raw flat HDR output here on the bottom left and then just pushed into a couple different color space examples here in this case like the day mood night mood and this is by being able to edit those different color layers, um, which is not a standard feature of any other video uh generator out there. Um, and then moving on to that reasoning model that I alluded to before, the way that it's able to reason its way through your generation, judging its own output, refining its own output all by, uh, understanding your instructions and try to, um, accomplish the best result possible without you necessarily even being there. It annotates on its own and, and finally comes up with a result that it feels fits your instructions the best and then we'll upscale that. Allowing you to uh sort of be hands off and saving some credits in your experimentation, uh, if you will. From there, uh, touching on the visual annotation mode here, uh, something that's really, really cool for creators to use, supporting the ability for anybody to just sketch or scribble directly on screen telling the model what elements to move where, when, how, and, uh, for example on screen as you can see. Directing that fish go this way or as you saw before bringing in that UFO element from the top moving the cow up in a certain direction, etc. and that cat jumping from stone to stone all while you're literally just on screen annotating um with your mouse or uh your uh wake up pad for example. From there another really awesome feature of uh Rai 3 is this draft mode, and draft mode allows you to create these low res 360p previews before actually committing to a uh uh high res upscale, um, allowing you to kind of stay. In that creative mode a lot more seamlessly and conserving your credits and being able to experiment a whole lot faster uh than with your standard 720 or 1080 and then you can upscale into those other uh resolution modes within Dream Machine as well. From here touching quickly on the compatible features of Ray 3 starting with image to video of course turning static visuals into cinematic motion text to video we all are very familiar with this, bringing your ideas to life through your written instructions, modify video, the ability to transform videos by either editing a start frame or just with a simple prompt as well. Extend key frames and loop extend being the ability to extend the length of your videos by 5 seconds at a time, either at the beginning or at the end of the video. Key frames transitioning from one frame to the other, um, and then loop being able to seamlessly create, uh, or to create, uh, videos that are seamless and endless loops as well. Um, just touching on the image to video aspect here, as you can see, this is my original key frame image down here left, and then, uh, just looking at some of the motion that that key frame is hinting at and the model is picking up on this. I'm not using any prompt when I'm creating this, just feeding it the image and letting the model do its thing and as you can see there's kind of this, um, motion that's inherited, inherent that is inherently built into this image that you can tell the video is picking up on. Um, where even those, those, uh, like particles on the sides that are moving fast and, uh, the, the beard and the hair kind of moving really gently in that breeze, those lightning bolts showing the that delicate kind of endings to them as well, even down to those reflections on that like wet skin surface there, uh, doing a great job here with that image to video, uh, instruction only. And then touching on the text to video aspect of things here I'm using a really short prompt. I'm just controlling the details that are really important to me, uh, like the shot type macro close up, the subject, a rattlesnake in that moment where it's about to strike, and, um, I'm all the other details that are missing from my prompt. I have more trust that an advanced model like Ray 3 is gonna come up with those details on its own in a way that's really pleasing to me, um. And so in this case it's doing really well with this kind of super short direct prompt and then touching a little bit further on that prompt adherence piece. Ray 3 is really improving those instruction understanding and adapting to all kinds of different prompt style, different prompt lengths as well, and just showing really accurate results, uh, and being reliable with all kinds of different style of prompts, whether it's that super long story style prompt that maybe you get out of an LLM. Um, where it has all kinds of color, very, very detailed, is doing great at those long prompts down to kind of a concise version of that on the, on the right top side over there, um, where it's like the 1 to 3 sentence prompt doing great there over to the bottom left here that. Kind of laundry list style of a prompt where you're just kind of rattling off those keywords doing great with those kinds of prompts as well and then over to that really short prompt that you saw in action before um I kinda tend to live over on the right side between the 1 and 3 sentence um. Style prompt that's just kinda my preference, but grade 3 doing great at understanding all kinds of different prompt styles as well, um, then touching on modified video, a really cool feature here I'm just showing a side by side of, uh, this basic Newton's cradle, uh, video here. Where you with by just editing the start frame or by giving it a basic prompt I'm able to edit that video and isolate certain objects for example those those metal spheres changing them into something completely different uh very quickly and very efficiently here. Uh, from here I wanna touch on that keyframe interpolation that, uh, start and end frame animation where, um, it's just doing great at transforming these stills into truly dynamic sequences and generating these really smooth natural transitions, um, and it just feels super seamless and cinematic and it, it feels really natural and as you can see down here on the bottom left my start frame and my end frame. Um, and then these interpolation markers that I'm kinda testing against, starting with that seamless scale shift as you can see that, uh, start frame is a medium shot and the end frame is a close up shot, and it's going from one to the other really seamlessly making it seem like it's truly a single shot from the beginning. Um, then that material fidelity going from, uh, these sort of hard edges in that in his armor, uh, to this like translucent very like ethereal look in that close up and doing it really naturally really feels like it was like this, uh, in a single shot. Um, the lighting continuity, uh, again, not over brightening, not darkening, just keeping it really true to my original instruction here in these, um, in these key frames, the cinematic flow again, it feels like it's a single shot and even with that camera movement as it zooms into that close up shot, it kinda even jolts back a little bit, uh, making it seem like it's truly part of a single shot. Um, the structural coherent, nothing is really falling apart here. All the lines, all the different elements here are staying together really well, uh, and then the color harmony. I did pick, uh, two frames that are sort of similar in color, but I just wanna showcase how it doesn't deviate from that. It truly holds true to the color, um, that came with those, uh, key frames. Then touching on advanced style adherence and the way that Ray 3 is able to excel at this image to video feature preserving that original keyframe look and feel, the vibe that comes built within that keyframe and just doing it really well across all kinds of different motion across all kinds of different styles as well and then putting it up against these aesthetic markers here just to point some of these out, the lighting again, not brightening it, not darkening it, just staying really true to that original key frame. Uh, the camera affects that shallow depth of field. Even as the camera slightly zooms in on his face, you can still see that that um that focal plane is staying true according to that original key frame, and the bouquet lighting in the background looks really exactly the same as what the instruction told it to. Um, the film grain and that texture looks really, really accurate to that key frame. It's not over sharpening it. It's not making it look very AI-ish. It's not making it look too kind of Photoshopy. It's really keeping true to that original key frame, uh, the moody atmosphere, really reading this key frame, and I'm not using a prompt here either. This is just image to video without any reinforcing prompt and. It's picking up on that kind of like intensity that's built into that key frame and doing a great job translating that not changing that too much at all. The color palette also something really important in this case, um, where you can see those blacks are very muted the highlights are also muted, and here it's staying true to that it's not darkening those, it's not making it look like something other than what that key frame is telling it to. Um, the surface details, those droplets of sweat maybe on his face, the raindrops in the background, even down to those individual hairs as well, everything is staying really intact. Um, the retro aesthetic, I'm a huge fan of like old school kung fu movies, and so I generated this key for him to sort of test against that holding up really well. It truly looks like this was still shot on film. It's got that true 1970s kind of look here, um, and I absolutely love the way that it holds true to that input. Um, and here, uh, talking about the video extend feature before I play this video, I just wanna point out that this original key frame that I used, um, you can see that that warrior is standing already standing at the top of that cliff, um, but as you look at the starting frame here, he's actually climbing up and I'm just trying to point out that you can extend before or after at the tail or at the head of the video and as I play this, just take a look at how smoothly and how fun this sequence can really. Be as he sort of flips off that cliff, turns into this eagle, and then goes across that valley back over to this mountain, he continues to climb up this hill and then gets picked up by the snow leopard, apexes this hill, and then it completely switches over to a whole new scene seamlessly going down and kind of, uh, immersing you into a whole other, uh, scene here and for this video I made this in a day, mind you I'm not a VFX artist and. It took me about 90 different videos to kind of put it together in a way that I really liked it and that translates to about $90. Um, there is no way that I could do this in a day with any traditional tools and certainly not with a $90 budget either. And then finally touching on that video loop aspect um starting with this key frame down on the left here I'm creating a seamless looping animation that really you cannot tell where it starts or where it begins it's just taking you through this journey without even uh knowing that this is actually looping um I was. Uh, uh, a GIF animator, uh, like close to 10 years ago now, and I would've absolutely killed to have this kind of control and these kind of tools back then, um, so all of that I will switch it back to Jason, but just to give you kind of an insight into, uh, all the different features and all the different capabilities of, uh, of Ray 3. Thank you very much, David. You bet. So Now that you've seen what happens behind the scenes with Ray 3, let's take a look at what it's capable of in the hands of professional technical artists. We have a, a short sizzle video here so that you can kind of see what the range looks like on Ray 3. When you close your eyes. What do you see? When we talk about reality. How do you know what is real? For what is simply Your imagination. Now open your eyes. So for enterprises this isn't theoretical anymore. This is real and in market, and video generation is the first real step towards multimodal AGI. It's already transforming how companies create product videos, campaign assets, cinematic scenes, and world scale environments. So let me show you what this looks like across the industries that we serve. We're going to go through some cool use cases. Across every brand that we work with, we hear the same thing. It's that content demand has outpaced human production. Video production still costs thousands of dollars per finished minute. Social platforms keep increasing the pressure, and 2/3 of major brands have already rebuilt their production pipelines in just the last 4 years. Models that understand the world visually and physically unlock a path to producing content at the speed that modern media requires of us. People now expect content that speaks directly to them, their tastes, their culture, their region. But brands need 10 times more video assets per campaign to deliver that level of personalization. They, they simply can't keep up. You can't build teams fast enough for this. So this is where generative video goes from nice to have to existential advantage. And one of the first questions that we get from enterprises is ownership, copyright, and data safety. So we built Luma Enterprise around these three guarantees protected inputs and outputs, your data sits in a walled garden, full commercial ownership of everything you produce, and indemnification on what you produce. Luma has your back legally. And enterprises produce with Luma in two key ways, as we've already touched on. There's Dream Machine, which is our web and iOSAS platform, and you can access all of our AI models through that platform. And then you can also access each of our models such as Ray 3 through direct integration by API for high volume pipelines. So let's start with one of the most immediate areas of impact, consumer packaged goods and e-commerce, near and dear to my heart personally. So creative teams tell us that the hardest part isn't the finals, it's exploring enough ideas quickly. And Ray 3 lets you generate dozens of directions, refine, remix, and then produce fully realized video assets from those explorations. As you can see here, you know, we've got the same hoodie across a bunch of different models in different scenes, different backgrounds, different poses all generated with AI in a fraction of the time. This would typically require multiple photo shoots. It would be a very expensive process. With one product photo, brands can instantly generate product lifestyle photos. So something that used to require a shoot, location, lighting and a full crew can now be done sitting at your laptop any time of day, any place. We're also able to reframe videos, so every platform requires different aspect ratios TikTok, Instagram, LinkedIn, YouTube, YouTube Shorts for content teams, this is a big burden and a big bottleneck. So Ray 3 lets you reframe a video to any aspect ratio you want. It will maintain composition, lighting, shot intent while adapting to 916, 16.9, 4 by 3, and plenty of others. And some of the cool things about it are it will dynamically outpaint the missing content based off of the context of the video, or you can give it a text prompt and tell it what you would like in the missing space. Something else that's cool. That you can do is you can drag the subject of the shot into different parts of the frame so that you can change what the subject is or change how it's positioned. So if you want to go from a center frame shot and then turn it into a Quentin Tarantino style, you know, 1/3 video shot where you've got 3 people in frame, or for marketing teams, if you want to free up some space in that shot to put marketing language superimposed, it allows you to do that as well, which is really powerful. From a single product shot, we can produce multiple hero angles, macro shots, motion passes, and product demos, as you can see here. This compresses days of studio workflow into literally minutes. We can also reuse old product shots as new assets. This is huge for brands with large archives. Old photography becomes new campaign video without any reshoots required. Now let's talk about PDP. For those that aren't in the e-commerce space, this is product display pages. So PDP video lifts conversion rates. Any brand in the room that handles e-commerce volume knows that product videos see a click-through and conversion boost over still images. Ray 3 creates seamless motion loops from keyframe images of your products, so brands can enhance every SKU page without needing new footage. That's an immediate and easy revenue bump for brands. Large brands spend millions of dollars on product video shoots, and SMBs can't afford to turn their entire product catalog into videos, until now. So here's a cool example where rather than animating the whole video, we've just told the model we want to animate the ocean in the background, keep everything in the foreground static. No problem, it can keep the foreground static, we've just added a little bit of life to this image for a product page to increase our conversion rate. That translates immediately to increased bottom line revenue. These apparel examples were created from a single still frame each, and these examples are one-shot examples. These are not cherry picked. This is the very first generation out of the model. It adds natural body mechanics, fabric motion, camera drift, and depth cues, all while maintaining brand consistent lighting and color. When people shop for clothing, they want to see how the fabric moves, how it drapes on a human body, how it's going to look on a real person, and this allows us to simulate that really easily. We can also do fast ad variants. Performance marketing teams love this. One product, many environments, many styles, many moods, each generating new creative for AB testing at a massive scale. We can do demographics specific assets as well. Localization is no longer a bottleneck. Ray 3 generates culturally specific scenes for different regions, different holidays, different seasons without any new reshoots. This is crucial for global brands who need to distribute assets across global market segments very quickly. We can also produce new complex scale, and this is where world modeling becomes really obvious. Crowds, dynamic camera moves, underwater worlds, complex environments, these used to require huge teams and huge budgets. Now a single creator can explore these concepts at full scale. It's incredibly empowering. I love this ant example. This is great. And we can replace the flags in this with any team we want, any corporate logo, even a made up team. So sometimes brands want precise control, where an object should appear, how it should move, how the scene should flow. We can interpret annotations just like a creative collaborator. You can just speak to it like another human. You should be able to do it. So all this functionality allows brands to create targeted ads in a fraction of the time and a fraction of the cost. Here's an example for Adidas Floral. that What Tashi was. Mingo Oh She was. She was. Put you back after this. So, in film and episodic production, we're seeing some of the most exciting adoption. Let's start with pre-production where speed matters the most. Directors can now move from script into storyboarding instantly. We capture tone, emotion, framing, and visual continuity. You can see here we're exploring concepts for a bird getting lost in the woods, finally finding its parents, and we can explore dozens and dozens of different visual concepts for this before we settle on what direction we want to go. Not only that But Ray 3 also understands cinematic language, not just images. Orbit right, Dolly's uh Dolly's zoom, roll left, it understands camera instructions and other native filmmaking language. So this scene demonstrates lighting realism, atmospheric physics, performance consistency, world coherence, all hallmarks of cinematic quality. Here you can see how the dust particles filter through the flashlight. That is typically a very hard shot to achieve. And now we can get the shot just by prompting for it. This is a cute one we threw in. Everybody loves the hamster in the race car. This one with our animated pirates, we were recently speaking with a former Disney exec who worked on Pirates of the Caribbean, who told us that if they had these kinds of AI tools back then when they were shooting the movie, they would have saved over $20 million on production costs. So animation is another area where Ray 3 reshapes workflows. This 32 2nd animation had a compute cost of just under $630. It was made by a single person in a single week. Now, for those of you that don't know, get ready for this. If you were using a traditional animation workflow with an animation studio, the same 32 seconds of footage would have cost between $95,000,000 and $180,000. It would require a crew of 14 to 20 people and would take 6 to 12 weeks depending on iterations and approval cycles. Just think about that difference, $900,000 to $180,000 versus $630.01 person, one week. That is an enormous difference. So Ray 3 isn't just for net new content either. It's also a powerful VFX and post-production tool. Modify video lets you relight, restage, or re-theme existing shots. This is a great example where we've taken a flat CG render and we've produced a fully lit cinematic final pixel version, and we can make that scene look any way we like. It can be a snowy blizzard, an apocalyptic fiery hellscape, a city bustling at night, whatever we ask for. This is a great example of relighting mood shifts and fine-tuned selective adjustments like inserting logos or brands. This task normally requires multiple teams across multiple multiple departments. Uh, now one person can do it in an afternoon. So here Rai 3 can interpret hand-drawn sketches or blocking passes, and it can use that to generate final shots. It dramatically accelerates the production phase. They had a lot of fun making these, you know, you can see we've got, we just sketched the outline of a car over our model, and now they're driving a car. We can restyle the shot. We can turn a toy into a fully realized finished shot. So our video to video pipeline preserves the structure and motion of the base video while transforming its style, theme, or subject. And you can change the level of imagination on it as well. You can adjust the strength of how, how readily it it changes things. It's uh extremely useful for creators that wanna try multiple variations of the same performance, the same environment, or different camera moves. You can see we can pose with a particular product, we can turn John into a realtor there, or we can completely change the clothing on this model as many times as we like. You have control. So I'd like to draw attention to how the depiction of the sheep really snaps too as soon as the actress goes down onto hands and knees. The model immediately recognizes this person is dressed as a sheep. We're going to make them look as a sheep. And with something as simple as tying two shoes together on an iPhone in your backyard, you can turn that into a high octane police chase. So speaking of high octane, let's talk about the gaming industry. In gaming, Ray 3 accelerates the pre-production and the ideation stages across genres. Whether it's world exploration. Or dungeon interiors. Or high intensity combat. It handles spatial reasoning, dynamics across very different game aesthetics. We can also reinterpret 2D sprite sheets. That's so annoying. It can expand on animations. It can smooth motion cycles. It can create new frames that maintain the original character design. This is an example of interpreting a text prompt to display a targeted specific motion with the Sprite for 2D games. We can make this orc make any kind of movement or action we ask it to. This is huge for 2D game production and mobile games. So advertising is one of the verticals where this technology is already having an immediate impact. What you're seeing here are 4 completely different concepts. Different brands, different tones, different visual languages. All created in just a day. Each of these could easily pass as a TV spot or a paid social hero asset. But the key point here is this. These aren't AI experiments. These are production grade ads created without crews, without locations, without sets, props, cameras, or post-production teams. So for agencies and brand studios, Ray 3 collapses the entire traditional workflow pipeline. There's no scouting, there's no scheduling, there's no retouching or compositing, no expensive hero shoots. It lets a single creative team explore a dozen directions in parallel and land on a polished high impact idea in a fraction of the time and a fraction of the budget compared to a conventional production pipeline. So this sneaker commercial is a great illustration of what TV quality in 48 hours looks like. Everything you see here, the slow motion kick, the atmosphere, the dust particles, the rim light, uh, it all normally requires a full production team. Here a single creator produced the entire spot end to end. And for performance marketing teams this is a huge breakthrough. You can launch dozens of premium hero assets, iterate on them instantly, and then tailor every version to a specific audience without ever shooting a second time. From the neighborhood the neighborhood to the to the hardwood. From potential superstardom. They say you have to make waves to be remembered. I disagree. When they told me to make the ground tremble, I stayed light on my feet and walked on clouds all the way to the top. Step light. OK, this one is special because this one isn't just speculative. This is a real commercial campaign. This holiday spot was created by Coca-Cola with Luma in an incredibly tight turnaround window. What made this possible wasn't just speed, it was that Ray 3 understands brand language, so it understands the warmth, the glow, the atmospheric winter feel, and the emotional pacing of a Coke Holiday ad. With traditional production, this level of craft takes large crews, controlled environments, specialized lighting, a long post-production pipeline. And with Ray 3, a small team produced a single campaign quality globally consistent holiday ad at the speed that just wasn't achievable before. For global brands like Coke, that time compression isn't just operationally important, it's strategically transformational. Hello friends. Today we are going to look at something very special because you know they're coming, the holidays. Welcome to the making of the Coca-Cola 2025. Holidays are coming ad. It really feels like this work is, you know, actively shaping how storytelling is evolving. It shows Coca-Cola really reimagining the creative workflow, especially in this AI era. The The days are coming. The look they settled on is really interesting. They've tried a few things out. I can't see it. But they landed on this super expressive hyperrealism, really cinematic scenes. The big question is probably how a tiny team like 5 specialists, how they managed to churn out and carefully refine over 70,000 video clips. 70,000 in just 30 days. That level of precision suggests the team had to go way beyond just simple prompting, shaping the animals, getting the texture right, those big sparkly eyes. Exactly. OK, so let's get into the, uh, this customized workflow they built. Right? So it basically starts with generative image models, then it moves into these powerful video models for the actual animation. There's still a little post and color tweaks, quick cleanup, but the big shift is clear. What we used to fix at the end, we're now building right from the start. Post-production is the new pre-production. Advanced reasoning models let artists plan and solve them early and making scenes feel real before production locks in. Last year was different. Extensive post-production fixes were required. Uh, wheels looked like. They were sliding instead of rolling. Physics needed correction. Logos had to be adjusted and repositioned. Now AI is enabling creators. What used to be patching at the end is now designing at the start, and they needed a big toolbox to handle 70,000 segments, a whole suite. Combining human creativity with AI to turbocharge expression and imagination, giving creatives more freedom, speed and control than ever before. And that's real magic. Exactly. So to close, I wanna come back to a broader look at where we're heading. So this reel highlights the mission behind the technology. What we're seeing is the beginning of a major shift. From video generation as a creative tool to video generation as a world modeling engine that empowers anyone to produce cinematic quality stories without legacy production barriers. When we started Luau. The founding vision of the company was, if we are able to capture and then simulate the world. That would have an immense impact on humanity and what we're able to achieve. And what Dali proved was that from nothing, you can generate images, but in order to generate and understand the world, we need to go way beyond. It's called the Dream Machine, created by tech company Luma, and it's available starting today. Grade 3 was a very small glimpse of what the pinnacle of the previous generation of models looks like. That's not enough. Our goal Is very simply to solve. Multimodal general intelligence. So this is what multimodal AGI looks like in practice. It's models that understand the physical world well enough to help us create inside of it. When we started you very much. The founding vision of the company was.