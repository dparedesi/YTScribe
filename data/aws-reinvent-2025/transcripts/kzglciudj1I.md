---
video_id: kzglciudj1I
video_url: https://www.youtube.com/watch?v=kzglciudj1I
is_generated: False
is_translatable: True
summary: "This session, \"From Documents to Decisions: Unleashing AI-Powered Business Automation\" (SMB204), features David Medford from AWS and Priya Vu, VP of Data Science and AI at AArete, discussing AArete's transformative journey in overcoming data extraction challenges using AWS Bedrock and generative AI. The core business problem was the inability to scalable access valuable data locked within complex unstructured documents, specifically provider contracts between health plans and hospitals, as well as vendor agreements. Traditional manual methods were inefficient, slow (processing only 100 documents per week per person), and inconsistent, while rule-based OCR approaches using AWS Textract, though better, still struggled with diverse document templates, yielding only 50-60% accuracy. To solve this, AArete built \"Doxy AI,\" a generative AI-powered metadata extraction and interpretation tool. The solution architecture leverages a Next.js frontend, AWS Cognito for authentication, AWS Lambda, and Textract for initial OCR, but crucially adds a layer of strategic chunking and prompt engineering built on 17 years of domain ontology. This is processed through AWS Bedrock using Anthropic Claude models (3.5, 3.7, and 4.0) to achieve human-like understanding and data extraction. The results described are staggering: processing capacity skyrocketed to over 500,000 documents per week with 99% accuracy, achieving a 97% reduction in manual effort. This innovation directly translated to $330 million in savings for clients by identifying overpayments through precise reimbursement data comparison. The tool has processed 2.5 million documents and 442 billion tokens in just 22 months. AArete is now evolving Doxy AI into a SaaS platform to integrate directly into client ecosystems for real-time contract lifecycle management, aiming to make it a living part of business process automation rather than a one-time solution."
keywords: AWS Bedrock, Generative AI, Document Intelligence, Healthcare Payer Contracts, Doxy AI, AArete, Unstructured Data Extraction, Business Process Automation, Anthropic Claude, Cost Savings

All right. Thank you for joining everyone. I'm David Medford, AWS commercial sales leader, uh, based out of Chicago. I have the, uh, luxury of building teams to help our customers, uh, build solutions, and, uh, I'm excited to highlight one of those today with Priya here. Uh, but today's session is documents to decisions. Erit lives in the healthcare and financial services space and serves as a consulting firm and helping customers extract data from documents and make uh decisions through their consulting firm or through their consultants, and they face a unique challenge in getting that data out of the documents and that's what we're here to discuss today, so. Uh, without further ado, I wanna introduce the leader of the program, Priya. Thanks for coming to Reinvent. Welcome to your first reinvent, and we're looking forward to hearing more about, uh, Ari and your problem. Absolutely, Dave, thank you so much for having us. This is fantastic. Are you all having fun here? Absolutely, me too. I feel like I'm a kid in a candy store. Exciting things happening everywhere. So myself, Priya Vu, VP of data science and AI at AIT, um, AET is a global management and technology consulting company headquartered in Chicago. We help healthcarepayers, you know, the health insurance companies primarily. We, you know, process their claims. We help them understand, you know, do we improve their medical and administrative cost reduction. And then also help out with member engagement, improve, uh, member retention, and also do help them through care coordination, care management. We also have a financial services, uh, practice, and I lead the AI practice that serves both the industries. So really excited to be here and share our success story. You know, our success stories essentially, uh, we found a business problem. A challenge and then we applied generative AI to solve that problem at scale and that's really the story that we want to share here today um can I have the thing. Awesome so the challenge that we are, we have solved for is data is locked up in documents, you know, we, we have processed a lot of provider contracts which are contracts between health plans and also the providers which is, you know, the hospitals and physicians so there's a contract that's written that details about, you know, all the payment terms, the contractual terms. And also we have seen member of vendor contracts, right? That process for uh organization through the vendors with various SLAs, KPIs, and other metrics. Data that is in those contract documents is locked. And in order to get it out, the current options essentially fall short, you know, you need a lot of institutional knowledge you have to throw a lot of people at it anytime you want information that is needed to be extracted from documents you need, uh, you know, uh, all of those kind of additional resources. It's not scalable, right? You need, you have more documents you need more people to do the work, but and then also the, the contract life cycle management solutions and products that are out there. They're not doing a much effective job because they only cater to the configurations for a few fields, but then a lot more other fields they get that you, you do, there's no process now for contract life cycle management tools to incorporate. So with that, what happens is really a lot of friction and downstream use cases if you want that to use that data and documents for analytics, aggregated metrics, KPIs, insights. You're out of luck. So that's the challenge that we have solved for using Doxy AI. So our doxy AI is a metadata extraction and interpretation tool that is built on generative AI. So simply put, what it does is it looks at the document, understands it like a human would. Extract the information from it into a structured repository that is then usable for downstream needs so the primary, uh, perspective or the primary uh uplift is in terms of removing that subjectivity, you know, you need a lot of people to do all the work, but different people with different experiences would do it differently, right? But. By using this technology we've been able to remove eliminate that variance and subjectivity within the humans and every document is processed the same way across the board by, you know, large language model that's the primary benefit. And the scale of course, but the secondary benefit is now you have extracted that data and then you can use it, it's aggregable, queryable, searchable, and you can use it for downstream needs and that's the secondary benefit from it. Yeah, and I know. Korea, we're talking about the generative AI portion of it now, but like any good story, there's a start, middle, and end, and you all have been on an evolution of trying to figure out how do we get this data out of the documents. So talk to us a little bit about the journey you all went on at AE to get to this point. Definitely, Dave. So that's a perfect segue because. Our journey did not happen overnight, right? We, we wanted to definitely start off with prior to 2020 we were doing this manually. We had, you know, 17 years of ontology experience doing this manually. And obviously our consultants were not too happy with it because it's a mundane work they got they wanna be able to do something more effective that is more strategic with the clients and so but then the scale of that was we were only processing 100 documents per week per person. During using that manual approach and then with 2020 we decided, you know what, we need an accelerator for this because it's not really ideal to just put human resources on this problem, right? So we used AWS Textract as an OCR tool, by the way, that's the best OCR tool out there if you're wondering. We use that to convert PDFs to text and then use that text we wrote rules based, um, you know, engine on top of it we basically said go to this page, go to this line, grab these characters and for this field put it into this JSON structure so we created that rules. Based engine which worked for quite a a a good time but then there we also had a challenge with it, you know, the challenge was not all documents are created equal if it followed a certain template, the results would be consistent if the documents had varied level of information at varied locations. We were out of luck and then we had to again go back to human resources for solving that problem and so it was giving us essentially, you know, 50 to 60% accurate results. So with the advent of generative AI in 2024 we did rapid prototyping and then we said this is the perfect candidate for using. Uh, generative AI for solving this problem and then we actually got very positive results and our, our impact at that point really was very significant. Yeah, I think it's, it's great to see the sort of construct and where you were going and the idea of getting the work off the consultant's plate. It was really low value to put them back in front of your customers. This is great. I think the, um, the numbers start to speak for themselves in terms of what you were able to deliver. So can you walk us through the numbers and what you saw as a business? Absolutely. I think we. We, as you told, as I mentioned, right, we were at 100 doc we were processing 100 documents per person per week, and we went up to actually even 500,000 documents per week. This is actually a lowball number, but the scale that we were able to process this. Using our generative or doxy AI approach was tremendous, and this is something that we could not have otherwise done if not for uh the technology obviously along with the ontology and the domain expertise that we incorporated into it but then. Who would want a solution or a product that is not accurate, right? You, you, it doesn't, it doesn't matter if you're able to do it at that scale. You need to be accurate, and we've were able to get it at 99% accuracy level, which again a lot of our clients and, and our cust our team members were extremely ecstatic with that. And also this also created a a reduction in the manual effort that was going into from our past experience so we got a 97% reduction in the manual effort all in all enabled a lot of savings for our clients, you know, we were able to get a $330 million of saving. Savings for our clients as a combination of direct and indirect savings. So the direct savings for our health plans was we got the, the reimbursement information from the provider contracts out and then we compare it to the claims and then identified overpayments and then that's a direct recoupable opportunity for the health plans. And that's the direct savings that they were able to uh find from an indirect savings percent it's obviously all of the human resources that they have avoided in order to be able to do that kind of work and the scale at which that was done. So if it, if doxy wasn't that accelerator that was used then they would have to use. A lot of, you know, manual team members for this, so that's really the savings that we have seen. Yeah, I appreciate how you shaped up the business problem and now you're seeing clear returns and now the business is excited about new G AI products through your team and what you're capable of delivering. I know this is uh oftentimes technical audience, so can you walk us through the solution, how it's architected? Absolutely. So by the end of this year we're also launching a SAS platform on Doxy AI. That's, that's a great achievement for us. And number 2, we also submitted for patent, so it's patent pending, um, invention as well. So we've put all of our eggs into AWS baskets, so to speak. So we've built our end to end solution on AWS platform. Um, you know, our external users would log into, you know, a frontend interface that's built on Next JS, uh, authenticated by AWS Cognito, and when they upload the documents that gets stored into an S3 bucket, and from there it invokes a lambda function that calls text track service to then use that to convert. PDFs to text and here what happens is if you have, you know, tables, if you have forms and check boxes if you have signatures that identifies signatures and if you have, you know, even handwritten notes it identifies the handwritten notes and converts it into text all of this happens within the text track process and then that data that output gets ready. To be processed using it runs on ECS as uh as a containerized service using that compute but then what then happens is essentially we, we strategically chunk the document, the text, and include our prompting that is built on our 17 years of ontology and domain expertise and together. We make API calls to um AWS Bedrock and then the output gets extracted so we use Novaro uh anthropic sonic models 3.5, 3.7, and even 4.0. So we're very heavily invested into AWS Bedrock and we were also one of the top, uh, uh, consumers of AWS Bedrock until recently. So from there, uh, that output gets stored into Snowflake database, which then is again usable by the front-end users from our interface. Yeah, and I, I think a couple of things to point out here for the, for the audience is you touched on it a little bit, but you've used 4 to 5 models now, and you've been able to swap those out when you found better returns and a and a cost performance. The other thing I'll say is you work in heavily regulated industry. So you were able to put this in in your own VPC in your own environment, and I know it for particular clients, high trust was a big issue, and you're able to accomplish all of that in the solution. So, again, by the numbers, I think it's interesting to look because everybody now know what a token is and know it, now everyone is familiar with the size, but talk us through what the actual numbers look like and as the outputs of how you're leveraging this architecture. Absolutely the numbers really speak for itself. We've been able to process about 2.5 million documents using our tool, uh, which, which amounts to like really 442 billion tokens, uh, in the just in the last 22 months. So the scale at which we were done, we were able to do this, would not have been possible without Dave and his team, you know, Brett and Sanketh and Garish and everybody here. It, it was, it was not something that we achieved, you know, in one go, you know, we would constantly keep hitting the, the limits like the tokens per minute limit and also the rate per minute limit. We would keep hitting it and then we would work with Dave and team to help increase those numbers and finally we've gotten them to a pretty high level so that we're able to process things at the scale and keep up the delivery that we had to do for our clients. So absolutely. Great success story. We wouldn't have been able to scale without the the Bedrock, uh, platform that is built by AWS and also for this partnership. There's not like a vendor customer kind of relationship, but we're together in it to to be to help and deliver. So very thankful to Dave and team. Well, we appreciate the trust that EEI has put into AWS and I, I think what's interesting now is you have a solution that is in a consulting world living and breathing. And I think when you start to look at maybe your health plan customers, when you, uh, look at them and how they can go out and can consume Doxy AI, how do you see this product changing your business and how you serve those customers? Absolutely. So, so Dave, you know, our, our product, we don't want it to be a once and done kind of a thing. Anytime you process documents through Doxy AI, you, there are contract documents, but then you go back and renegotiate and do amendments on top of it, right? So these documents are living and breathing and changing over time. And what we want to do with doxy AI is make it an integral part of a business process automation so that it lives within the ecosystem of our um healthcare payers and their environment that way each provider contract or vendor contract when they are written, when they're signed it processed through docs. gets the metadata out of it and then we proceed towards either claims adjudication, a claim accuracy, or even invoice accuracy and pricing at that time. So we want this to be a living and breathing thing within our clients ecosystem and that's our goal and that's where we're heading towards through our SAS platform and subsequent integrations through API. That we're going to build within, you know, our payer ecosystem. Yeah, and I think the other thing we've noticed is it's unlocked other opportunities for vendor management, uh, other contract management opportunities that are maybe outside of the core health plan space, and you're able to move quickly now, uh, and you've got the confidence in the business and, uh, your customers now have the confidence in you, so. Uh, thank you, Priya for walking us through the solution, the challenge, and how you and Reed and AWS partnered together to go, um, solve this problem, and, uh, again, enjoy your first reinvent. So we'll be, uh, at the end here, uh, down to the side. Priya will be signing autographs, and, um, yeah, thank you all for coming. Thanks, Priya. Appreciate it. Thank you so much.