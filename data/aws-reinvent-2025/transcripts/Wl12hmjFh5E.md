---
video_id: Wl12hmjFh5E
video_url: https://www.youtube.com/watch?v=Wl12hmjFh5E
is_generated: False
is_translatable: True
---

Hey everyone, um, thanks for joining us. This is uh an amazing conference. There's so many people around. Thank you for taking the time out of your busy schedule to join us here. Um, how's everybody's conference so far? It's good? Amazing response. OK, great. So my name is Jessie Butler. I am a principal product manager in the Amazon EKS service team. I'm joined by my colleague Shriram. Hey everyone, I'm Shriram Ranganathan. I'm a product manager with the Amazon EKService team. So we're really excited to talk with you about something we've been working on for several months called Amazon EKS capabilities. So, uh, I'm gonna start with some context, go through sort of some of the theory of why we built this, uh, this set of features and how we hope they will benefit you, and then, uh, Shriram will come back up and dive into some of the details. Um, so just starting with kind of setting context, um, if anybody's seen any of the launch announcements, you know that this is related to platform services and, uh, growing sort of your cluster experience through these foundational services for GOps. Um, I like this quote from Alan Kay who's the father of the small talk programming language object oriented programming. Simple things should be simple, complex things should be possible. This is a great analog to abstractions like why do we abstract things in systems. A little bit more on point to our discussion is one from, uh, from Diekstra. The purpose of abstraction is not to be vague but to be creating a new semantic level in which one can be absolutely precise. This is a better sort of software engineering concept around abstractions. We don't want. to hide the abstractions because um of the complexity under abstractions because often there's power in them, right? If you look at AWS or look at Kubernetes there's so much you can do with that cluster. The last thing we wanna do is put a sort of layer on top of it that obfuscates. We wanna actually elevate. So Kubernetes itself is an abstraction or set of abstractions. Kubernetes makes complex things possible. Um, we like to look at Kubernetes as sort of the platform layer for many customers. It's the front door to AWS. It's often said that Kubernetes is complex itself. I personally have worked on fairly large software projects, scheduling thousands or hundreds of thousands of processes across hundreds or thousands of computers. That is possible, but it is hard. It takes a lot of concerted, deep technical effort and a lot of resources. Kubernetes democratizes, uh, scaled distributed computing. And this isn't my opinion, right? The world's kind of agrees, uh, so 80% of enterprises are using Huinities in production with another 13% report piloting or, um, kind of investigating for future adoption. So this has become a de facto standard in cloud computing because it is a good abstraction because it is a powerful way to layer, uh, functionality into your systems. So simplicity is one of the things that we're looking for and we'll talk a little bit about how EKS is one of our sort of tenets of making simpler experiences for our customers. We also need to think about consistency at scale. This is where we have things like declarative configuration and how these kind of cloud native practices have come to be. System consistency is incredibly important at the scale that many of you operate at and if you're just getting started in your scaling journey. The scaling level that you aspire to consistency is paramount. We also have extensibility as a core requirement for building platforms in the modern world, and Kubernetes is incredibly extensible. These abstractions are good and powerful, but you can also extend them and build out any of your own custom needs right into the cluster. So one of those things around extent extensibility is kind of the result is this huge vast ecosystem of tools and projects that are available for you to use, right? Kubernetes itself is incredibly resilient and very, very powerful, but it is not itself an end to end production ready application platform, as many of you probably already know. Um, you need to do things. You need to install things in the cluster to make it be what you need it to be to ship and run software at scale. So the CNCF and uh uh has over 200 projects at this point, hundreds of compatible tools. This ecosystem is vast. There's really unlimited customization beyond that. You can build your own controllers and have your own custom resources in the cluster to do just about anything you want, from managing AWS cloud resources to ordering Domino's pizzas. When we look at simplicity and all of that um amazing abstraction and extensibility, we're really talking about relative simplicity. Kubernetes itself isn't a simple thing, but it is relatively simple when you compare it to other things. For example, the core system has about 1500 methods, um, and AWS SDK has about 10,000. Now these are vastly different things with different purposes. AWS is a bunch of features and products and other things that do a lot of things, but there is relative simplicity if you compare the two. And what's nice with EKS and using uh Kubernetes as the front door to AWS is that you're able to capture some of that complexity and powerful primitives that you can abstract for your end users or for yourself as you're scaling your workloads. So by 7 years ago when we started, uh, Amazon ECS, we launched actually right here at Reinvent in 2017. Our main motivation was to make, uh, Kubernetes more accessible to AWS customers. We knew that this abstraction, this primitive, uh, kind of, uh, the set of primitives and these, these kind of standards would evolve, and we were right. Um, we're very happy to have been right because, uh, the growth of EKS has been enormous. Um, EKS started as a managed control plane, right, so we knew that doing Kubernetes the hard way while educational was actually hard. So the managed control plane, um, was where we started and we've evolved ever since with this mission to, uh, help you build reliable and stable and secure applications with Kubernete's clusters, um, and it's very important to note that you know it's Kubernetes, it is, it is upstream compliant Kubernetes, so we get to benefit from all of the extensibility as well as you do. So if we look at one of the feature layers for EKS around um the data plane and node management. So when we started with that managed control plane, we had self-managed nodes. This was fine, right? The real complex part was in the control plane and actually customers wanted self-managed nodes. They wanted that autonomy and over time that became burdensome to manage all of those nodes directly. And so we put a life cycle API on it with managed node groups, um, allowing you to create groups of nodes that you could manage with API calls and manage that life cycle. Um, moving to last year reinvent, we announced auto mode which is a fully and, and fully managed, uh, data plan. So that gives you an end to end push button cluster where, uh, you just, you don't have to worry about nodes at all. We fully manage everything as well as the storage and networking integrations. Right, so this is an example of the evolution of EKS starting with what we know customers need at the moment and evolving over time based on your feedback and also based on, you know, where the direction of Kubernetes is going as well as the direction of AWS. So when customers start with EKS we're talking about building and managing clusters primarily, uh, and EKS makes it really easy to do that, right? At this point with auto mode, you go into the console, you click a button rather than going and growing coffee beans and roasting them and making pots of coffee, you can simply get a cup of coffee from the kitchen and come back and pretty much have a cluster up and ready, ready, ready to go. So our mission since launch has to been continuously improve this cluster life cycle management for you to like to, to make that a better experience um over time, adding new feature layers like auto mode, hybrid nodes, these are all sort of commitments toward that vision. But when you look at scaling your workloads and using more clusters and expanding your workloads across multiple accounts and multiple regions, you're really scaling with Kubernetes. EKS helps you build and manage production ready clusters. It's up to you to manage and scale your Kubernetes, so workloads, uh, networking components, storage resources, all the integrations that you have, plus any custom solutions that you need. Um, all of those things come into the cluster and become things that you manage maybe with helm charts or maybe with other methods. Um, this can work really well to get started and even at scale can work really well. Um, there's a lot of people that are very good at automation, they're very good at governance, but as scale continues, you have more regions, more accounts, depending on how autonomous your teams need to be. Compliance gets hard, auditing gets hard, and scale just brings more operational burden. So if we look at how most customers are uh dealing with cluster management beyond the API, there's a lot of terraform use, some cloud formation, and we're seeing some Kubernetes IAC. Um, so what we have here is, is sort of the imperative external to the cluster IAC, mostly terraform, and this works really well. We have version control declarative configuration that's reconciled against the source of truth. Um, terraform does its thing and you get a cluster, and this can work as you scale as well, right? Having two or three teams going across a couple of regions as you start to scale, this can work incredibly well. But at some point we do hit a point of friction. We experience this problem, uh, where there's just too many, either too many cooks in the kitchen or there's too many pipelines for one small team to manage. And if something goes wrong, we end up in this position of trying to find where it went wrong, how to remedy it, who even owns it, right? This can be a, a problem with scale. So we start with single clusters and you want to get to this all seeing all dancing cross region cross account scalable thing. How do customers who have been most successful with EKS get there? What we see is that there's this inflection point around the point where multiple clusters come into the mix. Um, and you want to get to that next level of abstraction and automation. This is the point where we see a lot of customers start investing pretty heavily in dedicated platform engineering resources. We move away from, uh, sort of decentralized teams, um, and more toward a structured platform engineering path to get you to the next level. Um, this is something that we see commonly with all of our most scaled customers. But all of them do it a little differently. This is bespoke investment, right? There is engineering work to be done here that is not necessarily the business logic that you wanna ship to customers. Um, at our largest customers there's dedicated platform teams whose customers are the internal development teams, right? So this is a, a place where we see a lot of investment and a lot of commonality, but things are slightly different. One thing that is true is that these platform engineering efforts follow these sort of core cloud native characteristics that we see in Kubernetes also repeated throughout the ecosystem. Declarative configuration and continuous reconciliation are things that are just the kind of bread and butter primitive of these systems. Uh, programmatic discoverability and observability, these are really important at scale. You know, if you want to run 400 clusters with a team of 5 people, you need to be able to automate and you need to be able to watch where you are, um. Active drift detection and automated self-healing is really a sort of holy grail, right? Wouldn't it be great if the system using its underlying primitives could keep itself whole, right? What if an imperative change comes in? What if something fails? Can something automatically fix that at 2 a.m. for me and, and slack me so I can see it with my morning coffee rather than getting paged at 2 in the morning? And on top of all of this, we want these things to be standards based, you know, Linux took off because it not is the, it wasn't the best operating system on the planet. It was an open standard that we could all contribute to and make better. That's what we see with Kubernetes. It's an open standard. It's portable and it's governed. So when we look at these characteristics and we think about Kubernetes, we check all of the boxes, right? This, this is how the system was built. And I think it's important to note that as we start looking at something like EKS capabilities and wondering, do we need this? Do I need this in my world? Um, we often find that people who are new to Kubernetes and new to DevOps are learning DevOps through Kubernetes. They start using Kubernetes and they find that these things are the aspects of the system, where in fact these are kind of just DevOps best practices that through trial of fire and and ice, uh, have become the, the principles and the standards that we know. Kubernetes was designed with these principles in mind. So we see that Kubernetes is in fact a reference implementation of these underlying characteristics that are ideal for systems. The reality is that there is no ideal system for everyone. Every customer has different needs and different requirements. Um, you may have, uh, different, um, governance and compliance, uh, for restricted, uh, regulated workloads and restricted environments, um, and just generally culture, right? It's different across every customer. Uh, everybody has different needs, so we can't say, oh, take the, uh, GetUps off of the shelf and I'll, I'll buy that product and I'll just use it. That's not how that works, right? We have to think about all of the different ways that we have to keep the standards open and keep the primitives light so that you can build what you need to in order to scale. One thing that we see across all of our most common, um, our most successful scaled customers in EKS are these common platform, uh, foundational components really we're looking at workloads, infrastructure, uh, for cloud resources and infrastructure for the underlying clusters and related, uh, you know, production databases, that kind of stuff. These are the uh the foundational components that every team has to solve for. You know, depending on the line of business or the, the culture or the, the even the, the point of scale, right, all the other things may be different, but these are three things that everyone has to account for at some point as we begin the scaling journey. So we looked to GitOps and I think this was a very trendy and buzzy word for a while. Um, we've been, uh, really, um, consistently guiding customers toward GOps since 2019 or 2020. So since before it was a buzzword and through it being a buzzword and now it's no longer a buzzword, um. I think of GitOps as a reference implementation, you know, it takes these best practices, these cloud native system fundamentals, and sort of how we think of a system running well in the cloud today, and it turns it into a reference implementation. These four characteristics are what makes a GitOp system. You can build it with corn shell scripts on a raspberry pi if you want to. It doesn't have to be Kubernetes, but it does have to have its desired state expressed declaratively. Declarative configuration is a requirement for practicing GitOps. That desired state is immutable in version, which just means if it's a YAML file, it goes into get. It's versioned, then it's immutable. Uh, the desired state is automatically applied from source. So when you push something to get, it automatically ends up in your running system. Now, these three first things are anybody using Git and doing ops. That's GitOps. The last one is most paramount and the thing that makes such a good fit for Kubernetes is the desired state is continuously reconciled. So that means imperative changes that come in from a junior admin who hits the wrong button all the way through to nodes falling over and having to repair themselves through a software update. All of these things come through from the system out, not imperatively from you getting paged at 2 in the morning. So the desired state is reconciled by agents within the system. This is probably the most defining characteristic of what a GitOp system is. That's why Kubernetes is such a good match. So GitOps using this reference implementation, we can work toward building ideal platforms. We can't build ideal systems for everybody. Everybody has different requirements, but the platform can be an ideal and one of the things we were motivated to do, um, earlier this year is to see if we could find some commonality and some things that we could help, uh, provide in EKS as native features to help with this practice of. GitOps for workloads, for cloud resources, and for, uh, for clusters themselves, you know, building a fleet management system has never been possible for us because every other customer we talked to has wildly different requirements. So with these, uh, this reference and implantation being extensible, you can actually build your own and we can help you, uh, support you with that. So GitOps works really well and it is open source software that's usually happening on the Kubernetes cluster, for example, Argo CD you might use ACK or cross-plane for cloud resources or any number of other uh solutions. Argo CD is kind of a de facto standard. Um, so looking at the system, you'd say yes, OK, now we have manageable growth. Um, and you can add more clusters and you can manage with a single control plane those deployments and you can add more clusters and eventually we end up in this situation where we're scaling out, uh, decentralized GitOps environments which are based on open source that you're self-managing in your clusters and we're kind of back in the same position. Um, so we see teams ending up spending time here and this is where the shift to we, we need dedicated platform teams comes in. Um, we end up managing the thing that helps us abstract things away, but we end up kind of having to manage the problems with that abstraction and then you end up just chasing your own tail and then you end up with issues in a system like this as well. So Kubernetes based platforms, this, this is the way that you scale, but we think that we have a way to help you do it a little more efficiently with that foundational bespoke um complexity being something that we handle. If you look again back at our feature evolution this time thinking about things in your cluster. We start with helm charts. We point you to the CNCF and say, have a nice time. There's, uh, there's 4000 helm charts, right? Um, EKS add-ons was a step in the right direction just like manage no Groups. It puts a life cycle API around some of the most commonly used operational add-ons. So what comes next? So that's why we're really excited to introduce you to EKS capabilities. So this is the first time that EKS has had a, a, a new feature layer. We expect a full part of the roadmap dedicated here. We're really excited about this, that this, uh, we're moving beyond managing cluster life cycle to also help you build and scale with Kubernetes with your clusters. So with EKS capabilities you're able to click a button to start using these things, um, they will evolve as you evolve by using them. Um, we're gonna be adding more features and more refinements, uh, but you'll be able to increase your velocity from day one, remove some of that friction. If you're just starting out on your scaling journey, this is a great place to start, even if it feels like it's all too much and you won't need it. If you start using uh GetOps either open source, self-managed, or with capabilities with a, a few clicks of a button, which we'll talk about in a, in a few moments here, we'll dive into some of the details, but starting here means you're ready for the future. It means you won't ever have to do that refactoring and figuring out how to put all the fires while you're doing it. You just start with these, these primitives and you're ready to scale. It's all open. It's all based on Kubernete's open standards. Uh, right now the three capabilities that we've launched are all based on well-known, um, open source projects are either de facto standards, emerging, uh, kind of innovations, or things that, uh, AWS stands behind with a capital S support. So these are all things that customers already use and are interested in using more of. Um, so Argo CD is where our GitOps engine comes in. Um, this is, uh, used by most of our customers practicing GitOps. Um, Argo CD has really emerged as its own de facto standard alongside of Kubernetes. Um, what we do see a lot is this use of Argo CD is, is self-managed and predominantly driven out of the platform team. A lot of customers actually use Argo CD particularly for their platform where their development customers aren't actually using it to deploy workloads at all. Using, you know, imperative pipelines with Bash Grips and Jenkins has worked just fine and so we see this kind of coming out of the platform team and starting to evolve. We also have ACK and Crow, which gives us the infrastructure part. Another piece that we see with uh customers who are practicing GitOps, and some for years, really enjoying it, having a lot of success with it, they stop short of managing infrastructure with it. They only work on, for example, um, uh, cluster add-ons or maybe some just do internal workloads. What we think the real power in the system is when you can bring cloud infrastructure to the party and start managing your clusters, your workload resources, everything together. So Crow and ACK help you do that. So really quickly, I just want to be clear before we dig into some of the details of how these work. Um, this is what a diagram might look like for a self-managed platform with the software components. You see, EKS uh runs the cluster for you. In our account, we run the entire control plane. If you're using auto mode, we'll also manage your nodes for you. Um, and in your account you're Helm chart installing, um, Argo CD ACK, and Crow, and you have the CRDs to work with and you're creating applications and resources and instances and therefore having S3 buckets and RDS instances in your accounts. With EKS capabilities, we're running all of the controllers and their dependencies in our service accounts just like we do the cluster control planes. So Argo CD, ACK, and Crow are fully run in EKS infrastructure. They don't take up compute resources and other resources in your clusters. They free up your compute and your pod slots for your workloads, and we manage the life cycle patching. Scaling and the resilience and availability of of your capabilities for you, we do install the CRDs in your cluster, so you're still in full control of your applications and your ACK resources and other custom resources that come through the capabilities. So our vision for this new feature layer is shown through the three initial capabilities that we're offering. They're all open standards. They're Kubernete's native experiences, which we think is very vital. You're choosing to use Kubernetes whether you're starting out and ready to scale or you've been with us for years. You're choosing it in part because it is a standard. You have a standardization layer, you have multiple environments, multiple teams potentially, uh, deploying workloads to multiple cloud environments or on-prem. You want that standardization. So it's very important to us as we build more features and into the future that those are portable for you as well. We want to reduce friction. So, these will be always fully managed for you. There will never be a capability that runs or deploys things to your cluster. However, we might have uh additional CRDs in the cluster to actually help uh accelerate velocity and help you fine tune your configurations. So we'll talk about IM role selectors with ACK in a bit, but there might be others of those in the future. Um, and we're also really excited about new innovations. So Crow bringing Crow into, uh, EKS capabilities is really the first time in EKS where we've offered, um, you know, not GA software, right? This is software that is evolving. It's something that the community is really excited about. Um, it's recently was, uh, donated to SI cloud providers, so it's very, um, visible in the ecosystem, but it is not GA. I will clarify that EKS capabilities are GA. Our ability to manage and run that software for you on your behalf is fully generally available in all commercial regions. It's available where EKS is. Uh, however, what you might find is that we will lean into new innovations and new trends in the, in the Kubernetes ecosystem, and this is a way for us to be able to do that for you. So I think I'm gonna pass it to my colleague Shriram. He's gonna dig into the features and talk a little bit about the details. Thank you. Thanks Jesse. Hey everyone. So what is EKS capabilities? EKS capabilities is an extensible set of platform features that extend your EKS clusters. Unlike self-managed installations, AWS fully manages all EKS capabilities for you. What this means is you don't have any operational overhead. You don't have to worry about installation, configuration, patching, upgrades. All of that is automatically taken care of by AWS. If you try to cubecuttle into your Kubernius cluster and try to find ArgoCD controllers, or let's say for example, ACK controllers, you're not going to find them. They run in AWS own service accounts, they're fully managed, and that's why you won't be able to find them. How do you go about creating a capability? All capabilities follow a common pattern when it comes to creation. When you create a capability, you first have to identify which capability you want. You have to give it a name so that you can refer to it later. You want to decide which cluster you want to enable the capability on, and then you have to pass an capability role. Depending on the capability, there might be some additional configurations needed. We will touch upon those as we go through the next slides. And just to note, all of these capabilities can be created using multiple modes. You can do it through the CLI, you can do it through EKS Scuttle, any of your favorite IAC tools or through the EKS console. There are no restrictions. So when we build EKS capabilities, we have introduced a new service principle that you see on screen here. When you create the capability role, you need to ensure that the trust policy trusts this particular service principle. This is key. It's not your OIDC provider. It is the EKS capability service principle. You can always scope this down to specific resources by adding additional conditions. Once you're done with the trust policy, now comes the time for permissions. Depending on how you want to use the capabilities and what integrations you want, you might need to add certain permissions for each of the capability roles. In case of Argo CD, depending on if your source is coming from, let's say code commit or from ECR or from code connections, you have to give it appropriate permissions. With ACK, depending on the AWS resources you plan to use, you have to correspondingly give the underlying permissions. As an example, if you plan to use S3, then you need to give S3 permissions. If you plan to use RDS, you need to give it RDS permissions. With Crow, there are no specific permissions required. It is confined to the Kuberus cluster. But let's say for example, if you plan to use ACK along with Crow, then you need to make sure that whatever AC AWS resources are governed by the Crow RGD. You have to give the corresponding permissions to the ACK's capability role, not to the crow capability role. We will walk through hands-on examples of three EKS capabilities, Argo CD for automation with deep AWS integration, ACK for infrastructure provisioning with scoped flexible IM roles, Crow for creating platform abstractions. These are the building blocks. You decide how you want to use them. What is Arbo CD? I know Jessie touched briefly upon what is Arbo CD. Argo CD is a GitOps-based continuous deployment tool. Your Git repository becomes the source of truth, and Argo CD ensures that your cluster state matches what you have defined in Git. It supports drift detection and automatic reconciliation. What I mean by that is if you go and make an imperative change, ROCD can automatically detect it and make sure that it brings back the system state back to the desired state so that there is no drift. You have options to turn that off, but it is typically not the best practice. One of the good things about Argo CD is it lets you connect a central Argo CD instance to multiple different clusters. So if you have applications that needs to get deployed to different clusters, you can always do that from a central cluster. In this illustration, we have the EKS capability, which is an Argo CD capability enabled on a central cluster, and you can connect multiple different EKS clusters and push your deployments to it. The good thing is these EKS clusters can be in the same account, different account, same region, different region. It does not matter. You can connect to any EKS cluster within your portfolio. Typically when you're connecting to multiple different EKS clusters and different accounts, different VPCs, different regions, you need to worry about the networking aspect of it. How do you do VPC pairing? How do you set up transit gateways? The cool thing about EKS capabilities is you don't have to worry about any of it. That is fully managed for you. You don't have to worry about how to reach the clusters. We take care of that for you. You just need to give us the cluster arm and we establish the connection. Here is a specific example of how do you go about creating the actual Argo CD capability. Here we have the type of capability selected as Argo CD. We give it a name. We identify the cluster to which Argo CD should be enabled on. There is a roll-on which is nothing but the Argo CD capability role. And in this particular case, we also have an additional configuration. Uh, AWS EKS Argo CD capability is integrated with AWS Identity Center. This is how we enable single sign-on for your Argo UI and Argo CLI. So you need to ensure that your identity center is enabled and you pass on the identity center configuration when you're creating the capability. This illustration is showing how you can create the same Argo CD capability if you're more comfortable with console. You can do it through the console. It's pretty simple. Most of the information is pre-filled for you. You just select the role, select the identity center provider, and you click on next, and the capability will get created for you. In case you don't have a role already available, there is also a nice option available wherein you can create the role directly through the console itself. It is mainly for getting started experience. If you want, you can always scope it down when you're going through the role creation process. But if you are OK with the getting started experience, you don't have to change anything. Everything is pre-filled for you. You just need to agree to it and it creates the role and automatically selects it for you. Once you create the role, this is where you will land. It gives you basic details about, OK, what is this capability, when was it created? What is the ear corresponding to it, and if there are any health issues or not. At the top right corner, you see the capability issues. If, let's say for example, for whatever reason you ran into an error or degraded, you can click on it and it'll give you details in terms of what was the error and how to recover from that. Argo CD is fully managed, but you still have access to your favorite Argos UI. You also have access to Argo CLI. You can configure that from Argo UI perspective, you don't have to do anything. We provide you the hyperlink from the EKS console. You just click on it, you enter your credentials, and you're logged into the Argo UI. So there are no restrictions compared to what you would do typically with a self-managed installation. Now that we have created the capability, let's dive deeper into some of the Argo CD core resources. One of the main resources from an Argo CD perspective is the application resource. The application resource typically tells where my source code needs to come from and which cluster is it destined to. In this case, we have the source coming from my report and it is going to the demo cluster. But what exactly is demo cluster? How does Argo CD know what this random string called demo cluster really means? Let's put a pin on it. It will become evidently clear as we go through the next slides. There's also an attribute called as project. Again, I want to put a pin on it. In subsequent slides, we will cover how project plays a role in this entire setup. Anytime you want to use a cluster as a valid target for your Argo CD, you need to first register it with Argo CD. And how do you register your cluster with Argo CD? You create a Kubernator secret and pass the special label that you see on the screen here. You give the cluster a name, the demo cluster which we previously referenced. So this is where the mapping between the actual cluster to the AN happens. In a typical self-managed installation, instead of the AN, you will be providing the Kuberative API server URL with EKS managed capabilities. You don't have to worry about the URL. You can just specify the AN and it will work. Similar to the clusters, we also need to register all of our source reports. And how do you go about registering the source reports? The same as clusters. You create another secret in coordinators of type repository, and we would be able to connect to any of your private GitHub, GitLab, or Bitbucket clusters. The good thing about that is EKS capabilities comes integrated with AWS Secrets Manager. So if you want to store all of your get secrets in Secrets Manager, you can store that and just reference the Secrets Manager on. What this means is we do not pull any of the Secrets Manager credentials into the cluster and store it. It's just read at runtime authenticated, and it remains in Secrets Manager. Typically in a production environment, you do not want everyone deploying any code to any cluster. There needs to be certain security guard rails. App project is the way how Argo CD enforces these constraints. App project, you can think about it as what are my valid source reports, where the code can come from? What are my destiny, what are my valid destination clusters, where my source code can reach, and who can really deploy those. And that is defined by the Argo RBA that is highlighted here. So, Argo CD A Project acts as the container which gives you the security boundaries. Next, take a look at some of the EKS specific integrations that comes with the Argo CD capability. In this particular case, we are pulling a helm chart from ECR. So if you see the repo URL, it's not a GitHub URL. Instead, it is an OCI URL. What this really means is with respect to any of the EKS specific integrations like ECR, you do not really have to create a repository secret or do the credential managements. The capability IAM role can automatically authenticate into your ECR repository and pull your helm charts into your cluster. With self-managed installations, that is not going to happen. You have to install the repository secret and worry about the credentials management. The same thing goes with AWS Court Commit. AWS Court Commit. You again specify just the URL of code commit. The IM role can automatically authenticate into it. You don't have to worry about credential management or creating any of the repository secrets. There's one more integration that we have, it's called AWS Code Connections. Code Connections lets you connect to your GitHub, GitLab, or Bitbucket without managing personal access tokens or secrets. The way it works is through an OAth handshake, you need to first register it once with your Git provider and once the authentication is established, you can directly reference it from your Argo CD applications. Again, the authentication automatically handled for you. You don't have to create the credentials. You don't have to worry about managing the secrets. So what are the key takeaways? The key differentiators is there is direct integration with multiple AWS services like Secrets Manager, that is ECR, Code commit, Code Connections. So it simplifies how you connect to different source repositories to the Argo CD instance. This is unique to EKS capability for Argo CD. Now let's take a look at what is ACK. ACK or AWS controllers for Kubernator lets you manage AWS resources using Kubernator custom resources alongside your applications. Define an S3 bucket or an RDS database in a YAML file and apply it to your Kubernator cluster and ACK handles the rest for you of provisioning those resources. ACK is again built on GitOp's principles with continuous drift detection and reconciliation, so ACK can always make sure that whatever you have defined within your Kuberneus cluster is the source of truth, and when something deviates, it can bring it back to the desired state. How do we go about creating an ACK capability? Like we previously said, all the capabilities follow the same creation pattern. You tell what type of capability you want, which cluster you want it on, what is the name of the capability, and what is the IAM role that the capability should assume. This is just showing it through the console as well. I'll quickly skip it. Here is an example of an ACK resource in this case we are creating an S3 bucket. If you look at it, it just looks like any other Kuberator resource, but if you know the kind is bucket, bucket is not a native API that is supported by Kuberators. It is a custom resource, and this is enabled by the ACK controllers that are enabled by the ACK capability. So once you apply this particular Kuberneus YAML to your cluster, it is basically going to create an S3 bucket with the name that you have specified and the policies that you have defined for worsening and blocking the public access. IAM role selector. IM road sector is a really important concept for sophisticated ACK deployments. It lets you map multiple different IAM roads to different name spaces. It also enables cross-region resource management, team isolation, and least privileged access. The IM role you specify here is used by matching namespaces. In this case, the ACK target account role is used by names spaces which is production and anything starting with broad hyphen. If you do not want name space specific roles, it's fine. Just get rid of the namespace selector and it will apply cluster by. It's completely flexible, it's up to you how you want to manage it. Let's say you want to do cross-region deployments using ACK. It is as simple as just specifying the annotation for the region, and the resources will be automatically provisioned in the corresponding regions. Let's say you have one Kernator cluster. There are multiple teams working on it. They all have different resources, different requirements for spinning up different resources within AWS, and they all need different permissions. How do you go about doing it? You basically can define multiple different IAM role selectors, have multiple roles, and assign them to different name spaces. Kuberneator RBA controls who has access to those different name spaces, and IM permissions controls what resources they can spin. So this way you can also operate a multi-tenant ACK environment from a, a single cluster. Key takeaways from an Argo CD perspective, sorry. So we have seen, uh, two capabilities so far. One is ArgoCD and the other one is ACK. Argo CD handles the application deployment with direct AWS integrations. ACK handles infrastructure provisioning with sophisticated IM patterns. This covers the deployment and infrastructure, but developers still need to worry about the underlying resources. How do we abstract that for our developers? And that's where crow comes in. Crow, also known as Cube Resource Orchestrator, lets platform teams create custom coordinator APIs by composing existing resources. Think about it this way. If you have a web application, a web application could be composed of a deployment, a service, an Ingress. It can have an S3 bucket and an RDS database. Today, developers need to know about all of that. Would it not be convenient if you can combine all of them into one entity and call it as web application so platform teams can create a new custom resource using Crow on the fly called as web application, and they just allow certain things to be configurable by the developers. Developers do not need to know the underlying complexity. They can just create instances out of the web application by passing some basic parameters using simple AAL, and they don't have to worry about any of the underlying complexity. Creating a crow capability is similar to what we saw with the ACK capability. I'm not going to dive into the details. The only changes the type of the capability is going to be Crow in this case instead of Argo CD or ACK. The same thing can be done through console. I'm going to quickly skip that. And now we come to resource graph definition, which is the core to how Cube Resource Orchestrator works. The resource graph definition is where platform teams define custom APIs. You encode your best practices here. The schema section defines what field developers can configure. In this case, they can specify the app name, the number of replicas of the web app, and whether they need an RDS database or not. The resources section defines what gets created. The RGD creates a. Web application resource type in this particular case. This is what cust developers really need to create. The platform engineers have created the RGD. When developers need to create a web application, it's as simple as this. They come, they create a custom resource of the web application. They specify the name. They tell how many replicas they need and whether they need the database or not. And you can make this as simple and as sophisticated as you want. Crow also empowers you with the power of prescriptive patterns. Developers only need to know to specify, let's say, the name and region when creating an S3 bucket. In this particular case, an S3 bucket is embedded within a Crow RGD definition with all the best practices built in. The platform team has already encoded saying that whenever somebody in my organization creates an S3 bucket, it needs to come with versioning enabled. The life cycle policies for cost optimization, public access is blocked, and encryption is enabled. So if you really think about it, these settings are immutable from a developer perspective. They create the S3 bucket, but the best practices are already encoded for you when the platform administration team has distributed the different RGDs. Let's say you need to make an update as part of platform engineering to your existing RGD and you want to change the life cycle policy of retention from 90 days to 30 days and add intelligent tiering. You make the change to RGD, you deploy it to your clusters. All the underlying resources automatically get the updates. It's completely opaque. You don't, completely opaque from a developer's standpoint as they do not even know that the change has happened. Versioning resources by definition means that platform teams can evolve their abstractions over time. And this is very critical for large organizations with many teams with backward compatibility. Standards are enforced through code and not documentation. One of the cool things about Crow is that it comes with something called a cell expressions. Cell expressions is how you embed logic into your resource graph definition. As you see here, In this resource graph definition for your production environment, it comes with larger DB instance sizes with multi AZ enabled and also the backup retention is for 30 days as against 7 days for non-production environments. So you can put this kind of logic within your RGDs depending on the type of instances they're created. It will make sure that these policies are applied for you. Before imperative. With independent custom resources, customers would need to work out of the order of these resources, inject configuration across them, and create custom patterns for teams to consume. Now let's see how that changes with Crow. With crow, you can easily create composable abstractions and group different resources together. So if there are certain resources that need to get created before other resources, you can control the order. In this case, let's say for example, the VPC needs to be created before the cluster gets created, you can control that. So that crow waits for the VPC to be created, grabs the VPC ID, passes it on to the EKS cluster creation before the EKS cluster gets created. All these three capabilities work together to create a complete platform engineering solution. Crow provides the abstraction layer. ACK handles the AW's resource provisioning. Argo CD handles the GitOps deployment. IM road selector ensures proper permissions. This is modern platform engineering in AWS. What are the key takeaways? These features demonstrate why EKS capabilities is more than just managed installations. They provide deep AWS integrations and sophisticated IM patterns that aren't available with self-managed solutions. This is infrastructure management designed for AWS. With that, I will pass it on to Jessie to cover the next slides. Thanks for your Honor. Hey, thanks everybody for uh kind of following along with all of the how to get started and watching all of the details fly by. Uh, I wanted to call out that we have documentation, we have blogs, we have these things you don't have to actually remember at all. There's a lot to cover as well. We're, we're just sort of touching the surface here. So, um, I think we'll have more content as we, as we move forward and we'll have work. Workshops and things like that so I just wanted to call that out. Uh, we're giving a very high level treatment as part of that before you leave the room today, I wanted to give you some considerations when you're thinking about capabilities, you know, primarily like which ones you'd wanna choose for which types of workloads or what type of uh requirements you have, uh, what your governance and operational model is or you want to be. Um, what multi-cluster system designs might look like and how that impacts multi-tenancy considerations, um, and also like permissions, lease privilege principle, and things like that, or you wanna actually plan ahead before you start clicking buttons and onboarding teams, how do you wanna manage those permissions and how do you wanna design that into the future so you can scale and obviously there's considerations on using self-managed open source components versus using capabilities. Um, so these are all things that you should be thinking about as you start kicking the tires and looking at this set of features. So EKS services are designed to interoperate, but they're not required to do so. This is the first thing to think about. If you only want to use Argo CD, you can. If you're only interested in Crow, that's fine. These are designed to work together, but they're all independent. We also support any compute type on EK. Whether it's hybrid nodes, auto mode, or self-managed nodes, anything in between, so there's some really powerful stuff where you can hit a button in the console, click a few more, and have an auto mode cluster with all three capabilities, but you don't need to use auto mode with it. So these are all kind of our primitive features are meant for you to design and build the system that you need. So operational model is probably the biggest thing you want to look at as you start automating a lot more of your infrastructure and your workloads. So centralized management is sort of the classic platform engineering model where we see their centralized management cluster or fleets of management clusters. Uh, we're working to orchestrate workloads and multiple, uh, cloud resources across the workload clusters, so you're onboard teams probably into name spaces or other type of tenancy model. And you're gonna be provisioning AWS resources and clusters through that management, uh, the management cluster or management clusters. This can really simplify operation for, uh, for, for teams, uh, but it's a bit more of a burden on a platform engineering dedicated investment. Whereas decentralized might help you actually achieve a little more velocity up front, but there are trade-offs to consider around each cluster, each team having some amount of autonomy. Can you keep compliance in check? Can you do your audits, right? That's where actually GetOps comes in really handy, um, as far as the extensibility of that system being literally anything. It's all in Git. It's all verifiable and it's all immutable artifacts, so you kind of get the best of both worlds here where you can start fast but you can keep it on the rails, um, so I think the thing to think primarily about is how many teams do you have, will you have, and how much autonomy might they need in the future. Multi-cluster system designs is sort of the bread and butter of modern platform engineering with Kubernetes having that management control plane. Uh, what does that look like? There's hub and spoke and there's local cluster. So these are really the two options. Um, primarily we see, uh. Most very scaled customers with very mature large platform engineering teams prefer hub and spoke because it centralizes operations and costs around managing these components, uh, whereas with local cluster we see that a lot more with scaling, uh, platform engineering efforts. Or when a large organization has multiple efforts that are all working independently from each other, you know, potentially different business units, um, there's trade-offs here to consider, particularly around scale and how much you expect a team to be able to take on operational burden. Until it starts becoming um a situation where your small decentralized teams start building their own dedicated platform engineering efforts that probably isn't ideal so those are the trade-offs you look at for scale now but also into the future. It's really, really important to respect both IM and RBA in the cluster, how they work together, especially bringing Argo CD to the party that has additional RBAC controls, and you wanna, I think, think about this in a, in a con, um, a continuum of, uh, defense in depth but also just in specification, right. Think about IM as in the capability role and in other roles that you would use potentially with IM role selector, um, as granting service permissions to the capability. This is what the capability can do or can be configured to do versus the controls in the cluster about who can do what where. Pardon my voice. So can this, uh, RBAC is gonna control the Kubernete's resources and where they can be created. These are intimately connected, particularly if an Argo CD application defines a crow specification which creates ACK resources. Now you're talking about anyone with access to be able to create that resource in your cluster having access to create those AWS resources in your accounts. This is transparent and it's very obvious probably to us sitting in the room, but it's something to really get fluent with when you start designing out systems at scale. It's very powerful. It's just like terraform or answerable and salt and. Um, you know, things before it, it's when you think about infrastructure as code, you're really talking about, um, taking the keys to the kingdom and putting them, uh, at somebody's keyboard. So you have to think about that with Kubernetes Arbeck. You have that whole next level of defense, saying the system can do these things in AWS, but only these personas and principles in the cluster can do them. Uh, the key takeaways on all of these observations, and there's many more, we're actually gonna be building some of these considerations into extensive narratives and guidance for our best practices guide, um. The, the, the things to take away right now as you start looking at this and thinking about how you might be able to use it either as a scaling customer or somebody with a mature platform, uh, effort already is how you want to simplify operations. Is it simpler for you to have a push button get ops for teams to get started? Do prescriptive resources actually make sense to put teams on the rails for some self, uh, self-service, right? Or do you actually need to potentially for regulated workload requirements or just organizational standards, do you need to consolidate into a platform team? If you already have a mature platform team, uh, are you planning for scale? Do you know where you're headed and can capabilities help offload some of the foundational pieces? Likely if you're a mature platform team, you already have an enormous amount of um differentiated value built into your platform that we would never presume to replace but these foundational components can maybe free you up for building more of that. Um, ultimately you wanna build and ship faster and safer. So with EKS capabilities, this is, uh, our next step and our next set of evolution of features to help you do that. Um, we're continuing to execute, which is one of the reasons I love being in the service still after, uh, 6 years. We're continuing to execute on this vision to help you focus on building and shipping software, uh, and not on managing the small pieces of a cluster that all of your, uh, colleagues at the conference also have to do. Anything that we find as a commonality, we're gonna try and do on your behalf so you can offload that undifferentiated heavy lifting to us and you can focus on your differentiated value. So before we leave, I wanted to call out some uh additional sessions at the conference and some resources. Um, so there's some sessions here that are worth taking a look at, uh, builder session and two workshops. These are gonna be foundational, uh, GitOps and platform engineering content for you. Uh, I believe these workshops, some of them are self-driven as well. Uh, we have a ton of workshop, uh, content and learning materials, uh, through our websites. And also session resources we always have our documentation EKS workshop, EKS Blueprints. These are all of the things that come along with the service we support all of these directly. So uh if you ever have ideas for new content or you find yourself looking, you want to learn something that we're not quite covering, let us know and we're happy to help you with that. Uh, thank you so much. Thanks for taking your time. I hope you have the rest of the conference is great for you. If you see us in the hall, please, uh, flag us down and we're happy to talk. Thanks a lot.