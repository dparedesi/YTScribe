---
video_id: OxbY7DgWIqo
video_url: https://www.youtube.com/watch?v=OxbY7DgWIqo
is_generated: False
is_translatable: True
---

Hi everyone. Uh, first of all, let's, uh, just wanna say thank you for joining and taking the time to attend our session. And the one today we're here to talk about, talk about cloud mesh and potential different solutions. I'm Frank, and this is Jacob. Hi Frank. Hi everybody. My name is Jacob, and I first wanted to ask who has heard of AA Proxy before? It's a lot of people and who uses HA proxy right now. Awesome. So some of you see my colleague in the back. We have a t-shirt for you. Thank you for using HG proxy. If he runs out, then, uh, please see us at the booth and we'll get you one. All right, so for today we wanted to talk about multi-cloud chaos, the unified cloud mesh architecture. Uh, we wanna tell you some tales of the field. Uh, we work on the enterprise side and we talk, we talked to a lot of different enterprises and they're looking at service mesh solutions. So I'll start with a statement. The service mesh was designed to solve connectivity. However, for most organizations, it simply shifted complexity elsewhere. Let's dive into something, some of the issues that we've observed. If you want, if you look at today's modern infrastructure, it's a it's fractured. It wasn't built, it evolved. There's multiple clouds, multiple regions, on-prem data centers, legacy applications that can't change. And most most service meshes weren't built with this in mind. They struggled to integrate the brownfield enterprise. Then you have the mesh stacks, uh, the two most prominent architectures and service meshes, you have the sidecar architecture and the sidecar list architecture. When you look at the sidecar architecture, it's basically one sidecar, uh, one sidecar per pod or, or per instance uh of your service, and this simply becomes too expensive and complex as you scale. Then you have the sidecarless architecture, which is better. However, complexity, complexity is shifted elsewhere to different components. You have the layer 4 proxy per usually per node, and then you have layer 7 proxy, which is a layer 7 proxy, so it's split into distributed components, usually per name space or service account if you're using Kubernetes and these types of things. So it's less overall, but there's still operational complexity in dealing with that. And crucially, both architectures still fail at preventing resource sprawl as you scale, managing north-south traffic, they primarily focus on east-west only, and federation or multi-crowl region setups have difficulty or or emerging right now. They require additional components or weren't were not considered with the initial architecture. So the problem isn't the service, it's the boundary. We've been hyperfocused on services, but the real challenge is the network boundary. What if we stop trying to manage sprawling sidecar and proxy deployments and instead deploy strategic gateways at network boundaries. This, on the HA proxy side is what we call the universal mesh. So if you take a look at it, uh, this is a mock-up, but if it's basically, you can have an outer edge and an inner edge because HA proxy is capable is omnidirectional, so it could do north, south, or east-west. And you could take a, uh, on the outer edge, some of the most popular things are TLS termination, security, centralized logging, rate limiting. And on the inner edge, it's mostly east-west, east-west traffic. So Uh, think about, uh, think about rate limiting, think about traffic splitting, other complexities that you see in, in today's modern infrastructure. So if to sum it up, essentially, our universal mesh is simpler, flexible, and a more powerful architecture, backed by HA proxy with 20 years of proven reverse proxy excellence. Uh, uh, we focus on convergence, federation, and that gives you the universal mesh. So, on the convergence side, it's the ingress, the mesh, and proxies all become one. It's one HA proxy managing that. Federation, it's out of the box. We've built this, we built federation with this in mind, so it's effortless effortless multi-cluster or multi-cloud or multi-region, and that gives you unified security, connectivity, and observability, which is the primary purpose of the mesh. Frank, I would just add that I think that the multi-cloud kind of connectivity and multi-cluster connectivity problem is real. I don't know if you've noticed, but AWS announced, I think, today morning that they're, uh, they have a new solution called MultiConn or something like that where they're actually partnering with GCP to do connectivity between AWS, GCP, and then other network. Works so something that you were able to do before, but now there's a single API call to connect all of these clouds and they operate on a layer 3, so different than us, but this kind of shows you how uh how much demand there is for basically multi-cloud, multi-connect multi cluster connectivity because everybody's trying to solve this problem. So we think the universe solves impossible problems. Um, it connects everything. It's a single pattern that simplifies operational complexity and resource utilization. It's multi-cluster and, and federation ready, so it understands resources, it understands infrastructure, spans cluster without forcing architectural compromises. It's the same pattern everywhere, and there's flexible deployment, uh. You can run gateways external to Kubernetes, inside Kubernetes, inside EC2, uh, EC2 instances, EKS ECS, it's really flexible. So if you want to take a look at this is kind of a a a typical mockup of an architecture diagram, uh, you'd have the control plane which is fusion. All, all service measures kind of have, they're broken up with a control plane and the data plane, and then you'd have the data plane layer which is HA proxy, which empowers all of the features that you can see on the side, and that this is proxying to any number of services if we're talking about it. AWS you have, you can use ECS Elastic container service, EC2 auto scaling groups as well, Kubernetes, basically anything we can route to we can we can route to anything and then we can also do the same for any virtual machine or any on-premise or any other region. It's basically all controlling, all controlled through the control plane in a single, a single management plane. So that gives you kind of the building blocks of a modern mesh. On the inside, uh, the, the, on the inside you want to talk. The most important features are you're looking for performance, you're looking for zero trust, you're looking for multi-layered security, authentication and authorization, and, and federation again as well. So let's dive in, let, let's dive into our sweet spot. Uh, we are the world's fastest application delivery and security platform, and we do have the proof to back it up. In 2021, we, we released an, we released our article showing that you're can achieve 2 million requests per second on a single AWS instance. It was a Graviton 2 at the, it was a Graviton 2 at the time, and basically, This is, this has been never seen, so it's really sheer performance and it's one of the core tenets of HA proxy in everything that we deliver, it's performance, reliability and scale scalability, all, all of that is in mind. Uh, what happened though, is after that, after that release, uh, in 2023, we realized there was a dra dramatic performance loss in the new LTS version of the current TLS TLS library we're using, and efforts with the authors failed to yield acceptable results. So what did we do? In 200 in 2024, we reworked our entire TLS stack to support modern TLS TLS libraries, and in 2025, released a performance build using AWSLC. That was the one we, uh, that was the library we sle we selected as the default uh TLS library, and we also have modern alternative support, so compatibility supports for anything that's delivered by DOS as well, if you wanna use that. And it's always our, our commitment is to do whatever it takes to deliver the best performance. And then, uh, we didn't really release a benchmark, but we just released HA proxy, uh, HA Proxy 3.3, and we're sharing some of our internal benchmarks. So, what we've done, what we've done is we did the exact same test that we did in 2021. And obviously the natural, the natural sentiment would think, oh, since 2021, the hardware, the hardware has become so much more powerful, you should, you should see an increase. But the reality in today's world, since 20121, 2021 to 2025, we've actually 10 separate versions. And what you're seeing in the in modern software is the phenomenon of software bloat. Every release has more features, more, more, more, it takes more resources, so it basically takes away from the advantages that you're gaining from the hardware. So what we saw when we when we benched it on the exact same test, right now it's pretty much the exact same instance, except it's a Graviton 4, the most recent Graviton that's been released. We were able to achieve 2.54 million requests per second. Um, so, and to make sure that we're testing on the same platform, we re-ran the exact same version on the exact instance, so it's not anything related to hardware. So we took the same instance, we ran 3.3, then we ran 2.3, which is the one we ran at the time, and we got, we yielded better results. But however, the reality is that Even in 2021, we reached hardware limitations. So when you're doing such benchmarks, it's, you're really limited by the communication in, in the internal CPU cache, right? So if you're doing millions of operations, the CPU cache and the communication between it, there's, there's some resource problems, they're just not there yet on, on the. Chips and then you're also limited by the, the, the network packets per second. So on even on the most modern CPUs, there's only 16 network queues available, and that does seem like a lot. But when you're talking 4 to 5 million packets per second, you're there's a lot of chances that stuff gets into user line, so it's it's consuming resources. Uh, so what we decided, uh, because we're reaching the, the hardware, we, we decided that instead of trying to go for a bigger number, because in reality, nobody really needs this much traffic, or if you do, um, it's great to have something that does it on a single instant, uh, uh, on a single instance, but we, we, we fixed other pieces of our architecture, such as we optimize more load balancing algorithms, or at least gonna, uh, our, our, our, our least con algorithm, our default algorithms, we switched, uh, we switched it from round robin to random. And we, we focused on a lot of tuning in our queues and, and in our counters to enable to help with other bottlenecks, cause once you're at, once you're at the reverse proxy level, you're, you're load balancing traffic in front of application servers, and most people provision their application servers not to full utilization. So if you can use the load balancer to get more efficiency on your entire form of application servers, then you're really gaining overall. So. Advan our queuing system, it really allows you to utilize a 100% performance of your application service. So we're not only empowering performance on the load balancer level, we're empowering performance on your entire application level as well. And then if I'm a, if I put a If I'm on the other side of the fence, and I'm actually running something that needs 2 million requests per second or more, I wanna know how much that costs, right? That's really the most important. If you go to your CTO, he's gonna ask you how much will it cost me to, to actually run this entire, uh, or this entire workload. And if you look at the quick back of the napkin nap, um, nap back of the napkin math, you can realize that a billion requests. A a billing request actually costs 41 cents on this instance type, and we've benched, uh, we've benched it that we can get it down to something on the 20 cents range, and, uh, if you go for, if you don't need that these amount of cores, 64 cores and that stuff. So in all this to say, all this to say that when it comes to performance, there's nothing that can match HA proxy. I would say, um, Frank notice the the CAGN instances uh my team uh actually benchmarks all instances on AWS with HA proxy and uh we are a very network heavy application and so if you go with like CAG instances you will very quickly run out of network limits. So notice that a lot of the benchmarks use CAGN. Instances with the network optimized cards because otherwise you're gonna run out of PPS limits and uh bandwidth limit exceeded and everything else on the instances so in these cases it's very important to use the GN instances and as you can see we're very big fans of the graviton ones as well so they're the sweet spot for H proxy. Then it then it comes to federation. So to HA to HA proxy, NIP is an IP as long as it's routable. So that that empowers us to use the control plane to configure each of our, each of our, each of our proxies independently. So if you take a look at this picture, the control plane can send a different configuration to US East and to US West to each one of those instances. However, since the control plane knows about US East and We West, it can also send that information to all the proxies. So if there's ever a situation and we have a lot of use cases like on the payment provider side where people are looking for the redundancy or backup, so when a transaction costs money and if there is a failure, there are no servers available in a in a single instance, they do want to cross over, uh, even though it may it may cost uh may cost the monetary value. All of these things, having the control plane outside talking to the infrastructure. Being aware of the infrastructure really enables flexibility and allows us to be truly any platform, any service, any infrastructure, and a lot of the use cases that a lot of the use cases that we see take full advantage of take full advantage of this. If you take a look at, we have a lot of, uh, we have an ad tech, uh, red tech recommendation for HA for HA proxy in partnership with AWS. We've worked with, uh, a lot of the. 5G or mobile providers to really expand their coverage and wavelength zones for AWS where we've install, we install our own, our own software and then we discover resources for the 5G Kubernetes cluster and basically we offer native load balancing for a 5G network in a remote region where there's only wavelength zones and then as I mentioned on the on payment provider side, a lot of payment providers, uh, payment providers nowadays, popular ones that are. You can take a look at our website. Um, basically they have redundancy across 3 or 4 different regions or zones, and they, and they wanna switch over. Uh, they don't, they want, they want region affinity, so they wanna stay on the one, the first one, first one as much as possible, but in, in the event of a service failure, they allow them to cross over and they want, they want us to prioritize, prioritize which ones to, to cross over to, and that's all configurable out of the control plane and out of the configuration. Then a key point of a service mesh is zero trust. So in terms of, in terms of certificates, this is all handled between HA Proxy and the control plane itself, either either renewing the certificates, accepting, verifying the certificates, even on health checks to the back-end servers, client certificates, er. Cates, all that stuff is handled out of the box with HA proxy, and then we slide back to the slide before you look at the performance. Uh, it's, we optimize performance at level and we monitor it as I mentioned before. We even moved to, we, we even moved to AWSLC which is kind of giving us more options on. The new types of crypto uh qua quantum, um, encryption and stuff like that. I haven't seen many use cases yet, but we're trying to move to a modern TLS library that can enable these work, uh, workloads in, uh, in the future, and we're always on the precipice of new techno uh of new technology. We are looking, we, we just released our experimental version of kernel TLS and, uh, encrypted client hello, so really being on the forefront of all these platforms and our, all these are RFCs. Then another piece, another piece that's really interesting, uh, when it comes to a service mesh is really authentication and authorization. Um, basically, this diagram kind of shows you how authentication and SSO modules work. We do have modules on, on the, on the enterprise side. Uh, we, we can handle anything from Open ID, SAML. We even have, uh, ADF, uh, proxy protocol as well. Uh, there's a specific Microsoft standard for ADFS. Uh, we have an, we have a module for that, but in, in theory, basically. Client presents itself to HA proxy. HA proxy can re redirect or or talk to the IDP in the background and then pass on, pass on that information to the back end server. So it's kind of seamless. Then you just have, maybe have APIs or you're using JWTs. We have native support for JSON and JWTs. So basically, these functions can help you find the algorithm, uh, the issuer, the expiry, the scope. All that information can all be natively done in, inside, inside of HA proxy, and has been for a while. And then you can, based on, based on what you find, you can, uh, you can actually deny. Approve and you can build your own access control list on that side as a side note, uh, HA Proxy 3.3 that was released yesterday, not yesterday, last week, last week, last week added support for JWE tokens. So those are the encrypted JWT tokens so you can actually now not just verify JWT tokens, but you can decrypt them first and then verify them. So that was added to 3.3. And then common uh uh sometimes you wanna have a language to, uh, based on the results that you're gonna get and we see a lot of it in the field we see a lot of OPA, so, so open policy agents, so we do have support for that as well. So if you need more complex, um, uh, more complex policies, you can define a policy in, in, in Rego and basically we parse the input and we, we, we decide to allow it or not based on that. So it's really super flexible in terms of authentication and authorization. And then I'm gonna hand it over to Jacob to go over our multi-layered security capabilities. Thank you. So I'm gonna talk about multi-layered security and really first introduce what it means to me to talk about multi-layered security and then I'll talk about how we're achieving that in HJ proxy in context of performance, right, because you've noticed that we talk about performance a lot because, uh, in many cases security means performance implications and we don't think that should be true, right? So. So, uh, to me, multi-layered security means a lot combined with authentication, authorization, everything that Frank talked about, but also detecting application layer DDoS attacks, web scraping, brute forcing, comment spam, vulnerability scanning, some specific targeted exploitation, and I'm gonna talk about the details here. Uh, the web scraping to me is one of the most important ones, uh, because obviously. Um, AI, since we're all using AI, right, there are a lot of websites out there announcing they're, they're struggling with AI bots scraping all of their sites, and there are many protections, and you can definitely use protections from HE proxy, but there are many others to stop AI bots from scraping your site while you're fighting some targeted attacks from AI scrapers that are just trying to get everything from your website. So, um, I usually define this as multiple layers of detection and decisioning. And the reality is that I'm gonna talk about these layers as 4 separate layers, but they all actually work at the same time, so the fact that I have chosen to place these layers in this order is just my choice, right? In the end they all, all of what I'm gonna talk about happens at the same time, so I should be able to choose which layer reacts first. Or just do all of the detection first and then make a decision later with some exceptions, right, because for example here we're applying access control. So if I'm applying the access control and for example um uh I'm denial listing a specific IP address, well if I deny listing an IP address, it doesn't make sense to try to detect if it's a bot or not. It doesn't matter to me. I'm gonna deny it anyway, right? So I can just deny right away. So access control like deny specific IP addresses, allow internal IP addresses, allow specific monitoring systems, or deny specific fingerprints or IP addresses coming from bots, right? This is where we're kind of starting that idea of the bots, the idea that um we use in HA proxy and this is. Thanks to the HA proxy community, support for large files is that you can actually put millions of IP addresses inside HA proxy and say deny based on this list and looking up that IP address in that list, whether you're looking up the first one or the number 1 million, will take a constant amount of time so you don't have to worry about um. How to scale for a large amount of IP address blocks. Uh, this is all thanks to the, uh, Willy Taros, who's the founder of HE Proxy created this unique data structure called Elastic Binary Tree. I recommend that write up if you ever Google, uh, Elastic Binary Tree. He has a big write up on how it works and how it scales to millions of items. Um, then I start rate calculation, so I'm not denying, but I'm calculating specific rates and even here I'm saying is it over a rate limit, but I, I don't care if it's over a rate limit yet. I'm just calculating a rate, right? So I'm figuring out, um, based on an IP. Address bandwidth label of a bot URL. What's the rate of the requests from this specific client? So rate limit is all about grouping clients or in identifying individual clients and saying what's the rate of their requests so I can make decisions later. Um, the problem is that in our experience static limits are hard, and we think there's a better way. I don't know if you've ever ran rate limiting in practice in your own AA proxy or any other reverse proxy, but every time, I think Frank in production you ran this right, every time you set up rate limits you get it wrong. Every single time and now you don't know what the rate limit should be, right? Should it be 100 requests a 2nd, 1000 requests a second? What if my traffic is normally 100 requests a second, but I spike to 1000 requests a second every day during a specific hour, then I have to set the rate limit up to that specific very high rate, right? But then I can be attacked during the low traffic periods, so. We've built something we call a global profiling engine that allows me to aggregate statistics and aggregate rates across multiple instances of HA proxy and then calculate statistical aggregation so I can do things like, hey, I'm going to rate limit bots over 1.5, the 95th percentile of the rate, same hour yesterday. So I take a look at what's the rate of the requests during the same hour yesterday. And then I take a 95th percentile of that and that's a specific number and I don't wanna probably uh block or deny on that specific number because that's a 95th percentile, right? But let's decide on a ratio 1.52 depending on who you who it is and I start blocking on that or rate limited on that and that gives me dynamic rate limit so I no longer have to set static rate limits. I do it dynamically. And I basically the rate limit actually changes every hour or every second or every 5 seconds depending on how, how I set it up. Next is bot detection. There are a lot of bots out there. There are bot scraping companies at the expo right next to us selling bot solutions, right? And so the reality is that a lot of people want to detect if the request came from a bot or not. And I'm not saying that if the request came from a bot, that's a bad thing. It might not be a bad thing for your specific situation, right? But maybe you need to know. Um, and he needs to be able to identify those and identify crawlers, and that's the AI thing right now, right? Can you identify crawlers that are based on AI? There are good companies out there that will say, hey, I'm open AI, right? And. There are bad companies out there that will hide their AI scraper and try to hide that from you so they present themselves as a standard browser while it's one of the top 4 top 5 AI companies out there. I'm not gonna name there. Um And then, so think about this. I, I did access control. I determined rates. I determine if it's a bot or not. Uh, now I need to do some security in terms of, um, attacks, right? So, um, I should run in the web web application firewall, and the problem with web application firewalls usually is latency and throughput. So if you just enable standard web application firewall like one of the open source versions, you will find out that your latency goes up. Up and your throughput on your reverse proxies goes down because it just takes a lot to actually detect the traffic so we actually built a WAF with a latency that is not detectable so we had uh we had our own conference and we had a customer of ours came in and said we enabled the WAF and then we got confused because it felt like nothing happened. Because they couldn't detect a statistical difference in the first time to bite when they enabled the bath. And then I did a lot of detection and the reality is, as I mentioned, uh, you should decide early in specific situations if an IP address is on a denial list, we'll deny it right away. We don't need to go all through the hassle of bot detection WAF filter and everything else deny it right away, right? So you can do some early decision in but very often we see the customers do decision in later once they do all of the detection. And they decide on a response so I can deny the request. I can allow it. I can tarp it. It's a very popular thing where, uh, for example, in our network, I don't know if you wanna mention that we tarpit customers or bots or bad clients instead of denying them because if we deny them they will detect we've denied them and uh they will adjust. Right, so we tarp them instead. Yeah, blocking is not something we often see or people usually have options to, to do so there takes a lot of decisions on the product side or as an organization has to go through change requests and stuff like that. So it's not something we see commonly. So we do offer you the ability to make any decision, right, or label it, observe it, and make the decision later. So this is, it's completely configurable and that's some of the power that we're offering as well. Yeah, most of our customers start it or send traffic to limited back end. So like if you have a backend with your real servers that can handle 10,000 requests a second, they'll send the bad behavior to a back end that can handle 100 requests a second. So the users will not get blocked. The bad users will not get blocked, but at some point they're gonna run out of resources and think, oh, I succeeded. I brought your website down and they might stop, right? But in the end your normal users are still going through the fast back end because you just limited the bad behavior, uh, so you don't show the users, the bad users, what happened and what you're doing because again once you start blocking them, our experience is that they start adjusting and so you will be playing a cat and mouse. Um, the other thing that very often we see is show a captcha or some kind of a challenge. This is one of the popular solutions recently. I don't know if you've heard of like JavaScript challenges, Anubis, and a few others, which is really, I'm gonna allow the request, but I'm gonna ask you to calculate, uh, two prime numbers first, and I want you to take 500 milliseconds doing that. Right, to slow them down to with the idea of I'm not gonna block them but I wanna make it a little bit more expensive for them to succeed to kind of reduce the amount of traffic they can run. Um, and then again we said we're taking all of the layers and we're making business decisions now and this is the important part. This is you should not let anybody to say, hey, this is what you should do. It's a business decision to allow a request or to deny a request or block a bot or give them a JavaScript challenge. And then finally we route them right and so we're a reverse proxy we're a load balancer, so we route the requests but we augment them with data so GIP data we rewrite the headers, device information, and a few things like that and some, some of the standard routing you're used to from reverse proxies like based on. I cookies path or anything else in the request we uniquely have access to the entire HTP request right? so we can decide how to route we can decide to route on an AI model if the request is for a specific AI model we can route it to a different back end based on the data in the request itself. So again, the, the idea is that we're gonna protect against application layer attacks, web scrapers, brute forces, spam on forms and everything else, targeted exploitation, scanning and everything else, um. I talked about this actually last year at a different conference, but there are obviously edge cases to this right at the edge as I call it. So for example LLMs, everybody's doing wipe coding now everybody's doing AI, um, and the idea of securing AI is pretty unique and new so we're seeing customers doing new kind of protections like rate limiting based on tokens instead of requests because requests. Are cheap tokens are expensive. PII detection. This is very prevalent in like big enterprises where they don't wanna send PII to the AI. They don't wanna output PII, right? But most likely, uh, most often they don't wanna send the PII to the AI, so they wanna detect that as quickly as possible, um, and figure out if there's any PII and block the request and then API key protection which is standard API gateway. So, um, I'm gonna show you an example implementation of this, um, and then actually when I do my demo I'll show you everything running. Uh, but this is an implementation that I did myself. You don't have to implement the same thing. You can if you want, but I did it myself, so I've decided that we're gonna do a couple of things. We're gonna deny rate limit, challenge, sandbox, and then allow. And so, I'm denial listing bots and IP addresses that are on a specific list, um, um, uh, denial listing anybody who's triggering a rave over 3 triggers. That's a human. This is another thing that we've noticed, um, even our largest customers very often they say, you know what, we're running a RAF, we don't wanna block traffic even with the RAF because that's a big decision, so they allow one or two violations to go through before they start denial listing the traffic. So if I'm a human, um, I can trigger the RAF once, twice, but 3 times, then I'm gonna, I'm gonna be denial listed. So we see that all the time. Um, rate limit bots over a specific anomalous threshold, right? Like, um, very often what I do is, um, allow humans to do 3x, the 95th percentile, allow suspicious clients to do 2x, and allow bots 1.5, right? So humans have the most leeway in terms of how much, how much rate they can actually run. Versus bots have the least amount of uh leeway and in in today's in today's day and age you kind of have to have both sides of the coin. You have to have reputational signals which we're providing here as well, but you also have to behavioral because with AI and agents nowadays it's almost it's almost undetectable headless browsers and these types of things. So you need a lot of combinations and this allows you to kind of perform that with all of our tools, yeah, absolutely, and so, um. We talked about the what and let's talk a little bit more about the how. So this is how we do the configuration and this is how we show our customers how to do the configuration and we ultimately centralize everything into the control plane. And with the control plane we centralize all of the policies so you can then deploy them across all of your public clouds, VMs on premises, Kubernetes, and every anywhere else and the way we do it is through um a unified control plane where you basically. Deploy configuration once and it's deployed to all of your HA proxy instances and you apply security profiles so think about like all of the security you need to configure on all of the configuration when I first made the security recipe I've shown you before it was like. 200 lines of configuration. It was not code, but it was configuration. We've abstracted away that specific configuration to a security profile so I can basically on the left at all the signals. Those are the detections and on the right I do all of the decisions, right? So on the left I say, hey, is this web scraping on the right I decide what's the decision I'm gonna take to actually uh react to the web scraping. And when I do that I can then deploy the security profile to all of my instances. So again, our customers don't run one or two instances of load balancers. They run hundreds across all of the clouds, all of their on-premise infrastructure, all of their regions, all of their availability zones as well, because sometimes they try to avoid cross AZ traffic for many purposes. Um, so basically I take all of the steps on the left again and I decide what are the decisions on the right to take. And uh very quickly I'm actually gonna get to this and then I can start inspecting what's gonna happen to the traffic so in this case you can see we, I think this was a screenshot I took for the last 5 minutes. So in the last 5 minutes we saw 65,000 requests we allowed about 46%, so 30,000 of them, and we took an action on about half of that as. Well, I notice we're calling it taking an action because we can't say we denied something we don't know in advance what's the action we're gonna take, right? And in this case, the action, I'm sorry, uh, in this case the action was we're gonna target some brute force attacks we're gonna deny high risk countries and everything else, so that's the actual action that we're taking. So Frank, do you want to talk a little bit about some of the service patterns we've seen? And again, uh, all these patterns again, they're all possible with HA proxy. It's just a question of config uh configuration. So you see the common ones that, that we have are on the API gateway or circuit breaker within an API gateway, and then basically you'll see the users will hitting, they'll be hitting HA proxy, and then you'll have SSL termination rate limiting or throttling as you've seen before and different microsurfaces or cir uh circuit breakers of of services down or if they're consuming too much time, uh, consuming too much time. And then you have other traffic splitting initiatives which are kind of like simple weight weight based or uh other value based so like a typical canary deployment. So if you're sending 10 10% of traffic to uh to version B and but your stable version is version A, uh, if you're in Kubernetes you can enroll deployments or, uh, uh, and these types of things. Blue-green is a similar if you have a blue cluster or a green cluster, HA proxy makes the decision based on the value, however you pass it to it, and then it switches over the traffic. And it's really flexible. It's really flexible for any pattern. Again, the whole point of this, the control plane makes it a lot simpler. All this configuration is all possible in the HA proxy. Control plane gives you API and client libraries to be able to simply, uh, simply do this and make it more seamless and apply it to different clusters. Oh And now let's, there's been a lot of talk. Thank you for your patience. Let's see some of this in action. Cool, thank you. So, um, we introduced a couple of concepts, right? We introduced the concept of universal mesh. We introduced the concept of security and so you, uh, traffic through the universal mesh flows through either externally, that's the north-south traffic or internally that's the east east. US traffic and I'm gonna talk about it later. So imagine that you're an Acme Corp and you're running a universal mesh on mesh. Acme.com and the point is that we're doing explicit routing of traffic. So if I'm a service or if I'm a user out there, either an external user or you're mapping to some public domain, you wanna talk to a payment gateway that's on premise, right? You're gonna call mesh Acme.com/payment gateway, and what's gonna happen is. Your first HA proxy will ask, hey, where's where's payment gateway? and we'll find out uh, payment gateway is on this AA proxy that's on premise, so we'll send the traffic there. And the HA proxy on premise receives this traffic and says, Where's the payment gateway? Oh, that's the spot in this on-premise company cluster, and we'll send it there. Compared to internal traffic, which actually looks exactly the same if I'm a service in a pod in USE West 1 in AWS and I wanna talk to the payment gateway on premise, I do the same thing. I call mesh Acme.com/payment gateway, and the AA proxy that's closest to me will find out where's payment gateway. It's on premise in this specific data center. We'll send it there and that HA proxy will send the traffic there again so I've, I've made a demo of exactly this. So if you wanna scan this, I hope the QR code still works. If you wanna scan this, it's the page that I'm gonna show now. Um, so you should arrive on this page meshhappy demo.com/payment gateway, right? So now I'm sending traffic externally from my browser to the universal mesh and back to this is actually a port in a Kubernetis cluster. So if I load this page I get an answer and it this is because again the external HE proxy decides what's happening, finds out where to go with the traffic, and then sends the traffic over. Um, what I'm gonna do is I'm gonna, uh, actually I'm gonna hack myself, so I'm gonna say path equals ETC password. There's a path argument, and if you give it a path of a file, it will give you the file. It's kind of fun, right? So, uh, you can do that, but obviously that's not gonna work because this is a demo, uh, so you get 43 forbidden. So let's take a look at what happens in the background, right? How's this traffic being routed. So, um, I'm gonna go to, uh, the control plane for HA proxy, and the first thing I'm gonna see is there's this, uh, request payment gateway slash, uh, or path equals ETC password, so that's the one, and you can see we got a 403 response, uh, but when I go to the detail, uh, we actually took an action and the action was we blocked this request because we deemed it suspicious as a client. And at the same time it triggered a web application firewall so that was my business decision, right? I talked about the business decisions and my business decision is if it's a suspicious client then. Do more than to humans and you can see the bot management said that this is a suspicious client because I have some things in my browser installed that made me look suspicious so um uh as a suspicious client we decided to log this and we we triggered an obvious probe with the path ETC password that's pretty obvious, right? So we don't have to talk about that much more. Um, the other thing that we're gonna look at is. Instead of looking at the specific request that we blocked, let's actually take a look at the request that worked, right? Some of you actually loaded these, so some of you will be these, uh, specific. Uh, these specific URLs, uh, obviously I can't say who's who, uh, in the, in the audience, right? So somebody requested slash payment gateway and we routed this request to US East. That's the AA proxy inside US East, and then we traversed to the other AA proxy, and I wanna know what happened to that. So what I'm gonna do is I'm gonna click on the search here next to a correlation ID. So now you see the two requests as they as they traverse the network and in fact to the client this is a single request, right, but we see it as 2 because the first one right from the client to the HA proxy, the second was from the AA proxy to the far end HA proxy that originates or that's the destination of this traffic. And that's this one and in here we're sending the traffic to this specific payment gateway service in Kubernetes. If I make this even bigger, you can see that, right? So I'm automatically detecting, uh, all services inside my Kubernes clusters and here I'm sending the traffic to the specific Kubernetes cluster. The last thing I'm gonna um demo on this is we're actually gonna do some terminal work and I'm gonna deploy a new demo so here if you take a look. Um, if it lets me through. If you take a look, I have this new manifest and I'm calling this manifest demo one, right? And this manifest says that I'm gonna deploy a new service and the path of the service should be slash demo one in the US East and um at the top of it I have some web server with just basic information so I actually applied it before but I'm gonna reapply it in my Kubernese cluster. So, uh, I've created the service. I've created the deployment. So if you take a look, I have some parts of the deployment demo one, so that's running. I can actually scale the deployment. I think it was called Demo one. So give it 3 replicas instead. But now if I go to. Mashhappy demo.com. I actually get the answer and the point is that the HTML is the same as the previous service, right, but suddenly without me doing anything just by deploying a service in Kubernetes and saying hey service path should be slash demo one, every client in the network and every AE proxy now knows how to route demo one. So if I now go to meshhappydemo.com/demo one from. Externally in this case and internally at the same time, I arrive on the parts of this specific service and if I scale the parts again, if I change the number of the replicas, oops. If I change the number of the replicas, then this will be automatically updated and I will be sending traffic to the 5 replicas now instead of the 3 replicas that I had before. So just to recap, we talked about um service discovery and basically creating services inside Cuberris and automatically configuring them in HA proxy and running multi-layered security on top of them where I'm observing in the control plane what's happening. Um, and we think that this is really a solution to the connectivity problems that a lot of our customers are having in terms of talking between services inside AWS, but also AWS to GCP and a few other clouds that they're seeing out there and they're requesting solutions for that. Frank, I don't know if you wanna close us up. Oh, in the end, just, uh, oops, yeah, there you go. We just wanna say thank you and you can come see us at uh booth 117 1179 and you can visit HA proxy to see some of our other solutions as well but in general, uh, we're happy to discuss and just another thing, all these things are available. HA Proxy is available in the marketplace in AMI, uh, so it's definitely any form factor everything works and, uh, if you have any questions, feel, feel free to come and see us and thank you for your time. Thank you.