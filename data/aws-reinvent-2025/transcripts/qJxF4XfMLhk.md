---
video_id: qJxF4XfMLhk
video_url: https://www.youtube.com/watch?v=qJxF4XfMLhk
is_generated: False
is_translatable: True
summary: "This code-focused session, \"Observability for Reliable Agentic AI with Strands SDK & OpenTelemetry (NTA406),\" addresses the challenges of deploying and monitoring AI agents in production. Solutions Architects Gabriel Paredes and Luis Torres from AWS Financial Services demonstrate how to use AWS Agent Core and its observability features to debug and improve a multi-agent financial advisor application. The session uses a practical \"break/fix\" approach, where they identify issues like high latency and outdated knowledge transparency. They show how to use the AWS CLI to tail CloudWatch logs for real-time debugging, use CloudWatch metrics to monitor token usage and cost, and leverage X-Ray traces to visualize the full trajectory of agent interactions and identify bottlenecks. The speakers also illustrate how to update agent models (moving from Anthropic Sonnet 3.5 to Nova/Haiku and Sonnet 4.5) and configuration parameters (like temperature) to optimize performance. Key technologies demonstrated include the Strands SDK for building agents, OpenTelemetry for standardizing traces, and AWS Bedrock Prompt Management for versioning prompts."
keywords: Agentic AI, Observability, AWS Agent Core, Strands SDK, OpenTelemetry, CloudWatch, X-Ray, Multi-Agent Systems, AWS Bedrock, AI Performance Optimization
---

My name is Gabriel Parede. I'm based in Bogota, Colombia. I'm a solution architect for the financial services industry, and I've been in the company for the last 7 years in which I had the privilege to support many customers within Latin America, uh, in the use of a GNAI for their business and in this case in artificial intelligence, and with me is. Hi there. Happy to be here. My name is Luis Torres. I'm a solutions architect manager for financial sector industry in Colombia, and I have been working for AWS almost 6 years, mainly, uh, helping in different kind of services, but very interested in analytics and GAI. So now that we tell you more details about us, we would like to know more about you so we we are going to personalize the session. So the first task for today is if you can help us, we are going to have a small survey just to know what is your knowledge about Agentic AI and the AWS services and that will allow us to personalize this session so. It should take 2 or 3 minutes to complete the 5 questions that we have here. So let me switch to my screen. Na. QR is working. Can you, can someone confirm it's working fine? OK. So we have 27 participants. We are going to give 20 seconds more. And then we are going to start with a few questions. 36 10 seconds more. Let's go. All right, let's go. So the first question that we have for you is, have you deployed any AI agents in a production environment? Yes or no? So we have 50/50 50/50. OK. OK, very good. Next question What are the main challenges you have faced when deploying agents to production? It's a cause of cost, precision, latency, integration. What's the main issue that you have seen? Precision, OK, and integration. OK, OK, that will work for what we have prepared today. Next, Have you worked with the strands? We know that maybe you have heard a lot about the strands these days, but we want to know more. If you have already used it or if you are using different frameworks, which is fine, we are going to talk about it. But just to know So we have 25% of the people that already have used strands, we are going to show you here some things about the strands. Very good. Have you used agent core or other agent frameworks? Uh Or more than other frameworks it is more related to other services related to AWS for Agenic, but that's fine. Let me, let me check how many of you already know about Agent Core. 29, so more than 50%. OK, we are going to show some interesting things there. And finally, the last two questions, which observability tools do you use for monitoring your agents? Are you using Agent Core observability or Land fuse, Langsmith, other? Only 2 people here have been using Agent Core observability. That's why we have this session to show you what you can do there. It's very good. Lance Smith, land fuse, and other. A lot of people in other, OK. And finally, the last question. We, we told you that we are from Colombia so we want to know if you already know where Colombia is located. Some people confuse our country with the state, so actually it's different how you can write both words. 24. Ah, there is a lot of doubts here. These people, 36. OK, very good. So you gave me that a little bit more of time. Yeah, definitely. Someone is reviewing there. OK. OK. North of South America, very good. Central America, no. I don't know. Very good, thank you. So yeah, Colombia is in the north of South America. OK, thank you very much for your answers. That's a way as we want to know more about you so we can customize the content of the session. The agenda that we have prepared for today is start talking about agent core and talking about observability and what, why this is important. Also, we are going to build this is a code talk, so most of the time we are being. Uh, building an agent. We already have an agent with different issues and we need your help to improve the issues that we have there. And finally we are going to share some resources that can that can be useful for you. So, if you mention that 50% of the people here mentioned that already has agency in production. In case, in case you don't have good observability, there can be different mistakes or different bad behaviors with the agents. So what we want to achieve with this session is that more people know about how to use the AWS services to improve the visibility of the agents that you are deploying in production. So we have as a goal when, when we have observability. We are looking for a way to control the non nondeterministic behaviors that we have with foundational models. Basically you can have an agent that is providing different answers to each, uh, customer, to each user, so we want to know what's happening. Also, sometimes the companies are deploying agents in production to achieve a business objective like for example, increase the revenue, but that's not. Happening so we need to have more visibility to know what happened there. Uh, the other thing that we are going to see during the uh demonstration is that these systems are complex. So we have different system knowledge bases. We have different NCPs that are integrated there and we need a way to know what is causing, for example, problems with precision or with latency. So with that said, what we are building today is a case that we already have in Colombia with a customer. They want to create a financial advisor. So this financial advisor has for this case and and we are going to be working on that, has, um, um, chat where the customers can ask for a new credit for a new loan. And obtain information about if the customer is eligible for a credit, if the customer has a good, um, behavior or performance for the bank, a good score, also we are going to verify what can be the rate for the credit, uh, and more information so. Gabriel or Gabo, I'm, I'm gonna call you Gabo, but it's cool how I call you in Colombia. So Gabo, do you know what kind of issues we have right now with this agent? Yes, and we have, I have seen with many of, of situations when deploying agents into production that they become like a black box for some of our customers and basically there are a couple of issues related. One to the evolvability of the architecture. Like for example, we need to move from one model to a newer model. And we don't have the right tools to measure if the quality of the responses are improving or decreasing compared to the previous model. We are also see situations in which the prompts might be contradictory and the tool use becomes a little bit uh nuanced because the instructions can be overlapped. Uh, I have seen other situations in which the integration with external tools are not clear and basically there is not a way to identify why the agent is taking a, a decision, uh, and what is reasoning to use a specific path, like for example in a multi-agent. Architecture, OK? OK, very good. So we have all those kind of issues today. And with your help we are going to improve it. But before going further, you mentioned that some of you know about Agent Core. So why Agent Core is important for this project that we are doing here. Now with Agent Core we are looking for all of our customers to move away from POC, from mass POC to production with confidence. Basically Agent Core allows to build, to focus on building the gigantic part to building the specific. agents and not worrying about the hosting the infrastructure. Agent core is building in a modular way or in a component mindset in which you can implement each individual components individually based on the needs of your specific agents, OK? And that also renders basically into reducing the time to market. Basically you can start building and let agent core to take most of the responsibility in hosting the infrastructure. And also the integration you have the integration with native services and also with third party tools for logging and monitoring. Very good. Agent Core has different components that work individually, so if you prefer only go by one, you can do it. In the case of observability. If you already are using other frameworks or if you are also right now using a different platform to deploy your agents, for example EKS or ECS, but you want to use observer, you can do it. So for today we are going to focus on a run, Amazon agent core um for the the side the run time and we are going to have also the observability. Those are the main components that we are going to use today. But something that I also want to let you know is that this is enabled for different frameworks. So if you want to have different not only strands but other frameworks running in the wrong time, that's possible. Or if you have a different kind of uh observability tools and you want to export all this information there, that's that's possible because we are using open telemetry. So what we are going to have in the console is first a chat agent where our customers can ask different questions about a financial credit or loan then we are going to use strands to define an agent and multi-agent. That strands agent has the capabilities to integrate with different tools and also we have created an agentic flow there. And also we have some knowledge base to answer questions to our customers. So all these flows you are going to see in it. We want you to know this. We are going to integrate also with different external tools. And the agent that we have here it's gonna be running in agent core running and the idea that we can see logs, metrics and traces using agent core observability. So at the end of the of the session we should improve all the issues that we have right now with the agent, but also we are going to have the possibility of see everything that is happening in a dashboard with a lot of detail. So let's build. So let's start and as Luis mentioned, uh, we will require your help and also to help us build and fix this the issues in this agent. So we are going to be, uh, iterating over, uh, the agent that's already running and if you can switch to the going there. Well, first question, can you see the, the code? You should, OK, let me make it up a little bit bigger. Mankind. Hm. Settings and settings and let me find font. Increase a little bit. Font size. I think that should be better. OK? Yeah, it's better. I think so, perfect. Now we're, we're going to start with a, I would say a fast run over what is built in this moment. So we have in this section an agent that is built on the strand SDK as we mentioned. And it's basically is composed like a uh like a simple structure we have at the top of the agent all the imports for models. We have, uh, the configuration for the environment variables in order to set the prompts and to set the the the models. Uh, you can see there are some of the things that we are going to be solving during the presentation. We have a couple of utilities, uh, for Grabbing the prompts from the prompt management because the prompts are not embedded directly into the code and basically we have two tools, one for the calculating the loans and basically obtaining the the monthly payments and we have a tool for retrieving information from a knowledge base, OK? And then we have the creation of the 4 agents. I'm going to open one of those so you can see because the pattern is almost the same. So we have a declaration of the model that we are using, some parameters, then we have uh the agent as we create the agent and provide the tools and then we return the agent back to a single orchestration point in which we fix and stitch all together into a swarm architecture. As an architecture for agents, it's like a full mesh of communication between the agents. So we have an entry point which is the orchestrator, and then, uh, the orchestrator is responsible for handing over the transition to other agents autonomously in order to solve an issue. So as you can see we have one. An agent that can check uh the rates based on the documentation. We have an agent that is a specialist in doing a loan interest rate calculations. We have one agent that can check for eligibility and we have an agent that basically the responsible for orchestrating everything, OK? So far, any questions? No, perfect. So the first thing is that I wanted to show you that this agent is already running in strands SDK and running in strandser runtime, I'm sorry, in agent core runtime. So With the uh quick start and the the quick start from agent core, you can deploy agents easily. Basically you configure the utility to grab the code, to grab the dependencies, to set the uh features that you want to enable like for example memory, observability is enabled by default, by the way, uh, if you want to change the where the traces and the logs are being stored and once the agent is deployed, you can check it with agent core command, agent core status. And you can start seeing like when was the agent created, in which account it's running, what's the run time ID, uh, the deployment time, this was deployed, uh, last time was deployed today in the afternoon. And finally, you are going to see this handy command that it's here. Have you ever seen this command before? No, first time. Well, this command, it's part of the AWS CLI, basically it's for cloud watch locks and this is a tail command that allow us to basically follow the logs as we invoke our agent. This is not a functionality exclusive for agent core. It's basically for every log group that you have in your account. You can like an old Unix system, I would say like tail a log file in front an Apache server. That's the same that you can do with a cloud trail, a cloud watch logs. OK. So I'm going to copy and paste that command because that's the first observability point that we have with uh agent core. Let me copy and paste this part. And we will have a tail of command. OK? Now, we are not going to see anything right now because we don't have any invocations in our agency at this moment, but let's switch to our er Or Uh-huh, to our agent user interface. So this is something that we built for this for this session specifically. It's a pretty basic uh user interface built on the streamlet that allows to select the agent core runtime and also the version you can see we have made a couple of testing in version 27 and then we are going to fire a prompt. Let's see what happens. So what are what are the current more Rates for a thirty-year fixed loan. And then we wait. Have you experienced something like this with your agents? I mean, response times, super elevated. What should be the normal time for an agent to respond or the time that you are expecting to have an answer? Less than 10 seconds? Less than 5? What do you think? Depends on the Depends on the use case, but for a normal interaction, what is a good time? 10 to 20 seconds. So how long it's taking here right now? Taking more. You can see right here. OK. 13.5 seconds. Now, because we left the console on the tail locks, we can see what's going on behind the scenes with this agent core invocation. So you can see this allows to see when agent core runtime receive a request, the parameters that are being sent into the request and also you start seeing like the reasoning of the agent. Like for example, in order to find information about the mortgages, I need to question another agent and then I need to hand over to the uh specific agent or the specialized agent and then that context is passed to the second agent that is the collaborator and then the final response is is assembled by the orchestrator agent. As you can see, everything took 13 seconds and this is locked directly by default when you launch with the quick start on agent core, OK? Now, let's try to find some potential, uh, I would say hypothesis of why this is happening, OK? What do, what do you think is happening? These are kind of things that you review whether you are having an issue with latency. In agentic. Some of the things that you can review is the program, for example, or the variables. Exactly. Now, like for example the prompt is one, something that we have set on purpose in this example, look at this value temperature. Now it's quite high and normally when you set the temperature high for any foundational model, the model becomes more creative and basically it has a more probabilistic distribution of words that's going to select as the output. So when we want to make the model a little bit more constrained in the probabilities, we could set the temperature back like um 0.5 or even 0.2 when we have when we want to make it more strict. Uh, there are some use cases like for example, we might set it to 0 when we want like a prompt router or to have like a conversion from text to Jammu or other structure format because we don't want creativity in that part. Um, the other, the other parameter, it's about the max tokens. So max tokens allows to control the, the length of the response from the from the LLM. So in this case we set uh 400 tokens, but we can decrease that. Yeah, to a lower value because we know that the responses are not going to be that long. OK. What about this? What do you think about this configuration right here? It depends what kind of model you wanted to use for this suitcase. Exactly. This might be a good model. That that's that's important and this is kind of funny because we, we, we refer to anthropic sonnet 3.7 like an old model but I mean things move super fast in the GI space. The reality is that there are newer models that we can implement and to take advantage like for example uh Nova 2 which was released just yesterday and also uh Sonet 4.5 and also another pattern that is quite useful when we have a multi-agent orchestrated system is that We can have like a super er I would say a more optimized model for the orchestrator and then we can have a more specialized er model for the specific agent. So we are going to roll this change through this agent and then we are going to push the change into production, OK? The first thing that I'm going to do is to clean all the uh I would say parameters of invocation of the agent. I'm going to set everything to 0.5 on the temperature, 0.5. And even in the orchestrator I'm going to set it like a quite low value like 0.1 because we don't have much, we don't want much creativity in that part, OK? The second thing we are going to set some of the environment variables uh to use the new models. So this project I have built with Agent Core basically relies on an environment, environment variables, external variables with this file. So basically I can change the model right here in order to upgrade the model. So let me open my terminal once again. And let me see and check for that model. OK? So, AWS. AWS Bedrock list. It profiles. List inference profiles. OK. Now, inference profiles, it's a mechanism that we have in order to er select like for example cross region er invocation or cross region er in Cross region invocation for models. So basically we use the capacity of multiple regions for a single model and we are going to look for the endpoint of a haiku. 4.5. Then we are going to grab this value here that is US anthropic clots 4.5. OK, let me grab this one. And let me place it right here like bedrock uh orchestrator model. I'm going to add a new variable. OK? And let me seek for the other ones on it. 8.5. OK? And this is the end point. So we are using an inference profile because it's better for resilience in case something happen in one specific region and having more capacity that case is needed. That, that might, that's one of the reasons also to have uh enough capacity because we are distributing, well, the service is distributing the, the inference call to our model replica in each of the regions. We can scale in capacity. OK. Now, I have set the the environment variables. Now I need to roll these changes into the code. So I'm going to copy this environment variable and I'm going to add a new one. Orchestrator model. There we have. And finally we're going to set this change into the Let me close it right here. Let's created that into the orchestrator model. Orchestrator model. OK. So basically we modified uh to use haiku on the orchestrator, which is a, a lighter model compared to Sonnet, which allows to gain a little bit of time in the first response and then we are upgrading the model from 3.7 to 4.5 in the rest of the models. OK? Now, we are ready to roll this change into production and The way we do that is with the same agent core specification and the same agent core utility agent. Status as you can see, we already have uh the the agent running. I have an an utility babe for the sake of this workshop that basically use a make file to deploy uh the agent. So let me also roll this change here. Mm, this is not what I want. So here we are using the agent core tool kit that allows us to deploy and simplify the deploy of new agents in production. Uh, if you haven't used it, uh, we are going to show you the resource so you can access it. Uh, I was testing it different times and you can deploy an agent in less than 5 minutes, so it's pretty easy. So the changes that Cabo has completed here. We can now send them to the new version of the agent in the wrong time we are going to see the new version. I don't know why we don't need that much. It's a little bit annoying when it tries to to recommend blocks of code that you don't need actually. OK, but I think we are, we are set. We have, let me double check. We have the orchestrated model, orchestrated model, then we have orchestrator model and then we have that into our variables. Now, let's launch that. OK, now, it took the change and we have haiku on the orchestrator. We have anthropic model on sonnet in 4.5. This is something that it happened when I, you might need to take into consideration. It happened to me when I was creating this workshop. Uh, by default when you launch an agent core, uh, agent base, uh, you need to pass all the environment variables one by one with within the launch command. If you, it doesn't take the M file by default, so it can go to a like a very difficult, uh, I would say fault conditions that are difficult to spot, but I mean it's part it's in the documentation. No. Well, it was really quick and then what it happened it's basically it took our project, it sipped all the dependencies that were already running, it created uh there is in the local project a folder that's called bedrock agent core multi-agent, and this sip file, as you can see by the output outputs of the log, are being uploaded to S3. And then it's created a deployment package. Open telemetry is already running. It's part of the things that are enabled by default and then we have our new agent created. Now, if we go back to our agent, to our user interface, let me refresh this. And we have version 28, OK? Now, let's see the difference. Oops, OK, there was a difference there for sure. Let me see once again. Whoops. Let me refresh. A little bit once again. It should, it should be faster. I promise. No. While the user interface loading, we can see that in the locks. What's happening behind the scenes. OK. This was a successful invocation. Handing over to the new agent. Handing over and then. There we go. No, it improved. From 30 to 0.5 to 12 seconds, but we are already seeing an improvement in this in this check, in this aspect. I mean, we can continue to improve it uh by making the prompt more specific, we can make other optimization, but we are going to get there in a couple of minutes. Now, any questions so far within what we have seen? To this moment. No? Our friend, OK. No, let's try with another one. OK? And let me check er 1 2nd, now. Let me refresh. This user interface once again and there we're going to ask for a different question. I'm going to check if we are eligible for a loan, uh, but Gao, before going there, can you please show us the metrics that we can see in CloudWatch related to agent core? Ah, for sure, yeah, because we were seeing the latency in the console. We were seeing that there are some problems, but also we want to show you here is that we have agent core observability including in Inside CloudWatch. So there are different kinds of metrics that will allow us to improve our agents in the future. So what we can see here, now we can see right now that we have 2 agents configured to endpoints. We have 25 sessions created. Some of those are created, those with a with a low test script. And then we start, we can start to see the trends in the sessions, the traces, the errors, and then we can start seeing like uh runtime specific metrics that are quite useful, like for example, the number of invocation, the number of BCPU by hour because that's the unit that the service is charged by and also the memory consumption by gigabytes. We have 2 agents right now, the default and some other customer support agents that I created previously, and we can start to check a little bit further for the specific agents, the token usage, which is also an important value for capacity planning, uh, the number of sessions, the number of traces, the number of errors were from the system, which is, for example, if the model failed to be invoked and also from the client side. Uh, whether it is some parameter misconfigure or even a header that was misconfigured and then we could even have, uh, the inbound and outbound authorization request count, you know, it's part of the components of aging core. Something that that that is nice about agent called run time is that you have a surveillance environment so if you are creating your agents, you don't have to worry about any assignment of BCP or memory. Everything is happening behind the scenes and is managed by the service, OK. Perfect. Now, we can also invoke the agent from the agent core console. So let's let me show you where you can test. Within agent core service, you can search for test and agent sandbox and basically you can select the agent, the default end point in version 28. And then you can pass a this is Jason structure where you can pass your prompt and also you can pass additional parameters if you want or you need, like for example headers from your application to give more context about the user. So I'm going to create another prompt right now about the eligibility for a loan. So am I elig eligible. So something that we want to achieve for this financial company is that they can. Verify if a customer is eligible. If it's the first time that this customer is coming to the to the bank that we can have an agent that verify the user history and that can er confirm what kind of credit or what kind of loan can be approved for this user. OK, now let's fire this prompt and let's see the responses. What was the question? Uh, checking if I am eligible for a loan with a 680 credit score and a 60K annual income. In this case, the user is providing directly the data. Uh, most of the time, the companies will verify that. Exactly. OK, and the answer and the answer is excellente noticia basando melos criterios delihilividad. The many people that can speak Spanish here. OK, very good now. Now this is something that can happen. It's quite common when you have a multilingual agents. Normally you have like specialized prompts based on the on the language because uh like for example, in Spanish we there are words that have multiple meanings and We can, we need to have a more concise prompt for the specific languages. It's a good practice and this can happen, OK? Now, the thing is that we already saw, let's go to the code and let's see what could be happening, OK? Now, Let's see where are our prompts right now. Let's close this. Let's close this, and let's close this one. Uh-huh. No, it was for eligibility, so let me search for the eligibility agent. Uh, no. So we have a belt truck model, a model ID, a temperature, and then we have a prompt, basically get prompt and eligibility agent prompt ID. Now, what's going on behind the scenes for this specific agent? We are leveraging a functionality that's called a Amazon Bedrock prompt management which is a service that allows to store version um I'll say the whole life cycle of the prompts in which you can set default parameters for each of the prompts so We need to seek for the eligibility. Prompt. Now if we move to Amazon Bedrock console as as we have a previously confuer open here. As we can see, we have the uh rate checker, the loan advisor, the Spanish agent, the eligibility agent and the loan calculated agent, OK? So most likely what we have here is a problem with the configuration parameter for our agents. So let's check what's the agent ID. In this case, this is L8J and let's see in our environment variables. That Exactly, that's what we have here. So the fix is quite simple. In this case, we don't have to modify uh the code of the agent itself. We need to move from one parameter to another. So let's seek for this one, as you can see, you are a loan advisor special, it just says customer qualifications and recommend blah blah blah blah blah. Let's let me. That should be better for you guys. And then let me copy this ID. The one that ends in 98. Let me modify that into the environment variables. OK? And then I think we are ready to deploy once again. So it was, it was a problem with the parameter that someone modified and left the prone in Spanish. Exactly. So. We are seeing different customers implementing multilingual, um, agents using multi-agent to allow the customers to have conversation in the language that they prefer, but sometimes it can be useful to have a specific parameters for each language, which is what we have here and also the ability to have the prongs there. We can have the problem inside the code, but when you have that as a parameter. You can make more simple the modification and the new versions and also having a version of the plans. Now it's up once again and I want to show you a feature that's quite useful in these cases that we are testing and testing constantly. Uh, within the agent core, uh, let me grab this prompt before I go. Be in within the agent core runtime, there is a concept of end points and also about versions. So every version is immutable. Once you deploy it, you cannot modify it. But then you can have multiple endpoints. It creates one that is the default one and it rolls. The latest version is always mapped to the default, but then you can have like production, pre-production, Spanish, er, Latin America, States, Europe, Spain, Portugal. You can have many endpoints and then on the application side you can select which version of the agent is being invoked, OK. Now this is version 29, just got up recently and let's test once again in the agent sandbox. OK. I'm going to fire this invocation and let me show you once again. The locks. Because we are now going to see another aspect of the observability. We are going to switch to the traces so you can see what's going on with all the visibility now. As you can see, the responses are flowing back in English. Great news. Based on your profile and your current eligibility created, you are eligible for a personal loan, thanks God. And then we have the invocation successfully completed. Now, With each invocation you are going to see a useful parameter which is the session ID, OK. Session ID is created by the client site. It's a 30 long character, normally a unique identified value that you set or you can customize the session ID, but with this session ID, let me copy and grab this value. We can go to the observability console and then we can start to drive on and to drill down into the specific of this invocation. So let me first of all, uh, show you a part that I didn't show at the beginning of the presentation that uh this functionality of traces uses what is called X-ray traces that needs to be enabled. This is in the documentation of agent core observability. And basically you need to enable transactional search for open telemetry spans. That's what we are basically using right here. This is the open format for uh setting the logs in a standard way. So let me go to the observability. And let me open the sessions. All sessions. So every time that we have a new user we have a new session. Exactly, even the session can be a set. It can last for at least 5 15 minutes and uh up to 8 hours. The session can last for up to 8 hours. That's right. Um. Let me find this session ID. Session ID equals. No. With this session we have one trace and the trace will allow us to map what's happening behind the scenes. We already depicted what's going on in the code and let me, let me open this and let me close. This little windows, yes, I don't want to see that. Now, the trace console within agent core observability, as you can see, has a 3 view and also it has a timeline so you can see what's going on. Uh, the first time, the first thing is that it was a post invocation, then we have, we grab the prompts from each of the four prompts that we are using, then we voke the swarm. Then we invoke the FSI loan advisor, then we go into a loop in which we can hand over the request to the agents within the within the swarm. And then we invoke the model which was anthropic haiku 4.5. And then we can see that we execute the event loops for each of the models. We can drill into the specifics of the model invocation like for example, this model invocation right here uh was completed in 2 seconds, uh, 193 tokens input and a 1000 tokens input and 100 tokens of output and then we can start seeing what is the, I would say the system prompt. You are a you are a main FSI loan advisor. You work with some specialist agent to help customers delegate the specialist that you need. We are going to see like uh available specialist, rate checker, loan calculator, eligibility. And also you can see the trajectory. This is quite cool because you can see it in a in a graphic way what the flow of the invocation. It's what's happening. So, the first time we receive the invoke runtime agent core, we received that into a post, then we grab all the prompts. Then we book the loan agent and then we book the eligibility, as you can see the loop cycle, uh, the many that were required in order to complete this request. Any questions, doubts so far? Yes, yes. Are the events on the previous page the same ones that would be passed in the memory if you had a configured? Ah, perfect. Now the memory has its own observability space. So if you, in this case we don't have the memory enabled for the sake of, of the, of the purpose of the demonstration, but in aging core observability. You are going to see that we have agents and also we have memory observability. So if we had observ uh memory enabled, we will see like all the events for short term and long term memory right here. Would you see the breakdown of the different strategies and everything as well? Yes, you will see the breakdown of the strategy. What's being stored in each of the, of the memories. It, it, yes, because in sessions then, uh, you can see the that information from the, from the memory from the detail. We don't have a, a memory right now. When when if configured you will see it right here. And metrics from that. OK. Now let's try to fix another one that is quite common also and let's ask the model for uh Let's see. The last one. Now, let me ask The agent. Let me refresh this to have the latest version. Version 29. Now, regarding Uh, what is the current interest, interest rates for 3? No touch let let me. OK. We can see once again, the locks that are being invoked. And also something that we would like to have here is for example that the interest rate can be evaluated by the agent and provide a different rate according to the customers and Allow them to obtain a credit or a loan in a few minutes. So that's why we have created different agents here and We are going to talk about the multi-agent architecture at the end. OK. Now, there is a slight issue with this agent as you can see the information we have, it's outdated. It's from 2023. This is something that might happen because of the maintenance of the knowledge basis in order to ingest and basically to process new information and also this is as Luis mentioned because this is an agent that provides advice, financial advisory. This can misguide the customer to take a decision because the this information does not reflect the actual reality, OK? Now, I'm going to grab the session ID once again. And now we are going back into the cloud watch observability and the agent core observability and I'm going to search for this session, session ID. OK. And the objective in this part is that within the traces we are going to see uh in the trajectory the requests that are being er performed towards the knowledge base. OK. No. Let's moved back all back to the end and then. How many knowledge bases do we have here? We have just one for the sake of the of the session, but the important thing is that if you have many others knowledge base, which is a common architecture, you will see that Why did the agent er start selecting one over the other and you can see that even the chunks that were retrieved by the request from retrieve er er retrieved from the knowledge base. So let me check if I can find a retrieve. This one here. OK. No, this is something that as you said, I'm going to select retrieve and it didn't bring any information, OK? Uh, when I, when I show the X-ray configuration, the default configuration is to ingest, I'm sorry to index 1% of the traces, OK? You are seeing the overall trajectory, but if you set the value as the default that is that it is, uh, you might encounter this. And that not all the information is being shown into the into the traces, OK? Now, there is a trade-off. If you increase the value, it's going to give a pro it's going to cost more because the number of events that are being processed will increase. Yeah, in that case you can decide if you want to have 100% of the logs being ingested and indexed or if you prefer in case, in case you have a production environment, choose less than that. Do you have a question? How does it? I'm sorry, what? How does it decide what to record if it's not set to 100? Like what's the algorithm or how is it determine what events to persist? It's a random sample, basically a sampling. So you could get lucky on some runs could get unlucky exactly, exactly, exactly, OK. that just as it happened right here, OK? Now I'm going to fix this real quick and basically er I'm going to move to the knowledge base. In this case, this is the knowledge base that we have for this session and this is the source bucket that we have configured. And I'm going to replace all those files in a second. So let me grab this. So this is something that is very complex for our current customers is, uh, is how to have updated all the knowledge bases. So that's something that we should see in this case we have Bedrock knowledge bases that are using as a source an S3 bucket and what Gao is going to be the. is going to do here is to update those files. You have a question? Um across the Like if you have all that set up. Now, let me, let me go there to see if I can hear you better also for the rest of the people. Well Can you please say that again? Thank you. Uh, yeah, I was asking if the session ideas persisted, the same session ideas persisted across like if you have. Uh, gateway set up or memory set up and you have traces enabled through all of them. Is it, is it a single session ID you can connect them together? Yeah, that session ID, it corresponds to the agent code runtime session. So for the agent code run time it's going to be the same if you have a different session, uh, from the memory side. You are also, we're going to have the same ID exactly. Um, basically because you can set the session ID from the client side, you can persist the same, uh, the same session ID. Now, the default timeout, it's 15 minutes if I recall well, up to 8 hours. So when the run time, uh, dies, basically, uh, you need to, if you create another set with the same session ID it's going to be another uh runtime environment, OK? Now, I'm fixing this and while this syncs, uh, this is, this was a quick fix. I want to move over to another uh theme in the code that might be useful for you folks also in the, I would say to optimize for. That's basically that in this situation as you have seen, most of our interaction is orchestrator specialized agent, orchestrator, specialized agent. So basically Uh, we have a full mesh, uh, environment. Let me see if I can draw something real quick right here. So we have something like this and then. OK. Then disconnect with this one and disconnect with this one and this with this one and this, I mean, it's a full mesh of communication. Now, this is the the architecture that we have right now with our agent in In this part, let me show you where it is. The agent core entry point is it's a swarm with an entry point with a max hopes of 20, but we can aim to simplify this in another iteration. That's basically if we switch to a, for example, a workflow orchestrator or basically to move to a uh like a principal agent and a subordinate agent, we can remove all those connections with Uh, simpler architecture and also that can simplify. The the interaction between the agents and can decrease the agent times response. OK, very good. So, we only have 3 minutes to finish. Something that we want to show you here is that you can use agent core to deploy your agents, but in case you want to use a specific component, for example, if you decided only only to use observability, you can use that, uh, you were asking about memory, uh, so from the agent core side we can have locks. And traces and and metrics from runtime memory and gateway that are ingested inside agent core observability you can see those uh that information in cloudwatch but if you prefer to use another observability tool you can send the logs and information because we have the open telemetry format also. Different concepts that Gabo was showing us here is that we have the the session ID is like the main data, but from that you can have also traces, pans, super expans that allow you to know what's happening with the agent and with each interaction that the users are having there. Some of the key metrics and silence we were seeing here 3 different issues, but there are different issues that you are going to face when you have an agent in production. So here we are showing you where you can find the issues. So if you have an issue related to latency, we have a specific matrix for that. Or if you are having issues with invalid schemas, you can go to the locks and check user errors. Also we have good information about talking. I know these kind of projects are very price sensitive. I have, for example, a customer that has a good use case for a GAI for agents, um, but the problem is that this doesn't is giving them pro profit because it's very expensive. So you can trace here the cost and verify it is good for you. Exactly. And finally we want to provide some of the resources, er. If you haven't used agent core started toolkit, I fully recommend to use it because you are going to deploy an agent in a few minutes. You are going to understand more that we were showing you here. Also, if you want to see code samples about how to use, for example, other frameworks to send data to agent code observability, you can do that. We were using strands, but that's not the only option. And also if you want to go and dye the in agent court, this is a good, uh, workshop that you can check for the component that is better for you. I, I have, for example, some customers that prefer only using the gateway. That's OK. You can see what are the components are there. I don't know how if you want to add something there. No, I think that, that, that's it. OK, so thank you again for your, uh, attendency here. Uh, we are going to have a survey at the end of the session. I think I think you have there in the app. So if you have any questions, we are going to be here. Thank you again for your time. Thank you so much for being here. Thank you, thank you, really appreciate it.