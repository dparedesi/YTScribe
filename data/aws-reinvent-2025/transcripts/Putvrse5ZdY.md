---
video_id: Putvrse5ZdY
video_url: https://www.youtube.com/watch?v=Putvrse5ZdY
is_generated: False
is_translatable: True
---

Right. I hope you can hear me. Uh, good afternoon and thank you for coming for today's session. So we're gonna be talking about our 2 year journey in 20 minutes. So just bear with us as we go through this, uh, incredible experience we had where we looked at AI as a powerful tool to emphasize. The finance transformation journey for for one of our clients, but before that, maybe just a quick introduction of the three of us here. I'll start with myself, Rakesh Shetty, so I lead our finance and transformation practice for PWC Canada, finally focusing on financial services and optimizing finance technology for the future. Uh, along with me, I have my client and my colleague Ram. Just a quick intro on your side. Hello guys. Uh, my name is Ram Perdue. I am the chief data analytics officer for risk and finance at the Bank of Montreal. Hey guys, uh, I'm Anna Ravi. Uh, I lead the, the cloud data and AI practice at PWC for financial services. So, so we're gonna keep this interactive. I'll play probably the moderator role for a bit just to kind of tease out some experience that we went through the journey. So my first question will be for Ram, just an overall, we've been at this for 2 years. Maybe we can just power a little bit, talk about the bank and their AI journey in 4 key things. Why transformation? Why? In corporate functions, what are some of the challenges we went through the last two years? What are the obstacles we face, not only about implementing the solution all the way from concept to production, but more important, energizing our business users from a business adoption standpoint as well and the experience to it. So maybe I'll just talk a little bit more about the bank and your journey today. So, uh, again, I, I'll be sharing some of the use cases that I've been working on and, and as Rakesh was mentioning, this is a journey we started about a year ago uh looking at finance and risk, what we could do from the AI perspective and obviously we are a financial institution, we are a heavily regulated industry like a banking. The first and foremost thing that we need to. Look at was build an AI trust, uh, where we are looking at the AI governance side of it, whether it is going to be a fairness, whether it's a transparency, whether it's an accountability, all the things that you've been learning at this conference from the AI governance perspective, that was the first and foremost that we needed to make sure that we are doing with that. And then the biggest challenge, as you can imagine, is building pilots. It is always easy, but taking a pilot from, uh, uh, once you do the pilot, how do you take it to the production? The lead time between a pilot and a production, how can you close the gap? What are the challenges, Rakesh, you were asking what are some of the challenges is how do you bring trust into the numbers that are coming because it's a finance and risk. The numbers are very important. For us, even to the decimal levels, how do you make sure that the numbers that are coming out from the AI perspective, they are the right kind of numbers and, and what kind of guardrails are you going to put on that, right? So that's, that's where we started our journey. We, we have some interesting stories to share as we go across, uh, then we are also looking at the risk use cases uh within the BMO perspective. We'll be sharing more of them. Yeah, and I, I think, uh, largely one of, uh, what we're observing across clients and even our learnings from BMO as a whole is, uh, it's the shift from moving from individual use cases all the way to transforming an entire organization. And when you get into that path, there is a lot of synergies that you need to be mindful of between different use cases. So like Ram was mentioning about having a finance use case, having a risk use case, but maybe content generation and content summarization are two common things that exist between the two use cases. So in terms of how you're architecting your models, how you're leveraging the different. Services there could be a lot of common patterns that you could build within or between these two different use cases. So that was one of the major learnings that we were having as we went through the implementation part in itself is talking about is building more of the capabilities. We all tend to go one use case after the other use case, but can we build more of the capabilities. That can be used across the different use cases. For example, we started with Amazon Bedrock as a foundation, but now once we build that, whether it's a rack or an agentic one, once we build it the same kind of capability, for example, it's a content extraction. You build it for one use case, you can definitely apply the same kind of thing against other use cases as well. Just, uh, continuing about finance and some of us who have actually worked in finance, right, uh, the last time something great happened to finance was Y2K 1999. Some of us remember that done to beat the clock and implement a GL. Here we are 25 years later transforming finance. What we learned is when you are looking at your ROI and how do you identify the use cases, put them under 3 by 3. So the three pillars are. How can I make something like the process more efficient, whether that's your closed process, whether that's your variance analysis? How can I be more effective as RAM called up the accuracy and the degree of precision is key because we are a regulation and we actually are reporting these numbers to the street, so it has to be precise and the most important thing what we have also understood. AI as much as we love it, it's here to augment what we do. So it's all about the enablement of the employee experience and how do you actually empower the talent for today when you think about digital upskilling, when you think about having finance as an example, it's not about just learning accounting anymore. They need to be better storytellers. How do you actually come up with. Thinking outside the box, so some of the soft skills about enabling that, uh, employee experience is something that we've learned from our perspective. Now you married that to how do you actually measure the performance, and there's a question for um. We start off with what, 3 use cases. It was all focused on productivity, right? Can I reduce the amount of time I I, I, uh, I take to generate like an MDNA report or something related to narrative or commentary, and then we moved on to. Can I do transformation now? Can I actually stitch a few processes together? And the third one is obviously disruption, but I would love to get your view as a CDAO. How are we transforming, moving from Gen AI and of course now agentic AI. I think the fundamentals are still the same, right? We all start with a business strategy. What is it that we are trying to achieve? For example, we stand here and we are looking at what we want to. Achieve from the finance perspective, from the risk perspective in the 2026, we take that broad umbrella and say that, OK, how can AI help from, from this perspective? And, and when you look at the business case standpoint, we always go after the business value, right? What is the business value that this use case can generate? Sometimes it is the efficiency, but there are a few other things that you might also want to consider. One of them will be feasibility. Yes, this is a great use case, but given the capabilities of what we have within the bank or within the industry, can we actually solve it and how long does it take to solve it? That is a feasibility side of it. Then the second thing you are looking at is how aligned are we as a bank and as a team. Is the team ready to work on it? Is the business leaders, executive support is there. The business alignment. Side and since again we keep coming back to the regulation side, there is sometimes a risk of doing or not doing it. For example, a risk use case, there is a regulatory pressure on us to actually solve certain things. It could be a policy management, it could be something else. We're looking at that pillar as well. So one obviously is the value that you're going to generate. The second thing that you're going to look at is the business alignment. Third thing is the feasibility of it and the risk of doing or not doing. So that is going to be the, the pillars I would look at and, and as we are going through, we are getting better and better at each one of them and taking them forward. Question for you, Abinov, your favorite question. Uh, another thing that we learned, and some of us are learning on a daily basis, right, that you heard something brand new this morning, uh, when we looked at the keynote. So what we have learned is every week. There's something new that is coming in. So for example, a use case that I built 3 months ago using a particular model, whether that's cloud or something else, 3 months later, there are newer gadgets on the tool. Now when you marry that to a regulated environment, what you see on TV doesn't mean we can use that at work. As well, right, it becomes a balance between what AI as a hobby versus AI in practical. A, you worked at other banks just like Bank of Montreal. What has your experience been about how do you actually leverage some of these new tools which are still not approved in a regulatory environment and have that balance? I think um this is a question which could probably prompt its own session in itself because this is a challenge that I'm sure like a lot of you are also like dealing with on a day to day basis. Um, there are two lenses that we've kind of started applying. One is from a use case life cycle standpoint. The other is from your overall STLC or your software development life cycle standpoint now. When you, when there are new tools, it's always pretty encouraging, um, like how do we start leveraging some of these new tools, uh, they are probably not G8, uh, some could be used in a lower environment, some could be used like all the way up to production, but I think the real question that goes back to is when you look at the overall use case life cycle, like you, you have a requirements extraction step, you have a solution design step, there is a development, then there is deployment and then monitoring. And testing. So at which stage do you want to really subject your new tool to be used is what again gets complicated within a regulatory environment where like within monitoring and deployment, you could probably leverage a tool, whereas it's a lot unsafe when you're starting to use it for generating requirements in itself or even more risky when you start using it to select models dynamically. On the fly as well. So it's a critical thinking that we need to actually layer in from a use case standpoint as to where in the development life cycle do you want to really leverage some of these new tools and new offerings that are coming about. I want to talk about one thing exactly. I was gonna come to you, talk about speed to value, and you talk about new tools. An average life cycle of a use case, and I'm just putting traditional agile. Where you do your your design, build, test, and deploy, but the key is about how do you actually capture requirements. So think about some of us here as finance folks, we don't think what the good looks like because we're so consumed in what we do day in day to day. Rah has a great example where he took something that would have taken 3 months and he was able to do that in 8 hours on the policy management. I mean it's just an example, Ram, I would love to get your. Experience there. It's more of a challenge, right. We had a session together, both from the internal and external folks. We just got into a room and we said that look, traditionally it takes 3 months to bring a product as an MVP and put it in front of the users. How do we challenge ourselves to see what we can do because as Rakesh is mentioning, sometimes what happens is we build a use case, you take it to the users, and the users are looking at it for the first. and they're saying that I wish there are 3 other things that are there. Or I'll take an example. 6 months ago we started on a project and we built it, but suddenly now it is not at that time when we started, maybe it was not agentic and now suddenly the user is saying, hey, I probably I'm seeing the other things in the market that are more agentic that are doing much better things. How do we change it? So the big question for all of us is how do we think about speed? Speed meaning that. Instead of spending 3 months to put something in front of the user, can we challenge ourselves to put something in the days? The things that were taking months, can we convert them into weeks? The things that were in weeks, can we convert them into months? That was the challenge I posed to the team and said that, hey, you go to these hackathons. I'm always impressed with the people that do the hackathons. Again, it doesn't mean what you build in a hackathon can go into production, but the idea is you put yourself. In a constrained environment, constrained time with the tools that you have, can you build something that you can actually take it to the user and showcase? So it is going to be more and more important for us as things are changing every day. New tools are coming. It is going to be important for us. One thing I want to touch exactly on the tool side of it, the users are coming and saying that, hey, I know that I'm in my daily life. I'm probably using it. Why is it not in the? That is something we need to answer. But from our standpoint, what we are doing is we built inside the BMO. We build AI for all as a curriculum. We also build more of AI for finance and risk as a curriculum where it's a curated kind of a course content where people can go through. They're not just a periphery. They know that what is AI, but if you want to get a little bit deeper into it. That is what they're spending time. They, it, it could be on the foundations of the AI. If you want to go advanced, you can do that. If you want to learn about agentic, you can do that. But you need to bring people along, right? We all are here and we can build tomorrow, we can go, go into our own institutions, we can build the applications, but we need to bring people along. The way to bring people along is obviously we call it change management, but part of that is educating them, getting them. Not just about I know AI, but getting them more educated about, hey, tomorrow you might be using these tools in your day to day job in what you're doing today. Can you, for example, we rolled out copilot for the rest of the bank. Are you using Co-Pilott or not? Or when you're doing it, are you still doing the manual kind of requirements? Are you able to use this? The other one is, can you bring your partners inside of the bank because of the regulator, are we. Our risk folks very early on, model governance folks very early on, are we bringing our legal folks very early on or IR folks very early on. So what kind of the people we need to bring together to start these use cases, that is going to be the success. It's not so much about we go build the use cases, bring it back, and hope, hey, we have built a great product, it will be successful. We need to walk through this mechanism to make sure we have the right people in the room from the day one for this to be successful. And I think, um, like, now looking back over the last 6 months, probably if you look at a data migration or a cloud migration effort, it's not going to be the same how it was being executed for the past couple of years, right? Like the whole mapping aspect of mapping your source and target, running your test scenario. Or test strips or even generating your target code, all of this could potentially be automated uh end to end. So you don't need the same number of data engineers or the same number of testers for you across the whole life cycle. So it's all about how we could get it faster, quicker, cheaper, and probably in the most efficient and accurate way. 11 other point and might be a question for both, and then I go back to finance. End of the day, you know, when you think about data sensitivity, right, especially in a public cloud environment. When it comes to bank, we always talk about PII data, right, the employee data or customer data, which is very sensitive, it's critical, and hence it's confidential, but finance, if it's unpublished data, it is considered to be registered confidential. That means you can't just use a production set in your lower tier environment. It has to be regulated. So what would your experience be? and I know we went through a whole journey about. Encryption and putting some guard rails around it. So a governance question and for you more about leveraging AWS and how we're able to do it. So first start with you on governance. So again, this is not something new right today, whether you are using AI, whether you're not using AI inside of the bank, we have a role-based access controls that we have, but maybe because of the agentic stuff you might have to enhance some of those guardrails and the. Controls, for example, I was in the keynote session this morning and we were talking about creating a policy to say that OK, agent can do this, agent cannot do this, agent can access this, agent cannot do this. For example, there is an approval, there is a general entry in the finance. If it's $10 or below a certain threshold, maybe $100 we will allow the agent to make the transaction, but above a certain threshold, so we've got to create those policies. We've got to create some of those frameworks. We do have the AI governance, as I mentioned. The big, the big points of that is the fairness. It's the transparency. It's the accountability as we are doing it. You might have to build a few more controls as we are thinking from the PII level. But one other thing I would say is data is always a challenge for building AI. But some of it, when you are in the MVP stage, you can also build a very High quality synthetic data for you to start working with and building stuff, something for us to think about where we were 3 or 4 years ago versus what we can do with the synthetic data today is much different. Exactly, and I think like and we have like anthropic stall like close by. So at BO itself, as you may have seen, like we started building a number of these agentic systems where it's not only about like creating that synthetic data. But how can you make it as representative as possible for training your models and taking it a step further where you're having the agents generate what should be your different parameters to fine tune the ML model that you would actually be building that runs on those training data sets to further refine it. So it's just like taking synthetic data existed for quite some time, but how do we make it like take it to the next step all the way and go all the way into the ML uh enablement too. So that's our journey to date. I wanna just finish off. We've got 1 minute left to maybe a last question to Ram to kind of wrap it up. What's next, Ram? What does the next 5 years look at for finance? You know, hopefully we have a better story to say every year we come here, we'd love to get your view and then, uh. Yeah, so look, last year was all about experimenting, but this year is going to be all about return on investment, right? So it all again starts with a business strategy. What is that business wants to achieve in the 26 time frame and how do we help from the AI perspective? We have a really high quality use cases that we have both on the risk and finance perspective where we are putting together with a clear, like I said, the business value of it, the risk of. Doing it or not doing it, you need to bring that framework together. Put like, what is your top quadrant, where, where are those use cases that are high in a business value, high in the feasibility, and go after them. But things are going to change and I'm really excited for what is being, uh, uh, what I'm seeing in the conference. And it would be a remiss not to say thank you for the PWC for working with us and, and accelerating our work. And and also uh from the AWS perspective, uh, from providing the tools, the platform, and, and helping us, uh, more accelerate our work. Awesome. I wanna say one last thing before we go. We all grew up saying people process technology. We're gonna walk away today thinking about people process and performance. Technology is like electricity. It has to be here. It's how we use it for the best. Thank you so much for, uh, joining the session. If you have any questions, we're gonna be here for uh thank you.