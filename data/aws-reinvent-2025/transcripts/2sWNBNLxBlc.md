---
video_id: 2sWNBNLxBlc
video_url: https://www.youtube.com/watch?v=2sWNBNLxBlc
is_generated: False
is_translatable: True
---

All right. How many people here is this their first reinvent? 5th reinvent or 5 or over. 10 or over Any hints? No hands. OK. So 5 and over. Um, so welcome to day one of reinvent. Um, hope you're enjoying yourself, uh, so far, this is your first session. Welcome to your first session. Um, this is an advanced AI security session. This is a 400 level session where we're gonna go deep into how to secure the AI workloads using AWS Native capabilities, open source frameworks, other things like that. My name is Reece Goodman. I'm a principal partner solution architect at AWS focused on AI security. With me, Jason. Hey everyone, I'm Jason Garman. I'm also a principal security solutions architect here at AWS focused on our AWS industries customers. So happy to be here today and excited to help you out today. Awesome. All right. Um, the way that we have this presentation structured, we break it up into different phases. And the reason why we call it phases is depending on where you are building different AI workloads, we'll determine exactly what type of security you're looking at, whether you're doing things with tools, whether you're doing things with agents, data sources, other things like that. Throughout this presentation, we'll have these little thought bubbles at the bottom. Uh, they might be thought provoking questions or just additional comments about the presentation. We also have QR codes. So, um, QR codes, while I'm still speaking, they will pop up, so it's not just like a click on it and it, uh, then goes. But we will have QR codes throughout. So if you're looking for documentation, um, there's a couple of blog posts that we mention on here, but just additional information to get the knowledge that you need for, uh, what's on the slide. Again, this is a 400 level session and so the way that we're trying to break apart those black boxes is to show it through API calls and also show it through code because people uh hear LOMs, people hear agents, people hear tools, but a lot of times you just understand like. agent does something, but you don't understand how it works. And so the overall goal, whether it's showing Amazon Bedrock, Agent Core, strands, all those different types of capabilities and managed services from AWS is to break apart those black boxes so you can understand it from a security perspective and what needs to be done. How many people here are familiar with Agent Core? OK. There's a reason I put this slide in here. Um, Agent Core is a service that we launched at New York Summit, um, that is a lot of different primitives in order to build agents on AWS. It includes things like Agent Core runtime, memory, identity. gateway. I put the documentation and the um QR code on this to get more information. We're not gonna spend a lot of time to talk about what Agent Core is. There's a ton of sessions this week if you want to dive into exactly what those are. But with that, let's talk about the first phase. So when you're first building anything with generative AI workloads, this is normally how your application looks like. You have an application. You're talking to an LOM, OK? It could just be a simple generative AI chatbot, something like that, but it's just an LOM. It doesn't have data sources, it doesn't have tools, it doesn't have agents, anything like that. This is a 400 level session, so let's dive into what that actually looks like under the covers. Amazon Bedrock is a managed service. And so we deploy it in an Amazon owned account. And for your app to actually connect into it, you have to deploy it in a VPC and connect to things like VPC private endpoints. You can also connect through public endpoints if you want to do that. On top of that, you might do things like an ALB or some type of thing that will low bounce across your application in order to get access to the application that then goes to the LOL. OK. Let's talk about security with us. Security groups Roles, permissions. All those things still come into play when you're talking about generative AI workloads, because those are the traditional controls that still come into play. Then you have to think about things like, OK, do I wanna, uh, do DDoS protection? Do I wanna do something with WAF at the application level? I wanna do something with identity. I wanna do something with permissions. And then on top of all this. You have things like cloud watch, cloud trail, guard duty, to get visibility into what the application is doing. These are traditional security controls. Probably 80% of what you do with anything with AI workloads is traditional security. It's that extra 20% that we're gonna spend most of the time on. That you had to do things differently, you had to think about from a threat modeling perspective and other things. And a lot of that comes down to this guy right here, the LOL. How many people could talk about exactly what a large language model is? I want to see one hand. I got one hand. OK. So, large language models. A lot of people view this as a black box because it is complicated. If anybody has ever read the Attention is all you need, uh, paper that came out in 2017, 2018, it goes into granular detail about how to build these transformer architectures to ask questions like, how many R's are in strawberry? Sometimes the LOM gets that right, sometimes it doesn't. But if you look at exactly what the LOM is doing, it is complex math. It's looking at associations between words, associations between tokens. So when I ask the question, how many R's are in strawberry, it can convert that into numbers. It can uh put it through multiple different layers, and then at the outcome it makes a prediction. That prediction could be the number 3. Then it turns it back around again. Then what's the next token? What's the next token until it gets to a stop sequence that says, I am done and you can send that back to the user. Now my question on this is, where is identity? In this architecture Do you see anything about rows, columns, tables, anything like a database? And the reason I point that out is from an LOM perspective, all it's doing is matrix multiplication complex math on the data that it's trained on. And so when you think about it from what it's doing and what it's not, it's not an object store, it's not a database, and it's just doing the complex math, but you had to figure out how do you put security around it to make sure if it comes back with something that's not gonna leak sensitive information or anything like that. Fine tune the models. So you can take one of those large language models that was trained on petabytes upon petabytes of data. And then put your own data with it. So this is called fine tuning, and it's question and answer pairs that you can add to it. But then the question comes into play of what happens when you put sensitive information with a large language model or a fine-tuned model when there's no identity that exists inside that architecture. What type of data or what type of users should get access to that, if it does have sensitive information. Alright, so going back to this architecture. We talked about those traditional controls. Now let's dive into this part. This is where we're gonna spend the majority of our time today of talking about how applications talk to a large language model, add data sources, add tools, other things like that, in order to produce the outputs to provide value to your customers. One of the APIs with Amazon Bedrock is called the Converse API. OK. This is one of the APIs that you can use in order to interact with models on Amazon Bedrock. When you actually call these models, it's a certain API that you can call. You add some natural language query like, shall we play a game? Now this is a very special model. That likes movies, and so it can come back with something like, how about global thermonuclear war? Or a nice game of chess. It's very dependent on what the context is, how you ask the question, what temperature and top P top K settings that determines exactly what that output is. But remember, it doesn't implement anything with authorization. All it's doing is just predicting that next token, depending on what the context is when you're adding, uh, stuff into the API. And so this gets into one of the biggest points that we'll make in this presentation, and it's about data reaching the model. That because the LM doesn't implement anything with data authorization or identity, anything that you send to the model, either the user or the agent needs to be authorized for that. Now why is that the case? Like I said, LOMs do not, uh, implement anything with data authorization. And so once that data hits an LOM, it's just gonna do what it knows to do. It's gonna do the complex math and make a prediction of exactly what the next token should be. It's not saying, OK, who's the identity provider? What type of uh JWT that it has or anything like that. And so because it doesn't have anything with identity in, in it, when you try to do any type of authorization afterwards because it's natural language. It turns something that could be a deterministic control into a non-deterministic control because it is natural language. It's about interpretation at that point, compared to what data source are you gonna send context with or who the user is from an identity perspective and other things. And so the term that we always use is implement security outside the model. Don't hope in LLM that sometimes acts like a two year old, sometimes listens, sometimes doesn't. To try to implement security inside the model. Right. Now, there's some things with, uh, the types of content that you wanna come back with, or whether you wanna, uh, prevent hallucinations or harmful or uh prompt injections, where you do want to look and do interpretation of exactly what the content is and what it's coming back with. We have something called Amazon Bedrock guardrails, and the focus on Amazon bedrock guardrails is not on the security, but more on the responsible AI. All right? And what do I mean by this? It can do things like deny topics. I don't want it to talk about a certain topic. Um, content filters, sensitive information filters. And when I say sensitive information, I'm talking about PII data. PHI data. Because it doesn't understand identity, it can't say whether this user should be authorized to data that's being sent to the model or not sent to the model. Word filters, um, and also automated reasoning. And the overall goal is you put the user input and then the output through bedrock guardrails. And it will tell you whether or not, um, it's hitting some of these filters and whether it needs to filter things out. All right. And so, again, identity doesn't exist in this. And so there's deterministic controls that exist in here like word filters. Can you just do a pattern match on that. But most of these are non-deterministic controls. So example of this. I'm gonna do a prompt injection and say ignore all previous instructions and do something, right? In that API it has a guardrail config. That guardrail config can be configured in multiple different things, like it might uh match on uh prompt injections or word filters or other things. And depending on whether or not it matches on something, it can come back with a response saying, sorry Dave, I'm afraid I can't do that. Again, it likes movies. One of the most important things with the guardrails is that information that comes back in the API. Whether it actually hit it. What was the reason it hit it? What part of the guardrail, uh, did it hit? And it's for two important things. One, to make sure that harmful content or hallucinations or bias is not coming back. But also, if someone is not trying to do something malicious, and they are hitting guardrails to get the visibility that you need in order to say, maybe I need to tweak this guardrail, or maybe I need to add additional context or other things in order to make sure this is working for customers. So, in summary on the foundational layer. Couple of things to think about. Raise your hand. Are large language models deterministic? Yes or no? Yes. No, cause you're reading the slide. OK. Um, they are not deterministic. And we say it as functionally non-deterministic because what that means is that the math with matrix multiplication is deterministic. What's not deterministic is the types of hyperparameters that you can use. So if you use temperature at 0.05, then it can be more creative in other things, compared to if you set the temperature to zero. And so it's not gonna, uh, predict the most probable token every time, right? Can you filter specific data out of LMs? It's not a table. It's not a row, it's not a column, it's not an object store. Whatever data the model is trained on. You have to assume that the user could get access to that data in the LM if they ask the right question. Do models do continuous training? The answer is no to this. Once you actually train a model, the model is static. Doesn't matter what you put into it, all it's doing is just matrix multiplication to predict an output. It doesn't do continuous training on your data. And we talked about this at length. There's no authorization that exists in the model. OK. Phase 2, data sources. There are a lot of places that you can get data to include as part of the large language or to the prompt for the large language model. There's data that exists in the model. But also a lot of data that can exist outside the model that you send to it. OK? Things like context engineering or system prompts and other things like that. You have vector databases or knowledge bases or RAG, where you can uh do a vector search in order to understand what data I can include in there that is similar to the request. Tools, this is a big one. But I'm gonna let Jason talk about that because we have an entire section on that. And then memory. Memory is one that's coming up a lot with agents. Specifically on how you can add additional information, whether it's old session data, or facts or other things that you can include in as part of the context in order to get the LM to respond back the way that you want. So we're gonna go through each one of these. Context engineering. So context engineering, when you have an authorized user. Interacting with some generative AI application or some agent. You can put additional context as part of that prompt. That can come from user data, that can come from system prompts, that can come from other data that exists in the application. But the overall goal is when the user asks a question, you can add additional context in order for the LM to respond back the way that you want. And so all that goes into the prompt. And so again, when we talk about the authorization and what data should be included in there, you have to think about what is the data that you're going to send to the LM? Is that agent, is that user authorized for that to make sure that you're not gonna leak sensitive information or give it data back that they shouldn't have access to, right? Second one is retrieval augmented generation. Some people call this knowledge base, some people call this vector databases. But the overall goal is you have, um, unstructured data that you wanna include as part of the search in order to get specific data that you can include in the prompt. And so let's talk about how that works. The first thing that you have to do with RAG databases is indexing. And so what you have is you have all these documents or unstructured data. And you have to divide those into chunks. We are not gonna talk about how to do the chunking strategy. That gets into implementation of the application, but chunking strategy is actually very important to make sure that you're including the right data in each chunk. Those chunks are then converted into numbers that then get stored in a vector database. The overall goal of the vector database is being able to search in order to get access to the chunks and data that you need. And so when you're actually doing the querying of the vector database. You have a user query that you send, convert it into embeddings, you send it to the vector database, and the overall goal is to find similar chunks to what the user is asking for. And so if you think of like a multi-dimensional space, where is the user question getting close to some of these chunks, and it's going to return back some of those chunks, depending on what the user asked for. And then that will go into similar chunks that then go into the prompt. OK? One of the things that you can do with vector databases is add something called metadata. Metadata allows additional context to be added as part of each individual chunk in order for you to do things like filtering. They're like key value pairs that you can add on top of it. One of the important notes with this. Is that with any chunk or with any metadata, it's applied to the entire document. So it's not applied to every single chunk where you can say, this chunk has this metadata, this chunk has this, this metadata. It's applied to the entire document. And it's important to remember that when you're actually doing this metadata that you can't take a single document that has different sensitive information in and put uh specific metadata in there. So As an example, We are now as a, at a wizard school. And we want to add metadata to this vector database on the different types of defense spells. That you have. And so for example, you have a student year, so let's just say I'm a student year in year 4. Spell type charms, uh, difficulty level 77, and use cases defense. Right? And so as part of the query that you send using the converse API, you can do filters with this. And the overall goal is I want to filter out anything that's above your 4. And also filter out anything that is not a defense spell. So I'm matching on defense spell and anything that's less than or equal to year 4. And so I can ask that question of the retrieve uh query, go into the knowledge base, and say, can't, what are the defensive spells that I should know? Right? And so I include that uh retrieval configuration and that uh vector configuration with the metadata in order to filter out anything that's above your 4, or anything that's not defensive. And when it comes back, what it's gonna come back with is multiple things. First, the first chunk is gonna come back and it's going to give you exactly what the text was of the chunk. What the location that chunk came from. So if it's an S3 bucket, it's gonna give you the S3, URI, and it's also gonna include information about the chunk from a metadata perspective. If there's additional chunks, you can also get that additional chunk. It's also going to include the score or the accuracy, how close it is to what the user is asking for. And so you can see on here that one of the defense spells is on year 41 is on year 3. Because of what the query is, and it's looking for stuff that's less than or equal to. All right. Now, one of the important things, uh, with anything with retrievable augmented generation or vector databases is permissions. Cause one of the things you have to think about with anything with vector databases is you're taking a data source that has certain permissions, and now you're copying it to somewhere else. Copying it into a vector database. And so permissions is one of the, uh, a very important thing that you have to think of, because permissions could be lost when you're copying. And so let's talk a little bit about that. When you are copying anything from a data source, the permissions that you have usually stays at the data source. Yes, you can copy them over, but what happens if you change the, uh, the permissions that exist at that underlying data source? It can cause things where you have to reindex and other things like that. And so there's multiple different ways. That you can configure permissions to make sure that the only thing that you're returning back, especially if it has multiple different permissions, um, where certain users should only get access to certain data, other users, users should only get access to other data, how to configure that to make sure that you're not leaking sensitive information. OK. And so let's talk about a couple of those architecture patterns. First one, if everybody who's getting access to that vector database is authorized to any of the data in the vector database, you don't have to do filtering for a permissions perspective. You can do it from metadata in order to filter out specific chunks that you don't want, but from a permissions perspective, you don't really have to do too much cause everybody should have access to it. You can do post retrieval filtering. And what that is, is when you receive chunks back, you can look at things like where did this chunk come from, and look at the underlying source database or source data source to see what the permissions are. So one of the examples, and this is the blog that I wrote, um, is S3 access grants. If it's coming from an S3 bucket, you can say, OK, does this user with this user identity or does this group have access to what this underlying data source is. You can do things like per user and per group vector databases. And what this is, is each group or each user is gonna have their separate vector database. So the application is making the decision, should, uh, I send this user to this vector database? Should I send this user to this vector database? So it separates it. And the last one is pre-retrieval metadata filtering, where you can add. A metadata search in order to filter things out. That is a filtering process. It's not using anything with JWTs or anything like that, but it allows you to filter things out as long as you understand exactly what data exists in that data source, which is what the thought bubble is with the data governance. Alright. Last thing with data sources, memory. Memory is one of the things that I said is coming up more and more with agents, which we'll talk about. And there's two types of memory. Short-term memory, which is keeping track of things of the existing conversation. It might summarize, uh, the context of what exists, if it gets too long, other things like that. And then long-term memory, where it maintains things like facts, process information, previous conversations that you can do semantic search on in order to get context with a current session. Right. So the overall goal, just like you have with uh rag or vector databases, is to add additional context as part of the search query that you have in order to send the data that you need to that large language model. So, one of the most important things with this thought bubble is, it is dependent on the application. To configure the memory properly to make sure you have separation with things with memory. So for example, if you only want a certain user to get access to a certain group information, same thing that you did with retrieval augmented generation, that you wanna make sure that you have that separation. OK. One of the way to do that is with, uh, memory name spaces. This is something that we're implementing with, uh, Bedrock Agent core memory, where it allows you to separate memory using a hierarchical format with slashes and other things in order to say, this is one group, this is another group, this is just this user session, um, being able to separate that in a database type format in order to give access to the right data that the user needs to get access to. So Example, Let's say that you are working in a restaurant. And they have a certain uh rule about the number of pieces of flare. That you had to have Every time that you are, uh, working there as a waiter or waitress. And so as part of the memory, you can say, what is the minimum number of pieces of flair that I had to have? And one of the things that memory could be stored in memory is the number of pieces of flair, and it can come back saying that with this policy, you have to have a minimum of 15 pieces of flair, right? It's, it's similar to what rag is. It's just using it in a different way, in different contexts. It's not just unstructured data. This is more structured data that you use, uh, primarily with agents. OK, so in summary. Authorized users interacting with a generative AI application or an agent with an LM Data sources can come from multiple different places. They can come from the query that gets added to the prompt. System prompts. Come from rag, come from, uh, memory, come from tools. A lot of different places where this can come from. But the most important thing to think about when you're building security with data sources is authorization needs to happen before it actually hits the LOM. OK. So, with that, I'm gonna turn it over to Jason to talk a little bit about tools. Jason. Excellent. Thank you very much, Rich. All right, let's talk a little bit about tools. So we went through all of the basics of the large language model itself. We went into some of the data sources using tools such as or using techniques such as retrieval augmented generation. So now let's enter into tools where we want to add additional context into our large language model from other external data sources. So what are tools? So tools let those AI applications uh take actions. For example, you could control a web browser, you could write a file, you could call different APIs. So these are all external data sources that provide additional context or the ability to take actions into the large language model. When you think about using tools with LLMs, you are thinking about how am I going to define why the LLM should choose my tool and then go ahead and define what are the different parameters that my tool will both take in and will generate back out for the LLM to reason about afterwards. All of that is wrapped up in what's called a a tool definition. And so when an LLM receives a list of tool definitions and a user prompt, what happens is the LLM will reason about what tools or if any tools that should be selected in order to uh answer that user's prompt, and then it will generate a tool call parameter, a tool call based on the parameters that it needs to call with. One very important note uh thing to note here is that the LLM never interacts directly with the tool. The LLM is simply generating context, additional text, which is then interpreted by the application. And then the application is what's actually calling the tool so you can see here this is a way for you to automatically instrument your your application so that they're going to do additional security checks based upon identity of user, the type of tool that's trying to be called, any additional parameters, etc. and that way you're going to be able to uh instrument that security outside of the scope of the LLM inside of instead of trying to get the LLM to do that security for you. So let's take a look at what this tool definition actually looks like under the hood. It's a structured format and it's a way that you define what the tool looks like into the LLM. So we have an example here for a very, very simple tool to add two numbers together. So you can see the tool definition has a definition of what's the name of the tool. It includes an English language description of what the tool is. This is very important because this is what is going to provide the LLM the reasoning as to why it should call your tool or not. And then of course there's a list of different properties that are going to be the tool's inputs and a definition of what the tool will be generating as a result. So what are some security implications from these tool definitions? We'll take a look at those in a second here. So if we look at taking our tool definition and input it into the converse query API, here's an example, we go ahead and we say, here's a message from our user, what is 15 + 27? But you'll see at the very bottom of the JSON structure, here's where we add in the list of tool definitions. So we're just gonna substitute what we saw from the last slide right there under the tool config tools object in that JSON structure. And if the LLM is doing their job correctly, they're going to reason that it should call the add numbers tool because that's what the definition of that tool describes, and that's what the user is asking for. And so what will happen with the converse query is not that it's going to come back with an answer. Instead, what it's gonna come back with is a request to the application, hey, can you please run this tool on my behalf and give me the output, and then I can answer the user's query. How did it know what the input for the tool should be? Again, the LLM is going to be generating those parameters for you and putting that into the converse query response based upon its natural language understanding of what the user's query is combined with the tool definitions that you gave it. So to kind of summarize all of that together, the LLM is going to decide based on those tool definitions and the context, any additional context that was there from either your system prompt or user prompts to tell what variables and inputs are used for tool calls and really to do a tool call at all, right? It's going to make those decisions. It has that autonomy and that agency to make those decisions. So it's gonna go ahead and convert that natural language context it's gonna make that decision and then policy enforcement you can implement inside of your application because you are in charge of that deterministic uh call that you're going to make on behalf of the large language model and this is why permissions and identity are one of the most important things that you can do with tool calls, especially when you're talking about interfacing with very sensitive data or actions. So once we get that um you know tool call request into our application we can go ahead and make the tool call we can go ahead and call the converse query API back with the result of our tool call and you can see here in the. User, uh, segment of the request you see the result of 42. That's a result that was given by the tool, the add numbers tool, and you'll notice, of course, as well that we, uh, this is a multi-turn conversation. So every other message both from the user and the request from the LLM to call that tool, all of those are gonna be concatenated into the converse query API because we have to give the LLM that context every single time so it can keep track of what was already said and requested. All of those come as part of your converse query API, uh, your, your, your request. And so of course the response comes back that says uh it interprets the tool result and it goes ahead and gives you a natural language response based upon whatever that tool result was. Now this can get really cumbersome when you are dealing with lots and lots of tools and so this idea of, you know, creating tool definitions, it's a very model specific thing, you know, different models take different types of tool definitions, there can be multiple different ways of specifying them. This became very problematic and so you know. A year ago, Anthropic came up with this idea of the Model context Protocol, or MCP for short, and so that's where MCP comes into play. It's how we can define a standard way of creating tool definitions and exposing them into a large language model so it can consume those and to use those for all of their processing. So here's an example of what MCP looks like just from a communications perspective. You have an application on the left hand side. It has a number of MCP clients, and then those MCP clients will communicate with an MCP server to go ahead and take an action or retrieve a result. That communication protocol can happen either remotely over HTPS or it can happen locally on your desktop or laptop using standard IO. So using MCP AI applications can connect to data sources, tools and workflows. It allows you to access key information and perform tasks. And again, think of MCP as this universal protocol for defining these tasks and these tool definitions so that you can connect your AI into those APIs and data sources. So let's look at how this request flow actually looks under the hood. So when we have these different principles, I have them listed at the top, there are 5 different principles. You have the end user who is um actually using the tool, the uh the MCP host, the application itself. You have the application is the 2nd principle in this diagram, the MCP client, the MCP server, and then whatever backend APIs that that MCP server might be using to get the results. So when you launch an application that's using MCP, the first thing that that application is going to do is it's going to be configured with a list of MCP servers it knows about. And so it's going to go ahead and query for all of the different tool definitions that are available on that MCP server and it's gonna do that through the MCP client that that request comes in. The MCP client calls into the MCP server. The MCP server responds. back with a list of tools. Those tool definitions look just like what we just saw a couple of slides ago with an English level description of the tool with a list of the parameters and the output definitions that gets pumped back into the MCP host. It can squirrel that data away. And now when the user comes back and says, hey, I want to I want you to answer this query, now the MCP host, that application can take the list of available tools, concatenate it with the natural language query. It will select a tool, the large language model will select a tool based upon that and a a set of parameters, which then get communicated back into the MCP server through the call tool command. And then that may, you know, end up triggering additional downstream, uh, requests, right? It could be calling another API. It could be querying a database, right? Whatever that MCP server needs to do in order to fulfill the structured request that was just, uh, that it just created, and then that response comes back. That's a structured response. The MCP server goes ahead and formats that back into the client. And then that gets back into the MCP host with that, that uh downstream application and then that large language model can interpret it and give the user a natural language response. So I'm gonna circle back to that question we had earlier, when we talked about tool definitions, which is what are some of the security impacts of exposing an agent, in this case to an untrusted MCP server. Same thing as we had with those tool definitions. Well, as you can see here, before the user even has a chance to interact with the application, we are already ingesting potentially untrusted data inside of our large language model. Those tool definitions, or in this case the MCP server's list of tools can include natural language descriptions of those tools themselves. And so you can prompt inject directly from those descriptions, and these are the sorts of considerations you need to make when you are exposing your large language model based applications into MCP servers and other tools such as that. Kind of rewinding a little bit, this is how you can create MCP servers. In uh in in Python, in, in code, so MCP servers, as I mentioned before, they expose tools. There's also two other types of resources that you can expose in MCP. You can expose resources which are things like files and other things that have a, uh, you know, kind of a static identifier associated with them, as well as prompts. So these are the three different types of things that you can expose via an MCP server. I'm gonna focus primarily on tools here. So here you see an example of using the fast MCP library in Python to create that very simple add numbers tool that we described before. So you can see in here, it's simply a fact of adding a decorator into your Python application. That add numbers function could come from anywhere. You add that decorator of MCP.tool on top of the function definition. And what that will do is actually take the dock string that was defined there, add two numbers together, that becomes the tool definition for the large language model, and it will parse through the list of parameters automatically, and that will become the list of parameters and their types as well as the return type that the LLM will be expecting. So it actually just kind of does a bunch of magic under the hood for you. And so what happens here is when you do the MCP, uh, the, the communication from the MCP client into the MCP server, it's gonna create this JSON RPC structure which then goes ahead and says, hey, I want to call your ad numbers, uh, your ad numbers tool MCP server tool with these parameters, right, and that's what it looks like under the hood. And use that same flow that we described before where this is a request that comes from the LLM back to the application, the application handles it, and it goes ahead and makes that call for you. So what if you want to require authorization? As part of MCP for an MCP server or a tool. OAuth was recently added into the MCP specification, uh, not long ago in about June of 2025, and so OAuth allows you to do both delegated access as well as service authorization, uh, to tools using MCP. So this allows you to assign distinct user and agent identities so that you can secure those agent uh actions at scale. And there are two different um patterns, OOF patterns that we have seen uh customers use, right? in order to use OOF with MCP. So those patterns are two-legged or off and three-legged off, and we're gonna look at the use cases for both here. And really the reason why you would want to choose one or the other is this main question right here. What identity do you want to use in order to authorize access to this tool, to this data source, whatever it happens to be? Should you use the user's identity, in which case you would be looking potentially at three-legged OOth flow, or is it something where the agent's identity itself, sort of like a service identity, would be sufficient, and that would be more of a two-legged OO flow. How do you decide this? First of all, you think about data ownership. If you are dealing with user-specific data such as emails or documents, uh, then you wanna think about three-legged OO and a user delegated approach versus something that's a system or organization owned data that might be shared resources, etc. Then you might be, uh, able to use that service, uh, approach of two-legged OO. User interaction. If the user's present and can perform an authorization step, that again is a user delegated action potentially versus something where you have no user interaction and maybe an automated system, for example, and that goes into the operation timing consideration there. And then finally, permission scope. So if you're thinking about how permissions they may vary by the user and their consent choices versus consistent permissions that are designed at the agent level. So think about these things, and we're gonna look at how we can actually implement some of these as well. And so identity really at the end of the day is one of those most important aspects of architecting tools securely. And we know that from our traditional workloads. This is not a new technology, it's not a new terminology, but it is now something we need to figure out how to apply appropriately inside of our AI um application infrastructures as well. So how can we actually implement it? Agent core identity is going to be the primitive, uh, service that you will use inside of AWS. If you're using AWS Agent Core, then this is how you can go ahead and implement a two-legged or three-legged OOF flow for your MCP tool calls. So if we think about creating an MCP server configuration that has a tool that we need to provide a user delegated access to. That's gonna be a three legged OO call. So in this case, if we have a tool that is going to search a personal travel log for a, a variety of, you know, here's what I, you know, here's what I went to Vegas on this date and time, etc. etc. that sort of information, of course, should be locked down to only authorized principals to access that. So here's an example of using again that fast MCP Python library. We can see creating that MCP tool, you see that tool decorator at the top, but now we've also included a new decorator, which is the requires access token. And you can see that we've included the idea that we need to use a user federation or a three-legged OO flow for access into this particular function call. And so this is going to go ahead and get us a token that's associated with the end user so that we can make further downstream authorized calls to get those logs, for example, and to uh you know to to uh to to create that request, that response back to the user. Of course, uh, with three-legged Oauth you see that last one where it's on off the URL, so that is going to be the URL that the user will be, uh, redirected to so that they can go ahead and grant that authorization. In contrast, the two-legged OA is going to use a uh a static token, right, that's going to be associated with this particular agent. It's gonna be a machine to machine authorization flow, and so that's going to have uh no authorization URL obviously because there's not going to be a user there to authorize that access, but it's going to provide the uh MCP server here in this case a static token that can be used to go ahead and access protected resources behind the scenes. So again, two-legged OA expects client credentials, whereas three-legged OAuth expects authorization codes with user federation. So what does this mean from a security perspective with tools? Well, first of all, you need to set the right permissions for tool calls and what identity you want to use. You want to understand as a decision maker, what is the LLM going to be authorized. To decide what are the different decisions that you're going to delegate to the large language model? Are you comfortable with it deciding what uh parameters to call to this particular tool? Think of it this way, if a tool has a parameter that says authorized username. Am I going to let the large language model decide what value to put in for the authorized username for my parameter for my tool call? The answer should be no. Right, the identity will be piped through to the tool call through aloth or some external mechanism, and then the LLM's decision making power is limited to what are the different parameters maybe for that query, right, as opposed to who that identity is in the first place. MCP provides a standard way to connect the AI with the outside world and finally OAuth is your standard for communicating whether a user or service is authorized for an action. And again, don't forget that this all comes down to. Putting things into a context window for the LLM. Everything just gets pushed into that large context window for the large language model, and where is the authorization inside the LLM? It's not there, right? Very important, very important to keep in mind. Alright, phase 4, agents. Right? We're moving up the stack, right? We're becoming even more and more complex. Here we have agents where we are delegating even more decision making power into the large language model, right? Before we had very specific workflows that were well defined inside of code and agents we are we are like taking away some of that decision making power and giving it into the large language model now. So an agent. Has a lot of definitions. I think every single software company defines agents differently. The way we like to think of it is as a software program that can interact with the environment, collect data, and then perform self-directed tasks that meet pre-determined goals. So it's a goal-oriented architecture rather than a task-oriented or like a like a, um, you know, interpretation like step 1, step 2, step 3. So in this case, AI agents, they act autonomously, without constant human intervention. They interact with the environment by collecting those data sources, and they're combining that environment, that data with domain knowledge and past context, and they're going to use tools such as agent core memory, which Riggs just talked about, in order to learn from past interactions and to improve over time. So how does this actually work under the hood? Well, everything with agents works with what's called an agentic loop. And an agentic loop is a piece of software, it's a deterministic piece of software. It enables those intelligent autonomous behaviors through this cycle of reasoning, tool use and response generation. So a prompt comes in, inside of this agentic loop, it's the input, and the agent can then go ahead and invoke the model. To get a response, to get a, to invoke reasoning, to figure out what tools that should be called as part of this agentic loop to respond back to the user's query. And so again, the agent is going to take that response from the uh from the model. It could include a request to call a specific tool. And the agent then will go ahead and take that tool request, execute that tool, get data. The data, the result of that data will get back into the agent. The agent will return that back to the model, and this loop continues, continues, continues until the model has decided that the answer has been complete, and the agentic loop can terminate and it will go ahead and return the final response back to the user. So again it's important to realize what parts of this diagram are deterministic code, and what parts of this diagram are non-deterministic AI based large language models. We have a tool, we have a library called the Strands Agents SDK. How many of you have played with Strands in this audience? Alright, I've got a couple of hands up, that's excellent. Strands is a great library that allows you to create production ready agents in just a few lines of Python code. And so we've got a QR code. Code up there if you wanna learn more about it, but what does it look like when you are actually building an agent with strands with the agent class? It's one of the key components of the agents, uh, uh, sorry, the strands SDK. Well, what we did is we actually kind of put together pseudocode for what this looks like. So inside the agentic loop we can see it is a loop. Alright. While true, we're going to call the language model based on the user's prompt. We're going to get a, uh, you know, we're gonna give it a list of tools and the messages that we've already processed through this agentic loop. And then we're gonna see, hey, what does the model think we should do? We're going to ask it to make a decision for us. Should it say that we are done, that's our end turn there, we'll go ahead and return the final answer back to the user. But most likely, the model says that, hey, we need to call a tool in order to further answer this user's question. So I'll go ahead and take a look at the tools that it has requested me to execute. Again, this is deterministic code sitting inside of the strands agent SDK and it's going to go ahead and for each one of those tool requests, actually just. Go ahead and call that tool, right, and add the results back into the messages that are gonna be concatenated back into this uh agentic loop for the next time around. And then of course, you know, you have error conditions and so forth. If I've overflowed the context window, I gotta handle that, that sort of thing. But you get the idea here that this is a constant loop that's happening, and it's basically a loop of LLM what should I do next? OK, I'll go ahead and do that on your behalf, and then tell me when you're done. That is the agentic loop. So what changes with agents in this case then, right? Well, think about generative AI applications, they're really about answering questions, maybe one shot or a couple shot questions. Agents accomplish goals. So agents are all about autonomy. Because they can act autonomously, we are delegating more decision making power into the agent to achieve a goal by planning and making its own decisions. They become more complex because they can be multi-step, complex workflows. They do increase our risk profile. Because we are giving agents more autonomy, we are taking on more risk. Because we don't know necessarily what the LLM is going to decide for us and it's aligned with what we want it to do. So we're going to put guard rails around it. And finally, learning and adaptability. So agents can use memory again, to actively learn and adapt based on the outcomes of the previous actions. So multi-agent workflows, what do those look like? So If you look at different ways that you can implement agents, there's a couple of different ways you can do it. One way that you can do it is by this sort of deterministic workflow. So we'll start with this one where we have an entry point, we have an example where we want to take incoming logs from a SIM. And we want to go ahead and enrich them and figure out what we should do and start, you know, interacting with our ticketing system and so forth, right? We may even want to do some auto remediation actions based upon some existing run books that we have. So this is what this could look like. I'll show you two different ways that you can organize it. So in this first way, we're going to have several different agents. Each one has a specific task, and we are going to wire them up as a specific workflow. So we have our enrichment agent that's gonna query the seam logs. We're gonna check threat intelligence to see what it might know about the particular indicators that we're seeing from the logs, and then that will be fed into a separate agent, a triage agent, to figure out what we should do about it. We can take a decision from the result of that triage agent. We could either execute an automated remediation action which is gonna block. That IP from our firewall or we can just go ahead and document what we did. It might be a false positive, who knows, but we're gonna update the ticket with the reasoning that we have in the resolution agent and that is the end of our multi-agent workflow. So this way we are breaking apart our workflow into multiple different agents and then we're going to go ahead and wire them together in code as you'll see in the next slide. So in this design, again, it's a specific workflow that's followed with each agent having specific tools that they can call. So you can see here in our strand pseudocode, we are defining each one of those agents, giving each one of those agents its prompt for what it should be doing. And then it has a list of just a couple of tools associated with that agent. So the first enrich agent has has two tools associated with it. The execute agent has a tool associated with it. The triage agent has no tools because really the purpose of the triage agent is to simply render a decision based upon the previous context that it's, that we've collected. So you can see each agent has a small number of tools with which it's going to be working. And then what we're gonna do is we're going to go ahead and wire all of that together in this graph-based workflow. So we'll define, OK, we have our should execute action. This is gonna take the result of our large language model's decision that it makes, right, as part of that if then statement that you saw in the, the pseudo or into the flow chart in the previous slide. And then we have the security workflow, it's all wired together as you can see there, where we have a graph builder, we've added a node for the enrichment. We go ahead and put the result of the enrichment into the triage node. The triage node enters into the execution and resolution nodes, and so forth and so on. So again, the point here being is that we are still relying upon deterministic code for the actual workflow and wiring all of these pieces together, but we are now delegating smaller pieces of decision making and contextual generation into the large language model. This is different from another way to design the system with the same goal in mind, in which we are gonna use a single agent. So let's take a look at that. If I wanted to create this and use one agent to do this entire process, I do something a little similar but very different, and it's a nuance. So let's take a look at how this works. We still take our prompting from the user or from an automated process. We put it into this agenttic loop, but in this case, the agent tech loop is going to have access to all of the tools and all of the capabilities all at once. And so the agent. Loop is going to be doing a lot more of the decision making and in fact there is no set workflow here. The agenttic loop is going to rely upon the LLM to make a decision about what to do next for every single step of this operation. So the agenttic, uh, the LLM could simply decide it could query the sim logs and then go straight to updating the ticket without even checking the threat intelligence, let's say, right? So this is delegating a lot more autonomy into the AI system, which can be very powerful. However, you have to, you know, weigh those risks with how you want to, you know, how much control you want to have on how that workflow is actually executed, right? Do you want to be uh more prescriptive about it and use the multi-agent workflow, or do you wanna have more autonomy that you've delegated into the AI and you're gonna use the single agent design? So with this design right again there's one agent it autonomously decides which tools to call and you can see literally there is one agent here in our definition and that agent just happens to have all of the different tools associated with our design at its disposal. So now we have that agent has a prop that says you are a, you know, a tier one network analyst, uh, here are the tools that you have. Give it some alignment and tell it what you want it to do, but ultimately the agentic loop and the LLM will be deciding how it's going to walk through and use the tools at its disposal rather than a deterministic system wiring them all up ahead of time. So compared to the multi-agent solution, there is way less complexity here as you can tell. However, as I mentioned before, you're giving the agent a lot more autonomy to decide what actions to take and when to take them. So how do we put human in the loop in some of these things, right? We would like to have all this autonomy, but we also want to have the ability to have some way to put human approval in there. This is where you can put things like strands, hooks into the into play. So hooks provide deterministic controls as part of your agentic loop. So for example, you can control that autonomy and perform checks as part of the agentic architecture. I'm gonna give you an example of. Of an approval hook in this case. So let's say that we have that high risk security remediation action such as blocking an IP from the firewall. We don't want to necessarily have that happen without somebody looking at that first, lest it interact with our production systems. So in this case we can define our approval hook and we can say here are some high risk tools such as block IP firewall. And so when we see that tool call. We can go ahead and go through and say, here's a request for approval, and now we can maybe send a Slack message, we could send a ticket to the appropriate team, and then they can review it and approve it or deny it before the agentic loop can continue. So there's a lot of use cases that you can have for hooks. We just talked about the, you know, one example, you can have intent breaking, goal manipulation, you wanna understand, uh, you know, and control for a disruptive or deceptive behaviors, all that kind of good stuff. So in summary, what does this mean from a security perspective with agents? Well, we understand now the agentic loop enables autonomous actions while having an ability to still inject deterministic controls into play. Human in the loop is gonna be one of those ways that you can keep, uh, you know, understanding of what is going on with your agentic systems, even if you do delegate a lot of autonomy to them. Agent design does have some implications here. We talked about single versus multi-agent, and of course you always wanna understand the agency that you're delegating into the AI system. So in conclusion. We want to understand that security and risk in AI systems is dependent on what the application or agent has access to and what actions it's authorized to make on its own. So this is a very classic thing here. This is uh uh sensitive data, untrusted data, and external access. You wanna pick two, not all three, because at the center of this Venn diagram is danger, right? If you have an LLM that has access to all three of these things at the same time, this causes security issues. You always want to think about that. All three of those provide a concern. Place security controls outside the model. If you take nothing away from this session, take a picture of this slide, you always wanna have a determinist to control around that data. You wanna validate your input and output, and you wanna use identity to do your tool calls, protecting the entire flow. Agentic identity as a combination of your agent identity and any human identities that you're operating on behalf of. And finally, I'm gonna give you one last slide, which is a list of all of the resources uh from earlier today. We've got a bunch of different QR codes for you to take a look at for our security blog, reference architectures, secure AI landing pages, and then we have over 100 different sessions today related to AI security. So thank you so much for attending. Have a great reinvent. Thank you.