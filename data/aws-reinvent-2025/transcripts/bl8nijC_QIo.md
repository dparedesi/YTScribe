---
video_id: bl8nijC_QIo
video_url: https://www.youtube.com/watch?v=bl8nijC_QIo
is_generated: False
is_translatable: True
---

Hello everybody and welcome to a modern approach to application migration with VPC Latts. I'm joined here today by my good friends. By the way, I am Jamie and I'm joined here today by my good friends Yasin and Ryan, and they'll be going and talking to us a little bit more about, um, using Lattice and how you can use it to not only upgrade and modernize your application but also to upgrade and modernize your infrastructure as well. So let's start out with our agenda. So the first thing we're gonna do is we're gonna basically talk about what we're starting with and the easiest way to show you how to use or to modernize, uh, with lattice is to kind of show you in a practical application with an actual architecture. Um, funny enough, when Yasin and and Ryan and I were building this architecture, we couldn't stop building it well architected because it's beaten into our heads so many times, so we had to add some things in there to say, OK, here are some things that you can improve. So then after that we're gonna talk about what needs to change and why it needs to change, and then from there we're gonna work in how lattice helps like you know what is, what is it, how does it help and some fundamentals. I do understand that this is a 300 level session, but we'll be getting into some fundamentals for some folks that have not used Lattice. How many of you use Lattice? OK, good, so like a good half spread. So for some of the others we can kind of catch up on real quick on some of the fundamentals and describe why we're choosing lattice to do some of the things that we're doing. And then also we're gonna be talking about how we're gonna be um modernizing with lattice like. Obviously, and then the last bit is Ryan's story from Goldman Sachs and how he actually went through this and his practical application of modernizing with lattice. So let's talk about our current landscape when we're talking about anything in business, right, especially when it comes down to IT, everything comes down to a business requirement. Those business requirements are always started with some need of the business, right? Business could be growing or it could be, uh, a change. You might be bringing on a new line for manufacturing or a new product that you wanna release. The other thing too is that we hope that all these businesses grow and as they grow, so do your requirements. Right? And then the other thing too is all this stuff has to happen within a certain time frame, so like we need to pick the best tools that help us get what our business needs as quickly as possible. So what that generally means is is that we tend to adopt more and more things, right? So obviously as we grow the number of requirements as I mentioned, none of the requirements ever grow, you know, one after the other sequentially it's always coming at you all at once and then lastly what we all have done, everyone here has ever built an architecture or built an application knows that things build on top of themselves. It's not like you scrape things away and they get to build new all the time. So when we're putting together this presentation, knowing these things we're like how do we address these with lattice. So let's look at our current landscape and, and what we're gonna be talking about. So here I have an architecture that we had made where we have a provider VPC that's connecting via VPN because they need bidirectional communication over the Internet to our front door, right, pretty much to where we things come in, right? We have our back end which is in VPC too. That's where we wanna modernize from a monolithic application to containers. We have bidirectional private link that's going to one of our acquisitions. How many of you use bidirectional private link? Have you heard of that? Yeah, we've got a couple. It's real fun to maintain, right, because there's a lot of moving parts, and I mean fun sarcastically. We also have our IPV6 offering. A lot of folks who generally deal with health care and in the US or government. This is a requirement. And then we have our hybrid solution, right? We've got a DX going from our acquisition to a database we wanna protect and then we have our firewall. So going through this, we're gonna kind of list out all of the things we wanna do. Our partners, they love the bidirectional, but they hate that every time we do an ad move or change we have to, you know, give them the keys, change IP addresses, all that. So we wanna change that. The next thing we wanna do is we wanna have a better connection strategy for our acquisition. The bidirectional private link works for going to specific services, but as the companies merge more and more, they need to add more services, and that's not quite easy to do with bidirectional because you'll have to add more listeners. You have a max of 50 on NLB. Um, it could be quite cumbersome for any, again, ads, merge or changes. And then next we wanna grow our IPV6 offering. We have to add more things to that offering, but as you saw in our, in our previous architecture, we're using private net and a bunch of things to go from our IPV4 environment to our 6 environment. And again, if we have to grow that, that connectivity becomes a bottleneck and it becomes a problem. And then next we wanna make sure that our mainframe only talks. The mainframe on-prem only talks to our VPC2. We can of course do that infrastructure wise with security groups and whatnot, but we also wanna know if there's a better way. And some, and, you know, as you know, if you're doing things infrastructurally, all those hops and all those pieces need to change. It'd be nice if we can change it in one spot. We also wanna use our containers. This is the big meet, right where we wanna upgrade those, those, uh, monolithic, um, EC2 instances and VPC2 to containers. And then of course with our acquisition it needs on-prem access to our database. There's gotta be a better way than having a separate direct connect. You see it's gonna take us through VPC lattice and how it helps us solve these problems. Thank you, Jimmy. So before we start, uh, asking, answering why no lattice will help, let's do a small refresher on what lattice is. Um, so here we see on the screen, which is, uh, VPCL lattice, we call it an application networking service, and it connects, monitors, and secure communication between services and resources, and you see here on the screen you've got the variety of compute type it supports from the traditional EC tools to the containers and even SeS. We also support compute outside of AWS in a hybrid scenario. It also supports databases, and we can do those communications over multiple protocols. And the last but very important is it also allows you to enforce your security requirements while keeping the monitoring and observability. So now let's take a look at the different building blocks that Lattice offers. The first thing, the core of the service is the services. So here you've got your application with any type of compute that supports, and you will put that application into one or multiple target groups. Once you've got that. You have this logical group what is what we call a service, and with that service it basically allows you to expose your compute as an endpoint and then you'll be able to apply different routing rules, load balancing options, and authenticate both policies. So that's the first building block. The second one is the resources. So you might create one or many services which I just described, and you've got the second type which is the application resource. And here basically it's where you put all your TCP enabled destination. It could be an Amazon RDS. It could be a DNS name, or even an IP address. And to configure that application resource, you will attach what we call a resource configuration. Next is the accounts. So with VPCLA is you might have your service and resources in one or many accounts, and here you see that we've got a bunch of services and resources in account C. And you might have those services be consumed by either, uh, either the same accounts or different accounts, and all of this cross-account communication is supported, and they will talk through what we call the VPCLLD service network. So as I said, there's the the con the concept of providers and consumers, and I'd like to first clarify what we mean by that. So here on the screen you see there are 4 services and what we call a provider is a service or a resource that you provide to the service network by either associating the service to the service network or to create a resource gateaway. So those are what we call providers. On the other hand, for the consumers, they'll be the VPCs and the service endpoints that are associated to the service network, and they will be able to consume the services that we have exposed. Another point I'd like to touch on and clarify because me and Jamie get that question a lot is how does VPC lattice compare to Transit gateway. And while they are very different services, they can both live very happily together, but because we get that question a lot, I think that it's good to clarify the differences that we can have with those services. So I'm sure most of you know what Transit Gateaway is by now, but when we talk about Transit Gateway, we really talk about what we call a core networking service, and it's the service that allows to connect all your VPCs and hybrid and create that central hub of networking, but we're mostly staying at layer 3 and 4. On the other hand, with VPC lattice, we're more on the layer 7. That's why we call it an application networking, even though we can support any ETCP based destination. But we have more a managed service that simplifies that communication between those services and resources, and you have that extra security, networking and observability built in. That's why we call that the application networking service. So that's the main difference between the two. Another difference is also with the pricing comparison. So here I wanted to show you if you fully remove a transit gateaway by VPC lattice, what the cost would be. So we start with the transit gateway model and for every application you will have one load balancer. So if you have 4 load balancers for a certain amount of traffic, it comes down to roughly $1300 per month. If you want to replace that architecture by VPC lattice, then you will need to create one service network and 4 services, and then that can come down to $750 per month. So that's to give you a comparison point between the two services. And I'll let Jamie talk about our migration strategy. OK, thanks, Cin. So now let's get back to that architecture and actually start doing stuff now that we've got some of the building blocks that you've seen has told us about. So bringing back the architecture just again to remind you of what we're going to talk about. And now let's get into it. So the first one was partners were complaining about the VPN, right? What can Lattice do to help us with the VPN? Now we know that lattice is multi-account, right? So it's a fit there. We also know that Lattice is bidirectional, so there's a fit there too, and it also has inherent security. So where do we start? We gotta create our lattice network first, right? So let's, let's just kind of take a look at that. So the one key piece for sharing lattice across accounts is RAM, right? So our resource access manager will allow us to share our service network. So the first thing we gotta do is create it. We create our service network and then we associate our VPCs to the service network, and that's gonna be in account one. The second account is gonna share their service network and account B and C will go ahead and share their services and we can then connect them together and Lattice will then watch based on policy which we'll get into in in the presentation, um, and as well kind of watch that traffic as it goes back and forth. And then lastly we have to create the services, right? That's where you create and share so you create the services on our side we're gonna associate the service network we've already shared our service network out on their side they're gonna create their services and join it to the service network and share it to us. So if we go back to our architecture, right, I'm gonna go ahead and just concentrate on the front door and I'm gonna create my lattice service network and I'm gonna go ahead and move that in. Now the thing that's important to note here is I didn't remove the VPN right away. I don't need to. I can do both and we can get people going and customers on board and partners on board without ripping and replacing what they're used to. There's no shock to the system. You also notice that the partner right now has a firewall and that firewall is to protect the traffic going back and forth, but because we have policies, those firewalls aren't needed. So we've got a couple of places that we're gonna put policies and then we generated some generic policies here so you know don't copy and paste them. These are, these are just examples, but what we're doing here is we're saying, OK, we're gonna accept traffic right from our front door and our front door has to have a token that says front door before we're gonna allow that traffic to go through. We're only gonna allow them to do a get and we're only gonna allow them to do a get on 3 different paths because we wanna restrict it down. We're a partner. We wanna be secure. The next place that we would put a policy is on the service network. Now normally we always say service network you wanna make it as coarse grained as possible, right? This is not the place you wanna fiddle with as much as you or as little as you can because the blast radius is wider, but in this case because we're sharing our service network out. To a partner we're gonna require a couple of tokens before they can talk on the service network and if we add more services, which we will, you just add more tokens, right? So you can make it as simple or as complex as you need it really comes down to your business use case. And then lastly, of course, our front door we're gonna do the exact reverse that we did for our partner. So we're gonna say, OK, here's the certain URLs you can do, you can only do a get and if you're talking to me, you have to have the partner one token. So we can lock all that stuff down. Once that's all up and set and ready to go, things can go ahead and start talking. Right, so we'll go ahead and add this to our architecture. Now we're starting to slowly clean things up. And we'll move on to the next So the next piece we wanna talk about is policy hierarchy. I just mentioned about coarse grain and and fine grain. Let's just kind of put this all in perspective in one spot, right? So the service network, that's where you want coarse grain policies, right? That's where you wanna be able to change the major things. Some of the examples you might see in our public documentation would be you have to be a part of my organization to talk, right? For us what we're doing is we're saying you have to have specific tokens to talk because we're sharing that out. And then the services is where you get more fine-grained as you saw, right, only allowing a get only gonna go to these speci specific services, and that's it. And then of course we've got the resource configuration. I'm gonna go back one for a second. Resource configurations, they are a little bit different, right? So when you add a resource to a service network, that resource, everything has access to it. So you're, you're gonna rely on your services to go ahead and tighten down who can and cannot talk as well as your service network. OK, now moving on. Accident button click. So let's talk about acquisition connectivity, and I hand it back to you. See. Thank you, Jimmy. So let's go back to our scenario with our acquisitions. And you know, uh, our leadership ask us to simplify, uh, our strategy when we acquire a company. Uh, how many in this room, you know, how you, you worked and you had to consume services or export services to a VPC that had the same IP range? How many of you had this overlapping the famous overlapping IP problem, uh, and there's nothing wrong with dealing with overlapping. Uh, IP, the only thing here is that because we want to consume resources and expose our own, we create what we call a bi-directional private link and again there's nothing wrong with using that architecture. It's just you can see at scale how the operation overheads might get really hard to manage. So let's see how we can use, uh, VPC lattice to, to help with this, and you will see that, uh, VPC lattice. Handles overlapping IP perfectly fine, and I'm going to walk you through how exactly the service works so you understand how does it deal with that. So when you have your service network, you want once you attach your VPC to that service network, it gets a VPC lattice link local ENI, and that ENI gets an IP from the 169254 address range, which is the link local address range. Once this is done, the route table gets also an entry with that IP address pointing to the lattice. And then the final step is you create a service, you expose it, and you will use a DNS name. Here you see on the screen it's a very, it's the default generating name. You can also use a custom DNS name. So what will happen is once you do that DNS resolution to the service, it will point to the 1169 IP address. It won't point out to the IP in the other VPC, and that's how you handle the overlying overlapping IP basically VPC that is makes that problem a non-issue. So we're gonna apply this and update our architecture. So here we've got our VPC 2 and the acquisition and we already have the service network that Jamie created. So what we're gonna do the same way we've done on the first step, we're gonna create our acquisition service and our back end service. Once this is done, we associate that to the lattice service network and here again we leave all the other components in place. We can do our testing, everything, make sure that it works perfectly fine. And once we are happy with the results, we can remove all the components and uh simplify the architecture. And the same way we've used uh the policy to replace the firewalling before, we're gonna apply the same concept here. So from the back end here it's the same policy type we're going to allow the acquisition to talk to us with the get request on a specific path and we will use the acquisition token. And to this, uh, to make this work, we'll also need to update our service network policy to allow the token to talk because remember you need to both um the service network and the service policy to allow the communication to work so we've done that on the back ends we can do the same on the acquisition service, same thing, same story, allowing the back end to talk to us using a token on the various paths. Now the question is, I've got a policy that talks from the back end to my acquisition. Can the back end still talk to my front door, front door, sorry. And the answer is yes. All you need to do is you need to update or edit your policy and add the new section that will handle that communication, and that's a very powerful feature because now your team, when they build the service, they can edit their policy and the security requirement can grow organically. They all have to edit the old policy to add the new requirements, and that will work. So now we've got our back end talking to our acquisition through service uh Lattice Service Network. We've also got our front door service talking to our back ends. Everything works perfectly fine. So let's add this piece to our architecture. So here we have dealt with the partner service. We've done also the front door and our back end and acquisition, and there is another piece that now we will tackle is the V6 workloads. So that's the new requirements we need, as Jamie said, when you deal with healthcare or government customers, they have this requirement and you need to talk to them over IPV6. And the way we do it now is we're using that private net gateaway to do that translation between V6 and V4. And we want to simplify this. So the same way as I said, lattice handle overlapping IP, it also handles the IPV4 to IPV6 communication. So it becomes also a known problem, and here we're going to do the same way we've done before. We create our V6 service. We add it to our service network and once we're happy with everything we can remove that private net gateway because this is no longer needed and then we have a working business workload. I'm not going to show you the policy here, but it's the same concept that we've shown you until now. We adjust the service network policy and the service to make everything work. So now that it's done, we've got our V6 talking to our bike ends. Everything is fine. Let's add this last piece to our architecture. So now we've modernized the partner service, the VPC, the front door, back ends, the acquisition, and the V6 workloads. So let's see what's come next. So I promised that we would talk about um hybrid as well as modernizing our application, our, our, our big piece in the middle, right, the, the uh um monolithic to EKS. Let's, let's, let's tackle the hybrid and you'll notice that as Yin and I are going through this whole thing what we're doing is we're kind of picking a lot of low hanging fruit. Right, we're getting lattice in the front door. We're getting it, uh, on our, our, our network, and we're starting to use it and see the capabilities, and I strongly recommend that's the best approach to start adopting lattice is look for these low hanging fruit things that we're doing and then do that before you do the big move because as you'll see that is a lot easier afterwards. So let's talk about our hybrid. So we wanna make sure that our mainframe only talks to our back end services and we wanna give it a path using VPC Lattice. We like the idea that lattice looks over our traffic, that service network switches the tokens every 15 minutes, right? We, we, everything that we know for sure using IAM is secure because that's how we even log into our AWS. We wanna take advantage of that and not have to change all the little pieces. So as you can see here, I've got my direct connect. I've got my uh transit gateway, and we already have our existing lattice network. Now the thing to note is that at no time is the transit gateway or the direct connect going to disappear. Lattice suddenly did not get the ability to go right to your on-prem, so we still need it, and this goes back to that point that Yasin was mentioning where lattice and Transit gateway work well together, and this is one of those examples. So what are our options to connect our database in? You remember that I was talking about uh service resources or you seen actually was talking about the resources um so we're gonna use one of those pieces for controlling resources called service network endpoints. So we put in a service network endpoint which just like private link grabs it grabs a local IP address, right? It's actually gonna grab a range of IP addresses, but it grabs an IP address that's local to that subnet and then those EC2 instances all they have to do is talk to that that particular um service network endpoint to gain access to those services. So if we're gonna apply this concept of using the service network endpoints with our mainframe. We're gonna go ahead and adopt that now. Remember that goes ahead and it gets on that network. We're gonna be well architected and we're gonna put a service network endpoint in both of our, um, subnets, but as you can see, the initiation of the traffic is gonna come from our mainframe. It's gonna go through Transit gateway. Transit gateway has its, it's, uh, connections already into our VPC and it has its endpoints, and it's gonna flow through the service network endpoint. Now I would have made it flow through both service network endpoints, but then this slide would be a mess. So we're just showing it through one, but in fact it's going through both. And now that we've adjusted our policies and we've got our communication all set up and we're allowing our communications from our policy for our uh back door or our from our back end to say OK we're gonna allow from this IP address from our mainframe. We're gonna add that to our architecture and again at no time does our transit gateway go away. It stays Direct Connect stays, um, but now we have a path and we know that our mainframe is going to and through our back end service. Now granted there will still be a couple of pieces because this is going over Direct Connect and transit gateway. Where we need to put in some security, security groups and and things of the like, right, but it's a lot less now that we have to change and we wanna add more of our services and lattice to, to go ahead and have that mainframe talk to it. We just need to go to those individual services, edit their policies, and say the mainframe can now talk, and it'll be able to talk immediately. So the last piece we have, the, the, not the last, but the second last piece that we have is the modernization of our application. Now we've already adopted lettuce. We've, I've already told you guys that we're doing the, we're gonna, you know, do all the low hanging fruit and make this part as easy as possible. So let's see what that looks like. So I have my VPC, right? I'm gonna go ahead inside my VPC. I'm gonna create another service now this doesn't have to be in the same VPC as Yasina told you. This works well with cross IP, and I've seen some instances like, say for instance, if you have to upgrade an EKS cluster like how we force you to upgrade every 6 months, some folks, right, have IPs baked into that, and it's very difficult for them to go ahead and put it side by side. So if you don't have like an extra database in this, we threw this little wrinkle in here just so we can talk about it, um, just create another VPC. Right, whole new VPC, create a new cluster if you want to do your cluster upgrade and then wait it. And say I wanna send 40% of my traffic to one. I wanna send 60% traffic to my new, and then when we're happy again, both of them are being used at the same time, at no time are we ripping the band-aid off and, and, you know, basically giving our customers a negative experience. We can just go ahead and consolidate and remove the older one. So granted, I do appreciate. That going from, you know, a monolithic EC2 to microservices and clusters, there's a lot of application work there. But a lot of times there's a lot of burden on us networking folks to also help that along. But again, because I'm using lattice and I'm doing the whole low low hanging fruit thing, this was really a non-issue for me. So I'll go ahead and add this to our architecture. And now that we're looking at this part we can see that we've now modernized our partner acquisition we've got our V6 workload we've now fully modernized and we have a path of doing continuous upgrades to our back end services and we can do the same thing if we wanted to to our front. And but we have to be a little careful because the front end service also has that Internet access coming in, right? So that's could be a little bit more disruptive and each of those pieces we've shown you a way to adopt lattice side by side with your existing configuration so that you can go ahead and cut over as you need, not immediately. It's not like, OK, I've had this new service, everyone's on this now, we're gonna be doing this, uh, you know, right before my kid's birthday or something, and then something goes wrong and, you know, you get yelled at. I, I'm speaking from experience. So then what about our database? Remember on that other side. We have something about our database connecting to our acquisition. Yin's gonna take us through that solution. Thank you Jimmy. Yeah, what about our database, the rest of databases, uh, so our acquisition, they needs to access that on-prem database, and right now they have a separate direct connect connection. They want to keep those things separate. They don't want the acquisition to get access to the rest of the network, so they've done, you know, their, their own cooking there to have their dedicated direct connects. But we want to change that and make things a little bit easier and eventually remove that second direct connect connection. So now we're gonna use the piece I was mentioning before which is the resource gateway and how we're gonna apply that here. So before going a little bit further, let's have a, let's take a look at what is the resource gateaway, and here on the yellow part it's really your ingress points of the traffic, and then the back ends will be defined in your resource configuration. So it can be either public DNS name or it can be an RDS database or even an IP address. So once you create the resource getaway, you attach a resource configuration where basically you define what is the back end of that sings response. And remember when you add RDS instance as the database scale up and down, the resource configuration gets automatically updated. You don't need to change it, but please keep in mind if you use IP addresses, then you will need to do that work yourself. It won't necessarily be automatically updated. So once you've got that resource configuration, you'll be able to connect to that resource gateway either by using a resource endpoint, which basically is your kind of a private link access to the resource gateway or through the lattice service network. And your clients in that subnet can be connecting to the service network either by associating the VPC to lattice or using the service network endpoints. So let's have a look at how we can break down the different steps to update that architecture. So as we said, we've got our database. I'm gonna create my resource gateaway, create my resource configuration, and I will define the IP address of my database. Once this is done, I can attach my resource gateway to my lattice Service network, and from that point on, I do not need that secondary direct connect connection. And here we can make sure that the acquisition of VPC do not have access to any other resources from on-premise because that resource gateaway only defines the back end to the site. So here it can only use, only connect to the database. It cannot access anything else even though it's using the direct connect connection. And see and here you see how the traffic flows between the acquisition backends all the way down to the on-prem database. So let's add this piece to the architecture. So here, you know, we have, uh, if we summarize what we've done so far, we showed you the starting architecture which was sort of very functional, but we had different, you know, different problems overlapping IP addresses, V6 to V4, modernizing our back ends. And we showed you how progress, like how you can modernize those step by step, and here to arrive to that final architecture we've got everything in place and you still see the transit gateway. So as I said before, it does not necessarily need to replace fully the transit gateway. You might still need that for another type of communications, but now everything talks through the lattice service network. We use our policies to enforce our security requirements. And now I'll hand it over to Ryan who's gonna explain to you how he's using VPCLRs. Hi everyone, I'm Ryan McDonagh. I'm with Goldman Sachs, and I lead the technology for our managed continuous deployment platform that we call Fast Track. Now, a number of my colleagues have presented FastTrack at different AOS events, but we primarily focus on what FastTrack does in terms of evaluating policy as code to enforce our security baselines. But one thing we haven't really covered is how we perform networking with this platform. And today we want to cover how we do networking in Fast Track and how we're now leveraging VPC lattice to enhance our network capabilities. But first, let's Talk about what Fast track is. So it is our primary continuous deployment platform for getting applications into AWS. We launched this in 2021 to really improve developer productivity and reduce the number of manual reviews we've had on applications. So to achieve this, we took a shift left approach in how we do policy evaluation and front loaded all this into pre-deployment. So our applications are all authored in AWS CDK. And when we pre-deployment, we synthesize that CDK application to cloud confirmation and from there our guardrails evaluate policy against the generate confirmation templates. So at this stage we're looking at things like, you know, does an S3 bucket use a KMS key? Um, is it the key owned by Goldman Sachs, and, or, you know, is, you know, the bucket publicly accessible or not, um. But there are some resources that uh we can't really cover pre-deployment right so we do have to layer in things like SEPs and RCPs and even permission boundaries to fill in some of the gaps now for networking we've. Use a collection of VPCs that are shared to user accounts and so let's take a look at what that what the workflow looks like. So when a user requests an environment, this is all self-service by the way, uh, anybody can come in and request a new environment and when we do this we provision two accounts. There's one account that is your service account. And this is the target of the CDK application. So all resources in that CDK application are gonna be deployed to the service account. The other piece is your pipeline account, and this pipeline account is gonna be exclusive to that service account. And this is going to run an AWS code pipeline that is gonna deploy your application to the service account, right? Uh, both these accounts are associated with, um, OU in our organization. And once that's done, everything is set up and we provision things like roles for break glass access and detective controls and additionally we also do some house cleaning like disabled regions that we're not gonna support and remove the default VPC. So once the accounts are ready, users can get to work and start pushing code to get and when this happens, the pipeline gets kicked off and then we run the guard rails. This process runs a standard CDK synthesis and deploy, but in between there we evaluate our guardrails, and if the guardrails pass, then we allow the deployment to proceed. If it doesn't, then we fail the deployment. So our guard rails not only check the configuration of cloud formation resources, but we also use it to gate different types of resource types. So for example, if AWS introduces something brand new, we have to go through a review process and we eventually onboard it. But one resource type that we've blocked up until now is the creation of VPCs. So you have to wonder if we are blocking VPCs, how do we do networking. So for different application teams or business units depending on the granularity you choose, we create a VPC and if and we share this to user accounts using VPC sharing if you're unfamiliar with VPC sharing, this is a mechanism where you can take a VPC and define it in one account and then share that VPC through RAM to a number of participant accounts. You're gonna do this, uh, against an OU. And that VPC can get shared to all participants in that OU. And if you wanna know more about VPC sharing, you can look at the routing loop, and you can see Jamie and Alex, and they have a really good discussion and get that in really in depth. So we chose this for a few reasons. Um, one of the things that we wanna do with FastTrack is to make the cloud a bit more approachable, and some of our users, they have really great expertise in AWS and cloud in general, but some teams, this is a new experience for them, and we don't wanna inundate them with having to learn things about ciders and all networking concepts that they may be unfamiliar with. A lot of these users are gonna be. Coming from an application focused background and having them to find their own VPCs and networks is a daunting task, so this really helps simplify that onboarding experience. Um, and from our needs it allows us to control the network and we control the perimeter and all the things that go on about it, so we felt this really fit our needs really well. So the way the shared VPC works is we have a, we define a VPC for teams and we define two network zones. We have a routable subnet and this is basically a set of subnets that uses IPs from our, um, our own network and with the right firewall rules this will allow you to reach endpoints that are deployed into that subnet and then the internal subnets are just, you know, they're not routable from our on-premise environment. But this is where we're gonna provision things like your AWS private endpoints and we also define all the endpoint policies and make sure that those are in alignment with our risk controls and things like that and to make sure that we can only access GS managed resources and access upon by GS principles. And then like most organizations there's gonna be services like your SDLC endpoints, identity, HR, EMDB, all those types of services, and a lot of these services predate Fastrack so they're exposed via private link. So what we do is we create these end points and we configure all the DNS configurations for these end points so that when users get on board these things are there for them. So when they provision their accounts from the workflow that we just talked about, they're gonna be assigned to the same OU as the shared VPC and then through RAM these subnets are now are visible in each user's account. Now what's interesting about this is when you log into the console, now remember we delete the the default VPC this is gonna appear like a VPC that is in your local account and but the nice thing about this is all your connectivity is preconfigured for you out of the box and all you need to do is know what your VPC is. Now, Working with a shared BBC is pretty much just like working with a. VPC that you've defined on your own, there are some limitations. You're not the owner of that VPC, so you can't modify the network. You can't create a private hosted zone here. You can't, you know, add or remove endpoints or create your own endpoints. Now to some users that might be a step backwards, but in a lot of cases this is fine, and this allows us to maintain control of the network, and it works rather well. Um, the only thing you have to know again, you have to figure out the VPC ID and thankfully CDK has a nice mechanism to look up these VPC IDs and resolve that and again you start working with it just like any other VPC and you can provision resources into it, no problem. So we found that this model worked really well out of the gate. It allowed us to really simplify the developer on boarding experience and have everything preconfigured for developers before they even provision their accounts. Now you have to be wondering why are we talking about VPC sharing in a talk about VPC lattice. Well, VPC sharing isn't without its challenges. There are gonna be some workloads that simply aren't gonna work with, uh, shared VPC. So as a financial institution there are some workloads that are gonna need much stricter network isolation. In this case, a shared VPC isn't gonna cut it, but one of the bigger challenges you're gonna find with a model like this is resource management and IP starvation issues because your routable IP space is gonna be, it's a finite resource, so you have to manage this. And this, these types of issues are a bit of a slow burn. They're not gonna happen out of the gate. They happen over a longer period of time because when you provision these VPCs in, in this manner, you're doing it at a point in time with the information you had at the time that you set it up. As time goes on, you know, as, as Jamie was saying, you know, your, your business improves. You're, you think you're building new things and adding things that weren't there when you created the VPC. And so you run into things like we mentioned IP starvation. You've got noisy neighbor issues and then handling resource quotas gets to be a big challenge. Now again, as we, as things start to come online, if we go back to that, you know, team one and team 2 and let's suppose we have a team 3 and that maybe has a custom vendor integration or that account represents something that, uh, you know, it's an acquisition and we need to punch holes into our endpoint policies to reach something external. Now the question is you have to ask yourself is do I wanna open up these endpoint policies for all participants in that shared VPC or just one? And the more of those types of use cases you get, the complexity of that endpoint policy gets really gnarly, and it may not be something you wanna go down and you really have to think about how that's gonna work. And then finally is getting the right granularity is really tough, right? Unless you're the application team, you're not gonna know how to right size this VPC and how many accounts you can associate with that VPC. And you may think like, oh well we can just switch to VPC ID. It's really not that simple. It's really a one way door and if you do need to move VPCs, this is now a migration, so it's not as simple as you might think, but we think we have a better option. But before we go there, let's go through some of the things that we want out of a next generation networking architecture. So we already talked about the stronger network isolation requirements, um, and we wanna avoid these resource contentions so we don't have people fighting for the same resources in the same BBC. But then there's some other things we wanna get out of this as well that we aren't able to capture today in the shared VBC model and get visibility into what endpoints are being exposed to what consumers across different accounts and we still wanna retain this simplified developer experience for networking so that enterprise services that we have already plugged into our shared VPC can be used just as easily as they were in that model. So let's take a look at what this looks like with Lada. So now we've got our account. We've provisioned these, um, these two accounts here, and now we have no more shared VPC. So now there's nothing in here. We update our guard rails now to allow the creation of VPCs and we also introduce a VPC, uh, sorry, an AOS CDK construct that builds this VPC in a shape that we would prefer teams use, but one thing that we do. Uh, do not permit is the creation of things like a net gateway or an internet gateway so that these don't have unintended access to the internet. And in addition to that, we leverage block public access which makes sure that even if someone's able to circumvent our guardrails, we no longer we don't have the ability to get out on the public internet unless we've explicitly done it, and this is done at the organization level. So now we have these VPCs that have overlapping ciders, but how do we get access to the services. So these shared services that we have that were previously using private link we've now updated these to expose them as resource configurations and in another platform managed account we create a what we call a shared services service network and this is basically a a service network that's gonna contain all these services. And so our construct then can take this. Associate these VPCs with the shared services network and now we have the ability to reach these services but from two different VPCs that have overlapping ciders. One thing that is really important to note here, and this is something that is a relatively new feature of resource configuration, is we don't need to create hosted zones in each of these VPCs to map the correct, uh, custom domain name to each service, and that is not gonna be a responsibility of the individual team accounts. Instead, the resource configuration can declare its stated custom domain name and without any interaction from team one and team two accounts they can resolve those services on the same FQDNs, right? So we don't need to do any of the things that we're doing with Private link and the name that is being associated to these services is. Declared by the service owner and not by the application teams. Now I wanna talk about private link because there is some confusion about, you know, can private link, do I have to convert everything to lattice if I'm using private link, and the answer is no. Private Li and Lattice work happily side by side. So in this case we have, you know, Team two has a vendor relationship, and that vendor is using Private link. A lot of vendors are still using Private Link, and they're not gonna switch to lattice just because you, you wanted them to, right? So here this works just fine, and Lattice and Private Li work great side by side, but I wanna point out something. In this model that is we think is a little better than what we have with the shared VPC model, we had to take the same vendor connectivity in shared VPC, you're now exposing that to all participants in that VPC, right? So in this case we wanna make sure that team only team 2 has access to that endpoint and not anyone else. So now we're problem solved. Now let's suppose that team 1 and team 2 want to create a, you know. Maybe it's a bidirectional private link, right? We could go that route, but we instead what we can do is we can create a new account that is gonna define a service network that is exclusive to the application team, and this is independent from the shared services network. So using a service network endpoint they can add in an additional service network. You know what you're thinking, well, why can't they just use the shared services service network? And the reason is, is we have different permissions on how we share the shared services service network versus the team one. With the shared services service network we control the permissions that on how RAM shares that service network. So the only users that would be able to associate services or resource configurations to that service network is going to be the platform service teams, right? So the your SDLC team, your. Identity team they'll be able to create associate services with that service network but for general consumers they won't be so this isn't a place where we wanna have everybody in the organization to just dump services, right? If you need to have your own service network you can spin one up define what accounts need to access it and now that is a private service network that is for your work group, right, instead of having an entire VPC. So one of the challenges that we had in on boarding this is how do we get all these service teams to leverage uh VPC lattice, right? We don't wanna say hey everybody is a new work item for you and go do a bunch of stuff, right? But thankfully it's pretty easy. So if we take a look at your, your high level private link architecture, you, you have your compute resource that is your service and there's gonna be a network load balancer and a private link service, right? So how do we get this to work with a resource configuration so. One thing we don't wanna do is we don't wanna disrupt the existing private link customers, right? Those are still gonna exist and we don't have to do anything. But what we can do is create a resource configuration and we can create a resource configuration from an ARN. We can create it from an IP address, but we can also use a domain name. And so in this case we simply take the domain name from the NOB and this is the AWS assigned um. FQDN and we use that to create a resource configuration and now we can take that resource configuration and associate it with the shared services network and instead of having all of these accounts request to be allowed listed for every single private link end point we can get everyone in the same OU path or um in the entire organization to have access to the shared services network so this really helps these use cases where you have a service that you wanna share in a one to many fashion. And you've got a very large number of accounts. So what do we get all of this is now we have a much stronger network isolation story. Uh, we don't have issues with resource contention or IP starvation, right? All those resources are exclusive to the owning account and we've got a really nice story around how we handle cross account access so we can use lattice, we can use resource configurations, and we can still use private link when needed, right? And these one to many use cases are very easy to solve now, right? We don't have to allow list a bunch of little accounts everywhere else. But I think the really important thing is that we now also have a very good story around simplifying the developer experience when we on board because just like with the shared VPC model we have a mechanism now where someone can bring up a VPC. And all their connectivity needs are met out of the box, at least the majority of them, not the bespoke ones, but this is a nice simple way to get this done and now and maintain isolation. So I don't want this to be like a VPC shared VPC versus lattice conversation, but the other different things. But we, we really think that they shared VPC has applicability for specific use cases, but at scale it can really present some challenges, so. Um, we've been pretty happy with how this has worked out so far, and we're looking forward to seeing more with, uh, with Lats down the road. I think that wraps it up, and, um, I think, um, we'll take questions at the door because they're shutting us down, uh, to turn the room over for tomorrow. So if you have any questions we can take it over there and thank you again everybody. Please, uh, remember to fill out the survey. Um, it's important to let us know how we're doing, and, uh, we'll see you at the door. Thank you.