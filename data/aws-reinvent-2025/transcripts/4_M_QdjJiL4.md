---
video_id: 4_M_QdjJiL4
video_url: https://www.youtube.com/watch?v=4_M_QdjJiL4
is_generated: False
is_translatable: True
---

Welham All right. Welcome everyone. Thanks for joining today. Today you can learn how Bundesliga uses GEI to scale content production and to enhance fan experience. And for those who don't know Bundesliga, it's Germany's premier soccer league. My name is John Michel. I'm principal data scientist with AWS and I'm the technical lead of the partnership between Bundesliga and AWS. And I'm very delighted to be on stage today with Alexander Eisenhoven. Director, product and Technology at Bundesliga, and Arthur Hardtwick, senior software developer also at Bundesciga. As an introduction, Alex will talk about the partnership between AWS and Bundesliga. Before walking us through Bundesliga's product strategy and core principles. And then we're gonna dive deep into scaling content production, and we'd like to do this by showing you three products of Bundesliga, which are prime examples of scaling content production. After this, we're gonna segue into fan engagement. And again we're going to do this by showing you an example of Bundesliga. Throughout the whole talk, we're gonna emphasize how Nova's price performance enables Bundesliga to scale content production and enhance Fed engagement. And with this, Alexander. Kick off. Thank you. So welcome any football fans here? Please raise your hands. Perfect. Any soccer fans? Yeah, perfect. So, uh, first test, perfect. So let's start. So welcome to the Bundesliga. Um, there are over 1 billion people over the world who are interested in the Bundesliga, and our goal as the DFL, the governing body of Bundesliga and Bundes Bundesliga 2, is to become the most fan centric football league in the world. And AOLS is helping us to come closer to this goal day by day. So we actually started building on AWS since 2016 and since 2020, AWS is the official technology provider for the Bundesliga, and we have been changing the game since then. In 2024, when we renewed our contract with AWS, many companies were still stuck experimenting with Gen AI. They were working on prototypes. But we were actually already in production with our first G AI solutions and served them to over 100,000 Bundesliga fans in our app. AWS and the DFL both share the same mindset when it comes to customer centricity, and now AWS helps us to drive our innovation. And delivers us, helping us deliver a groundbreaking fan experience and deeper insights around the game of football. Within our partnership, we have set up those 3 pillars you have just watched in the video before. So the first one is focusing on data, specifically sports data, where we build on the success of Bundesliga Matchfacts and for example develop new technologies for referees to take faster and smarter decisions. And the second pillar, we talk about media production, and here we leverage Gen AI technologies to scale our content production and localize content for our global audiences. The third pillar, my favorite one, we create unique personalized fan experiences to engage fans within our D2C ecosystem. So how are we going to do with this? First of all, compared to other football leagues, we are in full control. So the the DFL is covering the whole value chain, and we call this our glass to glass strategy, which starts at the glass of the camera lens in the stadium and ends with a glass of the smartphone or the TV where fans are watching Bundesliga matches live with our partners like ESPN and Sky. Or they are, they are consuming viral content within the Bundesliga app. And with this unique setup, we are able to build and commercialize products end to end. Through this this organizational setup, we have various touch points with our fans. So there are over 50 million people on social following us. And we are on track. To generate over 5 billion video views on our old social channels this season. Over 10 million people every weekend alone in Germany are watching Bundesliga with our partners. Or they take part in our self-hosted events like the Franck Franz Beckenbauer Super Cup, and they are maybe already customers of our commercial licensing partners like EA. And our goal is to bring all those fans into our D2C ecosystem and especially the Bundesliga, where we then collect 1st and zero party data. To personalize their fan experience within the app. And this enables us not to only increase loyalty and retention with our fans, but also it enables us to develop new business models and generate future revenue streams for the DFL and our stakeholders. When we rebuilt our app last year, we set up 5 guiding principles for the Bundesliga app. So the first one is about personalization. Everything when a fan comes in through the through the app should be personalized based on their favorite club, their region, their heritage, and their fan interaction. The second one is continuous pathways, so we want to bring in users in those constant content consumption loops where they can interact with content and find new stuff. The 3rd 1 is about video and story centricity, so we believe that video enriched media is the core of our app and is designed to drive retention within our fans. The 4th 1 is about finding new content, so discoverability where users can can find new and relevant content for them, and if they have a question, they will get the answer from us. And the 5th 1 is engagement. So we really see in our data that engagement drives retention and that people are actually interested to take part in quizzes, voting, and can engage with our content. In the next few minutes, my colleagues will show you 4 examples where we are trying to get closer to those core principles, and we start with scaling our content. So Arthur, let's go. Thank you, Alex. The The first project I'd like to present are the Bundesliga match reports, where we see that our fans love to read about uh match reports after the game has concluded. And they might have missed the match or they were unable to watch due to time zone differences, but our editors, they are swamped with work during such a match. They have to watch the game and compose live ticket entries. They in the meantime also compose Bundesliga pushes. And compose match stories. And we found if we automate this task we could alleviate the stress of our editorial staff and increase. The performance. Of such products, so let us take a look how such a metropod actually looks like we have an introduction, we have. Um, embeds, we have videos and paragraphs with associated images to them, and at the very end we have, um, statistics with the fantasy heroes. So we can see that we have a rather predefined structure when it comes to match report as we looked at all previous match reports. We have an introduction, as I said, a pre-game discussion which is often covered in the live blog commentary. We have the first and the second half which is discussed also in the live blog. We have stats and MVP which is decided by the editor, and all of those paragraphs are complemented with images which were taken during the match by licensed photographers, and we can tap into vast data sources like uh the LifeLock commentary which is composed by the by the editors themselves. We have match event data which contains um. Not only the clubs which play and everything else, but we have match statistics where we have, for example, the winning probability which might drive an LLM to uh focus on different aspects of a match that has been played. We have historical match data which is interesting when it comes to derby matches where we want to uh focus on the teams rather than the outcome. We have team lineups where all the correct spellings of all players are uh a part of this data. So we have all the all the recipe parts we need for actually come up with a LM workflow to generate such match reports. In the beginning, an editor kicks off this whole process. And we ingest all of the relevant match data into our open search database either during the game or after the game, and we use AWS lambda to transform this data. Into Into this type we need to actually uh pull off the prompt where then if the metropolitan has been generated a human editor can uh look at it and publish it in web and app app but how do we come up with um. Selecting the appropriate images for each of the sections that the LNM generates. Mm We opted to use a multi-modal prompting approach for this, where we first generate the match report text. And supply some assignment instructions and provide blobs of all the images which were taken during the match and get back references to those uh images and recompose the match report. The downsides to the often used vector search approach is, we have quite a high latency when it comes to generating such match reports. We use up a lot of tokens and. Most LMMs are limited to 20 images, but we know which images to take. We don't have to search any database. We don't. We don't have to come up with search phrases because we we can supply the image blobs to the LM directly, so this seemed like the perfect approach for us. And when it comes to actually prompting, we have a straightforward approach as well where we supply all the content blocks we need, we have the matchup and all the relevant data available. But as you might. See in Yogeni applications we are facing very Um, common pain points like hallucinations when it comes to incorrectly referencing stats, we have incorrect paragraph styling when it comes to the editorial content guidelines on how such text should be generated, and quotes were incorrect sometimes and we wanted to use British English instead of American English. And our solution to this was to not only have a human in the loop. Be in charge of everything from start to finish, but introduce a review by Amazonova at the very end, to help and assist the editor in correcting those generated match reports even quicker. So the human in the loop, the editor kicks off the production of such a match report by selecting the match and selecting a persona which might be different on the matches played, for example, like I mentioned before, derby matches often focus on the teams rather than the outcomes, but some matches. Uh, they were rather boring at the first half and were very interesting in the second half. You can supply additional instructions to focus on specific parts of your generated match report, all as an editor from a central content management system. And the generated matchpod. And then gets reviewed and Amazon Nova tries to find all the style uh inaccuracies and fact checks everything that has been provided and validates all the quotes. It's not 100% correct all the time like most LLMs are, but here are two examples of, um, what, uh, Amazonova has found and, uh, suggested to the editors. For example, spelling has been corrected and the editor can then manually. Um, edit the text appropriately and the top scorer was incorrectly mentioned for the statistical element at the very end of such a metro report. Um, we can summarize that having such an end to end solution where the editor kicks off everything, it is in control of every step saved us about 90% of the editor's time where he previously started writing such a match report in the second half, he now can with one button generate such a me report and save 90% of the time and use this time to generate. Not generate but write other articles and other other content pieces which amounts to about 20 additional content articles for editors during such a match day. Amazonova, as per our editors, um, found approximately 70% applicable corrections, which is very good and uh very fast. And with this we go over to the 2nd. Um, product we recently released with the Bundesliga stories where the Bundesliga match reports are. A long form content. Um, we see that the engagement in apps is, um, driven by the younger cohort by more engaging and, uh, image centric, uh, media rather with Bundesliga stories we wanted to create. A highly engaging short form swipeable content much like Instagram stories basically where we focus on. Images and the storytelling and our solution would be to repurpose existing articles which editors have already written. And the images embedded in those articles, alongside with our vast database with about 150,000 new images coming in every season and transform them into those multiple sets of slides of different types. And assign proper and engaging images to all of them. Um, let us take a look how this looks. Content creation. With the help of Amazon Bedrock and Amazon recognition, we create Bundesliga stories. We transform articles into engaging slides for our app users within seconds, and our fans, they love it. We're now able to scale our content production like never before by automating routine tasks with AI. We are empowering our editors to create better experiences for Bundesliga fans worldwide. As you can see we have here 3 examples of um slides generated by this process. We have a regular slide which is a text and a description. We have a quote slide with an engaging image and a preview image of a video which you can play in the app. Um, how, how did we proceed in architecting this, uh, LLM solution? We used a 3 step, uh, step function approach for this where we take the Bundesliga article in the first step and transform it into separate slides, each of them only with text and associated metadata. In the second step, we take all the images which are associated with such an article and much like in the match stories I presented earlier, we use computer vision to assign the appropriate images to the generated slides. We know that those images are relevant because they've been used by the editors for the original article already. And we use Amazon Nova for this because it's the best in price and performance and respond very quickly to our queries, but most of the time we generate more slides than there are images in such a story, and um that's why in the third step we use a vector search approach where we try to tap into our metadata store database where all our images are contained in. And find the appropriate images to complement the slides. Um, we asked the editorial team which images to pick. And they gave us 4 aspects on selecting such images. First of all, uh, relevance, we want to complement the storytelling, uh, with, uh, relevant images. We don't want them to be disjunct from, uh, the actual content presented. It has to match the mood and the intent of the slide. Uh, second focus, we want, uh, to have the teams or the players focused in each of our slides. Most of the time we have player images from matches, but we have interviews and, uh, other media which we take. And the third aspect are motives. We don't want to have, um, stadium images on all of our slides. We want to have a different variety of images present on the whole story. That's why we want to select them by the scenery, for example, not only stadium images, interview images, close up images. That's what, what we call, uh, motifs, motifs. And the fourth aspect is recency. We want to use the most recent images available. We don't, uh, we want to have the most recent jerseys and visible in those images, and we want to feature the most recent games. That's why um when we take all those four aspects into account, we have to think about how do we ingest our images. To make this all possible to select by those four aspects, and we came up with um a synchronous and asynchronous approach to the image ingestion workflow where in the synchronous approach. Or task rather, the image is uploaded by an editor or automatically ingested, and only the active data is extracted and stored in our metadata store, and the image is directly available to be embedded and recised dynamically and served via cloud front. Asynchronously we then determine the match related metadata, for example, competition ID, season ID, and we do this oftentimes heuristically where we, uh, determine, uh, based on the fixtures and the teams and players visible which match day and which match uh particular image is about we detect all the player faces using Amazon recognition and for embeddings we're using Amazon Titan multi-model embeddings. For the last aspect, the motive, we trained, uh, Amazon recognitions, uh, custom label feature with different categories. For example, action, celebration, goal, and we have several subcategories like, uh, general shot tackling, um, behind the goal, uh, sideways from the goal, and we trained them each with, uh, approximately 1000 images. And this is how this looks like for an editor when he's composing or uploading uh uh an image we have all the appropriate uh tags directly available in those images and. Uh, we, we know we have a very high detection rate of, um, our players because at the beginning of each season we take, uh, high resolution 360 degree shots of all of our players, uh, during the DFL media days which is very important to actually detect all of the player faces for all of our images uploading. Um, when it comes to actually scoring the images for selecting them. We let the LLM generate not only a search phrase but the motive and the player supposed to be visible on the on those images. And then we've come up with a scoring algorithm where we take the similarity of the generated search phrase phrase of the LLM. Which is a normalized value between. Uh, 0 and 1, then we multiply this by the motive boost where we detect whether or not a suggested motive is present on. Uh, On a, on an image which is being considered, and uh we multiply this by 10%. And lastly we give an additional 20% boost to any images which are most recent and as a developer I thought we just give a wholesale 20% boost to all recent images like the last images of 250 days, but this was actually not what the editors wanted. Um, they wanted a distribution and my suggestion was a linear distribution, but they actually value more, more recent images exponentially higher than older images. That's why we took such an exponential bias curve. Where we apply all of our calculations. And uh we stopped giving any uh multiplier boost um after 250 days of the image being taken. And if we combine combine all of those aspects together we can uh filter for the for the images and supply in Amazon Open Search Service a scoring script which lets us calculate the score for each of the images in our database and just quickly to see in the first line we calculate the similarity which the generated search phrase uh has um. Has been provided in the in the second paragraph you can see we calculate how old is the image and if the image is older than 250 days, we don't uh give any additional boost to those images we apply our. Exponential distribution bias curve and at the last step we only look whether or not any of the motives or subcategories of our suggested motives are present we multiply this value and we automatically choose the first image which has the highest score basically, which is most of the time the image the editor would have picked actually. And this is how the output uh looks like um for an editor and what the LOM has generated it uh suggested to use a. Text slide for this particular uh story and uh has come up with a title with a description it's suggested to be play a Harry Kane be visible which is part of Bayern Munich and a gold scene to be present um we have a search phrase which has been generated and a depiction is only there as a helper for uh to steer the LLM as a train of thought to help generate such a cohesive picture. And now that we have this whole solution in front of us, we, we know that without this comprehensive image ingestion workflow we weren't able to provide such a an algorithm. For our editors to use in the generation of such artic uh such uh stories we know our custom scoring script finds the perfect image in our vast database and with Amenova we can um assign images quickly which are already present in the article. And we know those Bundesliga stories have impact um. Bundesliga fans who use the app and uh engage with stories have 40% higher time spent in app. They occur for 70% more sessions and 20% uh increased one week retention. And if we compare this process by uh creating a story from an already existing article. To writing such a story from scratch, editors save about 80% of time in that, uh, in that endeavor, and Amazonova reduced the cost for this crucial step for us by 70% and is as fast and as as other L&Ms and with this I give it over to Jean Michel. For the next part. Yeah, thank you, Arthur. So, um, we looked so far in, uh, scaling content for fans, and we're now gonna segue a little bit into scaling content, uh, for partners of Bundesliga. And uh here can you here you can learn um how to build an on demand video localization at scale to content that you also might offer to international uh customers. So Bundesliga has more than 1 billion fans around the world across 200 countries. Um, however, Bundesliga produces most of its content in English or in German. And if you put yourself into the shoes of an international fan, how would you prefer to consume soccer content? Most likely you'd like to consume this in your own native language. And it's important to know that Bundesliga does not stream content directly to fans. They they offer a high quality product to broadcasting partners. At the moment, these partners need to invest time to localize that content, which limits the global distribution of that content. And that is why Bundesliga built a video localization that removes that language barrier and increases the global market reach. So first I'd like to show you a small example of this and um I would ask you to pay specific attention to the voices in the following video. He is the undisputed leader up front for Bayern. His special qualities lie in how he controls the ball and gets his shot off. It's a joy to watch Kane. In great control, Kane, brilliant. Game after game, week after week, King Kane is delivering. Beautifully controlled, expertly dispatched. His ability is clearly in finishing, but he can also score with his head, having done so 8 times. Harry Kane, a poacher's goal. So I think you could observe the two voices. First of all, it's a narrative voice which was translated from German to English, and there were live calls which were kept in the original English language. So this video is editorially produced with different specific components and to meet the high bar of Bundesliga, also the localized video needs to have the same editorial design. And just one word on the languages, so we picked a video in English to make it easily accessible here for this presentation, but Bundesliga targets markets from Latin America to Middle East to Asia and according languages. And this solution is offered to media partners of Bundesliga through the Bundesliga media portal. In the media portal, the media partners can discover the content in English or German and then request it in their native language. Once they make the request, that hits our API powered by Amazon API gateway. An Amazon um API handler takes that request and puts it on SQSQ and that's when a step function kicks in. The step function can be composed of different steps for the localization. Uh, but a few examples are, uh, the translation, which we do with Amazon Bedrock, transcription with Amazon Transcribe, and voice generation with an AWS partner solution from Deep U AI. Once the video is localized, we put it back on S3 and make it available for download to the media partner. So the core logic of the localization really happens in that workflow. And um we learned that the localization workflow needs to be tailored to the actual products which Bundesliga produces. So Bundesliga produces more than 20 products and all of them have different editorial design. So now we have a list of different steps which you can always choose depending on which product we want to localize. Um, it also allows us to use different inputs for different products, so some videos might have a shortlist, which describes what is present in a video, and that is very helpful for our localization. And only if we use both, so all inputs that are available to us and um preserve the editorial design of the video, we only then meet the high quality in the localized video that is actually required. And as an example of such a workflow, we start with demultiplexing a video, so that means we take away the audio tracks from the raw video signal, and that is because we found that processing the audio separately makes it faster and more cost efficient. Then we transcribe the uh the audio checks using Amazon Transcribe, and we also use an LLM based correction, for example, Amazon Nova Pro, to iron out a few mistakes in that description. Once we have the transcription, we run Amazon recognition to um segment the video. That is really important again to assign the editorial design to the video which we are processing. Once we have the segmentation, we go into the actual translation. With the translation we generate the new content, so voiceovers are generated and new subtitles, and then we multiplex again together the video and the audio tracks and possibly also the subtitles. So for translation we found that LM based translation is the best choice for us. And here we use either models from Anthropic or from Amazon Nova. And to make that call, which LM to use, we first need to establish an evaluation. And we settled on the word error rate for evaluation, so a word error rate measures how many words need to be changed, removed, or um updated, uh, depending on the translation made by AI to make it turn it into a correct translation. So with translations, there is this problem that for a given input text, there are multiple correct translations. And we solved this problem by using machine translation checks. So in that case, we make this AI translation first. And then give it to a professional translator. This professional translator would take the I translation and make minimal changes to make it correct, and that is then our ground rules going forward. And here's an example. So we have an AI translation to English, which has 3 sentences. A human translator checks this translation and changes two words. And changing two words in the sentence results in a word error rate of 5 to 5%, and that gives you an intuitive understanding of what these numbers mean. So a word error rate of around 5% means changing two words in a text of about three sentences. So with the Metric at hand we can now compare different methods for translations and as a first approach we compared Amazon Translate against Amazon Bedrock, so LM based translations, and we found that with LM-based translations we got lower word error rates, so higher quality translations, and that was mostly because we are able to avoid idiomatic errors in the translations for the sports commentary. Then we were comparing different LLMs for different language pairs we want to use for the translation, and we get word error rates as low as 2% or up to 5 and 7% depending on the language pair. So our approach was that we always choose the LM which gives us the lowest word error rate for this language pair. If we see matching results, we prefer to use Amazon Nova because of the price performance, we can reduce costs and latency of the translation. And speaking of quality, so there's one cool feature I haven't even spoken about. So our solution can automatically learn from human feedback to improve the translation. If you remember from the beginning, The media partners use the media portal to request a localized video. But they also have the option to review the result and resubmit the localization to get a corrected version of the localized video. So that is a very important feature for the media partners because it gives them control over the translation and ensures they have the high quality of translation they need. At the same time, this is also very valuable to Bundesliga because we can now process this human feedback. In this case we do this with Amazon Nova to turn the feedback into a correction rule which is then applied to other translations going forward to avoid the same mistakes, and that is how the solution learns automatically. And to make that a bit more tangible, let's have a look at an example. So let's assume a media partner requests to localize a video from German to English, and it contains this small sentences here with the word uncomfortable, which is not a great fit in this sentence. So a media partner goes in, change is uncomfortable to challenging. We process that change with Amazonova. And derive a rule, and the rule could be when translating from German to English, do not use the word uncomfortable when describing how tough a team plays. We put that on Amazon Bedrock knowledge base. And next time a media partner looks into a similar translation, we first look into the knowledge base and did we have a similar translation in the past, and then we avoid the same mistake. So now we use the word challenging instead of uncomfortable. Alright, so to sum it up, uh, Bundesliga built an AI powered content localization which streamlines the content production for international content at scale, and we found that the solution reduces the processing time when, um, localizing a video by 75%. We also found that when we use or make use of the price performance of Amazon Nova Pro, we can reduce the cost by 3.5 times. All right, so now let's shift gears. Um, we looked into scaling content production through 3 different products from Bundesliga. And we're now gonna look into enhancing fan engagement. And here you can learn how to use Gen AI to deliver a highly personalized experience to your customers. So Bundesliga knows that during the live matches around 80% of the fans juggle with different apps to get live data. In in a younger cohort, 70% chat during the matches, and Bundesliga wants to put an end to this second screen chaos. And that's why they want to provide the one-stop shop experience for fans which democratize the access to statistical data. And that's why Bundesliga built a fan experience which is currently under private preview and has the working title Matchma. So let's assume uh Berlin versus Munich is on. Um, and we of course support Bayern Munich. Berlin scores the first goal and we get a notification of that goal uh in Matchma. Then our team scores. In that case, Harry Kane scored the equalizer and again we get a scorecard with a celebrating image of that player, but we also see more so Matchma does research on that. Um, on that goal and provides some context to it. So what does this goal mean to the player? What does it mean to the club? And that is just how a live commentator would do this. So now we want to know what is the impact of that goal, and we ask Matchma through natural language, show me the top 5 of the table, and we get the live standings, and we see no surprise Bayern Munich is on top of the table, and we want to look a little bit further what is up next for Bayern Munich and again we just ask that through natural natural language. Then we get uh the next uh lineups and the next fixtures of Bayern Munich. So here the conversation could stop, but Matchma re-engages with us and offers a statistical analysis of the upcoming game. And we see that Bayern dominated the matches. But then we want to dive a bit deeper, and now we ask a statistical question to compare the both teams, so Freiburg and Bayern Munich, and MatchM shows us how the last matches went between the two teams. And then we can also look into um to get some some rich content in Matchma so we can easily ask show me our goals of Harry Kane scored in the half, second half at home, and first of all these goals are listed, but Matchma has a video search in the background which then presents these videos directly to the fan, and they can then consume it in the app. All right, so let's have a look at how this is made possible. So once a fan raises a question. That is through a back-end service put on an event bus powered by Amazon EventBridge. And then passed on to the chatbot service. And the chatbot service responds with a natural language response to that question. But we've also seen that fans can ask for videos, and that is enabled by integrating the chatbot service uh to a video service. But what makes Matchmade really fun is that it also proactively pushes relevant content to us, and that is what the nudging engine does. So the nudging engine uses events of the of the matches to understand what is relevant content for the fans and pushes it proactively into the app, like the research of the of the goal, which is interesting to us. And now I'd like to dive dive a bit deeper on how the chatbot service works, but also how the video service is enabled. So let's start with a small question, which teams score most in the final 15 minutes? So this is actually a statistical question, and we need to turn this question into an SQL query to respond to it. And that's what we do with a text to SQL workflow. Once we have the data at hand, we can formulate the natural language response back to the fan. For the first step to generate the SQL query, we use LLMs, and here we use either Enthropic or MZNova Pro, and we're gonna look into this in a second. So once we have the S2 query, we run that against Estina to pull the data from S3. And with that we can formulate the answer back to the fan. So now let's uh let's think about what questions fans can ask. There will not always be such simple questions as we see here on the slide. So there will be a vast variety of questions that will hit Mama. And if we would use one static workflow for all these questions, that would not be very cost efficient. And that is why we introduced dynamic workflows. So in a dynamic workflow we try to understand the question or the type of the question first and then adopt the workflow to have a very cost efficient workflow to answer that question. So in the first step for a question like how many goals did Harry Kane score, we try to answer what type of question the fan has, and that we do with Amazonova Lite. And we look into the query type which can be an individual player stat or a team stat, it can also be a comparison, and we look into complexity and here we differentiate between simple medium and complex questions. Once we have the complexity, we route that question through different paths. So for simple questions, we can make, uh, we can take advantage of the price performance of Amazon Nova Pro to answer it. For more complex questions, we routed through Sonic 4. And actually we found that around half of the question can go through the simple path which allows us to reduce costs quite significantly. So once we know through which model we want to route that question, uh, we now build the actual prompt to to run it, and that also makes use of the classification that we have done upfront. We also use it to pull a few examples which we use then for a few short learning in the prompt. So with that we run the S2 query against the LM which we picked. Then run it against Athena to get the data and come back to the fan with the answer. So we've seen how the chatbot service is able to respond to fan questions for statistical questions, but if you remember from the video, fans also have the chance to ask for videos in Match Made, and that is made possible through the integration of the video service. And let me possibly pause here for a second. Because if you think of video search, you might not think about a chatbot in the first place. But in our case, the fans, when they're looking for videos, they're looking for specific moments in Bundesliga matches, and that's what we use the chatbot for. We use the chatbot to identify these moments and once we have them, we we run a search on the metadata of video to find the videos and play them back to the fans. Let's have a look at this. So let's assume the fan asks, show me the last goal of Harry Kane. So first of all, we need to identify the moment in the match when the last time Harry Kane scored. So the chatbot service, coming back to the classification of the dynamic workflow, understands, now this is a query of type match event, so a match event should be returned. And in Bundesliga that is an event identifier which says for each event in a Bundesliga match that is the exact time stamp when it happened. Once we have the event identifier that can be passed on to the video search which runs a metadata search on Amazon OpenSearch to return the videos which capture this exact moment. Once we have the videos, we can play them back to the fans. So we discussed how Bundesliga makes personalized and interactive content accessible to fans. We found that Match Made enables Bundesliga to scale their personalized content by 5 times per user. With the dynamic workflows, we can make use of Amazon Nova, which allows us to reduce the costs by 35%. And with that, I hand it back to your ex. Thank you so much. So coming back to our conclusions, so I think Amazon Matchmade, you've just seen it, uh, as Jean-Michel said, it's not live or public yet, but I've been personally testing it for the last few weeks, and let me tell you it is awesome. So Matchma is everything what we've built over the last few years. So from UX content, sports data, AI and tech finally coming together in one personalized product within the Bundesliga app. So watch out for the release next year. So wrapping it up, um, so today you should have learned that Bundesliga is leveraging Gen AI and data to become the most fan centric football league in the world. And we are using, using this to develop new digital business models. We showed you how we localize videos for global audiences and scale content production with match stories and the match reports to and offer every fan an AI powered fan companion with our Bundesliga app and within all examples we use Amazonova as a key in infrastructure to run those Gen AI workloads in production in a cost efficient way. So that's the end of our presentation. One last note, we have another session tomorrow from the Bundesliga. We'll give you more insights about the chatbot service John Michel just presented and we finish it up with a small video.