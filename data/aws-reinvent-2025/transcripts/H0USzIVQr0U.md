---
video_id: H0USzIVQr0U
video_url: https://www.youtube.com/watch?v=H0USzIVQr0U
is_generated: False
is_translatable: True
---

Good afternoon everyone and welcome to day one of reinvent. We're very pleased to have you all join us to discuss training high performance AI models on AWS using Sage Maker. I am Michael Oguike. I'm a senior product manager on Amazon. And I work on AWS Sage Maker where we help customers train high performance models. With me today is Tomomori Shimomura, who's a principal solutions architect who also works on SageMaker and helps customers train AI models. We are very pleased to be joined by Denny Gupil, who's also a principal machine learning scientist from Roblox, and they're gonna be sharing an exciting use case with us. Today, We're gonna go through the need to train large AI models. We're gonna discuss the challenges that we see in training these large models, and then we'll discuss how Sagemaker helps to resolve these challenges then. Tomamori would come back up and show us an actual demo followed by um Roblox with Denny coming back up to show us a real life example of how they've built a 4D foundational model using Sage Maker and Hyperpod. Uh, before we go on, by a quick show of hands, how many of us in the audience have trained or customized a large model? OK, and how many of you have done that using SageMaker? Great, so there's gonna be a lot of uh capabilities that we share and a lot to also learn here but. Before we even go deeper, I think a couple of you may have noticed the growing popularity of this sparkle icon. You may have seen it on your reinvent app where it's helping you to try to get more out of your week, or you may have noticed it on your playlist where AI is trying to help you create a better playlist or even on autonomous driverless cars on the road. For me, even while building this presentation as I was trying to crop photos. AI was there trying to help me crop my photos better, but behind each of these icons is a trained model and as your customers and your users are getting more used to using AI to improve their experience, you're also gonna be needing to add more capable models to the experiences that you already deliver for your customers. And in fact we see this in studies. A study by McKinsey showed that 47% of companies who said they were using generative AI were also training or customizing these AI models to better serve their use cases. Now, we built Sagemaker for AI. As we've seen the trend, um, moving on forward for you to help to build more capable models for your customers, um, it's important that we have the right capabilities for you, our customers to build and train and deploy these models. We started this journey in 2017 when we launched SageMaker Training jobs. SageMaker Training Jobs offers you a fully managed API where all you need to do is bring in your training data. You bring in your training script. You tell us which instances you wanna run, and we take all that input, we spin up a cluster, we train the model, and we deliver the model artifacts to an S3 bucket. It's an ephemeral cluster and you only pay for what you use. And our customers had a great experience with that and they continue to use training jobs. But then we had customers like you ask for more capabilities such as being able to manage the cluster using Slm or Cubinetes. We had customers asking for more granular control and observability to these clusters, and there were also customers who wanted more persistent clusters so they could both run training jobs and inference jobs on the same infrastructure. So we launched Sagemaker Hyperpod to meet these needs, so. In the conversation today, and the two key capabilities that we have are 1, SageMaker training jobs, which is the ephemeral compute, or SageMaker Hyperpod, which is more persistent. These are the two key services that SageMaker offers for you to train your models. As we've continued to work with you and other customers to learn about what matters most to you for training large AI models, we've seen six key dimensions 1, computer availability. 2, performance. Then there's resiliency. There's observability. Number 5, ease of use, and most importantly, cost. Every lever that we move on each of these 5 dimensions has a direct impact on your cost, and in the next couple of slides, I'm gonna dive deeper into the 1st 3 computer availability, performance, and resiliency. And when Tomonorri comes up, he's gonna go deeper into observability and ease of use when you, when we walk through the demo together. With computer availability, let's first of all look at the trend that we've been observing over the last couple of years. We've seen that over the last couple of years we've been needing to use more compute to train more capable models. This is primarily driven by the fact that we need. More training data and we're also training these models with more parameters so they can be more capable, they're more accurate like using them in cases like in the legal field or in the medical sciences or in autonomous driving. We need bigger models so they are more capable. Now over the last 2 to 3 years we've actually gotten to the point where we're now using. 10 to 24 flops of compute power to train these very highly performance models and the scientific notation for 10 to 24 is wait for it, it's a yota flop, but let's make that more concrete. What's a yota flop? 5 Yota flops is the equivalent of running 1000 P5 GPUs consistently for one month. That's a lot of compute power. That's very expensive, and that's why it's essential that we optimize the utilization of these GPUs. But we also want access to the most performance compute so we can get our training jobs done quickly. To do this, Sagemaker offers. Multiple capabilities for you to get the best compute. Whenever you want it, so we have a wide selection of you have GPUs and accelerators from the H100 single GPUs to GP 200 ultra servers. You can get single GPU jobs or even a 72 GPU ultra server within a rack depending on your needs. But that's one thing, selecting the GPU. We also understand that your business needs and your timelines are different, so we have various options to secure capacity. Whether that's using on-demand capacity or you can get sport capacity for up to 90% cheaper than on-demand capacity and in fact, just last week on Sagemaker Hyperpod we launched support for sport instances which is great for fault tolerant workloads and experimentation. Or if you want a longer term reservation where you have guaranteed capacity, you can get a 1 year reservation or 3 year reservation with reserved capacity. And if you're looking for something in between where there's a calendar period, say next month, um, you want to get capacity for a particular period of time, you can use the flexible training plans. Well, Now that we've got GPUs, what are we going to do with them? As important as it is to get the compute capacity. It is even more important that we utilize them effectively to avoid wastage with SageMaker you have the option to use the training jobs managed API where you just submit the job and we handle the scheduling and the distribution of the jobs across the GPUs, or you can also use SLORM and Cubinetis depending on what your scientists are more familiar with. Or if you have a team where you have multiple team members and you have different type of jobs and you wanna prioritize the different jobs you wanna allocate capacity to different teams within your organization. You can use a capability we call task governance on Hyperpod where you can allocate resources to different teams or you can give a higher preference to say an inference job over a training job and whenever an inference job comes it preempts the training job. We also have an integration with AWS batch that also allows you to do this with Sagemaker training jobs. All these capabilities come together to ensure that you have the right compute capability and you're able to optimize utilization of these GPUs or accelerators so you reduce your cost for training. We then talk about performance and to discuss performance, let's quickly demonstrate the relationship between the amount of memory required by the parameters in a model compared to the available memory per GPU for a lot of the latest GPUs and accelerators. On this chart we have two trends. There's one trend that's showing the higher incline which shows the number of parameters in a lot of the popular models over the last 10 years, and we can see like we talked about earlier, this growth in the number of parameters required for these large models. On the other hand, on the gentler slope, we can see the slope on how the memory per GPU is changing over time. And we can see that the number of parameters is growing faster than the memory per GPU. Now, the implication of this really, we can demonstrate this with one example, say the LAA 370B model. The amount of GPU memory that we need to train a 70 billion parameter model, we can calculate that by getting the amount of memory each parameter requires and on average, the rule of thumb is like 18 to 20 bytes when you account for the weights and you account for the gradients and the optimizer states. So we say 20 bytes times 70 billion parameters. That requires almost 1.4 terabytes of data of memory on the GPU. Now, even the Nvidia H100 is 80 gigabytes of memory, so you do not have enough memory per GPU to load and train the model. So to overcome this scaling challenge, we then have to do what we call distributed training. And some of you are probably familiar with this, so I'll go over this quickly, but basically, for example, in the LAMA 70B model case where we have more, where we have more uh parameters and more memory from the model than we can put on one GPU, what we then do is to shard the model across the different GPUs and distribute the training across the different GPUs. And as training happens in step synchronously, the different GPUs can then communicate across each other to update their state. But then the data is small enough for us to replicate the data across the different GPUs. So each GPU is actually doing a micro batch of the training and then updating state as they go along. And that's model parallelism where we split the model but we replicate the data. Conversely, if we have a large training data set but we have a small enough model, say a LAMA 3.21 billion parameter model that we can fit onto 1 H100 because again it's uh 20 gigabytes of model versus 80 gigabytes of GPU memory. In that case, we can split the training data across the different GPUs. Replicate the model across the different GPUs, do the training in batches and have all the GPUs synchronized states as they train over steps and epochs. One thing we might observe here is at this point the networking. Becomes very important because the GPUs have to update state and communicate with one another over time. If we even step one step more complex where we are both distributing the training data as well as sharding the model, we've shaded the model across various layers in the neural network we've also shouted the model across different tensors. So in this case we're doing a hybrid of both data parallelism and model parallelism, but what's most important here is that your networking can quickly become a bottleneck. You need high bandwidth, low latency communication between all the nodes to ensure that you're not spending more time than you need on the training job. Sagemaker offers multiple capabilities to ensure that you're getting the most performance high-speed networking when you're running these training jobs. First, we have an automated, easy setup where you can use the CLI, SDK, cloud formation, or even the console to set up your distributed training job. We have access to multiple popular frameworks like PyTuch, Tensorflow, or even Ray. And on the communication libraries, you can use Nvidia Nickel or if you're using ranium instances, you can use the neuron Collective communication libraries. And for high-speed networking, you have access to the NVL technology from Nvidia or for internode connectivity, you have access to elastic fiber adapter from Amazon where you can get up to 3200 Gbps of bandwidth. All these together help to ensure that you have an efficient distributed training set up where your GPUs and your training jobs are are connecting and moving at a very high speed, which is essential for reducing your time to market and being cost efficient. Finally, we talk about resiliency. To discuss resiliency, we quickly bring back our favorite single GPU. Each of these GPUs has a mean time to failure. And when we scale these GPUs to a large number, say 100s or 1000s of GPUs, we then move from a GPU level resiliency. To a job level resiliency because every failure from every GPU in the cluster can potentially interrupt the progress of your entire job. And we've actually seen this in research, so research from Meta that was published in IEEE early this year showed where they ran an experiment on two large state of the art clusters for about 11 months. And if you follow the lower line, they saw that for jobs running with 1,024 nodes, the average time to failure was about 8 hours. So that's 3 failures in 1 day. One for breakfast, one for lunch, and one to wake you up at 2 o'clock in the morning with a page. But how does Sagemaker help us resolve these issues and prevent us from getting paged? We have a four-pronged strategy mitigation, prevention. Detection and recovery. And the way we achieve this is basically with mitigation we encourage and we make it easy for you to checkpoint your training so you're taking multiple safe steps. So even if something goes wrong 2 days, 1 week into the future, you're not losing all your training progress and we're able to restart from a checkpoint sometime this year we also launched manage managed tiered checkpointing. Which basically reduces the penalty of having to checkpoints frequently because there could be network penalties or even storage penalties, but with managed tiered checkpointing you have a more efficient, faster checkpoint process and you see that uh when Tomonorri also shows his demo. For prevention, we have multiple types of health checks with standard health checks or deep health checks where essentially before we add a node to your job, we do a burning test and we really pressure test these instances so if there's actually a problem with any of these instances of GPUs we can isolate that instance from the cluster so it never even makes it into your cluster. For detection, we have automatic health monitoring agents that are continually running through the cluster and running through your job to identify any issues and resolve them. And then finally, for recovery. In the event that something does happen, we identify like there's a slow GPU or a slow node we can automatically reboot the node or if that doesn't resolve the issue, we will automatically replace the node and then restart the job from your checkpoint. So once again all these capabilities come together and we've seen cases where it saves up to 40% of the time spent on training. Which essentially helps you save on your overall costs because you're able to respond to failures faster. So, I'll now invite our Tounori on to show us a demo of these in action. I. Thank you. Yeah, thank you, Michael. Am I audible? Yeah, thank you. So, uh, I'm Tomono Rishimomura, a solutions architect, uh, on SageMaker AI organization that's helping SageMaker AI customers every day. And uh now, Michael, uh covered the fundamental concept of, of SciMaker AI's training capability. So let's dive deep into the uh actual uh developer experience, uh including uh demonstration. So as Michael explained, there are two training capabilities, uh, SageMaker training jobs and Hyperbo. Uh, and, uh, um, SageMaker training job provide free uh managed APIs. So, um, before executing the job, uh, SageMaker training job automatically create a cluster, and, uh, uh, after the execution of the job, it automatically deleted the cluster. So essentially, The computer resource is allocated only during the job execution, so you can expect a lower cost. And uh uh you can use uh high-level Python SDK and uh it's a kind of wrapper uh Python library on top of uh Sage Maker's service API and you can easily use uh Search Maker's um training capability by writing Python code or uh running notebooks. And uh um because this is based on the ScisionMaker service API uh you can easily integrate the training job uh with your MLOps pipeline. This is an example, uh, called fragment of training job. And uh as you can see, uh, this Python code is importing uh a Python module called uh DecisionMaker core, and, uh, um, instantiating say, creating a training job object by passing some parameters like a hyperparameter configuration, instance type, instance count, and, uh, uh container image, right? By providing these, um say parameters, you can um Differ, I would say, uh, configure the training job, start the con uh training job, and automatically uh create a cluster internally. And uh um another training capability, uh, from Hype uh SageMaker is Hyperport. And uh unlike SageMaker training job, Hyperport is for, uh, persistent clusters. So it means, uh, cluster creation is not automatic, unlike SageMaker training job. And, uh, um, that you need to manual say explicitly create and delete the clusters. But the hyperport helps you create such a hyper, high, high-performance cluster easily. So you can customize uh the cluster uh based on your needs, and share the cluster with your teammates, multiple, uh multiple users, and execute multiple jobs uh on the same cluster. That's the difference uh from the decisionMaker training job. And uh um as Michael said, SageMaker comes with uh advanced resiliency capability. I would like to uh uh explain a little bit more later, and, uh, uh, I also included some demonstration, uh, in my, um, say, uh video. Yeah. And uh because Seime Hyperport is for AI uh development purpose, right, not a, not a normal uh cluster. So it's an AI development purpose to cluster. So it comes with AI purposed uh comprehensive uh observability dashboard, and also some useful uh libraries and tools to optimize your training. Hyperport supports two orchestrated options. One is RAM, another is EKS or KubanneS. EKS is Amazon's Elastic Kuban service, and it's actually Kubante, OK. And, uh, um, both supports, um, I think SAA and the EAS are both uh powerful framework to run uh distributed training, uh, but the, the, uh, developer experience are different. So I'd like to explain how, uh, they are different, um, briefly. So SAM is an open source, uh, job orchestration framework, and, uh, you can basically run a same, uh, batch job on multiple nodes, simple word, right? And, uh, um, in case of hyperport, uh you will uh create three types of node, controller node, computer nodes, and the login node. Typically, you log in to login nodes, and execute SRAM commands such as uh S, S, Sinfo, Spatch, and SQ, etc. to uh run SR uh features. And uh um another orchestrator is EKS or Kubernetes. I think I, many people have heard of Kubernetes because it's popular, right? And Kubanis is uh uh a framework to run or containerize the applications, uh, various types of containerized applications, uh including distributed training library training applications. And, uh, um, unlike, um, SRAM orchestration, Uh, in case of, uh, choosing, uh, Kubernetes, uh, the hyperport cluster only contains computer nodes because control plane, uh, I say EK, uh, plays the role of control plane. Uh, hyperport uh cluster itself doesn't contain control plane or login nodes. And uh uh you can learn uh say kubanate clients like a cube controller or your preferred um uh uh Kubinator client uh on your uh development machine even on your local laptop, right. Yeah. So which is better choice for you? Uh, this is a tough question because this is case by case. Um, so if you already have a preferred choice of orchestration, please choose it. And if you have, uh, want to, uh, directly access hardware, uh, you want, if you want to run application on host level directory, uh, so that may be right choice. If you like, if you love containers, maybe, um, Cuban, so I guess it may be the the better choice. Yeah, I think Michael already covered some portion of resiliency, but let me recap quickly before going into the actual demonstration. So hyperport resiliency, uh, so, so resiliency, uh, consists of three steps, right? Health monitoring. Rest instance replacement and the job auto resuming. So these three are the uh I say core uh the flow of hyperport resiliency. So hyperport constantly run health checks. And if we find some faulty GPU faulty instance, we trigger the instance replacement. And uh after the instance replacement, the suspended job automatically resumes. We provide such a framework. Yeah, so, uh, we would like to, to two demos. Uh, I captured pre-recorded videos, and, uh, partially, I, I accelerated the video because you, in order to fit this in limited time session. So, uh, starting fromcisionMaker training jobs, and, uh, um, I'm going to open a code editor on browser. DecisionMaker has a code, code editor browser, by the way. And uh uh I execute a notebook, uh, in order to trigger the uh training job. And eventually, we will see the uh execution result on training metrics uh by ML flow. Yeah, so, firstly, I'm opening code editor on SearchMaker, or clicking open button. And you, it looks like a say Visual Studio, right? So this is based on the open source version of Visual Studio code editor. And uh checking, uh browsing the uh training code. This is the code actually executed on uh computer resource. And I'm executing notebook, and the first half of this notebook is for uh data fully processing and uploading to S3 bucket. This is a preparation step. Yeah, visualizing the uh training data set. And the configuring the, uh, configuring some parameters for training job, like uploading S3. Right. And then, uh, creating or configuring the training job. Yeah Yeah, so you can see a model trainer uh class. Uh I means start setting model trainer class with some parameters. This is the Python API. And then, actually, uh, say, configuring input channels, the input data channels, and starting the training. And you can also see uh the list of high uh SageMaker training jobs on SageMaker Management console like this. So now, it's, it's showing training status. Training is in progress. And uh the progress uh can be also seen uh by uh visiting Cloud of uh watch logs management console. You can see output from container, right? Yeah. So, um, there are some options how to monitor the progress. You can see, uh, cloudWatch logs. You can use also a notebook, uh, to by interacting, by calling some uh method of Python object, you can get the log output as well. And also, uh, after this, I, I will show, but you can also check ML flow output, or your, your preferred observability solution as well. So Searchmaker has ML flow, manage the ML flow. So I click the ML flow button and opening the ML flow. And I'm seeing a list of executed uh training jobs, and you can see uh training metrics like this. Yep. So next demo is SageMaker Hyperpot. And uh um in this demo, I'm going to uh create a cluster easily and uh uh it's like configuring, so customizing the, the cluster uh by installing some other ones and uh uh learning some uh training uh job. And the event, and uh I'm, I'm going to, so in this demo, I did, um, let's say. Uh, simulating some hardware failure artificially by injecting uh the simulated error and verify that hyperport resiliency works, and eventually see the uh observability dashboard. Yeah, so you can easily create a decision maker hyperport cluster. Uh, and in case of quick setup, what you have to do is just to choose uh right instance type and instance count. Basically, that's it. I, uh, so cluster creation can be complex, uh, if you do manually, but, uh, we provide such uh, uh, uh, easy, uh, setup experience. And the actual cluster creation uh progress can be monitored on uh cloud formation management console because um it requires not only cluster itself, but also uh requires various prerequisite resources such as VPC or subnet Security Group, S3 bucket, and IM Raw, uh, etc. right. And uh when uh hyperport cluster is uh getting created, uh we can visit the decisionMaker management console and see the uh instance initialization progress at the instance level. Now, uh, so yeah, I, I, I think you just saw a deep health check was being executed. Now, let's install some add-ons, starting with uh hyperport observability add-on. It's just literally one click, but also, I'm enabling some uh additional, say, uh advanced metrics. So it's not a one click, but I am literally what uh what uh say one click in case of basic installation. And then, uh, enabling uh hyperport task governance. Uh, this is for, uh, let's say, uh, job task prioritization and a dynamic, uh, computer quota allocation between multiple teams. This is useful in case of. Um, how to say, sharing a cluster, uh, between multiple teams and multiple purposes. So I'm, I'm configuring the, uh, let's say, priorities, job priorities depending on the task types, like, uh, influencing, training, or experiments. And I'm also configuring two teams, Team A and Team B. And, uh, uh, defining 8 instances as a default quota, but, uh, uh, uh, allowing lending and borrowing computer resource between teams. And then, uh, I'm uh configuring training job, right? So as you can see in the second line, uh, hyperport training job, it's a custom resource definition, uh uh installed by a hyperport training operator, and I'm using it for advanced training. Installing uh additional software. Uh, and modifying the, uh, uh, checkpointing code in order to enable manage the tiered checkpointing. I will explain more later. Yeah, so using some decisionMaker specific uh Python classes uh to uh for, for uh checkpointing code. Now, on terminal, I'm monitoring three information. Top left, uh, I'm monitoring the node status, top right, uh portal status, and uh uh center uh is uh log output from training job. I executed the training job using cube control, that is uh standard uh CLI interface. Right. Yeah, training is progressing. Um, next, I'm going to, uh, let's say, uh, inject, uh, artificial GPU failure in order to simulate the, uh, and to verify the, uh, resiliency capability. So, uh, good, uh, feature of hyperpod is you have direct access to, uh, the infrastructure, so not at the container level, uh, not only just monitoring logs, but you can, um, log into, uh, computer nodes like this by SSM session. And in this case, I, uh, injected some error message to the kernel log, uh, to simulate the hardware error. And you can see, uh, yeah, so one node became not ready status. And one part, I think it became pending status, so it means your job got suspended because of the injected, artificially injected error. And uh yeah, so, um, so new node came in uh thanks to that instance replacement and uh a new container is container uh say uh creating status now and soon it should uh start executing. Yeah. So container, uh, a new, newly created container, uh, started and the job uh resumed like this. And then, um, Yeah, uh, job resumed. The next step, I would like to show you uh how the observability dashboard looks like. Um, this is based on Prometheus and uh uh Amazon uh managed Prometheus and Amazon managed Graphfaa. Uh, you, uh, so I'm uh using a task set, cluster dashboard first. The cluster dashboard uh shows you, uh, hardware resource, uh, related information at the cluster level or instance level, like this. And next uh dashboard I'm showing is task dashboard. Uh, it's pretty powerful. You can see key metrics like CPU GPU mem memory utilization at the task level, not the instance level. So, uh because uh hyperport is for distributed, so useful for distributed training, right? So I think you are interested in which team is Using GPU efficiently, which team need, need improvement and uh which job uh is let's say, uh say uh not, not performing well because of what uh bottleneck, right? So those uh informations are really important to uh use uh cluster. So uh this uh observability dashboard is very useful for you. OK. So, um, in the demo, I included some newly introduced, uh, features, uh, this year. So I, let me, uh, highlight some new, uh, features. The first feature I'd like to highlight is uh uh enhanced uh hyperpotse, cluster creation experience on management management console. And you can easily set up uh cluster by uh say, configure entering some parameters and Configuring cluster can be very complex because, um, say, uh, you need to uh not only configure the uh cluster itself, but also prerequisite resources. But you can use uh this browser-based experience to easily configure a cluster. Next feature is one click observability. And uh um so um observability is important for you, uh, as I said, but uh setting up is complex because it requires metric emitting component on the cluster itself and uh data, time series database, and a visualization layer. But uh yeah, you can literally install this by one click. And the hyperport training operator, uh, this is uh for efficient, uh, distributed to training, and uh it uh has um capability to, uh, recover in say, uh, intelligently. Uh, it has a job hanging detection. Manage the other checkpointing. Uh, this is to reduce the overhead of checkpointing. And uh um this, this is a feature I didn't include in my demo, but uh uh AWS uh sorry, training uh job has a new feature, AWS batch support, and uh A training job itself doesn't have a batch uh scheduling, uh sorry, job scheduling capability, it will just run a training. But uh by combining a decisionMaker training job and AWS batch, you can uh NQ training job uh in a, in a queue and uh uh intelligently schedule by priority. And this is also a new feature I didn't include in a demo, but uh it says, now you can learn IDE and hyper uh notebook on a hyperport uh cluster. So, you can use Nobook, uh sorry, Hyperport cluster not only for learning training jobs, but also for interactive uh development and experiment. And lastly, uh, we did this MCP server so for SageMaker AI. So, uh, you can use this MCP server to, uh, let's say, create and maintain hyperport cluster by natural language. For example, you can say, create a cluster with 4 G512X instances, or you can also say scale, scale up my cluster from 4 instances to 8 instances by natural language. OK. Um. So, yeah, so now we covered a fundamental uh concept of SageMaker uh training capability, and uh uh I say, I, I provided some demonstration. Now, I think you are interested in what the Hyperport or SageMaker AI capability. Can do in real world, right? So, uh, I'm interested, I'll say uh thrill you to, uh, say, uh, introduce Danny uh Gopio from uh Roblox. His team is usingcisionMaker yperpot for one of uh core component of, of their AI platform. Pardon? Thank you. Hello, can you hear me? Cool. Uh, yes, so I'm, um. I'm Donnie. Uh, I work at Roblox where I lead the AI infrastructure and AI platform team. And so today I'm going to talk about AI at Roblox to give you some, some context. Uh, and then we dig into our training infrastructure, uh, especially how it has been impacted by training our 4 4D foundation model. I'll finish by uh what we want to build next. Um, so if you're not familiar with with Roblox, Roblox is a platform where millions of people come together to create, play, and interact with each other, with one of the million experiments that we have, and those experiments have been created by our community of of creators. And so as of last quarter, we have 150 million daily active users, and we eat 45 million peak concurrency. That means that 45 million people come together to play at the same time in Roblox. That's pretty huge. And in terms of AI, our AI infrastructure supports about 1 million+ query per second across our 350 + 50 models that we have running in production. So as you can see, AI is pretty much everywhere on what we do at Roblox. Especially when it comes to safety, which is core to uh to our principle, we train and we serve a model for real-time moderation for voice and for text, uh just to make sure that there is no bad content that is happening within the platform. And actually those models are open source, so you can use it for your own use cases. For safety, we also do. Bad avatar moderation as well as boat detection, abuse reports, and clickbait detection. So a lot of use cases of AI use cases being used for safety. For our players, AI drive our recommendation and search system just to make sure and help our users to get the right content. So that means we have game recommendation, as you can see on the screen, as well as friend recommendation for social interaction and for assets that you can use for your game or for your avatar, we also have the marketplace search driven by AI. Um, Finally, for our creators, uh, we introduced a set of GAI tools to help them improve their creative processes. And so in that example, uh, you have a creator assistant built into Robloxs that helps you create a 3 by 3 orb grade. That's like one set of the assistants. The other set is a script that actually helps you. Um, Turn this blue orb into red and disappear when a player touches it. So similar that what you can do as a developer using cursor, we have the same set of tools for our creators. And more recently we introduced our Q model. So it's it's a 4D object generation, and by 4D I mean it's a 3D object that you can interact and is functional. So for example, as you get a car, you can actually open the door and get into the car and drive it and the wheel will turn. And so there's two examples on the screen. On the left side you have a gun, a watermelon gun in the shape of a banana that actually shoots watermelon. And on the right side you have a magic carpet built by our 4D model that you can fly. That's pretty cool. So you can, you can see that in the future our players and creators will be pretty much being able to create anything that they can imagine. Right, so now let's see how we actually train this foundation model and how our training infrastructure has been impacted by that. Uh, but first, let's step back and look at the AI platform as a as a whole to give you some context. So on the left side in purple, you have all our data components. So starting with our data lack with structured and unstructured data, our feature store and embedding where you can uh with online and offline retrieval. In the middle top, uh, you have the blue component which is used for training, so that's data processing, so you go from raw data to features for your training. And actually the training component itself. So when you go for features to like build those, those, those models. And then the yellow one is what we use for serving. So when you get your training and you have your model, you can push it to a model registry, and then you have our serving component that reads the model from the registry. And service for real-time use cases as use case in green and that's already described before. And finally on the on the on the right side you have the rest of what we use for observability, cost tracking, and our experimentation platform. So that's pretty much the AI platform as a whole. And so today we are going to focus on this red box which is our distributed training component. So how we do multi multi GPU, multi node in a distributed training fashion. And spoiler alert is going to use an hyperpod cluster, obviously. Um, so let's go back to the 4D model and to the scaling factor that Michael was talking about and so how it's going to be applied to our use case. So first was the data side. In our case, we have about 100 million 3D assets that are available. That's like roughly 6.5 petabytes of data. We don't necessarily use everything for training, but that gives you an idea of the scale. Then the model size, the requirement was between 1 billion and 70 billion parameters, so it's a very large requirement. We don't know exactly what's going to be built in terms of available hardware. All we know is we will need a lot of GPU memory to train those models, and so the new year, the better, and H 200 what we went for. In terms of training technique, uh, the research team doesn't have any specific requirement except that it has to be performance, uh, stable, and easy to use. And finally, it's probably the most important point for them is the number of jobs that that that they can run. Um And why it's more important is, as we never train a foundation model, and especially a 4D foundation model. Being able to run as many jobs as possible in parallel makes them more efficient and actually refines the hypothesis that they need to create that model, especially the model size. All right, so How does that affect our AI platform? Well, first of all, you go talk to your finance team and making sure that you have enough budget for it. That's the one. But once you get the budget, we actually sit down and look at what is going to break or what is missing from our current solution. And so we came up with 3 challenges. The first one was how we actually get capacity. We already have thousands of GPUs across training and inference, but those are usually like tiny GPUs for inference or they are used by other teams. And so in that case we just needed more capacity and more high performance GPU. And so how we get that was one changes. The second challenge was how do we ensure resiliency, as Michael was describing, as you, as you get more data, as the model size goes up, you're most likely to fail at some point during your training. And so we have to make sure that Whatever this new project is coming to the platform, we have hundreds of ML engineers working on other projects that should not be stopped because of a new project. And the last one is, OK, you get GPU, you get capacity, you get resiliency. The first one is how we make it cost efficient and making sure that we utilize, utilize GPU resources in the best way possible. Um, and that's where hyperpo come into the picture and help us get there. So the first one, first point was it's actually fairly easy to to to set up. It's not, don't get me wrong, it's not as easy as setting up an EC2 instances or not good for your AKS cluster, but it's, it's reasonable. It took us 1 month from protototyping to production, um, and so that includes making sure that everything that we do on the platform works the same way in an ipod cluster. Uh, with infosec review and actually setting up the network so our EKS control plane can talk to hyperpod nodes, uh, and we recently created a new EKS uh Hyperpod cluster and it took us less than one hour to do it, so I would say it's, it's reasonable. The next point was GPU capacity. As I was saying, we need this high performance GPU, and so hyperpod was one way to get to get there and especially that with hyperpod you have multiple options to get more capacity either with reserve capacity, sporting senses, or flexible training plan. We don't use flexible training plans today, but as we get more timely or ephemeral projects, I do see that happening in the future. Continuing on our purple help us with those changes. The next one was stability. Again, as we, as we scale up, a failure is most likely to happen. And so building those features ourselves will have taken a lot of time and most likely being delayed that project. And so hyperpod help us get there faster. And actually as I was preparing the slide for today, I couldn't find a single node failure in the last 3 months. Even so, I can tell you that there were a lot of GPU utilization during that time. So kudos to the team. But as I mentioned, that something that is actually missing though is some visibility into the the recurring node detection and like what failure can happen. I would have loved to show you actually a thousands of tests that AWS is running behind the scenes where everything is green and everything is fine. I couldn't find this information, so we should probably take that. Um The last one is flexibility, and honestly to us that was the most important point. Where you have an AI platform, we already invest a lot of time with that, where you have like tooling that we want to reuse as much as possible, and the fact that Hyperpod is able to integrate with EKS is phenomenal to us. And what I mean by that is when you create an hyperpod cluster, your hyperpod nodes are going to join your EKS cluster, and at that point is. Look like just any other node in your cluster. That means that you can tend those nodes. You can, you can do node selection. You can run any demand set on it, and any tooling that you have, you can work the same way. And so that means that hyperpod is actually like framework agnostics for both scheduling and training. And so in our case that means we were able to use Unicorn just like any other AI workload that we have on the platform, and we were able to use Ray for distributed training again, just like any other workload. So that was definitely a good selling point for us. Um, I'll finish with, uh, what we want to build next. Um, and that's what we call a decentralized, uh, compute. The problem still remains the same. Having access to GPU is always a bottleneck, and so hyperpod was able to solve that in a single region, single cluster environment. But as we want to get access to more capacity, we need to extend that. And so the solution that we come up with is again what we call a decentralized compute. And so we need a way to schedule workloads dynamically across all resources that are available to us, and it doesn't matter which which region, which cluster, where, where the capacity is, is all that matters, and we need to make that in a in a transparent way for our user. And so how we are going to abstract the scheduling process so the user gets a unified experience and. Just just schedule a job and submit it and it doesn't actually care of where it's actually running. That's where we want to be. And so finally that means that we want to extend the hyperpo support to get like multi-region and multi multi-cluster for that. And so we hope to work with with the sage maker team to make that happen and hopefully hopefully you can benefit from it as well. And that's all for me. I'll let Michael do the recap. Thank you. Uh, thank you very much Denny and Tomonori, and thank you so much for sticking around. We have just 3 more slides to close out this session and as a quick recap, we talked about the various dimensions of the critical things that are important to customers computer availability, performance, resiliency, observability, and ease of use and how those all apply to cost. I recall then he talked about flexibility of choice and on this slide we're just gonna summarize that right from the bottom of the stack for hardware we have multiple options, different compute options, storage and networking on the software and driver layer we have multiple pre-built images, different device drivers and toolkits moving further up for multiple training libraries to ensure highly performance distributed training. With PyTorch Tensorflow, multiple capabilities for distributed training strategies, and even various MLOs third party solutions, we integrate with ML flow or weights and biases for tools. You can submit jobs with Ray or Ce Cottle for observability like Tomonori showed. We have support for Prometheus, Grafana, or Cloud Watch. You can even use Sagemaker Studio or notebooks or even use it directly from your machine to actually submit jobs and at the top layer, whether you're using Hyperpod to orchestrate your jobs with uh Cubinetis on EKS or Slurm, those options are available to you. Or if you just wanna use a fully managed API, you can actually use the SageMaker training jobs managed API. So what this shows is we understand that for you your use cases might be different, your technology team might have different interests in using various frameworks. We offer that capability for you to pick and choose what works best for your use case. So We are sharing this, uh, link. It's a link to a blog where we discuss both the capabilities for training jobs and Hyperpod. It's a good resource that I usually recommend. It also has example notebooks on GitHub where you can literally just copy the code and run. So these examples are set up for you. We have hundreds of them available for you to get started. So thank you so much for joining this session and uh please uh leave feedback on the app at the end of the day or later on but we've really enjoyed uh talking to you today. We're gonna also be outside if you have more questions or things you want us to talk about but thank you very much and enjoy the rest of Reinvent.