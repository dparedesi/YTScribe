---
video_id: bu2cD1pCFTs
video_url: https://www.youtube.com/watch?v=bu2cD1pCFTs
is_generated: False
is_translatable: True
---

Hi everyone. Um, Brave people doing a co talk after lunch. Um, my name is Danny. I'm a, um, solutions architect. Um, I'm focused on G AI. I work in the, the Amazon Bedrock team focusing on knowledge base and agent core memory, and I'm here today with Akarsha. Karza, do you wanna? Hi everyone. My name is Akarha Seva. Um, I'm a gene data scientist with Asian Core Memory Team. Yeah. Um, so today, let me just go back a second. What we're gonna be talking, we're gonna be doing a code talk, so it's gonna be basically us going through some code, uh, so it's gonna be like a level 400 session, so we'll go, we'll dive deep into, into the details of the, you know, different, different APIs and, and the code that, uh, we're gonna be using today. So we're gonna be, uh, diving deep into agentic, uh, rack systems, right. Um, so we'll just start out, we'll set the layers and to the foundation. Let's talk about what agentic rack is, um, and then we'll see what we're building today, and then I promise I'll go out of the slides and go into the, into the code, right? Uh, I won't be an hour with slides. Uh, and at the end we'll give you some resources that we put together to, to get started. So, um, let's start with the basics, retrieve augmented generation. I hope everyone in the room has heard about this. We've been talking about this for the past 2 or 3 years. Um, so I just wanna go quickly over what, um, retrieve augmented generation is, right? Um, or how the workflow goes, uh, at the very, very high level we have our query that we send into our model, right? But before we send that into our model, we're gonna augment that query, right? We're gonna give it additional information. This can come from our, you know, um, internal data sources, or you just wanna provide external information to the model so it doesn't hallucinate, um, and then it will have the full context to, you know, generate, um, an answer and give you back the response. Um But Akarsha is, is that correct or not? I mean, it's partially correct. So when we think about agents, right, we have the LLM that has a lot of capabilities, but agents themselves are fundamentally stateless. So all we can provide from our side to make them better, to make them smarter, is in this retrieval in this rag component. So today we don't just consider RG to be retrieving from a knowledge base or a data source. We talk about um more in a generalized way which is called context. So today you must have heard this this new term popping up everywhere which is called context engineering. So essentially what it does and what it means is we are curating our context in a in a way that we are providing very precise information that the agent needs to give smart and more accurate responses, right? So that includes information from the documents, so the traditional rack component, um then it also includes curating our tools, so figuring out which tools to um are relevant for this current query. Um, or this current use case, so finalizing those and also making sure that whatever information we have provided in those tools is very accurate for the agent to know when to use it. And then we have the memory component. So memory component is where we are providing it with more information about the user to make it more personalized to make the experience uh for the user much better, right? So that's where the memory component comes in and all of this combined is what defines the context. Yeah, that sounds right. So now we have them we have now we, you know, we have all this, um, context coming in and we have a new way of, of retrieving our information, right? So we have our query, we'll have a res a retrieval, uh, that retrieval can, you know, import information from different sources as cars I just mentioned, uh, then we'll augment that, uh, query. And then the model will decide, hey, am I, am I able to answer with the information I retrieve, or do I have to go back again and, you know, look for a bit more information. Um, if not, we can get back the response. Apologies. So, this is genetic rack at a high level. Apologies. Um, so what are we building today? OK? Why are we here today? We are gonna be building an event agent, OK? Um, as I imagine you have your AWS event application right now and at the bottom there was a, I saw before there was a, a small text box which said, uh, so ask me something or something like that, right? And you'll have your common, um, you'll have your common agent now or your common chatbot implemented into all of these applications, but we want to not only be able to search across the uh sessions information, but we wanna deliver a truly hyper personalized experience to our users, right? So, I can say, hey, what sessions do you recommend me? And I expect, or we normally expect nowadays, these agents to have information about this, right? Remember some stuff about this and being able to recommend the stuff based on our preferences, right? Or questions like which session did we talk about last time, right? Has kind of history or what sessions are there on Monday, right? These kind of common event questions that show up and if you don't ask the app, you ask someone who is who is there to help. Uh we wanted to have it all in a single place. So we're gonna be building this, um, awesome event agent, OK? And this is gonna be basically an agent that has two capabilities. So, our users are gonna be able to interact with their agent. And that agent is gonna be able to do two things. One of them is, as I mentioned before, retrieve session information, um, from a knowledge base, and secondly, it's gonna be able to store and retrieve the conversation history that we have, right? And it's gonna be able to learn from our conversation our user preferences. OK, so, yeah, I think that's enough, um, slides at the moment. Um, let me go and show you how this looks from a code perspective. If there are any questions, put your hand up. It's a small room today. Um, we'll try to answer the questions alone. If not, we are happy to, to answer the questions also when, when we are outside, right? Cool. So, um. I'll go to the, I've, I've got the notebook ready with this event agent, and we'll make this available this week in our repo, um, for recon samples. We'll give you the resources for that later. So just bear with me and, and let's figure out what's, what's going on in this notebook, OK? So can everyone see this right? Let me just go to the text and see if we need to make it a bit bigger or not. Yeah, OK, good, good. I don't see anything because I've got this big lights here. Um, but anyway, so, um, this was the same slide that before. This is the, the high level picture of what we are building today. And we're gonna go through what we call different chapters, and it's gonna be creating the agent, we create the simple agent, uh, we create the knowledge base, that's where the session information is. We create the memory for agent to have that, um, persistence, and then we're gonna see how we can move this to production and do it in a secure way using, um, agent core runtime or agent core identity. So, um, if I start here with my, my notebook, um, uh, pretty simple, just installing some dependencies I need this to work with, um, I'll import some libraries that I need for the, for the notebook to work. An important thing here is that I'm gonna be using, um, uh, Cloud Haiku 4.5. Um, I'm gonna be using Amazon Titan text embedding P2 for my knowledge base, and I'll talk about that a bit later. Uh, I just want you to understand what, what's going on here. I'm just setting those. I'm creating a unique idea to have a unique resources, OK? Pretty simple. Um. For every code, um, cell, I will, I will show you the outputs. It makes much more sense. So let's start with the agent, um. What are we gonna, yeah. What is the database. Sorry, um, the, the, we'll get to that, yeah, give me, give me one chapter and I'll get to that. Um, so let's start, let's start with the agent, right? Let's start with the most simple step. We're gonna be building a simple agent, right, and that agent is gonna have a context, and that context is just gonna be you are an intelligent event assistant. Right, so I'm gonna be using strands agents. Um, Akasha, would I use any other framework here, or you can use any framework you want. So you can use Lang graph, lung chain, Crua, any, any framework that you're currently using. We use strands agents because it's a, um, it's, it makes it very easy, uh, for us to build agents, and strands agents is a I would say fairly new framework that has been out in the market. It's an open source framework that allows us to leverage the capabilities of the model itself to be able to plan, to take actions, and also to um. To revisit whatever has occurred is if it actually achieves the goal or not, to be able to reflect on that. Mhm. OK, then I'll go with Franz. Um, so, uh, what do I need to do to create my agent, uh, with strands? I'll just create, I'll just import my agent class and I'll import my model class. I'm gonna be uh using a model which is available in, uh, Amazon Bedrock. That's the, um. Uh, Haiku 4.5, right, and to create an agent with, with trans agents it's as simple as, let me just get my pointer out here, um, I'm gonna create my simple agent. This is gonna be my class that I wrote before. I'm gonna be defining a model, the model that's underneath this, this agent, and I'm gonna be giving it a system prompt, right? So I'm gonna give it a bit of context on what it's supposed to be doing. So it's as simple as saying, hey, you're an intelligent event assistant, OK. Um, simple as that. You create your first agent, one line of code if I take out the, the, the white space, um. So, when I go and interact with this agent, I'll say, hey, um, simple agent, which is a good session to learn about security and AI agents, right? And we see that the agent will respond, hey, I don't have enough information about specific sessions or events in your current context, and that makes completely sense because I haven't told it, I'm a dream event. I don't, I haven't given it any information about the sessions. It's just a blank agent and it's just, you know, using the, the model on the, on the hood with the information that it's been trained on. So that's completely normal, right? So, how do we start giving context to our agents? So, one way, um, is via knowledge basis, right? And with knowledge basis, what we're gonna be doing is we're gonna be providing the, um, reinvent. So in this case I'm using the reinvent 2024 sessions, um, I found around and I'm gonna be providing that to my agent, OK? So, for anyone who doesn't know in the room, Amazon Bedrock knowledge bases. It's a fully managed um service um inside the inside um Amazon Bedrock and it helps you create um fully managed workflows for Rack, right? So We are gonna be able to create a knowledge base where we're gonna say. We're gonna be able to define our data source layer and our data storage layer, right? So, when I'm talking about data source layer, I have right now NGA we have two available data connectors so I can basically bring in my data from S3. Um, or I can directly, um, send in the, the information into the red frack knowledge base. I have 4 different chunking strategies, right? So when we're talking about, um, rack, the flow is normally you have your documents, you get those documents and you parse them to understand what's inside those documents, right? And once you have that, you're gonna split those documents into smaller chunks. OK, then using an embedding model we're gonna transform those chunks into vectors and those vectors end up in a vector database, right? Pretty simple. Uh, rack knowledge basis helps you with this whole process in a, in a managed way, right? You only have to configure where your data is coming from, what kind of chunking strategy you wanna use, what kind of parsing strategy you wanna use, and then define what embedding model I'm using and the underlying vector store. OK, so all that information is gonna end up in my knowledge base and then I can. Use retrieve API to retrieve the chunks, so we basically doing a semantic search against the vector database, or I can use retrieve and generate which will combine that um process of retrieving my chunks and automatically sending it to the um um foundation model, right, and getting back a response. So I have those two options there. So, what am I gonna be doing? I'm gonna be creating a tool for my agent. And that tool is gonna be able to call knowledge bases. OK, so, um, here in my code. I'll, let me see, I'm going with time. I'm going good. Um, what I'll do is these are the steps that I'm gonna go through. I'm gonna download my 2024 session data. I'm gonna create my S3 bucket. That's where those documents are gonna end up. Um, and I'm gonna upload them. Um, I'm gonna create an S3 vector storage index. So for this use case I'm gonna be using S3 vectors, right? This is a fairly new capability from S3 which you support me and give support for, um, for vector storage, OK? Um, then I'm gonna go and create my knowledge base and do the ingestion job. OK. Finally, I will test that knowledge base and then I will create that tool for the agent to uh consume. So pretty simple steps here, um. I don't wanna spend too much time on this. Uh, the other one is more catchy, I think. Uh, but basically here, first day what I'm doing is I'm basically downloading my data, right? I'm downloading, it's, it's basically a single file which contains information about 583, if I remember correctly, sessions, exactly. It downloads that file and I'm just understanding what's going on on that file. Then I prepare those documents. So instead of having a single file, I'm gonna split it across 580 different files which will go into the knowledge base. So that's just basically what I'm, what I'm doing here, right? I'm creating my directory and I'll, um, split that single file into multiple files, OK? Pretty, pretty simple. Um, so I'm gonna be preparing those documents as you can see here, um, creating those 500 documents and putting it into a folder. Then what I'm gonna do is I'm gonna send those documents to my S3 bucket, OK? So here I'm just creating my S3 bucket. I'm gonna give it a unique name. That's where I'm gonna be sending my files to, and once I have created that bucket, I will upload the documents, right? Using in this case we're using uh Boto 3 to send the documents to the bucket. Pretty, pretty simple. Um, right, so as you can see here, I'm creating my pocket, uh, and then I'm uploading all the files. Um, this took like 6, well, this took 1 minute, it's OK, it's, it's not that much. Um, next, what do I need to do? I need to create my IIM role for my knowledge base, right? My knowledge base needs access to an embeddings model and it needs access, um, to the S3 bucket, right, in order to do all of its tasks. So I'm just creating a simple IMM role, uh, where I'm gonna be, um, creating it and, and I'll use it later, OK? All of this we can do it via a um via code which that I'm doing right now, but you can also do this via the the console, right? Uh, maybe a bit a bit nicer, um, but I have my notebook here. So, um, just creating my, my knowledge base role there, pretty simple, uh, and then I go and create my S3 vector store and index, right? This is where knowledge base is gonna do the whole ingestion job and, and the resulting, uh, vectors from the chunks that we created, they're gonna end up in that vector store, OK? And it's as simple as saying, hey, um. S3 vectors. I'm using the bottom three clients for S3 vectors create the vector bucket and I give it a name, easy, and then I go and create um vector index, OK. So here in the vector index, what I'm saying is I'm gonna give it, you know, where it's supposed to be that index in that in the vector pocket I created before. I'm gonna give it an index name and I'm gonna give it a dimension, right? And this dimension that I'm using here, which is 1024, I'm I'm stating that there because that's what Titan embeddings B2 is gonna be uh giving us back when we do that conversion from text to embeddings, right? Um, then we set other, uh, topics like, uh, distance metric, data type, and some metadata configuration, OK. Um, so here we're just creating our S3 vector store and index. As you can see here, we'll get an ARN for both of them. And now I can go ahead and create my knowledge base. OK, so I'm creating my knowledge base, simple API call. I'm creating my knowledge base here. I'll use my pointer better. Um, I'm creating my knowledge base. I'm giving it a name, I'm giving it a description, so it's my AWS reinvent 2024 sessions knowledge base. I'm giving it the role that I created before and then I'm doing, I'm filling in the configuration for that knowledge space, right? So, basically here what I'm doing is I'm saying you're gonna be using the Titan text B2 embeddings model that I defined at the start of the session. Uh, and this is the configuration for that embedding model. OK. Here's the 1024 that I, I used before for my, uh, index, OK? And then for my storage configuration it's gonna be using that index store that I set up before. Pretty simple. Any questions up to here? I explained it very good or I confused you even more. OK, I'll, I'll take it as the first one. so I just created my knowledge base, and now I can go ahead and import and, you know, ingest the data from, from S3, right? So I created my knowledge base and I'll go and ingest my data. I create my knowledge source, um, it's an S3 type knowledge source, um, data source, sorry, um, with my S3 configuration and giving it my bucket name. And here I'm giving the chunking configuration. Um, I'm using a fixed size, fixed size strategy. Uh, we have a fixed size, we have, um, no, no, no chunking, we can do semantic chunking, we can do hierarchical chunking. You can even bring your own lambda to do your own chunking con um configuration, right? Um, so there's quite a lot of flexibility there depending on your, on your use case, OK? But in this case, pretty simple fixed size max 300 tokens, um. And then 20% overlap, um, 20 token overlap percentage, um, right, so from there I can go and start ingesting my data. And I'll wait for that to finish. That took me around 2 minutes. Um, that's good. It processed my 583 files and I can go ahead and test my knowledge base retrieval, right? So if I ask a question, hey, what are the sessions about generative AI and security, I will go and use the retrieve API which I mentioned before. So I'm just retrieving the chunks. I'm just doing a semantic search, and it will give me back the three most relevant results for that query, right? Um, so if I ask that question, It will say, hey, I found these 3 results and these are the, the 3 chunks it got back, right? Um, these 3 sessions that I, that I have here, OK? Pretty simple. I just created knowledge base, injusted my data, and I did that all programmatically. Now fun bit, I'm gonna create my knowledge-based search tool and this is what the agent is gonna be using. So how tool works with um strands is that you're gonna take your Python code, uh, with your, you know, typical function. Um, and you, you're gonna put in here this tool decorator, OK? And with that tool decorator then you can pass it on to the agent and the agent will make use of that tool. So the nice thing about this tool is that I'm gonna be passing in um description and that's what the agent is gonna first read, right, and, and see if it needs to use that tool or not for the task it needs. So as you can see here, I'm saying hey this is a Uh, tool to search reinvent sessions from a knowledge base using semantic search. Um, use this tool whenever a user asks for a recommendation about reinvent sessions, right, and I can pass in a query and max results, right? And this is just a plain API called to Bedrock, uh, knowledge bases which is retrieve, OK? And I'm passing in my knowledge base ID, my retrieval query, and my retrieval configuration. OK, so with that I have my knowledge-based search tool created and I think at Kasha you can take it on from here. Yeah. OK, so we have one of the components of the agentic rag that we were talking about. We have that ready. And we have that ready for consumption as well because we have created it as a tool, right? So one key difference why you would use something as a tool, um, would be when you want the agent to decide you want to have it the flexibility to choose when does it want to call it, when does it feel it's relevant to call um the knowledge base, right? So that's when we're using the tool we would, we could take another approach with um the hooks. This is what we're gonna see uh a bit later here. But it's dependent, dependent on how dynamic or how deterministic do you want it to be, you would choose one or the other. So now let's add the 2nd component of RAG of context into our agent. So let's look at the memory, right? So what uh memory does to persist our agent conversation, so we added the rack so far with some generic information, right? Now we are adding memory to have more personalized uh context about the users. So how that works is our users when they're interacting with the agent. They have this conversation that they build up over time, right? So that information goes into agent core memory as events. So those events we term as short-term memory. Yeah, those events we uh we send into short-term memory which stores those events into a data store as raw conversational messages. Now those messages based on how you configure it, so you can choose to have more extracted insights from those events. And say that for example, hey I wanna create user preferences strategy. So I want to extract from all the conversations you've had, I want to extract user preferences um from that conversation. Or you can choose something between uh semantic or summary. So these are some predefined strategies that we have in agent core memory, or you can customize those as well. So you can either customize the prompts of those in-built strategies or create a completely new pipeline of your own. So that's, uh, completely dependent on you. Yeah. And if I can just add on to that just for a sec a second just to give you a use case, right, we have our agent which we are speaking with, uh, for a whole week. If I'm stacking up the conversation history of over a week and sending it over to the agent and for the LLM to process. Whoever is paying for the bill is not gonna be happy because you have this really big context and it's gonna be affecting the price, because it's a huge context, it's gonna be affecting the latency because, you know, it's got to process more and then it can even have problems with accuracy, right? If I've got this huge amount of information I have to process, it, it should be better if it was if if if it was smaller, right? So, Instead of taking that big context that big context and passing it on to the agent, it makes more sense to extract what's actually important from that conversation, right? And that's where the long term comes in where you defining, hey, from all of these messages that are going in between my users and my agent, I wanna extract specific things so I can Uh, extract user preferences, semantic facts, um, session summaries, or you can even find your own, you know, uh, workflow of what you want to extract, and I shut up with that, yeah, and yeah. For the short-term memory. What use to decide what is. For the short-term memory, you store deterministically, you send those messages in the, in the agent core memory. So there which one should I send like if I have a conversation. Ideally you send all of them. I'm just actually and you, you're not repeat the voices. Oh yeah, sorry, sorry about that, yeah. So is this based on the session of the user. shared the So, Asian, uh, agent core memory has, um, is, uh, is defining the separation based on the actor ID, the session ID, and the memory ID. So, when you're creating a memory, It takes in two more identifiers from you, which is the actor ID and the session ID. So the actor is usually the user, but it doesn't have to be. So it takes in a string that defines who do these memories belong to, right? So it can be the agent, it can be a project, it can be, uh, a team, it can be, um, a combination of these. You have flexibility of what you want to define in that actor ID and same for session ID, but those would be the identifiers based on which it will separate the memories one from the other. Yeah. So one for example to take the feedback that user has given. Sessions that they have attended the survey we hope for this session it's a 5 star. I'm just saying. And so to give recommendations on the next stage when they ask, would that be a memory? Would there be another tool that should be stored in the memory that should be in the memory, yes. So custom what we want we will, we, we'll get to that. We will look at the code. So I think code should clarify um how to use it, but this is just more on the conceptual level right now. Should we proceed? Yeah. One Um, still not getting the hang of it, to be honest, um, probably because. the accuracy right, so you're storing everything into Dynamo, and then when you have to get back, you know, information, you're getting back everything that is from Dynamo, right? That that dynamo would be um corresponding the the short term memory, right? But what, what the, the, first of all, this is a managed service, so you don't have to focus on building a memory storage solution. We, we'll give that for you, um, taking care of infrastructure, um. Uh, scale and, and all the other things that normally come with, um, managed services with AWS, but then We don't stop there. We don't stop at that dynamo that you're building. We are, should come, yeah, sorry, just we will explain, OK, yeah, I'll, I'll let the girl explain it. Yeah, I'll let it say what comes after. I think, I think we will explain with the long term of what you're describing, um, so maybe that, that helps. If not, then we can, uh, take this question again. All right. OK. So, um, based on how you've configured, uh, the memory when you created the resource, you can define one or more strategies for smart insight extraction, right? And you can say, you can define one or more of these strategies, summary, semantic, um, or, um, or user preferences or create your own. And based on that, it goes through an extraction module, so it goes through an extraction pipeline, which essentially consists of two steps. So one is the extraction step which is gonna look at all the conversation and extract only the relevant piece of information. So for example if you're talking about user preferences, not everything that the user talks about is talking about user preferences, right? So from all of that context it's gonna only retrieve wherever a user mentions something that they preferred they like they dislike, um, or you know whatever they prefer so it only extracts that information. Now the second step in this extraction module is the consolidation step. So now it looks at the existing vector store. So whatever memories it has existing about that user, it looks at those, retrieves the most relet most similar ones, and it it compares the new memories that it has tried to create and the existing memories. And based on that, it consolidates that and takes one of the steps, so it either can add. A new memory into the vector store or it can update an existing memory with a more complete piece of information. Or the third one, if it's redundant, if it's not adding any new piece of information, then it just skips it. So this way we do not have any duplication in the memory store that we create. So once those memories are ready, we save them into the long-term memory store, uh, which essentially is a vector store from which you can semantically retrieve the similar memories, uh, as and when the user asks something similar, uh, from the agent. So you can retrieve the long term memories which are more extracted insights, but you can also retrieve the raw events so you can list, let's say the last 5 messages from that particular session plus, uh, perhaps the, the, the summary of that session to compress the context so that we do not have to, uh, deal with the entire conversation window, um, sorry, the entire con uh context window so we can. Make our agent more. Uh, more aware, yes, speaking to. Um, all right, so how do we do that? So with strands, um, First of all, we create a memory manager, which provides us with the control plane API uh where we can call get or create memory. And we provide in a name of the memory. And the strategy that we want to create, so these strategies, we don't always have to provide one, but um we can provide. No or more uh strategies that we can add in here. So it takes in a list of a dictionary which takes in what type, what strategy type do you wanna define, what name space do you wanna have. So name space in this case is gonna take in a path-like structure which allows you to create that separation of concern in the long term. So in short term we did that by actor ID and session ID. In long term we're gonna do that by name spaces. So here, um, based on, you know, how do you, how granular or how global do you want your memories to be, you can define, um, a longer path with more variables, so you can add one, yeah, you can add zero or more of these variables so you can add actor ID, you can add a strategy ID, um, or you can add a session ID as well. So based on that it'll create um a path for you which you can then retrieve from. So basically instead of just having a very big vector database where you just throw in the new memories that are being generated automatically, you're able to create your own uh separation, you know, you create your own structure and how you're saving those memories so that when you have to go and retrieve those memories, you have it in a logical way. Yeah. And then you can define also an event expiry date, which defines how long do you want the short-term memory to stay. And that's, so in this case, it can, uh, it can be, uh, anything between 7 days to 365 days. And then you call that API that returns you back with a memory ID, right? So this memory ID now you can store somewhere and kind of use it to every time you want to store um the memory information for your users. So it takes on about 1 to 2 minutes. Yeah, I imagine you'll have your, your agent, and you will have your, you know, you have your agent and you have your memory resource. That agent will have to write those events to those memory, to that memory resource, right? So you have that memory ID. You have your actor ID, and you have your session ID, and you just have to send the information over to, to, to agent core memory, exactly, and I, I, I won't say anything else. OK, um, you should, you should. Um, all right. So, how do we add that to the agent, right? So we have created a memory resource. Now, how do we write to that? So first approach was a tool approach that we saw for rag. The second approach, uh, that I was talking about was hooks, right? Here we are taking, uh, control of the of different parts of the agent life cycle. So how it does is, uh, it enables us to write or to take an action at different points of the agenttic life cycle, meaning I can define that whenever the agent is initialized, for example, uh. Run a certain action, right? So that will I can hook something in that part of the agent life cycle so I can say every time the agent is initialized call the agent initialized event, right? Similarly, whenever the user sends a request, call the message added event. So this is then another hook that you can add. So we're going to use this capability, which, you know, you can hook at different points in, in, in time. So when the tool is being called or when the request ends, so when the agent responds back. So we are going to use two of these hooks to create and to add our memory and to retrieve from our memory. So how does it look like? So what we do here is, first of all, whenever the agent is being initialized. We are asking our agent to retrieve the user preferences from the long-term memory. So every time the agent is initialized, we essentially load a profile of the user to know what their preferences are. Right, and then every time a new message is added, we are storing the message that the user sent and also the messages that the agent has sent. Um, to our short term memory now behind the scenes it should automatically propagate to long-term memory so you don't have to do anything there, but that's how we'll do. And the second aspect was we already created a tool for the session data retrieval which was a rag aspect. So that's what we are doing, uh, for retrieving the event information from knowledge bases. So now let's see that. So here we are creating a class called Memory Hook Provider. Which inherits from the hookvi provider class provided by strands agents. So here what we do is we're defining mainly two functions. One is on agent initialized. So here I say that whenever the agent is initialized, search the long-term memories from this namespace. So this was a namespace that I defined when I created the memory. And with this query, so this way it can find similar information. So I already like hardcode that, hey, give me the user preferences, interests, and background so that I can personalize my um agent based on that. So give me 5 of those um preferences and those preferences I will then load and append into the messages. And this way, now my agent knows or has some context about what does the user prefer, right? And the second event that I'm going to add is on the message added event. So every time a new message is added, be it tool message, be it user message, be it agent message, any time a new message is added to the state. It is going to load the last message. And it's gonna, uh, add that message. It's gonna add that message into the, into our, uh, into the memory session Manager. So, it's gonna send it to Agent core memory. As a conversational message. So it's going to take in a message content, which is gonna be the string, the question or the uh content that the user provides, and along with the message role. So that's what we are sending to the to short-term memory for it to be processed for long term. Cars or the polarization and. Um, so this you have to define. So, for example, here, Um, what we are doing is, in this case, we are adding it into the system prompt, right? The user preferences we are adding in the system prompt. You can add it as a new message as well. And there you, you can write like literally in a, in a string message you can say these are my user preferences so it cannot literally separate between oh this is memory or uh this is a user message but you have to define that uh on how you want to send it to the agent. A nice thing about. This is that we give you the tools and we give you the flexibility in using those tools for your use case, right? Every I'm pretty sure everyone, every everyone here has a different use case and they're gonna be able to interact with memory in a different way than the person that's sitting next to you, right? Yeah. Or the privacy a large section is long. Council. So are you, are you worried that some people have access to other people's information? Yeah, we'll get to that, we'll get to that. Yeah. So right now you, you, when you're sending messages to the, um, short term memory, you're defining an actor ID and a session ID, right? So with that actor ID you can actually define who is that who does that um event belong to, right? So you have that logical separation in that in that way. And also you have IM policies that you can apply which contain conditions so then you can give, you know, permissions to an agent to only access a memory resource if the actor ID is corresponding to whatever you say, right? So you have multiple ways of of securing this. Yeah, but I'll cover it later, yeah. All right, so now we have created the hooks, um, and now the only thing we have to do is to register those hooks with the hook registry so that the agent knows that these are the callbacks that I have to do every time an agent is initialized or a message is added, right? So you add that into, uh, the hook registry. So now that is done. So we have now a way to call um our memory, right? And we've also already defined a way to call our knowledge base. So now we're going to initialize the class for the hook provider and you're gonna add that to the agent. So we have defined the memory hooks. And we're going to add that to the agent along with the tool that we created for searching the reinvent sessions for the knowledge base. Right? And on top of that we provide a state. So this is where we are giving it in a more deterministic way we are giving it the actor ID and the session ID. So this ideally should come from the application. Once the user has been authenticated, you get the user information from that from that session and provide the actor ID based on that and similarly create a, create a new session ID or get it from whichever environment it's running in. But that you pass in as a state, so to make it more deterministic and to make sure that it's the same information that is propagated within the agent. Yeah, you basically don't want to say date to the agent, hey, this is the user blah blah blah, and this is your system blah blah blah, because it will probably hallucinate and evoke away, right? You wanna pass that as part of the application code. Exactly. So now we have the agent ready. Um, now we can test it, right? So first thing we do is to give it some information because right now we haven't talked to the agent, so it doesn't know anything about me. So I give some, uh, I see in some information about myself saying my name is Alice. I'm a software engineer at Telcorp. And this ideally should be uh loaded in a more um gradual way so when you're actually building an application this will be built over time, right? But in this case for demo purposes we are kind of loading in information at once. So now we send that event, uh, we call the agent with this information. And the agent responds us back with, OK, you know, thank, nice to meet you, Alice. um, thank you for telling me about yourself. And Then, behind the scenes using the hooks, it should be able to store that information in the short-term memory. And then around about 30 seconds later it should also be um available in the long-term memory, right? So the next time we are calling the long-term memory records we should be able to see what we have already fed in. So if we list now the pre uh list now the long-term memory records from that certain prefix, it should be able to retrieve certain our preferences that we just sent in. So now the interesting thing to note here. Is it not just stores the preference but it also stores the context how it extracted that preference so it says the context is that the user explicitly stated that they work as a software engineer at Tech Corp and that's why I extracted the preference that they work as a software engineer. Um, so it kind of defines a more explicit, uh, defines more proper reason of why this preference has been extracted, and based on that, it, um, it also creates a preference. And now you can use this information across sessions. So if you create a new agent with a new session ID. Um, with, with the same actor but different session ID, you are able to retrieve, uh, the same information about the actor and you're able to use it within, uh, the question. So if, uh, so I asked this question saying, hey, which sessions should I attend at Reinvent, right? And I already mentioned that I'm interested in AI and security. So it responds to me back with Based on it, I'm happy to help you with the perfect reinvent session, and based on your profile, I can see you're interested in cloud architecture, AI and security, and that you prefer hands-on technical sessions. So let me give you some sessions, right? So based on that, it gives me, it searches the reinvent sessions and gives me that based on your interest, these are the top recommendations that I'll say. So it gives me some Gen AI, some cloud operations, um, and some security. Recommendations, right? So, it makes it more personalized for me of what I'm interested in, what's my profile, and based on that, uh, it responds. So now we know that our memory is working. Let's move to runtime, right? So what, I mean, we now have the agent running um in our, in our local environment. How do we move this to production? So we need something that is giving us a session level isolation that because as you said, we are dealing with user data here, right? So we have to be highly uh vigilant on how we are uh deploying this. So the session not just has to be isolated on the compute level but on the on the memory and the storage level. So this provides us, Agent Kurunim provides us with true session isolation, so. With the compute, memory and storage, and it provides us with um a fully um enterprise-ready production workload. Workload-ready, uh, system, and it is automatically, uh, scalable. So it's a server-less, uh, setup which allows us to, um, run hundreds of sessions in a parallel manner. So now how do we add this to the solution, right? So we have the endpoint that we created, uh, so everything else remains the same, we're gonna deploy this to Agent Kor on time, right? So this is gonna be our, uh, way to run this, uh, in a introduction. So how does that work? So first of all, what we have created right now is we have an agent with any model, any framework, so we uh chose our bets. But you can bring in any model, any framework of your, of your preference, and then what we're gonna do is to create um two endpoints for Pink and for invocations. Ping we use for health checks and invocations we would use to know where the entry point is. So how do I invoke? Um, and then we add a requirements file which defines what dependencies do we have to install. Now that will be what defines our agent. So the first thing we have to do is to deploy that or to push that in a more packaged way to the ECR. So ECR, once we have pushed the code to ECR, We will have our container image ready and we will provide some more configurations like we will provide the protocol settings, like which uh protocol do I want to use HTTPA2A MCP, what, how do I want to configure that uh that agent, um, and we provide some network settings and that's what completes our configuration setup. Now once we have that, they're going to deploy that to Agent Core run time. And then we have our agent ready for invocation. So then we can, uh, in a concurrent way, we can invoke multiple sessions, um, from the application that we built. And each, each session is a micro VM that spins up, loads that agent code, and you have a user that's gonna be interacting with that specific agent, your uh session ID. And when that session finishes, the micro VM gets shut down and everything gets deleted. Right, so that's why we have to persist the information somewhere else, because when the agent call run time session finishes, the context that the agent has is gone. Yeah. All right. So how do we prepare it for agent core runtime? How do we prepare the agent that we created right now to deploy it on the agent coun time? So we literally have to add 4 extra lines of code. So except the uh structuring of the code, we have to add 4 lines to make it ready for runtime. So first thing we add is importing the agent core application, right? And the second thing we initialize the app. Now the rest of the code is more about uh it's the same things that we already created um in the notebook previously so we created a hook provider, we created um a tool for searching the reinvent session, uh, we create an initialized agent which is doing the exact same thing as we did above. So it's importing the tools, hooks and adding the actual ID session idea state so we just package it into a function. And then we create an entry point. So this is the line number 3 of the code that we have to add. So, we add, we create this, um. Decorator to define that this is going to be the entry point of the application. And here we've taken in a payload of whatever the user passes in from the application, uh, from the front end. So that takes it, that, let's say gets in a prompt and the actor ID which is going to be done after after authentication. Um, and then that is what goes in, initializes the agent, uh, takes in the user input, and gives a response, right? And then finally you have to run the application. So this is line number 4. So that is it. So this is what you have to add to make it ready. So essentially creating something like a fast API server that allows us to run, um, this piece of code. Then we create an execution role and we are, um, then we configure and launch. So these are the two things we have to do to get it ready as an endpoint that is ready for consumption essentially for um for different users. So we have um. This agent Core starter toolkit provides us with um some uh tools to do that. So we have a dot configure. So first of all, we configure the application with an execution role. Uh, we allow it to auto-create the ECR, provide a requirements file, and ask us, ask it to, uh, take in the agent. And basically configure based on that. Now, once it has configured it, it creates a Docker file for you. If you want to customize that, you can create your own Docker file or you can also provide your own um endpoint from a different uh registry, essentially. Um, then once you have that, then you can call. launch, which is gonna launch the launch the application. And uh put it for you to warm-start, right? And here you can also provide some environment variables, uh, which takes in um your memory ID, knowledge-base ID model ID and region. And now we have it up and running, so this will take around about a couple of minutes for you to get started. So now I think the only thing missing here Danny is the identity. Yeah, you wanna I would just sort of small indication to see how it, how it looks like, OK, yeah. So basically we just passed from our local notebook where we have our POC working without our managers and now we need to get this into production, right? So that's where we use 1 core runtime. We pass from a notebook to actually having our agents running at scale securely. Um, and of course if you're not gonna show the how the invocation looks like, um, you can see there it says saving core runtime. invoke. I'm passing in my prompt. What sessions would you recommend, and in this case I'm passing my actor ID, right? This, remember right now is hard coded. The goal is not to have this hard coded. It should actually know who the actor is based on our login, OK? So if you see there, uh, we'll, we'll. Yeah, so we asked the same question. What sessions would you recommend? exactly? And more personalized recommendations to us. Yeah, OK, so that's basically our agent running in. 18 core runtime. Last bit of the of the architecture is identity, right? So here's where we use agent core identity. How is this gonna help us? But initially we're gonna have our user which uh interacts with our application, right? First step, our user logs into the into application using our IDP of choice, right? We support multiple um IDPs, right? Our user logs in, gets back an IDP token from that IDP. And then I can go and invoke 18 core runtime, right? So to that call that Takarsta had before, I will have a new, uh, variable that will say beer token, right? And there I will include the IDP token that I got back from my IDP. Now thanks to agent core identity, um, inbound authentication, so basically it determines who the user is and if it's allowed to call the agent. Every time we call agent core runtime before it actually does invocation, it will go call agent core identity. Identity will go back to the IDP and validate that IDP token, OK? And if that's valid, it will say, hey, you're allowed to go ahead and do that invocation. OK, pretty simple, uh, steps here. So how does that change at the code, uh, level? When we're doing in the agent core runtime configuration, we're gonna add two different things. One is the authorizer configuration. That's where we are configuring our JWT authorizer. In this case, I'm using Cognito. So, um, I'm passing in my discovery URL and my allow clients, right? And also very important bit here is my request header allow list and what I'm doing here is that I'm allowing agent core runtime to propagate to the agent code the authorization token, right? So what's gonna happen next inside the um Agent, if I show this here. Is that when my User logs it, huh, OK. When my user logs into the into the application, it will retrieve that identity token from Cognito. It will do the agent invocation passing in that cognito identity token and from inside the agent, I'm calling Cognito with that token to retrieve the user information. Right, um, and that's what the cars are showing there. So the same code that I showed before, it's the exact same functionality. The only thing that's different is instead of passing the actual ID directly, we are getting the user identity from the authorization header. So this is what is being propagated from um from the agent core runtime and this is being authorized um from agent core identity by Cognito. Exactly so no more hard coding either sessions or actor ID. The actor ID is coming straight from the identity token from Cognito session ID. We're using the same session that um agent. run time is running, right? So as you remember before, it works with sessions. Each session is a micro VM that spun up, loads the agent code, and runs. Um, that session has an ID, and that's what we're using as an ID, uh, for our session, um, inside 18 core memory, right? And I think that is it with 4 minutes left, that's it. Um, and exactly what Dani showed, um, on the screen. So when we're configuring it, we added these two configuration for JWT authorizer and the header configurations. So we have them here that it that allows it to know what is the discovery URL from where to authorize from and what would be the client ID. So what would be the allowed clients for me. And that is it. Cool. Keeps it. So let me just go back to my, we have 5 minutes to take, yeah, um, resources and questions. Did we put your resources here? We have some authentic AI resources to get started, um. At GiKI, we have, uh, GitHub Bedrock agent core samples. We'll be pushing the code that we saw today this week there. So if you go to GitHub and say Amazon Bedrock agent core samples, um, on their memory, it should be there, um. And yeah, uh, follow me on LinkedIn and the cars I will probably post it there also, um, very important, please, uh, complete the session survey. 5 star is recommended. Others are optional, um, and yeah, I'll take any questions now. Thank you.