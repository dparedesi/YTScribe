---
video_id: -8ySh_LN6JE
video_url: https://www.youtube.com/watch?v=-8ySh_LN6JE
is_generated: False
is_translatable: True
---

So good morning. I was, you know, it was kind of funny. I was walking here today and I took a picture because the, the sunset was, was beautiful. I don't know how many of you were up and I realized I can be a pretty good photographer with just the camera in my pocket. But to be a great photographer I need to know just a little bit more and so today we're gonna spend a little bit of time taking EBS to the next level and helping you understand how to, um, how to maximize performance of, of your EBS volumes. So I'm Mark Olsson. I'm a senior principal engineer on the EBS team and I've been thinking about storage and learning from our customers since 2011. Now you may look at my title and you may go wow, he's a fancy title. He just sits there armchair quarterbacking and designing stuff, but really the neat thing about being a senior principal engineer at Amazon is that I still get to write code and I carry a pager, right? Like I'm on call. I'm, I'm kind of in the trenches with, with you guys understanding exactly what's going on with my system, uh, and I get paged actually a bit because I'm on a few different on-call rotations keeping track of the entirety of, of EBS. Um, hi, I'm Jody. Um, I lead product management for EBS as well as a handful of other products like AWS backup, um, AWS DataSync, and, um, AWS Transfer Family. Um, I also carry a pager, um, so I can relate to a lot of the scenarios we're gonna walk through today, um, but with that, I'll go ahead and start, um. So I wanted to use, we're gonna use a couple of scenarios today to walk you guys through um things that you can do both beforehand as you're planning your application um to make better choices and eliminate surprises um that could happen in production later on, um, as well as um. Uh, a bunch of monitoring tips and tricks, uh, using some of the newer features we've shipped over the last year or so, um, that help you when something bad does happen try to isolate the problem, figure it out, and recover as soon as possible. So we're going to walk you through a fictitious, we're now two employees of the fictitious company, AnyHealth. Um, it's a medical device manufacturing and healthcare software company, um. So what that means is um durability is of the highest importance. um, this is, you know, medical information, etc. um, and it's highly regulated so we we need to do a lot in our infrastructure for resiliency um. In compliance, uh, Mark is actually the software engineer who is building the application. Um, he is on call this week. I thought like it was funny actually. I had to do a really complicated three-way on-call trade-off in order to not be paged on stage today. Um, but he's on call this week. I'm not on call because I did my, I traded my favors and got off call, um, and I'm the infrastructure admin helping Mark to make infrastructure choices, um. So really quickly I'm just, I know this is a 31 session. I'm just gonna go through a couple intro um items because they're always a few folks who aren't as familiar with the portfolio. So we are block storage, right? Object S3 file FSX, um, EFS, etc. um. We've also got um other services like AWS backup and um you know that coordinates EBS snapshots um as well as backs up all kinds of other AWS services. If you haven't taken a look, um, it's really worth uh a uh a look through because it's a, it's a great service, um, you know, it's got a lot of regulation compliance features, um, that pure snapshots don't have. Um, on the other side of the fence you've got Data Sync, which is a product that helps you move around data between on-prem and AWS, like between clouds. It's something that we've seen a lot of use for with AI workloads blowing up recently. And then if you're on the edge, we've got AWS outposts. Oops, go outside. Right, so you use volumes, you know, it's persistent network attached storage for EC-2. They're independent of your EC2 instance. Um, you can attach to any EC2 instance in the same availability zone, um, and the biggest difference is you stop start your instance, your ephemeral storage goes away, um, your EBS volume stays put, and you can just reattach it to another instance. So within the storage portfolio, we've got a bunch of different products split into the SSD backed products and the HDD backed products. Um, but for today's talk, I'm really gonna focus on our primary two SSD volume types, GP3 and IO2 Block express. Um, in addition to the cores volume snapshots, we've actually been putting a lot of work recently into our data services features. Um, a lot of EBS, uh, data services features historically worked on a kind of best effort basis, so it's very difficult to predict exactly when the data movement operation would complete. Um, so we started investing in, um. More explicit performance where you can actually provision the performance you want for those services first with a fast hydrate feature that Oh God, we named provisioned rate for volume initialization because that's an exciting feature name that rolls off the tongue. Um, I'm trying to change it, but for now that's what it is, what should I call it PRVI, um, but there you can actually set a throughput rate, right? You can say, OK, I want when I'm creating a volume from a snapshot, I want this to be done at this exact throughput level so you can know when it's gonna finish. Um, similarly, um, we. A time-based snapshot copy feature that does the same thing but with time. You can say, OK, for my RTO purposes I need this backup to complete within x amount of hours. You can set that and then, you know, the snapshot copy, which previously might happen really fast, it might happen really so it kind of depended, will happen in exactly the time frame that you need for whatever compliance or internal targets that you've been setting. So now I'm going to talk a little about planning your infrastructure, um, alongside your application and the kind of things you can do upfront to make the best choices. The first thing you've got to do is understand the kind of workload that you're going to be deploying or building, and before you start building, think about, think about what it needs. So in this case, um, for our patient medical records application, we're going to be building a, um, we're going to need a transactional database. So we're going to build a relational database that um. I needs super high performance, um, very, very high durability, um, and for that, I'll just flip to, here are a few workloads we have at any Health, um, for that, we're going to be looking at IO2, um, because that is a volume that has both the maximum performance, the lowest the lowest latency and latency consist best latency consistency, um, as well as the 59 of durability that we need. Um, but in other cases when, you know, when that's not your object, you can use different volume types. Um, and another thing that is useful to do with EBS, you have the flexibility to kind of break up your workload, um, and put, um. Different volume types or just provision the same volume type but different amounts of performance for different parts of the workload. So like if you can, if you're running a relational database, you could put the journals on IO2 or a more highly provisioned GP3 volume. Similarly with Cassandra, you could do that for commit commit logs, um, or even with Kafka, you could use something like one of our um HCD backed volumes, uh, for topics, for example, because that's, you know, a lot of, uh. Sequential, so here we go, um. A couple of things. So, IO2 is our, you know, is our fancy volume, 256K IOS, um, and it has the 5 nines of durability I talked about. Um, recently though, you might not have caught this, we launched larger and faster GP3 volumes. Um, so here we took performance from 16K IOPs to 80. Uh, we doubled the throughput performance to 2000, um, and we actually, it's not on the slide, but we increased the maximum volume size from 16 to 64 terabytes, which means, especially if you're running containerized workloads where you know you can't raid together volumes, now you can just grow one big huge volume as you need it, so that allows for a bunch more flexibility. We also updated our latency, our public facing latency guidance this year, which is something that we don't do that often, um, to state that IO2's got an average latency of under 500, under 500 microseconds, um, and then it has 10 times fewer, uh, outlier IOs than uh GP3, for example. Let's go take a second and look at the performance definition of these two volume types, um, because it's easy to get lost in the numbers and 9s. Um, GP 3 is designed for single digit millisecond latencies, um, so that means 99% of the time. Um, so that means anything from under 10 seconds to 10 milliseconds of latency is within the definition. In practice it way outperforms that, um, but that's the, the definition that, you know, the criteria that we're building against. IO2, on the other hand, is designed for sub millisecond latencies 99.9% of the time. So that's like an order of magnitude of difference in latency consistency, and you can see that as you start to look at outlier IOs. So both GP2's range for IOs is a lot bigger, and this is. This graph will break your brain because it's like the bottom is in log scale. It's crazy, but what we really wanted to show was as you go up from 99 to 99.9 and then you know later on to 5.9, how that's where the latency outlier difference is between GP3 and IO2, and we have to treat the products very, very differently to make sure that IO2 stands stays within that very, very narrow band. So, using, using an analogy, um, I was trying to think of like what job is really bad, like really, really bad if you get to work late on time, and I figured, uh-huh, an air traffic controller. Um, so air traffic controllers are on pretty narrow shifts and they've got to show up to work on time, minutes count, um, any, and God forbid you're multiple hours late because that means you've missed a whole shift, um, and, and things are gonna go very, very poorly. Um, so when you're trying to figure out a way to get to work, uh, and my airport, my air traffic controller takes the train, um, you have to decide whether you're gonna take the GP 3 train or the IO2 train. So, um, GP 3. is going to be on time as a commute. The GP 3 train is on time 99% of the time, so that means you're going to be late once in 100 days, whereas IO2 with the IO2 train, you're going to be with 99.9% in consistency. You're going to be late once in 1000 days, um. Similarly, um, your arrival time range for GP 3 is a lot bigger than IO2. For GP 3, you've got like a range of about 20 minutes, you know, I rounded up a little, um, where you're going to every day get to work. So you're gonna have to do some planning beforehand to make sure that, OK, I'm leaving a good. early to make sure that I, no matter where I land in that range, I'm gonna be able to get to work on time versus IO2, which is staying in a very, very narrow narrow band, so it's only, you know, 2 minutes, um, that you're gonna be, uh, that that you're gonna be, um, arriving for work within and so you don't really have to go out of your way to do much for it. Um, similarly, similarly with GP3, you're more likely to have a bigger delay, right? That's where the 99.9, 99% versus 99.9% really comes into play, whereas with IO2, not only on an everyday basis are you staying within a very tight millisecond range, um, you're also way less likely to have a bigger delay. Um, if you don't know though, generally when you're starting design, just start with GP2. Um, you can do some testing, see how that works out for you. Use our elastic volumes feature which Mark will talk about later to modify your volume, um, if it isn't the right choice, um, but that's a good no-brainer. So I'm gonna talk a little bit about here in testing, these are the things you can do beforehand, um, so before you've actually launched your application, you can do some testing so that you know upfront before your page at 2 o'clock in the morning, right? Um, how your application is going to respond to different scenarios, um. The generally speaking, the higher up the stack you get, the better. You want to be as close to your application as you can to simulate the things that happened and then what happens with the specific interaction of your infrastructure and application. So at the lowest level there's things like FIO testing, right? That's a flexible IO generator, um, that's just testing raw IO. It's great for testing your maximum performance limits. So if you say, OK, so is this, um, this volume says I can do 16,000 IOPs, can I check? Yes, um. It doesn't really tell you anything about your application, um, but it's good for that purpose. Um, if I compared it to like driving a car, um, it would be like driving a car on an empty racetrack. So it says you can do 100 miles an hour. I go all out, yes, if I'm brave enough, which I'm not, yes, I can, uh, and there you are, um. Another, another type of testing that you can do is I've, I've put up TPCC here which is a, a standard OLTP database benchmark, but there's lots of different benchmarks out there that can simulate the typical patterns of the workload that that you're working with or the database you're working with um. So for TPCC that's stimulating um things that typically happen under high load for a transactional database. So think like order entries, payments, deliveries, that kind of thing. Um, it's got a ton of tuning parameters, so you can, you know, mess with it and make it do whatever you want. Um. But it shows you um realistic read, uh, mixed read write IO patterns um that will show how, how you'll respond, how your application will respond to a typical um database-like transactional workload. Um, so it's getting closer there, but ultimately you're not really getting to real, real testing until you're starting to do some load testing on your application. So there you are. Looking at your own specific application and all its infrastructure and seeing how it responds to the conditions of high traffic that you feel is important to test. Um, so what's great about that is that it can reveal real bottlenecks, um, real retry storm. Um, and you're generally testing full end to end behavior, um, so that's something I know we're all have such compressed cycles and we're trying to like ship features, get products out the door, um, but doing a really thorough job on your load testing can save you a lot of misery, uh, later on in the game. And then lastly, we've got uh AWS fault injection service, and I'll talk about that one a little more because we did some new stuff with EBS here. So FIS is a service that allows you to test your application against specific worst case scenarios of your own devising. So here you're no longer measuring just performance, you're measuring your resilience, graceful failure, and recovery under controlled chaos. So this is a really, really helpful tool to use, um. The thing that is particularly useful about FIS is that you know when you're doing all of your other testing, you're doing it under fairly normal conditions. Maybe the traffic, maybe the load is high, but everything else underlying is basically functioning as expected. FIS allows you to do is simulate, you know, an infrastructure bad day. So these are things that aren't easy to test for because they don't happen very often, but when they do happen, uh, it might cause a lot of problems with your specific application and you know, and all your infrastructure, so. Uh, if load testing is like driving your car through rush hour traffic, um, in a, uh, in your own city to kind of figure out what impact that'll have on your commute, um, then. FIS testing is like testing against a horrible situation that you choose. Like maybe there's a highway closed or um there's a massive hailstorm, um, or there's, you know, um, like lots of fog, whatever, um, you can pick them out and test against them and FAS works for not just EBS. I mean it tests for like everything. um, I'm just talking about the EBS specific actions because those are relevant to our, um, uh, presentation. So in September, um, we launched some new FIS actions for EBS um we launched um. Latency injection that can stimulate degraded IO performance on your volume to replicate the real world signals like so you can see here how do my cloud watch alarms work? Are things working? Um, you can see OS timeouts. You can basically see the stuff I've baked into my application um and the the different like alarms and and rail safeguards I've set up, are they actually working when something really bad actually happens, um. So they've got uh some predefined templates in um in FIS and the EBS consoles you can see them um and they've also got um. customization. So you can start with, if you don't know what to use, you can just start with one of these, but then you can actually customize them. So if you're a super advanced customer and you know storage really well, you know the kind of things that can go sideways, then by all means have at it. You can do things like change the percentage of IOs in which your latency action will be injected. Right, so that's how, you know, how often is this, this happening among all of my IOs. You can change the amount of latency. So for IO2, you can, you know, inject a minimum of 1 millisecond latency. For non-IO2 volumes, you can inject a minimum of 10 mill. Seconds latency and then for that could go up to 60 seconds, so a full minute of let's say it's just stalled IO right like oh simulating a stuck volume or something like that then you can see, OK, what happens when this happens with the latency also you can Decide whether it's going to be persistent or intermittent. You can split it out from reads and writes, so there's a lot of things that you can do pretty easily with this tool that will help you know what what happens when you know a rare bad day happens on your infrastructure. You can also set, by the way, the duration for how long these things take. Um, so the minimum is 1 2nd, the maximum is, um, I think 2, I can't remember if it's 2 hours or 12 hours, but it's, you know, it's a long time, um, so you can see how, OK, if this thing is happening and for a while, what happens without actually having any of it happen to yourself and get you hauled up out of bed. So here I go. I'll transfer it to Mark and he will talk about OLTP databases. Cool. So we've gone through some of this benchmarking and testing, uh, and we've, we've decided that our application, uh, our medical records application, we're actually gonna have a couple different databases, um, for our, our patient information, uh, we're going to put it into a traditional relational database, um, and so for, for this we're gonna, we're gonna choose something like Maria DB. You could have just as easily chosen my SQL uh Postgrass, uh, or even a commercial database like SQL Server or SAP. And so we've determined that this application runs best on instances that have kind of a a 4 to 1 ratio of memory to CPU and so this fits into the M class of instances, uh, at in in EC2 and we're gonna use Graviton 4 to get, uh, even better price performance than than previous generations of graviton. So our database is replicated at the database layer, so we don't have anything below that that's that's doing database and one of the, one of the nice things about having an application level replication is that there's, there's a lot more uh state and knowledge of what the data that is actually getting transferred is and so the, the application can make smarter decisions and maybe not do blind transfers of things that don't really matter. As we choose our storage, uh, we're gonna split up a couple different things, and I'll, I'll talk about why in just a moment, um, but we've got a write-ahead log. Uh, write-ahead logs for databases are, uh, typically gonna be lower que depth, but they want lower latency for that as well. And so we're gonna use an IO2 volume for that to, to really get that, that nice low latency. And then our data volume, we don't actually have a high pressure for this particular part of the, the workload, um, but we know that the requests are gonna be large so we're gonna use GP 3 we're gonna provision some extra IOPs and some extra, extra throughput. And maybe that'll work. All right, so I'm gonna pause here. Databases are interesting We're gonna talk a little bit about how IO works in the stack. Now you don't have to memorize this image. It's pretty complex, looks like a Rube Goldberg, and it's also a little out of date, but that's OK. Uh, the important thing to recognize here is all those little orange snake looking things, those are cues. And so in any storage system, depending on like even if it's just the raw device, there's gonna be cues in the storage system. Now EBS, this is closer to about 5-6 years ago what EBS looked like on on our Zen instances, um, but there were a lot of cues in the stack. And so when your application wants to submit an IO request. It executes a system call and it puts the, it puts that request onto a queue. The system call is typically picked up by a kernel file system which maps the request to the locations on the disk drive. Now a disk drive, whether it's a mechanical hard drive, you know, with a swinging arm, uh, or, or an SSD they're gonna have an allocation unit. That allocation unit is typically called a sector and then the other important thing to note here is that disk drives have what's called a maximum transfer size. EBS as a virtualized storage system has different limitations and we don't necessarily show through what the actual media is doing and so we'll have potentially a different transfer size and and allocation unit than the end device might might see. And so why is that important to know? Well, as, as your request gets populated through all these queues and puts in different parts of the stack, it's gonna get split up, right? And it's gonna get split up probably in a smaller requests than the original and the kind of sub-quests and maybe put on different devices or at the very least like on an SSD on different chips within the SSD. And then everything's merged back together and we, we come back to the top of the stack and the IO is returned. Now why is this important for databases? Databases are really concerned about storing your data durably, and so one of the things that they're going to do is make sure that. Your data is actually written. They know that things can be written at multiple times to to different media chips and so one of the techniques is that they will, they will do what's called the double write or write it a couple times uh to ensure that at least one of those is gonna be good and that they can replay it later. If there's a power failure, And your, your, your storage IO request is only partially written, you're gonna get what's called a torn right. Now in nitro EC2 instances, uh, both EBS and local incident storage, uh, support much larger atomic units, and so typically an SSD will advertise that its atomic unit is 512 bytes. I don't know about you, but there's not a whole lot you can fit in a single 100 512 byte sector. Um, and so. This maximum size is advertised through the NVME device model for both of these, and I'll tell you right now for EBS volumes on all current nitro instances that maximum size is actually 16 kilobytes, which is nice, which fits nicely with what a typical page size is for a database and so with the right file system configuration, so you can do an EXT4 aligned at the start of the start of the device, uh. And using Direct IO and Big Alec on the on the EXT4 file system. Uh, you can probably disable the double we protection. Now I've gotta be careful to get this configuration exactly right because if you do get it wrong and there is an infrastructure failure, you could end up with corrupted data. But uh with RDS we've worked with them to to enable this on on their database platforms and they've seen a 30 to 35% performance gain because you're not writing twice and eating some of those that right performance. Sorry, I'm on call. Um. I, I forgot to hand off my on call, um, well, we're here, so let's go ahead and, and, and dive into to this problem. So we're gonna debug this one. we've, we've got page for, uh, a database backlog. Uh, in the interest of time I'm gonna skip a few steps, but really what I would have started doing is I would have gone to CloudWatch, uh, and with CloudWatch I probably would have used Application Insights or some of the observability tools that are available there to help figure out exactly which instances and resources are causing trouble for me. We're going to assume that we use that to find the primary database. Uh, so we found the primary database of our patient records. As the as the culprit of this page. And so let's walk through a few metrics examples that can help us find the problem. The first thing I'm gonna start with when I look for an instance is I'm gonna look at this instance status check. This is a roll up of all the infrastructure problems that could exist in your instance, and this basically says is this my problem or your problem? Is this an AWS problem or is this a my application problem? And so the status check failed is is the highest level. And so these metrics are gonna be either 0 or 10 is good, 1 means something's wrong. Stattic check failed just says there's something wrong that we've detected with the infrastructure. If that's high or one, I will, I will dive into some of these other metrics. If I look at the system, uh, that tells me is the infrastructure that hosts my instance failed. Cool, this one's low. Um, if it's high, some of the things that I can do and think about are I can stop-start my instance, which will end up placing that instance on a different piece of infrastructure. If I thought ahead, uh, I would have enabled EC2 auto recovery. And so what EC2 auto recovery will do is it will do that stop-start on your behalf basically look your instance will look like a reboot. Everything will come back on, on the other side. Now I might look at the attached EBS metric that says is there a problem with the EBS infrastructure? This is usually not something that's gonna be solved with an instant stop-start. EBS is going to be working on the problem to solve it. But if you need to make progress faster than we can, there's a few things that you can do. You can fail over to a replica, uh, you can potentially create a new volume if you've got a backup either in the same availability zone or if it's a larger scale failure, you can create a new, uh, volume from a backup in a different availability zone. And then the last one that I've got listed here is instance liveness, and this is us detecting is your operating system actually behaving from our perspective and so we look at things like are you looking at the network card? Like are you, are you pulling the devices and actually paying attention to them. So all of these are low, doesn't look like a hard failure. Since we saw a backlog of requests, let's take a look at some of the performance metrics. So earlier this year, EBS launched uh a couple of metrics that that do some math for you. So previously we had metrics that uh told you the total read operations, total write operations, the amount of time during reads, and the amount of time during writes. And so if you wanted to know your average IOPS or your average throughput, uh, you had to actually do math and sometimes it was kind of clunky and didn't work out right, um, so we launched these average throughput and average IOS metrics. And these are available on a volume that's attached to an instance, uh, which makes sense because if it's not attached to an instance, it's probably not driving any throughput. Um So we'll look here we've got the data drive and our our right ahead log drive, uh, both of these on the on the throughput side, you know, the, the right ahead log looks pretty stable, which is what I would expect it's just kind of writing requests and then. On the, on the data drive we see kind of a little bit of a dip and then a spike. And so that's interesting, right? We've, we've got a change in our workload behavior. Maybe something, somebody didn't tell us something, maybe we onboarded a new customer, um, maybe, maybe it's, you know, somebody, somebody doing load testing accidentally on production wouldn't recommend. Um, but then right where the vertical line is, which is where we got paged. We see kind of a drop off, like, well, that's interesting. We know that IO2 volumes and GP 3 volumes are supposed to have consistent performance, so I'm not gonna suspect them just yet, right? I'm gonna suspect something that might have some variable performance. And so if you recall on the instance. I noted that the M8G 4 extra large has something that we call burst capability. And so all EC2 nitro instances are EBS optimized by default. Which means that they have some amount of dedicated performance of IOPs and throughput available for EBS. Now, it is important to pick the right size instance um so that you can actually figure out that that configuration and your volumes and your instances live in harmony. And here we've got a total of 40,000 IOPs, but we weren't expecting to actually use that. Uh, it was just provision just in case and so our kind of ability to, to burst on this instance with the baseline of 20,000 IOPs, uh, and then a burst up to 40,000 IOPs for about half an hour once a day, uh, or like across the entire day would absorb peaky peaky workloads, uh, similarly, the, the throughput bursts as well. But our metrics didn't actually show that we were peaky. We, we had kind of sustained things, so maybe this wasn't the right choice or maybe whoever was running the test should have should have let us know ahead of time. So burst instances also have another cloud watch metric that you can take a look at, uh, that tells you the IO balance, the IOS balance, and the throughput balance. And so, uh, very similar, we saw the throughput metric drop and then go all the way to zero, and this is the available, uh, burst burst balance, uh, for throughput where IOPs recovered because the IOS workload isn't actually driving a whole lot of throughput. Once the throughput stopped, our IOS was, was able to, to recover a bit. And then the other thing that we just launched earlier this year uh is another status check based on those burst bucket limits and so you can plug this into auto scaling groups if you want, uh, to, to increase, increase the size of your instance, uh, maybe scale out your fleet if that's how your application is designed to support scale out. And so they give you an edge trigger that you can alarm on and instead of trying to calculate a rate and pay attention to a rate and understand what that is. So kind of putting these two metrics side by side, we can start to start to see the problem, right? We've got a throughput driven workload. The throughput really bursts as soon as we run out of that burst bucket, uh, that workload drops off to the baseline of 625 megabytes per second. Looks like we don't really have that spiky load that we were planning for, um. So we probably should increase our instance size, right? So in this case we can go up to the next instance, uh, the MAG 8 extra large has double the throughput and double the ops in the baseline, and there is no burst on M8G 8 extra large, uh, so we don't have to worry about running out of that burst, and part of the reason we added burst on smaller instances is so that you could size down if you knew you just had a few spikes here and there, uh, and didn't have to provision the peak. But you're probably thinking, great, you know your instances, so you could just thumb suck and and pick it out of air. How do I know my instances? And so there's a couple of ways that you can get that information. Now I added the note at the bottom. It is all in our documentation. You can read a giant table, uh, but. I actually find it more helpful to look at the described instance types API. The described instance types API actually gives you just more than EBS performance. It gives you a lot of information about the CPUs available, the network throughput available, different CPU configurations if you have licensing requirements to, to worry about how much memory is on an instance. Um, it's a, it's a giant JSON blob. Uh, for every single instance type and then you can filter it down by particular ones that you're interested in, um, and so. I quickly ran that query and then piped it through uh a JQ query string uh which. Looks like a JQ query string. I don't know who designed that, um, but that's fine. Uh, so you can get EBS optimized, uh, baseline throughput here and so I look for everything that's greater than 1200 megabytes per second. Um, you can also get in the EBS performance specifically you can get, uh, baseline I ops, you can get burst I ops, you can get burst throughput, um, and so you can use JQ to, to filter down to what you're looking for. And so this is how I picked uh the 8 extra large version. If I wanted a little more headroom, maybe I'd go to the 12 extra large. Uh I think I'm pretty safe though. I talked to my friends, they were just doing, doing uh a load test that they shouldn't have. So once you've identified that. How do you change it? If you've just done a run instances kind of on the CLI without any sort of auto scaling or other infrastructure management, um, the thing that you're gonna have to do is you're gonna have to stop the instance you're going to have to change the instance attribute and restart it. Now you can script this so it's kind of a long reboot, um, but unfortunately we can't change the number of CPUs and the amount of memory, uh, while an instance is running live so we need to do it across, across, um, across an instance reboot. Um, If you have launched it as an auto scaling group, you can just update that launch template. And then update your auto scaling group to to use that launch template. And then do an instance uh replacement refresh workflow. Make sure you choose launch before terminate. Uh, launch before terminate is really important. If you, the other option is to terminate before launch and then you've got a completely dead period as opposed to just a slow period, launch before terminate will make sure that your instance is available and alive with any health checks you've set up before it terminates the old one. Now as part of the stop start, the other important thing to remember is unless you're behind a load balancer, um, all your storage devices are gonna remain the same, uh, but if you did not assign an EIP or you're not behind an elastic IP or you're not behind a load balancer, the IP address of that instance is gonna change and so kind of a best practice here is make sure you've got something stable that you can reference that instance with. Now on the topic of EBS optimized instances, one of the neat things about us being a storage service as opposed to something you buy and put in your data center is that we're continuously innovating on your behalf. And so while I'm here telling you how to improve the performance of your application, We're also doing the same thing behind the scenes. And so if we look through the history of EBS, uh, back when we launched in, in 2008. This is kind of fun to look at. Uh, our, our first volume type, the only volume type, it was just called EBS Standard, and I think we actually named it Standard after, uh, we launched because, well, we didn't have to have a name at first. Um, it was about 100 IOPs kind of shared, kind of clunky. It was, it was hard drives. There was really no quality of service. Uh, the instance performance you also shared, and so if you overdrive your storage, you might not have enough performance to get that data out the network, um. In 2012 we launched provision IOPs volumes. Uh, it was cool back then. See light now. Uh, the first provision IOPs volume had 1000 IOPs, and then at the same time we also launched EBS optimized instances as an additional option to separate that performance from your network. Uh, and then those EBS optimized instances had 8000 IOPs. When we launched Nitro, uh, we went up to 80,000 IOPS and EBS optimized by default. So you no longer had to to change and select EBS optimized. You're always gonna get it regardless of if you selected or not. And then more recently last year R6IN had 400,000 IOPs, so we're, we're starting to climb that scale a little bit on what an instance can do at at the max. But even that wasn't enough. And so just a few weeks ago we launched the R8GB, uh, which. Actually gives us quite a bit more. Uh, we've got 1.5x, 1.5 times more bandwidth, and 1.8 times more IOPs, and so it'll do up to 720,000 IOPs and 150 gigabits per second of EBS bandwidth. Uh, so if you really need that high performance instance, if you've got a, a large scale up workload, scale up database like a lot of commercial databases are, um, this might be the instance for you. And so if we put it all together on that chart, um, Cool, so how did we do it though? Like, you know, it's not like we're, that's a pretty big leap to to almost double the performance, right? It's not like it's something that you can probably tweak in software. So we're gonna go back to the slide before I got paged. Um, like I said, this is what it looked like before nitro. And so some of the things that we're gonna focus on here uh is kind of all the steps that it takes to get your IO to the storage server and then back, right? So we've got a number of cues, um. And this is pre-nitro, so we've got a software-based EBS stack, so there's even more cues, uh, software-based EBS driver on the, the instant hardware, so even more cues, um, network of course has cues, all those little yellow snake things like I mentioned before. Now remember this picture. I'm gonna remove the cues from the picture just to simplify it a bit, right? And I'm gonna take a look at what at what nitro looks like. OK, maybe I simplified it too much, but that's fine, we can have the conversation. So there's still cues. I just hid them. Um, the important thing here is if you start with the device in nitro. A nitro ins or a nitro card presents EBS as a PCI device in your instance, and this is PCI pass through. So the hypervisor is out of the picture. We we've removed some cues there. The way that this works is the Nitrocar has a DMA engine. That DMA engine also does encryption, right? And so as we pull the IO data out of your out of your guest instance, we will encrypt that payload. And then there's another DMA engine on the nitro card that will put it onto the network and so we kind of bounce through the nitro card pretty quickly, pretty briefly, um, kind of stream it onto the network. It doesn't, doesn't really sit there for any measurable amount of time. We get to the network and we kind of do the same on the other side on the EBS storage server. We, we DQ it, put it into the software, do whatever we need to do on media. If it's a read, we'll just look at the local SSDs. If it's a write, um, you know, maybe we'll, we'll do whatever replication we've got. We've got some caching and, and things like that. Um, but not cashing in the right through, um, sense where your data is not actually persistent. We, we cash for reads more than we cash for rights, um, and then we populate the request back into your instance. So this is all, all actually pretty efficient in nitro. We've got some hardware offloads uh that support this, but it really wasn't this way. The thing that the thing that's interesting to zoom into here is the network and the network is where uh we can take a lot of liberties because we own that infrastructure and we don't have to present it to to anyone else, right? So we can do whatever we want. And so when we launched EBS and even into the early nitro instances our EBS steroid fabric was largely TCP based. It was pretty optimized, um, but it was TCP based and so our first step was to improve that. And so if you've been paying attention for a while, you've probably seen this slide before. If you're not, I'm gonna go through it pretty quickly, um, but one of the things that we did, uh, is we built our own transfer protocol for our, for our data center network. Uh, we call this SRD. Today this runs underneath every EBS volume attachment in EC2 nitro instances. And so our design goals for SRD, we took a look at kind of what TCP did and how we built our data center networks. And You know there's been quite a few runs at making TCP efficient in a data center environment, uh, but none of them really worked for us. As we stepped back, we questioned our assumptions, we took a look at our requirements, uh, and realized that part of the problem we were having is the TCP actually did more than we wanted it to do. If we put more of the logic in some of the higher level applications and so like I mentioned before with database replication, your application has more context about what needs to be transferred here we're doing the same thing we put more of the protocol context into the the EBS overlay and so SRD could actually be a a pretty great multi-purpose transport not just for EBS but for VPC networking and a few other things as well. Now that we've done this, this freedom allows us in SRD to route packets using multiple paths through the network and so for the case of EBS we can, we can route. Every IO request through a different path. Like once we get onto the network, there's multiple paths, uh, to get to any endpoint. We, it's called equal cost multipathing. And so The neat thing about storage is that while we don't want to reorder the data within an IO request. We can take advantage of the fact that anything that's in flight in a queue. We can complete them in any order. It's just a queue because that's how we get it onto the device. It's not a queue for any ordering perspective. And so as long as we complete the IO when we say we're going to complete it, we can complete the second IO before we complete the first IO that's on the queue. And so this is really cool because we can send every single IO request down a different path and constantly probe the network and looking for failures and reacting quickly to those failures and routing around those failures. Now in a data center, like if you look at how we built data centers 20 years ago, you might have some sort of routing protocols that would have to converge whenever there's a device failure and it might take multiple seconds depending on how you, how big your network is. With this we can route around failures in a matter of milliseconds. So what makes R 8GB even higher performance? So we knew early on in our journey with SRD that it was going to be pretty powerful for us. But we weren't really satisfied with what we had built like we we loved it, it, it allowed us to run our network more efficient, things got faster, things got just cleaner overall in our network, um, but we knew we hadn't yet done enough. We knew that the next step was to take more advantage of hardware offloads. Now if anybody's done anything with hardware. It takes a little bit longer than it does with software. Like software, I wrote a bug yesterday. I can fix the bug today. I can deploy that fix with hardware. If you've got a bug, you, your cycle is months, right, because you've gotta wait for the next spin of the hardware. Maybe you can do a rev on the existing hardware which is a little bit easier than a full tape out of a of a new chip, um, but it does take a while. And so we've been planning to on how we can use SRD and more hardware offloads within SRD for, for quite a while. And so with RAGB we're finally able to do that. And so remember how I mentioned that the nitroc has kind of did that double hop thing with RHGB we no longer need to do that. We have one DMA engine that can pull your data from the instance, encrypt it, and send it out on the wire to the EBS storage server. And on the EBS storage servers, we're now also able to steer those requests directly to the CPUs that are responsible for handling your volume data. So we got more efficient both on the on the instant side and on the storage side. And so that's what what gives us the ability to to launch uh RHEB with with more IOPs and more throughput. Cool part is we're just beginning down this journey. Well, we've got another page beep beep beep beep beep. Um, this time it's our application DB and so with, with this one, this is, this is where we're storing a lot of, uh, The doctor's notes and imaging and things like that and so we've decided to take uh a different database and so this is more of a hybrid database uh like Tidy B. Teddy B is a is a cluster database and there's there's a whole class of these cluster databases that rely on on quorum technologies uh to both be able to shard out and also absorb any performance spikes. And so the key differences in how we've configured this like we, we chose the same uh 4 extra large instance uh. This time we're managing this via EKS, which is obviously a more modern way made to to manage our our infrastructure, and then the storage prints and since we're scale out um. We're just doing these GB 3 volumes that have 400 megabytes per 2nd and 4000 IOPs, and then we're just going to put a whole bunch of these in the fleet. So instead of going through CloudWatch, I'm gonna go through something that we launched last year, which is our detailed performance statistics. And so these metrics can give you some of the same information that CloudWatch gives you. In fact, they give you a little bit more information than Cloud Watch gives you. One of the really cool things about these though is that you can pull them every second as opposed to every minute, and they're available right on your instance. So all you gotta do is, uh, run the EBSNVME stats command. You can query the device directly with NVMECLI as well or with with direct dioys if you're, if you really wanna get crazy, um, but the EBS NVME script wraps it all nicely together for you so you don't have to, to think about it. Um For those of you that are familiar with tools like IOSTA, uh, these are just counters, right? And so they're counters since the volume was attached or the, the host rebooted, there's a few times that it'll reset, um, but you want, you'll want to pull it and then look at the difference between the, the, those pollings, right? So it's just gonna keep incrementing and EBS NVMEats gives you kind of an Iostat-like interface where you can set an interval and a number of, number of times to pull it. Um, and so here we've got the total number of ops, operations, bytes, and time that, that, uh, your volume's been doing traffic. We've also got the throttle metrics, the ones that we showed before, for both volume and, uh, instance performance. But that's not the best part. The cool part about these detailed performance stats is that we now give you latency histograms. Um, so these latency histograms show you the number of IOs that land in these microsecond ranges of time. Right, and so you can actually characterize what the EBS storage subsystem is doing for your volume. Now these are after any performance throttles are applied, so any instance throttles or any volume throughput throttles, uh, and so it really does give you that picture of how is EBS behaving. If you run this side by side with something like IOSTA or maybe you you do your own uh trace probes in the kernel to to build your own histograms or even off of a benchmark like FIO. You might see a difference between these and what that difference is telling you is a couple things. One, you may need a different application tuning. Um, it may be showing you that you're throttling and so look at some of the other parts of these metrics, um, or it, it, um. Yeah, it could be, could be volume throttling, instant throttling, uh, or just an application or kernel, kernel tuning that you need to do and so it, it really gives you kind of a view behind the curtain so that you like a demarcation point like where you might find, uh, a network drop located. And if you're using the EBS CSI driver like we're doing uh here. These metrics can also be populated right to a graphfana Prometheus endpoint. And so for our platform we already had these populating, so let's take a look. We can see that the P99 right latency is starting to creep up. And so if we look at the graphs from earlier, uh, this, this log scale blow your mind graph, uh, log scale x axis, which is really weird, um, you can see that you like, of course we're gonna see some outliers on GP 3, so maybe one of the things that we should do is, is change it to an IO2 volume. And this is where elastic volumes comes into play. You're able to change your volume type uh and size and performance characteristics online, but there are a couple of caveats. For size that size is available immediately. So if you're running out of space, like if that's what the page was and you're running out of space, um, doing, doing elastic volumes, you'll get the size immediately. The volume will actually go through an optimizing size while we are optimizing state while we re-accommodate for that extra storage space, um, but you can use that space and we'll figure it out on the back end. Um, might have to resize your file system too. If you want to add I ops your throughput, uh, as we go through that optimizing state, the, the performance will be somewhere in between the original and the new higher performance, um, and so as we optimize different blocks get laid out a little bit differently, uh, we might need to reshard them, etc. The caveat here is with latency. Now I said you could just switch to an IO2 volume, but you need to be careful because during that optimizing phase sometimes you will see a little bit of latency impact, um. On average, your rights are gonna have less latency impact than your reads, uh, but that's not always the case, right? And so you need to think carefully about is switching to an IO2 volume right on this live instance the right thing, or should I fail over to a different instance, um, and see if that one behaves a little bit better, gets rid of some of the outliers while I backfill the rest of my clusters IO2. So let's talk a little bit more about why latency matters. Couldn't get out of an EBS performance, uh, performance talk without talking about Little's law. And so this is a common equation uh and a way to help us think about queuing in the average system. And so whether it's a single process, a network device, distributed system, or even a queue at the grocery store, Little's law is the thing that can help us reason about the, uh, capacity of these systems. And it briefly states that the mean concurrency in the system, and that's L, is equal to the mean rate at which requests arrive, or lambda, multiplied by the mean time each request spends in the system, W, right? Seems pretty simple and it's pretty straightforward. It gives us a pretty good reason to kind of reason about the a pretty good way to reason about the an ideal system and the long term concurrency of it. Now concurrency is a useful measure of capacity because it does give us a measure of consumption of resources, and those resources could be anything like CPU cycles, storage capabilities, really anything that's measurably limited. Indirectly, we can also use concurrency as a measure of contention. So if the concurrency is high, it's likely contention is also high. How many of you flew into Las Vegas this past weekend? Guessing everybody, um, I know I did. So here's an image like if we take a look at air traffic control, this is a sample of a 4 hour window of an average typical morning window of planes flying into the area. Uh, this includes both commercial and private flights, but there really aren't many private flights, uh, so it's mostly commercial. So on a typical day you've got an orderly flow of traffic. You've got airplanes converging on their final approach course about 10 to 15 miles out, um, and it's really easy to balance the system, right, because airlines have a pretty predictable schedule. They arranged that schedule to not overwhelm the airports, um, and sometimes weather happens and stuff, right? And so it's been said that 1 mile of highway takes you 1 mile, but 1 mile of runway can take you anywhere unless you're stuck getting to your destination, there actually isn't enough runway space. So the other week I was at the Las Vegas Grand Prix. This is a 4 hour window around the same time of day, uh, but just a couple of days before the Las Vegas Grand Prix. And you can see that there were a lot more flights coming into the area and a few interesting things to note here is that you start to see traffic getting diverted to there's actually 3 runways or sorry 3 main airports in the Las Vegas area, one where the commercial flights go and then there's 2, general aviation. Airports, um, but those runways are still a fixed resource they can't build more overnight and they really don't want to it'd be pretty expensive, right? And so we can think of those runways as the ability to support concurrency. Only one plane can occupy a runway at a time. As the amount of traffic increased, and that's our lambda, uh, the queue stretched out even further with many airplanes joining out 50, 50 and even beyond miles out as opposed to the original 10. So the other thing that happened is to keep an airplane from that takes off from the LA area. Having to go all the way to Texas before I can come back in is the FAA implemented a reservation program and so if I had planned to take my bonanza down from Seattle, I would've had to get a reservation, plan my flight, and oh, all the airports actually charge a whole lot to land on that that weekend anyway, um, or I'd be forced to turn away. So that kind of reservation system is an admission controller, a throttle, and you're thinking, OK, cool, airplanes are fun. What does it mean for my application? And so some applications can adjust, but sometimes it's really just a function of your workload. And so here's where latency matters, right? If you have a single IO in a queue and you don't put the next one in the queue until that first one completes, that's called a queue depth of 1. And so latency will dictate the I ops that you can achieve on that storage system. So say you have 500 microseconds of latency. Each Q depth can achieve about 2000 ops, assuming it's perfect, right? And so The lower the latency, the lower your queue depth, the faster we can return those transactions, and you can still get the performance that you need. So if you're designing a storage application, you're gonna want to think about how you can drive more traffic to your storage subsystem. Now databases often have a way to tune the maximum que depth, uh, but the minimum queue depth is really just gonna be, uh, a function of your load. And so on an IO2 volume with 256,000 IOs, you're gonna need a Q depth of at least 128. So a few notes to take away. Plan, think about your application ahead of time, test, use some tools. The higher up the stack you go, the more accurate it's gonna be. Uh, keep track of what's going on with your application and then react to those changes. And we've got a couple more EBS sessions, uh, this week at Reinvent. uh, storage 323 goes even deeper under the hood with EBS. It's a chalk talk, um, with, uh, two principal engineers. Should be a fun one. hope you have a great reinvent.