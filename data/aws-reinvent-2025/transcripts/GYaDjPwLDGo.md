---
video_id: GYaDjPwLDGo
video_url: https://www.youtube.com/watch?v=GYaDjPwLDGo
is_generated: False
is_translatable: True
---

There you go, OK. All right, so we're in the silent disco session, so if everybody could just try on your headphones, the first thing I'm gonna check and see is if everybody can hear me. Um, so we're, uh, SPS 402 is the session. Hopefully this is the one you're meant to be in. Uh, this is gonna be a 400 level session, so it's gonna be pretty, uh, technical, kind of deep dive. Um, so before I kind of launch into it, can everybody just give me a thumbs up if you can hear me? Amazing. Can you give me a thumbs down if you can't hear me? That's a hard one to answer. All right, uh, perfect. OK, so we're gonna be talking about fine tuning LLMs for multi-agent orchestration. Uh, I'm Hannah Marlow. Uh, I lead our model customization team within the Generative AI Innovation Center. I'm joined here by Charlina Kashava, who's my colleague in the innovation Center, and then in our session we're gonna be covering a cosign AI case study. And so we've got Alistair Pullin from Cosign who's gonna be giving you a deep dive of kind of how some of this stuff works in the real world in their product. Um, before we jump into the, the session, I just wanna get a kind of a gauge of, uh, who we've got in the audience, um, and also kind of try to make it a little bit more interactive because all of these like it ends up being us talking to you a lot. Uh, can you just give, give me a show of hands for who uses generative AI in their day to day personal lives? Awesome. What about uh at work, productivity tools, emails, development stuff, awesome. How many of you would say that generative AI has become like a core focal point of your role? Awesome. OK. How many people are fine tuning LLMs already? Awesome, cool, uh, how many people know what an agent is? Awesome. How many people are are building or deploying agents? Awesome. How many people are building or deploying multi-agent systems? Awesome this is great. OK, cool, um. Well, here's the agenda that we're gonna cover. So the point of this session is talking about, uh, multi-agent systems and then ways that you can specialize those multi-agent systems and why. Uh, why you would do that, um, and then also kind of doing that deep dive into how it looks in the, in the real world. So I'm gonna start out, I'm gonna give a little introduction to our team on the AWS side, um, and then I'm also gonna kinda walk through what our agents, what our agenttic systems, what are kind of the some of the benefits and also challenges of managing multi-agent systems, and I'm gonna set up like all the hard parts, and then Charlene is gonna talk through how specialization can help you kind of, uh, work through some of those challenges. And then I'll turn it over to Ali. He's gonna walk through kind of uh the cosign case study, how they leverage specialized multi-agent systems in their coding uh platform. All right, so Charlene and I are part of the Generative AI Innovation Center team at AWS. Uh, this is a program that was launched in 2023. Uh, it's a $200 million investment in, uh, enabling and accelerating customers to leverage generative AI. Um, so what we have is we have a mix of, of roles on the team. We have, uh, dedicated generative AI strategists who partner with customers and look at their kind of target business outcomes and identify and prioritize use cases with them for generative AI that are not just cool things that we could build that might sit on a shelf, but things that actually can go into production and bring back business value. So that's on the strategy side. And then we have uh builders, applied scientists, uh, machine learning engineers who actually embed with customers' hands on keyboard to help them build and deploy those solutions, um, so the innovation Center works on all kinds of use cases globally, um, more than 1000 customer projects uh at this point. Our team is kind of a subspecialist, uh, specialty team within that specialty group, and we, uh, focus on model customization, so we're also kind of this global horizontal team and we work in a few different areas, uh, so I would say, uh, most customers don't come to us and like jump in and be like I've never built something with generative AI before. I want to build a custom model. A lot of the customers that we work with are kind of the next steps, so they have already, they've already built their POCs. They're already running, uh, GO workloads and production, and they're looking at how do I optimize this, how do I improve it over time, uh, how do I make it, uh, better overall. Uh, so that's where, and how do I make it more relevant and bake in my data, my expectations, my brand alignment, those kind of things. Uh, especially with agents, uh, growing, growing use of agents, uh, this kind of multi-agent, uh, applications, we're seeing a lot of demand for how do I get better performance in a smaller, faster kind of a package, how do I do this specialization, so custom agents have become a big part of our business as well. And then the other side that we work on that kind of, I would say there's a big Venn diagram overlap in these customization areas is on the hardware optimization side. So how do I get my distributed training to be as efficient as possible? How do I leverage my GPUs or specialty accelerators like Tranium and get the most out of that hardware? And how do I make my inference super efficient um and really just get the most out of that? So those are the areas that our program focuses on. Uh, so I mentioned that a lot of this is through an optimization lens, uh, so we're looking for ways we have a, we have a system and we're looking for ways that we can kind of get the most out of typically these three areas, um, and if you've been in the tech space for a while, like you've probably seen these triangles, usually you're kind of trying to optimize around and like picking based on your requirements where you're gonna turn the levers, um, here we're really. Looking at like on one side we have accuracy, so we wanna get the best results possible out of the model or the system. Sometimes that looks like classification metrics or something that's really quantitative. Uh, sometimes it's qualitative like does this, does the output of the system align to my brand style and tone? Does it follow the right formats for my APIs? Is it too verbose, those kind of things. And then of course as you push up the accuracy, sometimes what you end up doing is increasing latency or increasing cost and so really you're trying to find a sweet spot based on your use case and that might look different if you have a real-time use case that you really don't have a lot of bandwidth for high latency. Maybe you're willing to sacrifice a little bit of accuracy. Maybe you're OK with 90 versus 95% accuracy. But for certain use cases you might have, you know, like you might not be able to sacrifice that, so use case dependent on what that looks like. OK, uh, so we're gonna talk about agentic systems, uh, and we'll get to multi-agents, but I actually wanna just start out with the question of like what is an agent. I think this is one of those areas that you could have a lot of kind of discussions and and disagreements about what constitutes an agent, uh, but here we're really defining it is uh this is an autonomous software system. That leverages AI to reason, uh, plan and complete tasks on behalf of a human or systems. Um, they're kind of like microservices in a way. Uh, and, uh, an LLM by itself is not an agent. So an LLM by itself, taking an input, it's giving you an output. Uh, an agent is a system built on top of an LLM. So in this diagram we've got kind of the LLM is the brain in the middle, um, and we're giving it agency to be able to actually take action for it, uh, for us, and so they can, uh, they have a higher level of, uh, automation. They can leverage, uh, memory and tools, and they have specific goals that they're working back from. Um, they solve problems and they can make decisions based on context. Uh, so this is a really, really kind of, uh, disruptive idea, right? So versus a rules-based system, these are very flexible because they can, um, adapt to, uh, something that's like when things go like a little bit different than expected when they don't totally follow the decision tree, when things change or there's a little bit of ambiguity, they're more flexible and they can, they can adapt. Um, and the idea of agents is not totally new, um, but there are advancements that are making agents more practical, and so I would say these are like key areas that have developed over time. One is model reasoning capabilities have improved so much over time. Uh, models have continued to get better. Uh, they're smarter. They have advanced reasoning capabilities, and we also have a lot more choice in models, so. Um, you have a whole zoo of models that you can choose from and really critically, especially for AI agents, is there's like different sizes of models, so you have like big beefy do it all generalist models and then you also have really agile lightweight task focused specialist models and you can use them in different ways and for different applications. Some models are really good at like text to SQL use cases. Some models are really good at summarization and, and you can kind of mix and match and do model right sizing to to find the right place to to fit them in. Uh, the other thing that's gotten better over time is data and knowledge-based integrations, being able to pull in the relevant context to your models. Um, also, uh, on the infrastructure and agenttic protocol side, we have scalable, secure foundations and protocols like MCP and A2A that make it a lot easier to connect agents together securely. Um, and then we've also gotten a lot better at agent development and orchestration tools, um, so patterns to deve to to design, deploy, and manage agents in production have gotten a lot better over time. And so that's why I think this year we've really seen this explosion of agentic use cases and more organizations starting to actually move into making these a reality versus kind of like a cool idea. Uh, this kind of typically what we see in terms of like the maturity scale for agentic AI. So on the left hand side, low agency manual human oversight, um, kind of like rule-based engines um you know you have a system that can execute a predefined set of rules. Uh, moving up from that, uh, towards like more agency and less human oversight, we have Gen AI assistance. So maybe this is a single agent you're able to, to give it a task, um, and it can give you a response back. It can kind of, uh, achieve a specific well defined goal, um, maybe like a chatbot or or something that kind of has a more narrowly defined scope. Um, and then we move into, uh, goal driven AI agents, so versus having a specific task that they're trying to do that one thing really well, you're actually giving them a higher level objective and having them figure out and actually, uh, multi-agent systems to be able to. Be like, OK, to accomplish this I need to, you know, start that task to go pull records and I need to figure out, um, you know, I, I need to go get the weather right now or what bookings are available, um, and work towards this higher level objective that you've set for the system. Um, and then, uh, this still has human oversight. Uh, you're helping develop the objectives and things like that as you go all the way up on the on the scale, and I don't think, I don't think many people are really here right now, but you, you know, you can see over time moving towards more autonomous agtic systems that require a lot less oversight and manual intervention and kind of can figure this stuff out on their own so that. It's more kind of uh governance and audit oversight for those. I would say a lot of organizations are um between maybe like the the first two steps on this. I think we have a lot of especially there's a lot of technical depth around like rules based systems um we see a lot of organizations that are that are leveraging AI assistance now and then what we're talking about here and where we're starting to see more movement towards are these multi-agent systems kind of that third rung on the ladder, um, where you're operating towards that higher level objective. Um, so what are some drivers for a shift? Like why, why do this kind of multi-agent systems? Uh, so I mentioned earlier, one of the things that's really been revolutionizing in this space is all of the model choice that we have, the fact that we can right size models, we can, we can get the accuracy we want and the latency we want, and we can, we have a bunch of things to choose from to put into the system. Um, that plus the ability to develop specialized agents where we can actually fine tune agents for specific tasks and applications, um, that gives us the ability to build a much more modular architecture and so if you, you know, uh, if you think back to like monolith versus microservices like vertical scaling versus horizontal scaling. There's not always a one size fits all answer there, but moving towards a more modular system with a multi-agent architecture gives you the ability to scale processes independently. It gives you the ability to be less dependent on a on a single model or a single solution because you can actually modularly swap things out and adjust as you need to. Um, so it's, it's very similar to that kind of micro server architecture movement. I would say this is a very common kind of uh approach to agents in terms of like how people orchestrate multi-agents currently. I don't know if it'll always be like this, but uh typically what we're seeing is this pattern where we have an orchestrator and we have sub-agent workers. And so you can see in the in the diagram we have kind of like user requests coming in at the top and so the orchestrator agent is taking in global tasks from a user, breaking that down into subtasks and doling it out into these specific specialized subtask agents to run their steps. The orchestrator gets to have all the information about it knows what's going on with the agents, it knows all the context that it has coming from the user in it, um, and this is typically a more competent generalist model I would say is common. Um, and then it's able to, it can spin new agents up and down, it figures out what it needs to accomplish its overall goal, uh, and then uses the sub agents to do that. So the sub agents just have information about, OK, what's my specific task? What is the specific. Context I need to do my task. What are the tools that I have available to me? API calls or different things I can do, and it executes on that and then comes back to the orchestrator. Orchestrator can pull different workers um and so this is kind of like common architecture that we see. Uh, so those are drivers for why you would do multi-agents and also, um, uh, kind of an architecture you would use and like the benefits of multi-agents, uh, but there's also a lot of challenges that come when you, when you start moving into these multi-agent systems. Uh, so one of them is the fact that like when you now you have not like one model call that you're making and then you're trying to have like this one giant vertical model do everything, uh, but you have different calls to different models, uh, if you're not careful and especially if you plug in the same big model as all of your workers, you're gonna end up with really high latency for like the full end to end process to get back to a result. That also equates to high costs typically. So if you're using a ton of compute to run these giant models to do everything, that can, that can also increase the price quite a lot. Uh, figuring out, uh, the right level like how to frame the system and the task and like how to, how to break down problems into kind of worker areas that's also more complexity here, um, figuring out how to manage like mid-task adjustments and things like that, managing context, um, and then also error propagation is a really big one as well, um, so. Uh, my example for this is a little goofy, but I don't, if people are familiar, there's a children's game called telephone. And the way that the game goes right is like you have children sitting in a circle and like one person whispers into into into their neighbor's ear and they'll say something like, uh, Grandma knits sweaters for penguins and then like that person will whisper it in the next person's ear and then by the time it gets back around it's like nana eats peppers with pigeons. So every step of that, like if if somebody gets it a little bit wrong, that goes to the next person and that error propagates all the way around. Um, so it's kind of a, it's a, it's a goofy example, but the idea here is if your worker agents are getting things wrong and that's feeding into a bunch of downstream processes that can, that can be in, uh, an area where you end up with error propagation coming in. All right, so I've told you all the, all the hard parts and the challenges. I'm now gonna hand it over to Charlina she'll introduce herself and then she'll tell you how to fix everything with specialization. Sure, thanks, Hannah. Um So hi everyone, my name is Charlina Cashava and I'm an applied science manager with same team of that team that Hannah's on the Generative AI Innovation Center custom model Program. And so we've, we've just heard about multi-agent architectures and how these systems are designed, the challenges that they face, but that leaves us with some questions like what do we do when our system costs are too high, when your agent is struggling to complete a given goal, or when like your orchestrator just isn't orchestrating. And so when we observe gaps in model performance, this is when we could consider some kind of agent customization approach so they can perform better on their respective tasks. So there are many ways to customize an agent, and today we're going to talk about 4 of the most commonly used techniques. We'll talk about model distillation, which is an effective method for optimizing cost and latency when you have agents performing well with large foundation models, uh, with large foundation models, Supervised fine tuning for when agents need to learn domain specific patterns, terminology, or structured outputs that foundation models do not handle well out of the box. We'll discuss preference optimization for when you need agents that produce outputs with specific characteristics like conciseness or specific tones and formats and also where there are multiple correct answers, but some of them are preferred. And this can be particularly valuable in multi-agent systems when uh you know you're working with an orchestrator or agents that communicate with end users or pass information between agents. And then finally we'll talk about reinforcement fine tuning for when agents must make sequential decisions with verifiable outcomes like tool selection, cogeneration, or multi-step reasoning tasks. So when we observe gaps in the performance of a foundation model, the next step that we like to consider is some kind of customization technique. Agent customization varies in terms of complexity and cost. Our general recommendation is to start simple and identify what gaps you're trying to close. Our, uh, before you consider any of the techniques I just mentioned. You'll want to make sure that you've extracted the most out of simpler ones like prompt engineering that leverage the existing capabilities of a foundation model or retrieval augmented generation that allow you to leverage your proprietary data. And then when these techniques fall short, consider ones that actually make modifications to the model weights like fine tuning or reinforcement learning. Leveraging these techniques, it does increase the complexity of the problem, but we also see that this leads to more significant performance gains in terms of model quality and also the agent's ability to accomplish a desired outcome. So these techniques, uh, I should also mention they're not mutually exclusive. They're also not really in any particular order on the slide, and they're typically actually used in a complementary fashion. So for example, a key component of model distillation is actually performing supervised fine tuning of a smaller model on data that's been generated by a larger, more capable one. And then similarly supervised fine tuning, it's also often used in conjunction with preference optimization. So you might start by fine tuning a model to help it understand the basics of your task and then perform preference optimization to align the fine tune model to your customer base. So the key here is really understanding what you're trying to accomplish and then selecting a customization tool or a set of those tools to match. So for any of the techniques that we'll discuss today, uh, there's upfront and reoccurring costs to customizing an agent which include the investments into data curation and augmentation, science and engineering efforts, and infrastructure costs, then of course the periodic cost of making updates to the model. But that said, we all, we generally see that spending a little upfront to optimize models, it leads to long term cost of ownership benefits, and we see that through optimized token consumption and also reduced inference expenses. So customized models, they just, they make fewer mistakes, uh, and they perform more efficiently against your business use cases than off the shelf agents do. And you can fine tune a smaller model to perform well, as well as a larger, more powerful one on your target use case, which will give you the additional benefits of reduced latency and reduced inference costs. OK, so we're going to dive a little bit deeper into each of the techniques that I mentioned earlier, and the first one that we'll talk about is model distillation. So in a multi-agent architecture you often have agents that are executing the same task repeatedly, so thousands or millions of times per day. So take your orchestrator model for example. It needs to process every request that's made to your system and also handle a variety of ambiguous situations. A large foundation model might perform well out of the box for this, but the inference costs and latency, they will quickly add up, and this is where a technique like model distillation comes into play. With this technique you take a teacher model, a large capable foundation model that already performs well on your task, and you use this teacher model to generate outputs, uh, for representative examples of your task. These teacher outputs become your training data, and you use this data to train a smaller student model to mimic the teacher's behavior, typically through supervised fine tuning. With this method, the teacher's knowledge then becomes your training signal, and the student learns to replicate not just the final answer but the teacher's reasoning patterns and output distributions. So leveraging smaller, faster, less expensive models to perform the same tasks, of course it has cost and latency reduction benefits. Smaller models, they require less compute power and memory and significantly, and this will significantly lower operational costs and enable deployments into resource constrained environments. We generally find that distilled models, they execute faster, they reduce response time, and overall this improves your customer experience. Moving on to supervised fine tuning, uh, so. In the wild, your agents will often encounter these domain specific scenarios that foundation models don't handle well off the shelf, like a medical assistant agent, for example. It needs to understand specialized medical terminology, follow specific regulatory formats. Produce structured outputs that downstream agents can parse reliably, and a general purpose foundation model might do pretty well out of the box, but it's unlikely to consistently handle all the nuances of your domain and perform well on the specialized use cases that have not been seen in its pre-training data. So with supervised fine tuning you train a model with input and output pairs that show the model desired behavior for your specific use case. For these examples, uh, the model will learn to handle domain patterns, terminology, and also your output requirements. In multi-agent systems, as Hannah mentioned with her example earlier, errors cascade. So one agent's hallucination, it becomes the next agent's incorrect, uh, assumption. So supervised fine tuning reduces this by teaching each agent to, uh, learn specific patterns and constraints of its domain. Fine tuned agents, they make fewer mistakes and they lead to overall better system performance. You also have the benefits of improved task decomposition. Orchestrated agents, they break down complex requests into subtasks for downstream agents to perform and fine tuning on examples of effective decomposition, it teaches them how to properly structure a plan, how to design its subtasks, which ones to create, and also how to sequence them for optimal goal resolution. So multi-agent workflows, they can span dozens of steps with information passing between agents, and fine-tuned agents learn how to manage this large context by maintaining relevant information. And filtering noise over these long running trajectories, focusing what matters on what matters for each of their specific roles. So as I mentioned before, supervised fine tuning requires high quality label data sets that show examples of input and output pairs that demonstrate your desired model behavior. So this data serves as a direct example of what the model should learn how to produce. So here we have like a little toy example. The user is asking the agent or the assistant to please help me solve this math problem. We cut out a lot of the subtest just to make it fit on the screen and then the assistant says, Sure, I can help you do that. Let's start by simplifying the expression. And so that's like a sort of a cute little example that just shows what the data format should look like. And then before we move on, I wanted to also talk about the different types of supervised fine tuning. You've probably heard some of these terms in the news or through the literature reviews that you've done. And so broadly speaking, we have two approaches to supervised fine tuning. We have full fine tuning, which updates all model parameters and gives you maximum customization abilities, but also comes at a higher computational cost. Then we also have parameter efficient methods, also known as PEFT where you update just a small fraction of the parameters through techniques like low rank adaptation or Laura. And so parameter efficient methods are generally they're they're faster and they're less expensive, and you can train multiple task specific adapters that share the same base model. Uh, this lets you customize multiple specialized agents without multiplying your infrastructure costs. And so that kind of brings us to the next question like when would we ever use full fine tuning? There are use cases for using full fine tuning, but we often find that fully fine tuned models, they have some things that they struggle with. One of them is a phenomenon called catastrophic forgetting. Which is when the model forgets information that it's previously learned when it's trained on new tasks, and this happens because the process of learning new data, it can cause the network's internal pathways or like the model weights as we would refer to them as to be overwritten. And that causes it to forget the knowledge that it acquired during its previous training rounds. So this is actually like a, a major it's not a new hurdle, but it's a major hurdle for AI systems and, uh, enabling them to continually learn and adapt without having to be trained from scratch. Full fine tuning also requires orders of magnitude and more data to accomplish, and also more computational power, which leads to increased costs for training rounds. And so for these reasons we find that parameter efficient methods are they're often preferred amongst our customers, and we actually recommend that you try them first before you try. Other techniques that are more involved. OK, so next up is preference optimization. Uh, in multi-agent systems, uh, being correct isn't always enough. Your agents, they have to return accurate information, of course, but the way that they present this information to your user base, it matters. Take a research assistant that has gathered information for downstream analysis. It might retrieve all the correct facts, but if it presents it in a verbose set of unstructured paragraphs, that's not very helpful to your end user. And so the next agent in the workflow, it will struggle to parse those responses efficiently, and that could lead to all kinds of downstream complications for some kind of multi-agent system. In another scenario, you know, you might have multiple customer facing agents that sound completely different. One is presenting information with a more formal tone. One is more casual, and that creates an inconsistent user experience. So with preference optimization techniques like reinforcement learning with human feedback or direct preference optimization, you teach the agents to align the responses with your customer base's preferred style and tone or your general preferences. So instead of showing only correct examples, you show pairs of responses, ones that are preferred and ones that are not. So both of these responses, they might be factually accurate, but one might be better formatted, one might be more concise, one could be better aligned with your customer's requirements. And so in this way, the model learns how to respond consistently across all agents and aligned to your customer's preferred style and tone also respond in standardized formats that are predictable and easier to parse in a more reliable fashion. So here's an example of uh preference optimization data. Uh this is again another little math problem. Um, we're asking the agent to solve an algebra problem. So you can see. In the bottom two brackets we have a chosen response that shows a clear step by step workflow that gives the user instructions on how to actually approach the problem, and then the rejected response is one that glosses over these details and uses unclear terminology which may be hard for a user to understand. And so while the answer, the answer is actually correct, but it skips all the explanation and it doesn't show any of the mathematical reasoning that led them to that answer. So by training thousands, training your model in thousands of examples like this, the AI is going to learn how to make responses helpful, how to provide clear explanations, follow the proper formatting, give step by step reasoning, and it learns to consistently produce these responses that are more like the chosen example and then avoid taking shortcuts like the one that's in the rejected example. OK, so last technique we'll cover today is reinforcement fine tuning. Some tasks, they require more than just learning patterns from examples. Your agent needs to make a series of decisions where each choice affects what comes next. Take a coding, a code generation agent for example. It needs to figure out what piece of code to write first, what tools it should use, how to organize a solution, and then also, when something doesn't go as expected, when something breaks, it needs to be able to recognize that problem and then pivot and try a different approach. So you could show it thousands of examples of what like a finished piece of code looks like, but that's not going to teach the model how to actually write and generate the code itself. So that's where a technique like reinforcement fine tuning comes in. The agent actually learns by doing, and a critical part of this approach is knowing when it has done something correctly. Which is achieved with a verifiable reward signal. So for a code generation example, you can run unit tests, tests which verify if a function has worked as intended. The agent learns through trial and error. It tries actions. It receives feedback on its successes and its failures, and it learns to favor actions that lead to successful outcomes. This differs from supervised fine tuning, where you're showing the model just examples of correct answers. Here, the agent is actually allowed to explore different paths, and then it learns which strategies work well and which ones don't. So here on the screen I have a diagram of group relative policy optimization, also known as GRPO, which is probably the most common reinforcement fine tuning technique we see with customers. For each sample problem you give the model, it generates multiple trajectories. So instead of just getting one response, we generate several different attempts at solving the same problem. And each of these trajectories, it receives a grade from our reward function and then from those grades we actually calculate what we call the advantage term. So some of these trajectories they. Uh, result in a completed goal. Other ones, they complete the goal but not in an ideal fashion, and other trajectories, they are examples of when the goal is not completed at all. And so what we end up with is a spectrum of samples that actually show what worked well, what didn't work at all, and that's the group relative part of GRPO that gives you much more nuance in your training data than a technique like supervised fine tuning where you're just showing it examples of what the output should look like. The model eventually learns not just to understand this is a good example, this is a bad example, but actually that this example is better than that one and here's why and it learns to do this over a whole spectrum of trajectories that we generate. And so choosing the right customization technique, it's, it's really a balancing act, and I, I hope to provide you here with a light framework for thinking through this investment. So first up, you can, you can think of agent customization as a way to leverage the most out of your proprietary data. But to customize a model you don't just need high volumes of data, you also need high quality data that is aligned to the outcome that you want to improve. We often work with customers that have terabytes of data, but only a small fraction of it is actually in a usable state or aligned with the gap that they want to close. And in these cases we help our customers work backwards from the data they have or even help them acquire more data and then prioritize their generative AI use cases and develop customization approaches. Next, you'll want to consider your compute resources. So as we've discussed, foundation models, they require high amounts of compute to customize, which adds to the total cost of ownership for your system and also impacts the size of the model and the complexity of the approach that you take. So a practical way of enabling agent customization is to design the model architectures and training strategies around the infrastructure and budget constraints that you have. And finally, but most importantly, is your business use case. Every other week you'll see a new headline or a new paper that's announcing the latest state of the art technique for fine tuning or reinforcement learning or other types of model customization, and I would encourage you not to be overwhelmed by the pace at which we're innovating in the industry in this space. So just be clear about what gaps you're trying to close. Start simple with the techniques you want to try and then fail fast so we can iterate quickly and learn from our mistakes. And at the end of the day, the best technique is going to be the one that actually solves your business objectives. And with that I'm gonna pass you off to Alistair. He's gonna walk you through a real world example from cosign on how they use multi-agent systems. Thank you very much, Charlene. Um, so. Hi everyone, yep, I'm Alistair, co-founder and CEO um of Cosign and uh Cosign um we build cutting edge coding agents specifically targeted at large enterprises, often in highly regulated spaces with large code bases and occasionally, um, with very niche stacks that we have to optimize for. Um, our product is deployable in a multi-tenant cloud setup in a single tenant VPC. Um, and in some extreme cases, um, fully on hardware air gapped on premise. Um, we're one of the only solutions that is able to deploy in that space. Um, we as a company specialize in post-training large language models, um, to become effective coding agents. Um, we do this all the way from. The largest, most powerful frontier, uh, models, um, where we use, um, a custom RFT, uh, pipeline, um, that then achieves state of the art scores on the most popular coding benchmarks out there, um, beating even the largest labs, all the way down to small, highly specialized large language models, um, which are, you know, for the strictest, um, enterprise deployments. Um, for instance, we recently built a model. Um, for a large quant fund that was specifically good at software engineering in MATLAB, um, we did that with a 70B, uh, model, um, and we brought that model above, um, the frontier model's performance in that particular language. So the product takes two main forms, um, the one, there's one in, uh, in web here, um, where you can give large tasks to the agent and it will complete them asynchronously off device, um, and also a CLI product which integrates with your IDE, um, and allows developers to work in collaboration, um, with the agent, very similar to Claudecode or Gemini CLI. You'll be familiar with those. Um, the agent can be also given access to sandbox code code execution environments, um, allowing it to test its solution iteratively, um, and then utilizes our custom scaffolding. Um, to ensure a tight coupling of the models that the agent uses, um, or the the tools the agent uses rather with the foundation model that we're post-training. This significantly boosts performance, we've noticed that, Um, by doing that tight coupling of the scaffolding that the agent will use in production at training time, um, we cut down our average trajectory length, so the number of steps that it takes to arrive at the solution by about 35%, so that reduces your overall task time and the amount of compute that you need just to run each task. Finally, um, the agent produces uh a reviewable pull request that can be, you know, reviewed and iterated on, um, and slots right into developers' existing software development life cycles, they're used to doing that kind of thing. So Why train multi-agent systems? Um, multi-agent systems solve one core problem, um, that we have encountered when dealing with particularly large enterprises, um, and that is the intelligent shortcomings of small models. Um, smaller, cheaper to run LLS. Out of the box, often struggle with long horizon tasks, um, and in a scenario where larger models aren't viable, um, or available, and this can be for a number of reasons, even the largest organizations sometimes don't have many GPUs or aren't able to deploy certain large models because of their origin, um, small models are the only way that you can go and um, Putting them together in a multi-agent system, we often see situations where we're matching or exceeding performance of these large models which are orders of magnitude larger in parameter count. Um, building an orchestrator um fills that intelligence void, essentially it allows, um, these large, long horizon tasks to be broken down into smaller manageable subtasks that the workers, um, Completes uh and then you're able to get that level of performance, um using multiple models, um, rather than just one monolith, so we created um our Genie Multi-gent solution um for these enterprise customers, so, Here are the advantages of a multi-agent system that we've noticed, so obviously in this situation we're hyper-specialising a model, um, this is obviously preferable for small models to start with because small models tend not to generalize as well out of the box as large models. Um, the orchestrated models are specialized to plan, breakdown tasks, delegate, worker models are specialized to be boots on the ground, actually traversing code bases, using tools, editing code. Um, orchestrated models essentially, as I've said, bring these, um, long horizon tasks into range, um, with these smaller models because they promote self-reflection and iteration and essentially keep prompting these, um, smaller worker models to keep iterating until the task is deemed to be complete. um. There are a number of reasons why small models out of the box aren't particularly good at these long horizon tasks. Um, sometimes it can, uh, it can range from those models were not trained on long trajectories at pre-training time, they weren't trained on tool cooling very well, or there are architectural decisions made in that model's creation that just don't lend themselves to, you know, long trajectories and long horizon decision making. Um, for instance, GPT OSS 120B, many of you might be familiar with that model or have used it. Um, that model under the hood implements, um, interleaved causal and sliding window attention, um, and the sliding window attention size is only 128 tokens. So what that means is that there, um, tokens within, um, that block can only see 128 tokens prior, um, which means that. As a result, on long trajectories, that model can feel like it has amnesia, it can call a tool, do a bunch of stuff afterwards, and then actually call the same tool again with the same input, because of the fundamental attention mechanism that it's using, not allowing it to actually attend to those tokens much further back. So, we have to use orchestrated models to like fill these gaps. Um, multi-agent systems also can help a lot with latency because you can paralyze a lot of work that would normally happen sequentially, um, so you're not, you know, you don't have these uh single threaded blocking workloads, uh, and then once you have a multi-agent system up and running in production, You can actually store all those trajectories um and you can use them to reinforce um signal that you've seen in the real world, so in certain cases, if a PR gets merged, for instance, that's a good sign that that task went well, we can use that as training data for subsequent runs for those customers. Moving on to the challenges. Um, uh, with added system complexity comes a lot of challenges with multi-agent systems. Um, what we've noticed in practice is if things go wrong, they go really wrong. Um, it's possible to encounter doom loops where like sub-agents get stuck or even worse, where the orchestrator becomes totally incoherent, um, at which point it's very difficult for the whole system to recover. Um, because the orchestrator and worker are trained independently, there is the risk that you get like significant policy drift between those two models, um, meaning they think about problems and operate in very different ways, uh, at which point you can get some unexpected misalignments. And another point um is similar to my attention point I just made, um, if context isn't managed well, um, you, you can end up in situations where, um. Your context is polluted significantly and that degrades model performance, quite significantly, particularly in the, uh, in the smaller, in the smaller models. Um, there are obviously additional constraints, particularly when working in on-premise environments, um, GPUs are often in short supply, um, and, um, these large organizations that deploy these kinds of systems still expect top-level performance, um, even if they're using smaller models, um, so in these situations, in, in the most constrained environments, we have to turn to these, um, multi-layer setups that Charlenena just mentioned where we essentially have different, um, personalities encoded into each adapter that we can just in time swap out, so your base model at one point is the orchestrator, and then you swap out which adapter is being used, then all of a sudden it's the worker. Um, Optimizing for the deployments that we do, um, and we've covered all of this already, but, um, we rely very heavily on model distillation, so what we'll do is we'll often start with, um, a leading really large parameter model, something like a Kimmy K2 Thinking or a Deep Seat V3.1. Um, and we'll, as a starting point, post-train that using RRLL to make it very competent at software engineering and to tightly couple it with our scaffolding, as I mentioned earlier. So at that point we'll already have a model that's pretty much state of the art on most benchmarks and is very strong. Um, and then once we have that, we can use that very strong model to create trajectories that we can then use to distill through supervised tune fine tuning into these smaller models. Um, so we'll throw loads of software engineering problems, um, at that model and we'll store the ones that it got correct, um, and once we have that data, we can then use that data to distill into the, the smaller LLM, um. We always aim to make um our entire product deployable on as few as about 4 H100s, which is generally the minimum that we recommend for an enterprise use case, so everything that we do tends to boil down to that lowest common denominator, can it run in that situation. Um, so the orchestrator model. The aim of that model is obviously that it needs to learn how to call tools, how to um delegate tasks to the worker agents, when, in what context, and in what situation should it follow up, um, how does it know when the task is actually done, um. The orchestrator is never never actually on the ground solving the problem. The orchestrator will always delegate the actual doing of things to a sub-agent, um, it merely coordinates them. So it calls sub-agents as tools, if any of you have ever done any tool calling with LLMs, it's just a tool, essentially, um, and then when it does that tool call, the tool response is the sub-agent's work, essentially, the workers' work, um, and that can, in, in our case, that, Um, that looks like chains of thought and also get diffs of the changes that the worker might have made, um, and then that orchestrator model is able to look at those changes and then deem, um, them to be satisfactory or not, and in certain cases it will say actually no, I don't think you've done that well, I don't like the way you've organized that, we need to continue iterating on the problem until we've solved it. So the process of training that orchestrator. Um, as I mentioned, we'll feed that RL post-train model, um, with software engineering problems, um, and. Once we've um got that sort of basis of a really strong frontier model, um we want to essentially create a prompt and a set of tools that the orchestrator will have access to. Um, so that will be, you know, you are an orchestrator of, you know, some sub-agents, you have these tools that you're able to, you know, read an agent's context, um, assign it a task, um, spin down that agent, and so on, um, and then at that point we'll actually start feeding that model problems and start generating trajectories, um. And very similarly to when we're just building a general um software engineer, we use those verifiable rewards to figure out which um which of those trajectories are good, which actually solve the problem, um and which didn't, and the ones that solve the problem we store and then that is the soft uh the, the supervised fine tuning data set that we then use for the orchestrator model, um, down the line, the, the, the small model that we're gonna train. um. The worker um is also obviously incredibly important, um, and particularly with small models, um, is, is the thing that you're relying on to actually carry out whatever work the orchestrator has asked it to do. Um, so we'll pick whatever model, uh, the, the customer is comfortable with using, oftentimes GPTOSS or LAMA 3.370B. And we will, um, to start with, we want to get this model into the rough frame of mind of um an, an, a, a software engineering agent, and the quickest way to do that to start with before you start doing RL um is just to do a supervised fine tune. So again, we'll take trajectories from a smart model of that smart model solving software engineering problems, distill those into that um smaller model, um, and once we have a, Uh, uh, uh, the smaller models SFT on that data, we're then in a place to start actually doing RL. Um, we have in the past experimented with skipping the SFT step and just going straight to RL. One of the big gotchas in that world is, you are, um, you are beholden to the base intelligence of whatever model that you're starting with. If those models are already quite bad and struggle to solve these problems effectively, You're gonna need a lot of time in um in an RL environment for it to actually start learning and getting signals, so the SFT sort of shortens that process. Um, then we start the actual RL process and this is, as Charlina mentioned earlier, we um deliver software engineering problems to the model in an online setting, um, it does them, we grade them, those grades get turned into advantages, activations, gradients and so on, we update the model weights, it gets better over time, essentially. Um, and that's how then we have a very competent worker model, um, at the end as well. So this is the architecture in action. Um, we get a prompt coming in at the top here, um, that, that then gets interpreted and broken down by the orchestrator, that gets turned into subtasks. Those subtasks then get completed by the worker model, um, and those then deliver the results, whatever the result looked like, in our case, yeah, diffs, um, and then the orchestrator will look at those changes and will say, OK, do these actually solve the problem? Is this, is, is this achieving the intent that the user gave us at the beginning, yes or no? If um if no, then we'll actually continue new subtasks maybe to multiple agents, maybe to a single agent, following up and saying, hey, OK, I don't like what you did here, I need you to do this additional thing and so on, or if it's happy, we just say complete and that's the terminal state and then we actually revert back to the user. So what are the things um that we get out of like these multi-agent systems. So we get significant performance and efficiency gains when things work. Um, so in our particular um use case we get um a a boost of 3X in terms of latency relative to a multi um a generic multi-agent framework, something like lang chain or something like that. Um, we also, thanks to the multi-law approach, see a 60% reduction in GPU footprint for our on-prem deployments. Um, that's because we're able to, you know, dynamically switch out the, the sort of mindset of the model, um, as and when we need an orchestrator or a worker. Also, because the, um, orchestrator exists, the, the quality and the number of errors that you find in the final code drops, um, by about 20%. That's because it is able to catch those issues before it gets reverted back and given to the user, um, and also because we have the orchestrator, it means that the process of RL becomes more stable because you have something overseeing it, um, at that time. So, what does this actually translate to in the real world? So one of the valves that we use regularly is an eval called Swelancer. Swelancer is um a, Benchmark that measures a coding agent's ability to do economically valuable work, um, these tasks came from Upwork, they are real tasks that humans did, um, and got paid for, and we know how much they got paid for, um, so this is an OpenAI benchmark that they then published, which, essentially you give these tasks to agents and if the agent solves the problem, it gets paid as much as the human would have got paid, and that's how we measure how good, um, the agent is. So this Genie 2.1 Mini on the left here. That is the base worker on its own, um, that is literally no orchestrator, no nothing, just give the problem to the worker and see how it does, um, and then on the right-hand side here we have in the multi-agent setting, and this is on a LAMA 70B model here, um, so relatively small, um, we got a 31% increase, um, by using a multi-agent system and bearing in mind we have no more GPUs running, we have the same number of GPUs running. Um, we're just swapping out a law adapter and then we get, you know, 31% more improvement, and you as a developer using the product feel that very, very seriously when you're in there doing, you know, code changes and so on. Um, So what does this mean for enterprises um using our products? Well, the fact that we're able to do this multi-agent system um means that we can actually do these highly isolated deployments, so it means that we have models where there is no egress from um that customer's VPC. Um, we're then able as a business to target highly regulated industries, things like finance, defense, healthcare, advanced manufacturing. Um, and so on, and it gives us a lot of flexibility. We can deploy this into Bedrock, we can inference through SageMaker, we can do it into custom Kubernetes runtime, and importantly, um, for these enterprises. Everything that is done is stored and fully auditable, and um this is one of the things that we've put a lot of thought and time into because you in these multi-agent systems need to be able to understand why what happened happened, um, and having the auditable trajectories for the orchestrator and all of the workers, um allows you to essentially play back exactly what happened and understand. The, the, the crux of, of, of the actions that the um the agent took. So. The key things that we have learnt um in building this system is that training an orchestrator model and training the, the actual worker are two very different disciplines. Um, and on this point, we have started experimenting, in fact, with doing RL of the entire system as a whole, um, where essentially we start feeding, um, We start feeding uh an agent model, a uh an orchestrator model with software engineering tasks where it has agents, um as tools available to it, um, and it will start using those agents um to perform tasks but in an online um training scenario, so essentially, everything is happening as it would at inference time and then we reward or penalize the model, um, in that situation. It's difficult to do because um you're doing every, like, the time horizons for these trajectories can be quite long, but it does boost performance, um, and the disciplines of training that orchestrator that is essentially breaking down problems and delegating to something that's on the ground and actually making code changes is significantly different. Um, one of the things that we have really been pleasantly surprised by, um, is that fine-tuning these smaller models in this way, um, you can often outperform, um, like the large monolithic models, uh, and for example, um, that eval slide I showed a couple of slides ago, the multi-agent score on Sreelancer was better than 03 high. Uh, I know that isn't the. The, the, the, the model of the day anymore, but that was still a very strong model and we're at a point where a 70B model is outperforming 03 in a, in a, in a tangible way, um, and that is huge for, and particularly for enterprises that could never dream to deploy or use an 03 for all the various reasons, um, that, that surrounds that. Um, one of the biggest things that obviously we leverage is distillation. Distillation is key, coming up with good supervised, uh, fine tuning data sets without distillation is very, very difficult. Um, about a year ago we released our, um, Genie one model, um, and that was manually curated, supervised fine-tuning data for a coding agent, um, before the RL paradigm, before anything like that. And it took a team of 5 people, about 3 months, manually creating trajectories and labeling how we would solve these problems, um, so without distillation, that is what you have to do, so distillation is a huge tool, um, to bring the, the abilities of these frontier models into these smaller models. Um, and my last point, and this is something that fundamentally, we already know in this space and, and has been proven out already, um. Reinforcement learning and as a result, real-world execution um data and trajectories, solve so many problems, um, and is a huge, is a huge performance driver for us. Um, as I mentioned in one of my slides, we use SFT to get the model roughly where we'd like it to be, um, and then we use RL to take it that extra distance, um, and particularly for small models, um, that real-world execution data. Is so invaluable er um at driving better generalization into these small models that would otherwise struggle with generalization um across the board. So, those are the key things we've learned in building our Gi Multi-agent systems. Um, thank you very much to everyone for listening, um, and that's us. So it's oh I was, I think we're about to say the same thing. One, yeah, one last request for everyone, um, the way that you can fine tune your future AWS sessions is to please go in and fill out. The surveys for the session, let us know what you like, what you don't like we'll run that through the algorithms and then next time you'll, you know, it'll get even better over time. So thank you everyone. We'll be available for questions afterwards if anyone wants to hang around. Thank you so much. Thank you.