---
video_id: f7mrL1G_DM8
video_url: https://www.youtube.com/watch?v=f7mrL1G_DM8
is_generated: False
is_translatable: True
---

Welcome to the last talk of the day, 5:30 on a Tuesday. Why not? Although it's Vegas, so you guys probably have like two dinners and like six happy hours between now and whenever you go home. Uh, anyway, my name is Sean Faulconer and my name is Cal Rub and, uh, welcome. This is, well, essentially we're gonna be talking about how to go from AI agent agent demos to essentially production. I think a challenge that many, many businesses, uh, face today. So first off, how many people here, you just put your hands up, have built some sort of AI demo, you know, maybe it's a single shot prompt, rag, agent. All right, keep your hands up. And then how many of you have made those demos turn into production use cases? Anybody? Yeah, a few. Great. So this is one of the, the challenges, and of course there could be many, many reasons why you haven't been able to move those demos into production. Maybe the demo was never designed to be a production use case to start off with, but what we're gonna talk a little bit about today is, you know, our experience working with a number of customers across confluent as well as Anthropic and some of our experience where we see customers getting stuck with moving essentially those types of demos and POCs into production. And you know, clearly AI is, you know, powering a lot of new experiences. We just crossed the 3rd year anniversary of Chat GPT. There's been such an explosion of technology. Last time I was at Reinvent was 2 years ago. I walk around the expo floor today and it's like unrecognizable to where it was 2 years ago because a lot of those companies either didn't exist or they're very much in their infancy a couple of years ago. Everything's about AI right now and it has transformed, I think, a lot of how we think about. You know, doing our jobs, if we're engineers, we're probably using some sort of AI assistive coding tool, cloud code, for example, or you know, if we're generating content, we're probably using some sort of AI based tool to help us do that. We're relying more and more on these systems, yet at the same time, a lot of companies are still struggling for where is the ROI for my particular use case or I want to solve this problem. How do I solve this problem and I can't get there. I'm still struggling to essentially bring that stuff into production. And while all this is going on, a lot of people believe also that like agents are the future of software, you know, you hear lots of proclamations like SASS is dead, it's all about agents, and regardless of the metrics that you look at, whether it's, you know, this chart that's predicting the agent market being, you know, 50 billion by the year 2030, or Gartner reporting that they think that by 2028, 33% of all software will have some sort of agent baked into it, you know. It's always hard to make predictions in technology. We tend to overestimate in the short term and, uh, underestimate in the long term, but even if you only believe some of the stuff is directly correct, it's of course something that we all need to be paying attention to when we work in technology and we're trying to all figure this out right now. And I think the, the dream or the promise of agents is amazing, like the fact that we could have some sort of dynamic autonomous system that's doing work on our behalf. Everybody gets like, you know, an intern in their pocket, really like, you know, the smartest intern that possible with almost perfect recall, that's amazing. So if we can turn that dream on for our businesses to solve all kinds of different use cases, of course we want to be able to do that. Now fundamentally what is different about, you know, these types of models and broadly speaking when we look at AI there's been sort of two different eras or waves we've had purpose built models and then now we have, you know, foundation models or generative AI, large language models, however you want to kind of term it, and one isn't necessarily a replacement for the other, but there are key characteristic differences between these kind of two different eras that's important to understand. So with purpose built models or predictive AI some people might call this, this is kind of like the AI that I grew up on and and was trained in and was a researcher in for a number of years. The way that that you build these models typically is you start with your data. So you start with your business data and you go through this batch offline process of feature extraction, feature engineering. You train your model, you test that model. Once you're happy with it, you can deploy it and you start using it within your application. And these are served us, you know, well for decades and will continue to serve us well for decades from here. But the sort of limitation of these models is that they're purpose built, so they're kind of singular purpose. Like once I've gone ahead and I've trained my fraud detection model, I can't just start doing image classification with it. I need to go through essentially a different training process, maybe even use a different type of model, probably different features to go and train that image classification model. So they're limited in terms of their reusability, but the advantage of the models is they know a lot about you because they were trained on your data and they were trained to solve a specific task. So they have a representation of essentially your business baked into the weights of that model, and foundation models come out of that opposite set of characteristics where there are these massive. Models that are trained essentially on, you know, all everything publicly available and only a handful of companies in the world have the resources and knowledge how to actually construct these models and they package them up in a nice API that anybody who knows how to call an API can essentially leverage the model and they don't need to know anything about sort of how the model was constructed or the guts of the model to use it. And this is democratized access to AI and I think led in many ways to this explosion of interest and people building software that leverages AI today and the other amazing thing is that they can be used for infinitely many things. I can use the same model for many, many different use cases, but the limitation of the models is that as smart as they are about public information and general information. They're kind of dumb when it comes to essentially your business information because they lack that context. They weren't actually trained on your customer data or on the data within your business, so you have to give them that information. So how do you do that? Well, typically that happens during this process of prompt time assembly. So when I'm about to call the API to the model, I need to go and gather essentially whatever additional data that I need to sort of to give to the model so it understands what the task is and it can accomplish its goal. And in an enterprise scenario this is kind of easier said than done because enterprises have data all over the place. You might have thousands of different databases. You have SAS locations, SAS applications where your data might be located. You have APIs, all these things, and this has been these data silos and data fragmentation and, you know, just understanding where your data is has been a problem. in businesses for decades now, and AI is really just the next sort of forcing function for people to think about deeply what their AI or their data strategy is, because fundamentally it's hard to have an AI strategy without a data strategy, because if your data's a mess, you know, slapping a model on top of it doesn't really solve that mass. So how do you essentially do that? And a lot of this essentially is grounded in this concept of context engineering. I'm going to hand this off to Cal to talk a little bit about how we've evolved essentially beyond the prompt. All right, thank you. So I joined Anthropic about 2 years ago to help start a team that we call Applaud AI, and it's Applied AI's mission and goal to help our customers build great products and features on top of Cloud. So when I first showed up at Anthropic, I was meeting with companies from AI native startups to large banks, meeting with their products teams, their engineering teams, figuring out what they're trying to build and seeing if we can do it with Cloud if Cloud can solve their problems. On top of that, everyone on the Applied AI team to make sure we're not just, you know, people that, you know, talk to customers but don't actually, you know, do the work, everyone on the Applied AI team also contributes to Anthropics products and so when Cloud Code was still an internal tool, I discovered it and. Fell in love with it and got to know Kat and Boris and the team and kind of joined that team as like a second job almost and so most of the system prompt, the tools, all the kind of AI engineering that powers cloud code, uh, I wrote or worked on in some way. And so between those things I've had a very nice I've had kind of the privilege to have a front row seat into how um we build on top of these models and get things into production and how this has evolved. So when I joined Anthropic in January of 2024, we had a model called Cloud 2.1. Has anyone here, uh, ever used Cloud 2.1? OK, no one, that doesn't surprise me too much. Uh, Cloud 2.1 was not the best model in the world. Uh, there were some cool things about it. It was on AWS Bedrock, something that our customers still really appreciate about Cloud today, and additionally, it had a very large context window of 200,000 tokens, which many of the other, uh, players in the space kind of topped out at about 32,000 or 64,000. So we had a few customers, but things weren't crazy. Uh, then we released Opus 3. Anyone has anyone here used Opus 3? Maybe 1. All right, Obis 3, when we released it was by many measures the best model, uh, in the world, and it really put us on the map and kind of set us off on this awesome trajectory. One thing we believe at Anthropic is we believe we are on this path uh to build better and better models and we think this is gonna happen very rapidly uh and we also believe that this rapid improvement in AI will have transformational effects on society and the world and how we work and we do a lot of work to study this and research it and get ahead of it. Um, but more practically as the models have gotten better and as continue to get better, the way we build on top of these models has evolved. So when I joined anthropic and I was meeting with customers in the cloud 2.1 days and the cloud 3 days, uh, most of the things people were building, not super ambitious, were what we might call kind of single turn prompts, things like, hey, I need to classify a whole. Bunch of product reviews or I need to write a first draft of an email or we did a lot of this. Here's 3 help center articles and a customer question can you like draft a draft a response, a lot of like Q&A chatbot sort of stuff. The model started to get a little better and people started to get more ambitious about what can we build on top of these LMs and we transitioned into what we call workflows. So one thing you could think about doing is you have your single prompts and a bunch of instructions you could just say OK I'll just. Put more in my single prompt and have it do more interesting complex things. Well, in practice that doesn't work very well. The model can, you know, maybe only reliably follow so many instructions at once. And so how do we build something that's more interesting and rich that we can't get done in a single prompt? Well, we try to decompose this task and split it up and chain multiple prompts together and maybe mix in some deterministic logic, uh, to get something more interesting and rich than we could have in our kind of single turn architecture. And so if you have used AI features and you know products even today, uh, if you were to peer under the hood, um, probably a lot of that stuff is powered by workflows. Now workflows in practice have two problems. Uh, one of them is that workflows are really only as good as kind of all the edge cases that you've kind of planned for, right? Like we have kind of this predetermined like set of, of things that this workflow can do, and uh that doesn't work very well for open ended tasks. So if you're trying to build an AI system that can just do day to day software engineering for you. Very hard to build that in a workflow. I've met with customers and seen customers that have workflows that are made up of 50 different prompts, and it can be very challenging to kind of manage this and keep this under control. The other thing that is tricky about building as a workflow is that if something goes wrong in the middle of this workflow, you're on LM call 2 and something, I don't know, unexpected happens, the LM makes a mistake or something like that, uh, it is very hard to build any sort of like error correction or the workflow like notices something went wrong and can kind of like self-correct. And so starting late last year and certainly this year we have moved into for many problems building on top of a different architecture anthropic this is what we call agents and with an agent in many ways it is much more simple, but we take our LLM, we give it a set of instructions and we give it a set of tools and some sort of open ended task and we basically say to the LM work on this task, call these tools as much as you want, let me know when you're done. And this helps with our open ended problem because we don't have to code for everything ourselves. We let the agent kind of figure the problem out on its own. We trust it to do it. We're giving it agency and on top of that, if things go wrong, I don't know, the model calls a tool and it gets back a response that it didn't expect or it gets back an error, the model can see that and react to it and adjust its course and try again and so it gives you a lot more of this reliability. Um, and you can build more rich, robust kind of products and applications on, on top of this architecture. And so at Anthropic uh we see both of these kind of setups in production today. They're good at different times. We have work flows when we have more narrow, very repeatable, maybe we want to mix in a little deterministic logic, uh, tasks, and then we use our agents when we're building more kind of open ended broad. Um, sort of, I don't know, working on those sort of problems and, and maybe we have a set of tools and, and things can go wrong and we're trusting the model to deal with more ambiguity. Great, thank you. So as we've evolved from sort of this, you know, single call shot prompting into uh agents and workflows, this is also, I think, represented an important shift in how we think about engineering and a shift in sort of architecting software. So with traditional software. We essentially encode the business logic of the software using precise logic and rules, you know, we're writing essentially that code. I click on a button, something happens that has been predetermined and and essentially baked into the software, but with AI systems what we're doing is since we're relying on these probabilistic models to make certain dynamic decision decisions, we're essentially steering the model with the data and the context that we feed into the model. So in a lot of ways the business logic essentially becomes a output of the data in the context that we're feeding in the model and. It's a shift represents a shift where instead of software necessarily being precise logic and rules, your data pipeline in a lot of ways becomes kind of your software application. And and the the same change is kind of happening in the world of testing where with traditional software we use unit tests, we use system tests where we can deterministically test the software, but now we're moving into this world of evals where we're using probabilistic tests against the AI systems and with unit tests we don't necessarily need to test our software using a real data because essentially the same input's going to generate the same output. It can be deterministically run. If the unit has passed, we have some level of confidence that the software is going to run as expected once we put it into production. With AI systems, it's hard to do that without some representation of the real data because the data is such an important input to orchestrate and sort of steering the model in a particular direction. So we need to be able to essentially evaluate in batch offline using some representation of the real data, but we also need to be observing these sort of in real time and also continually. Evaluating the AI system against a real reflection of the data to have some sense that of reliability of the AI system. So with traditional software we lived in a world of iterating on code, and with AI systems we're essentially moving in the world where we're iterating on data all the time. We need to iterate, sort of process, reprocess the data and test and iterate in an eval and get to a place where we have some level of confidence this AI system's going to do what we expect it to do. And talk more about sort of all the components of this like why context matters, what are all the elements of context engineering gonna pack this pass this back to Cal. OK, so remember when we were doing single turn back then we weren't really, you know, the models weren't very smart and you know we weren't quite, we didn't quite figured things out and so I would call this the era of prompt engineering last year when I was here at Reinvent I had a whole kind of talk where I was talking about prompt engineering best practices and. Uh, I don't know, prompt engineering was all about, OK, you know, the model's not very smart, so I gotta think about how exactly do I craft my instructions in a way where I get something useful out of the model on the other side. There are all these tips and tricks like, oh, how many examples should I use, and, oh, if I'm using anthropic models I better wrap everything in XML tags. I think that'll make the model work a little better. And so when you were kind of building on top of single turn prompts, what you're thinking about was, OK, how do I kind of what do I put in my system prompts? What do I put in my user message? There was a time at Anthropic where like one of the pro prompt engineering tips was never use the system prompt for anything because Claude was anecdotally a little better at instruction following if it was in the user message and so there's this all, all these kind of little tricks that people would, you know, kind of think about and, and try to iterate on and, and test. Now We are in kind of the kind of agentic world and so we have moved on from prompt engineering to what we call context engineering. And we kind of think of this as a more broad and interesting problem. Why? Well, we have our system prompt and user message just like before, uh, but we have other things that we wanna think about now we have our tools and if we wanna maybe give our agent access to documents and other, other pieces of text, how do we do that? And then remember the agent is the LM running in a loop, so it's not just one single API call to the LM. It's going to be calling the LM. It does something. You go do something else, then you give it back to the LM and it's running a loop and going and going, going. So we have the assistant message. What does the agent say? And we're gonna have all of our tool calls and results, and this is gonna build up over time. And then maybe to mix things up we wanna add some new tools, some new documents, and so the problem space, what we need to think about and work on when we're building agents, putting them into production. Little more interesting, a little more fun than than prompt engineering in my opinion. So that's what I'm gonna be talking about. So prompt engineering, back in the day, models weren't super smart, they were very, very sensitive to exactly how you phrase things and word worded things, what words will get me the absolute best output. Context engineering much more about just managing this whole information environment what tokens what what am I gonna present to my agent and when and how I'm gonna how am I going to do it? What is the optimal configuration of context to get my desired behavior out of this out of the system. I've said context a few times. It's worth mentioning that when I say context, I mean the context window. So all LLMs, whether you're using cloud or chat GPT or Gemini, they have this context window limit, and this is actually enforced at the API level, but there's only so much text or so many words you can pour into the LLM and get a response back. And it's not that we don't like know how to do the math where if you gave us, you know, 10 million tokens we wouldn't know how to process it. It's more about that the LLM at some point we start to see like diminishing returns or if you pour too much text into the LM, it starts to get confused and do a bad job. So we have to like set some thresholds where we think like it's an appropriate time to just say hey you shouldn't be passing any more tokens to small. There's this idea of context rot which is, OK, we have all these tokens, all this text that we could put into the model. I kind of hinted at this earlier, which is we could take all these instructions and put them into the, put it into the put it, give it to the model. We could take a whole bunch of documents and give it to the model. What we find is in most use cases if you just start dumping text in there, uh, the model tends to do a little worse at whatever you're gonna ask it to do. And so even before you hit this context window limit, you wanna be mindful of OK when my agent is working when it's calling tools and getting responses back, the initial instructions I don't want a lot of extra junk in there because it's probably gonna be impacting performance. A company called Chroma, they did a very nice technical report. It's a blog post. Look it up. Uh, they write all about this, and they actually like kind of break it down and talk about different ways that this can fail. The other reason you're gonna want to think about context, especially when you're building agents, remember I said that an agent is the LLM running in a loop multiple calls over and over again, is if you build your agent in the right way and you're thinking about context, you can take advantage of things like prompt caching. The idea with prompt caching is if you make an API call to an LLM. And then you make another call and there's some sort of fixed prefix like the text didn't change from one API call to the next. Anthropic can cash it behind the scenes and then we can pass on a very nice uh little cost discount to you as well as you benefit from some latency because we don't have to process these input tokens again and so if you are building with context engineering in mind, you can take advantage of these things. And so when we talk about context engineering we're really trying to kind of move three different levers. One of them is very practical and boring. It's the fact that our API will only let you pass so much text into it at once. So we need to think about, OK, how are we gonna deal with this context window limit and get around that and still get our system to do cool things. We can reduce context rots, which is this idea of even after we've kind of. When we're giving stuff to the model, can we make sure the tokens that we're showing it are useful and we're not hurting our accuracy in some way? And then finally, uh, if we do this right and we're mindful of prompt caching, we get some nice cost and lengy benefits. And so we do all these things in our own products we're building cloud code. Um, and we advise our customers to do this as well. What are some other things we can think about when we're doing context engineering? Well, we have our system prompts, the instructions that we're gonna give the agent, the model, we have our tools, OK? What is our agent gonna be able to do? How can it interact with the world and pull more information in and change things. We have data retrieval, which we're gonna talk about a lot, which is OK, this agent is only gonna be as useful as kind of the information that wasn't pre-trained into it. How do we pull things in about the business, about the problem, about the customer, so that it can make smart decisions. And then I'll talk a little bit about long horizon optimizations if we're building an agent that might work for minutes or hours or days, we're gonna have to do some fancy stuff to get around some uh more practical limitations like the context window which is going to fill up. So let's start with the system prompts. And when we talk about prompting today when I'm meeting with customers and talking about prompting, it's less about, OK, should I format my prompt as JSON or Markdown or Cal, should I be using XML tags and where should I be doing it? We're at a point where we've done so much work to make the model steerable, uh, that the little tips and tricks around, you know, XML tags don't matter so much. So when people ask me, hey Cal, what's like your best prompting tip? I always tell them that the best tip I have when you're writing a prompt for an LM is you should take that prompt and imagine you are giving it to your friend or your coworker that doesn't know what you're working on and you say to them, hey, with this prompt with these instructions, would you be able to do this task? And if they say yes then probably the LM can do it too and if they say no I'm a little confused, I don't really know what you're asking me for, probably the LM is confused as well. My team, what we do is we go meet with with customers and very often a customer will say something like, uh, we tried Cloud, but like I don't know, it didn't really work that well. I don't, I don't think we wanna use it in production. And always what we'll say back is like oh OK that's that's fine, but like can we take a look at the prompts like we might be able to help and 9 times out of 10 when we go and look at the prompts there's something confusing in the instructions and we can go in and point it out and clean it up and fix it, put that into production and all of a sudden cloud works well. And so there's kind of two failure modes, uh, even if you write good instructions where things can go wrong. So one example of this is I was working with a customer that was building a customer support agent, and what they did at the very start in their kind of POC is they took the standard operating procedure, the SOP, that they give their human support agents when they on board when they start on the first day, so about a 32 page PDF. They copy pasted it into the prompts and then they came to me and said hey this isn't working very well and I was like oh my god um and so this is where you can get too specific. If your prompt starts to look like something on the left where you have like this complicated workflow and especially if you have all of these ifL statements, you have this very complicated chain of instructions that goes on and on and on, especially for thousands and thousands of tokens, you're gonna overwhelm the model. It's not gonna be able to do this very well, and there's techniques and ways to get around this, but this is probably not what your prompt should look like. On the other side you can fail by being too vague and this is what I talked about earlier you give the prompt to your friend and they're like I don't know what you're talking about. I don't I don't know how I would do this task myself with these instructions. And so when we think about prompting, we're kind of trying to get into what we call this Goldilocks zone where we have just the right amount of details, but we're leaving things open ended and letting the model do what it does best, which is deal with nuance. We're not trying to like hard code in a workflow we're trying to give agency to the model so it can kind of work. And we want to just be telling the model like you know the general way of how it's gonna solve this problem and then the like non-negotiable things that it absolutely must do or absolutely shouldn't do and then everything else we should try to let it figure out on its own. So it can be too prescriptive signs of that is if we have a whole bunch of if else logic with these very complex and brutal instructions and we're trying to like encode every single possible scenario into the instructions and then of course we can fail on the other side like I said. That's the system prompts. OK, what about tool design? So we have our instructions in the system prompts, but we're also going to give our agent 12, 1050 different tools. How do we do that well? Well, we wanna remember that the tools, when we give a tool to the model, we should just think of that as additional prompting. What happens behind the scenes when you make an API call to Anthropic and you pass the tools array, we take that tools array, we actually just plop it at the top of the system prompt and it says something like, hey Claude, here's the tools you can use, and Claude's been trained to kind of know what to do from there. But when you're defining these tools and passing them in the API. You gotta remember that those definitions are going straight to the model. It is seeing what you are typing in there. And so there's some things we can do here to make sure that we do well, so one is we want just simple accurate names search customers. We don't want something that's just like, I don't know. X Y Z 123 we want detailed and well formed descriptions, and we can do things in our descriptions. Think of it like more prompting. We can do things in our exa in our descriptions like provide examples like here are good times to use this tool and here are bad times to use this tool. When I was working on cloud code, I was very often not messing with our system prompts, but I was spending most of my time in tool descriptions tweaking the tweaking kind of the prompting and instructions there. One failure mode that can definitely happen when you're building an agent that's more general purpose and can do a lot of things is in cloud. AI, which is our kind of like chatbot tool that we, we sell to enterprises. We had one team that was off building a web search feature at some points. They had built this tool that was something like search web. And meanwhile another team was building out our Google Drive integration so they had built this search drive uh tool and these, you know, both of these things were working very well and we shipped these at around the same time and we kind of put them in the you know the model now has the search. Web tool and the search drive tool and all of a sudden the model kind of there wasn't very good prompting here and so the model was confused about what when to use which tool and when what information would be hiding behind a Google Drive and what would be behind the web and so the model would be searching for things in Google Drive where clearly it should be using the web and vice versa. And so if our tools, the names and descriptions, especially on searches where we've ran into this the most, are too similar, uh, you can get into trouble and and confuse them all. OK. Data retrieval. Paradigm shifts. So back in the day, back when we were building single turn prompts and and work flows, the way you would give the model information about things it wouldn't know about because it wasn't trained on, it wasn't in the pre-training set was you do this thing called reg retrieve log. To generation, the basic idea is here, here is you have a step before the you call the LM where you go try to grab all the potentially useful information and then you give it to the LM and you say here's the information, try to do I don't know, try to answer this question, good luck. Um, Rag has a problem which is if you fail the retrieval, if you do a bad job at retrieval, there's nothing the LM can really do about it. If you give it kind of junk data, it can't really recover out of it. Agents solve this problem because we can give the agent a search tool and the agent can write a query to run a search, get some results back, and think, oh weird, I didn't get what I wanted. Let me try again. So that's sort of like I talked about earlier, the ability to kind of recover out of errors and kind of bad states. And so in an agentic set up when we're context engineering we're always thinking about OK what do I wanna preload into my agent? what do I want to always be there, probably the instructions and the tools and what information can I kind of hide away and tuck away behind my tools and the agent can go grab it only if it really needs it. So in cloud code for instance, uh, if you use this tool we have something called cloud. MD. This is like a special markdown file that is optional but we let our developers basically put in additional like information and instructions and preferences that are unique to them about how they want cloud code to behave and you know what they wanted to kind of know about themselves or the code base and so something like that we preload no matter what when the. Agent starts. We just take, we just look in the current working directory for a cloud. MD file. If it's there, we put it in the system prompt and we literally say something like, hey, here's some additional instructions this developer wanted you to know. So that's something we preload. But what we don't do in cloud code is when we start it up, we don't like take all the files in the code base and just like dump them in the prompt and say, Hey, here's all the files in this code base. Good luck. Instead, we give Claude two tools to basically just kind of grab and find and search a code base like you or I would. And so we're thinking about ways we can do progressive disclosure. How do we kind of. Give the model hints about OK what information might be available to it and if it thinks it's interested then it can go and look things up and figure out more about it. Has anyone here about heard about skills? Yes, so skills is kind of building on this pattern we saw this in cloud code where we could take files that might be interesting at some point and let the model kind of figure out and use them over time without having to load all the information up front. And so this is really what skills is under the hood. So skills is like this idea where you, as the end user, basically define, I don't know, mark down files, Python files, things like that that might be useful for the agent at certain times. So in cloud. cloud.AI, a good example of this is we want cloud.AI to be able to, I don't know, make a PowerPoint or build a spreadsheet. Um, but we don't have to take all the instructions for making PowerPoints and making good spreadsheets and put it in the system prompt all the time. Instead, in the system prompt we say something like, hey, if the user asks you to make a PowerPoint or build a spreadsheet. You go look in the skills folder and there's gonna be a whole bunch of useful stuff in there for you and so in the 10% of the time or whatever where the model where where you know the end user is gonna ask for this, the model knows how to go and knows how to go explore and get that information. And then the last thing I wanna talk about is long horizon tasks. So remember we have this context window that's gonna fill up and remember that the model is running in a loop API call, API call, API call, and we don't wanna bust the prompt cache. So that means we're not ever gonna like remove text from this conversation chain. We're gonna be appended only and so at some point our conversation's gonna get too long and we're gonna get to 200,000 tokens and what do we do? Well, in cloud code the way we solve for this is something we call compaction. When Cloud is getting close to the context window we can detect this, and what we do is we basically inject a user message in and the user message says something along the lines of, hey, I need to pass this task off to another developer summarize everything we were just working on and all the important details, um. And then we take that summary, we clear out the whole conversation, so we start over fresh, we put that summary at the very start and then we say, OK, keep working from here. But there are other ways to solve this more experimental memory can the model just write notes for itself and then also sub agent architectures. So system prompts, how do we get the instructions in the right shape? Tools, probably the most interesting and useful things that the agent can do. Think about iterating on the tools, data retrieval, which we'll talk about how do we get the right information to the agent to do useful things, and then being mindful of long horizon optimizations if we're building an agent that's gonna work for many minutes or hours. Awesome. So I'm gonna dig into this kind of data retrieval challenge and uh how do you curate context. So if we kind of pause for a moment and we think about like human reasoning, so if you're gonna cross the street. You know how many people would feel comfortable crossing a street based on essentially a snapshot of where the cars were on the street yesterday. Like most of us probably wouldn't feel confident about crossing that street. And when it comes to like human reasoning and decision making, you know, something as simple as crossing the street, like, obviously we, we have a lot of history of doing that. Like I've probably crossed the street successfully tens of thousands of times in my life. But I don't just rely on my historical reference, I also need essentially a fresh representation of what's happening in the moment. So I need to pair essentially both sort of real time signal information or what's happening in the moment or fresh context with historical pattern matching and so forth, and this is true of most sort of human reasoning situations, but it's also true of most operational use cases when it comes to software, you know, dealing with customers and so forth. So these are important things to to think about and this is in many ways what the context problem is when it comes to data retrieval because during this kind of online process of prompt time assembly, I need to go and gather this contextual information and if my AI system is doing something like helping doctors or nurses monitor patients, I don't want. That AI system to be making decisions based on vitals of the patient from 6 hours ago. I want it to be making decisions based on what's happening right now in this moment. But that's a hard problem in many businesses, and there, but there's a couple of different ways or two different approaches that I see businesses trying to tackle this problem. So the first thing is essentially taking all of their operational data stores, whether it's a database, an API, some sort of SAS application, and throwing MCP in front of it. And I actually had a call very early this morning with a company that is attempting this approach and It makes a lot of sense. It's like let's take all of our operational sources of truth and we'll throw MTP on front of it and we'll use tool calls into it and we'll get sort of the latest information about our business estate. But in practice there's a number of challenges with this, and it has nothing to do with MTP. MTP is great, but what you're doing when you sort of open up all of your different locations to. You know, a new workload like this is you're adding a pretty large security footprint on these existing systems that are probably all have their own ways of governing access and access control, so that's one problem you need to solve. They're also probably serving other workloads within your business, so you're adding a bunch of different new operational load to these databases and other systems. And probably one of the biggest challenges is that none of these systems were ever designed with the idea that an AI system was gonna use them. They were designed for humans to use them. So if I'm, you know, designing an API and I'm an engineer that's gonna do some integration with that API, well, I can read the documentation. I can read the API. Ice pack and I I interpret it and I write business logic that allows me to use that API. Maybe I'm only using a subset of the data or I need to write code to translate that data into a different format, whatever it is, I can essentially, I have the context to understand that raw data, but the model doesn't doesn't. So what you need is not the raw data. Because the raw data ends up with this problem of exploding token costs where we're feeding a lot of information essentially on the ingress in the model, and that leads to mistakes. What you need is refined data or a dry data set. It's like we don't put crude oil into a car. We go, the oil goes through a refinement process before we feed a car and end up with fuel, or you know, we don't take a bunch of raw ingredients like, you know. You know, salt and pepper and, you know, spices and, you know, a bunch of raw vegetables and so forth and jam them in their mouths. Maybe you do. I don't know, but usually you go through some refinement process. You cook it together in certain quantities to derive a data set or derive essentially a meal. You go through a refinement process. Same thing needs to be applied when you're building AI systems. You need to essentially refine the data, come up with curated data. So once you realize that this problem, where do you go to get derived data? Well, there's a place where many businesses already create derived data sets, and that's typically in the lake house or the data warehouse where they're already taking a bunch of their operational data, they're sort of throwing it, you know, left or right through ETL into the lake house or the warehouse, and then. They might be going through like a medallion architecture of bronze, silver, gold, and getting defining a data product, and that data products used to feed dashboards and so forth. They can reverse ETL that out to some operational database, maybe throw MCP in front of that, and then have their agent talk to it. So this solves the problem of getting derived data, but it introduces a new problem. And the new problem is that this is just too slow for most operational use cases, so it carries all of the same challenges that ETL carries with it, where you have different batch pipelines delivering data at different stages, you know, maybe it's 60 minutes, maybe it's once a day, and you're combining that data in some fashion where it represents essentially your data is only gonna be as good as the slowest pipeline. A lot of times that it is thrown raw, so into the lake house and the data engineer team has to figure out how to reverse engineer and understand what the scheme is, the lineage and so forth, ends up with a lot of reprocessing, remodeling, and a bunch of cascading downsides, but we've been using this to essentially feed dashboards and reports. But it's a challenge when it comes to trying to feed AI systems, especially in the operational use case. I mentioned patient monitoring. There's very unlikely that you'd be able to build a system that goes through this process that can monitor patients at sort of the level of the the latency that you'd really want to be able to make accurate decisions. Similarly, if we take like an example from airlines, if. Interfacing with a customer service bot that's powered by an AI agent and I want to rebook my flight. Well, that agent needs to know what flights are available right now and in the, you know, the essentially the, the foreseeable future. It can't make decisions based on a report that was generated a day ago. It also needs to know my itinerary, my customer, my preferences, maybe my status. All this stuff needs to be as fresh as possible. So how do we sort of marry these worlds of having both fresh data and derived data? That's, that's really the key problem. So we need a derived data set that represents the current state of the business, and we can do that using a combination of technologies. So streaming data capture is really good at essentially capturing data as it's generated, real-time data using technology and tools like Kafka. We can use stream processing, in particular Apache Flink, which helps you essentially. We bring both sort of uh um you know data at rest or batch data together with real-time data and just essentially batch data is just bounded streams versus unbounded streams so we can combine that data in the stream to create these dry data sets using stream processing technology and then we need some way of serving that data and ideally we have one layer of governance across all of this. So at Confluent we've been working on this problem for a number of years, and a couple of years ago we launched a product called Tableflow which doesn't quite address this problem but kind of starts to address some of the challenges. So Tableflow was designed to try to bridge the gap between the operational estate and the analytics estate where we can. Do streaming data capture. We can use stream processing to clean the data sort of as close to the source as possible, create these derived data products, and then materialize that data as iceberg tables or delta lake tables directly into your analytics platform and kind of taking out the ETL process. And this is great for sort of bridging the gap, and it can also work both ways, where as you create data in the analytics state it also writes back out to Kafka so you can serve your operational use case as well, but still a column or store is probably not the right sort of data layer for context serving. So what we launched a couple of months ago is what we call the real-time context engine whereas table flow you're taking a Kafka topic and materializing as an iceberg table or delta lake table with the real-time context engine what you're doing is you're taking your Kafka topic or your dry data set and you're materializing at it as a fully managed materialized table that's managed by conflict cloud and then is served as a tool over MCP. So that way from data creation to serving is very, very low latency and it's designed essentially for feeding these AI systems and the AI system doesn't need to be running on confluent, it could be running, you know, wherever you want externally, and it doesn't need to know anything about the streaming architecture, it's just talking MTP. And this is one of the core components of something that we call conflict intelligence which we launched recently as well. And the other two core components of that is streaming agents, which allows you to build event driven agents that are, they're not, these aren't chat-based agents. These are essentially system triggered agents that react to data as it's being generated within the stream. And then also built in ML functions or statistical models for doing things like anomaly detection, forecasting, and fraud detection. And all this runs on conference data streaming platforms so you can do your streaming data capture. You can define these data products, then you can materialize those into your analytics systems as well as power these different AI systems or streaming agents running on conflict cloud. All right, so let's get into a couple demos. So I'm gonna walk through two different demos. The first one's pretty simple. What we're gonna do is just show off sort of how some of these, this, this concept of the real-time context engine works. So I'm gonna take a couple different Kafka topics. I'm gonna materialize them as tables, and then we're gonna use cloud code to talk to the MTP server, and I'll kind of walk through that. So what we have here. We have a Kafka topic that represents customers. We have orders that are being made by those customers, and then we're creating, we're using Flink to combine those into an enriched orders Kafka topic, and then we're serving those as tools over MTP and then we're going to essentially use cloud code as our MTP client to talk to it. All right, so here on the left we have quad code and then uh there on the right we have uh Kale cloud, and you can see on C cloud we're just looking at the customer's Kafka topic in a particular event that was created. So the first thing I'm gonna do, I'm gonna say ask to show me the topics that I have access to. So Claude is going to, um, essentially. Reach out and there's 3 different tools available within our MCP server. One allows you to pull back the tables or essentially the topics that are being materialized. Here we have customers. We have orders and enriched orders. All of these have descriptions so that MCP clients and cloud can understand what these actually mean. And then we can ask for details in this case about the customer, so. We have table lookup, essentially metadata or schema lookup for these various tables, and then we also have essentially point time lookup so we can look up this customer information of Joe in this case. So pretty simple, and you can see that matches the information on the right, not, you know, groundbreaking. You could probably do the same thing with the database, but the key here is that as we generate new data, essentially it's immediately reflected here. So we will fully manage this as new data gets generated, it will automatically get upserted and reindexed or if you need to change essentially the data model, you can handle those kind of reprocessing steps. Batch up essentially completely remodeling the data into a way that's actually going to serve your use case. So we cleared the context here. So now we're gonna say show me the last order placed. In this case it's going to make two different tool calls. It's gonna ask first for what tables are available because it needs to know what tables are there, and then it's going to do the point in time lookup to pull in the orders placed by this customer. All right, so the next thing we're gonna do is we're gonna go over to Cono cloud and we're gonna generate a new event to update this customer's email. To show how sort of updates work, so we're gonna produce a new message, but what you can imagine here is if you had some, your customer profile page on a website, I could go into that page, update my email, click save, that's gonna be right as some event into a Kafka topic that I probably then do a database update at some point as well. But as I essentially send in that new message behind the scenes we're. Taking that data, we're gonna upsert it into the materialized table, reindex the table, update the index, and then that's immediately available through MCP, uh, for the AI agent to consume. So we're producing that message now. You can see that it's a a new message there and then once we go back over here we're just gonna clear the context and ask to show the the customer record again, the details of the customer. So there's really from it because this is all built on data stream, the data streaming platform too and sort of Kafka as the basis for that and Flinn, these are really designed for really extreme high load and low latency use cases, so it doesn't matter how complicated your data is or how fast things are updating. Essentially you can combine that in whatever way you want, mix it however you want to create these dry data sets, and it's going to be instantly available to your AI systems. So there we see the new email, and then we can continue to do this to show you the latest enriched sort of view of this data and so forth. All right, so let's go to the next use case. So the next one is, uh, an application for robotaxi customer service. Actually, before we, uh, we came up here to talk, we were talking about Waymos and what's the Zooks and all the autonomous drivers. Well, this is the next iteration of this. This is River Robotaxis. So these are like Waymos but for river boats. So imagine you're on the Mississippi in New Orleans, you pull up your River RoboTaxi app and you book a essentially a river boat. That river boat comes and picks you up. So we're not gonna talk about the fleet management. What we're gonna talk about is the customer service part of this. So with the river robotaxi, there's gonna be different spikes that happen in terms of ride requests and they need to take certain actions. Maybe they need to dispatch new boats to the area and so forth. So what the scenario here is, is that I've ordered a boat, but. It's taking longer than expected to arrive. I'm sure anybody's ever ridden in an Uber, uh, an Uber minute, you know, one Uber minute is really like 5 minutes. Uh, it's very frustrating. But so with this, what I can do is I can go into the River RoboTaxi mobile app, start a chat with customer service which is powered by an AI agent, and I wanna know where. Is my boat? What is taking so long? So for in order for it to be able to understand and correctly contextualize the response, it needs to understand the ride requests, the volume requests, what decisions have been made, vehicle location, telemetry, that's kind of streaming off of these different river rover, these different river boats, customer profiles all need to be taken into account. And what we're gonna do is essentially use the context engine to serve this data to uh the AI agent which in this case was built using Lang graph running an AWS and using cloud as a model. All right, so let's cue that demo. Little magic happening behind the scenes here. All right, yeah, so this is our various Kafka topics here. We have different, uh, topics that we want essentially materialize through the context engine. So we have completed actions which represent decisions made by robotaxi for dealing with spikes in, in, uh, ride requests and so forth, and we're just gonna enable the context engine for these. And we can do the same thing for ride requests. Once we've done this, these are immediately available as these materialized tables that can be consumed or MVP. I go over to the river support and I ask about my ride. On the left you can see the, the sort of chain of thought reasoning that Claude's going through to get essentially the customer profile and then also get the decision that was made about why. That there's a delay going on right now, it uses that to contextualize a response back to the user, letting them know that they're aware of a delay. They've dispatched new boats to the area to deal with it and to please be patient, but the only way it's able to make a decision like that or essentially be able to create a response like that is it needs access to that fresh contextual data to be able to do it. And this is again true for most of these kind of operational use cases. Oops, all right, well I just said next steps. All right, let's talk next steps here before we get into the questions. So if this is of interest to you and you wanna play around with some of the stuff, you could check out our streaming agents project product. You can also do, uh, there's a workshop that's going on with this. There's a, a quick start you can also use, uh, if you wanna sort of do a self-guided tour, and you can try this all out for free using Coffin Cloud. You get $400 worth of free credits for the 1st 30 days. I'll pause for a minute as people take pictures. And the last thing here is, uh, just this week we launched Confluent Marketplace, which is a marketplace for, uh, seeing all these various community connectors and a whole bunch of other community driven projects that you can use within confluence data streaming platforms. This allows you to ingest data from pretty much anywhere, any kind of uh uh source system within the enterprise. Really cool project. And you could try that all out for free as well.