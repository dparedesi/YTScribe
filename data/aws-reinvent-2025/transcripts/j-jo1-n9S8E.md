---
video_id: j-jo1-n9S8E
video_url: https://www.youtube.com/watch?v=j-jo1-n9S8E
is_generated: False
is_translatable: True
---

It's just to kick this off, today we're gonna be looking at log analytics and log management through the power of DynaTrace and the power of Davis AI. So looking at how we can do log analytics at petabyte scale, ingest and precise root cause analysis. And we've got a really interesting story to sort of back this up as well from, from the, from Alex here from Storyo Group. And also of course, with the partnership that we have here with, with AWBS and again. How of course DynaTrace AWS and of course our customers that we have together work hand in hand. So I'm John Griffiths. I'm the field CTO at Dyna Trace, covering primarily Europe, Middle East and Africa. And uh my name is Alex Hibbert. uh I'm an engineering director. I'm responsible for the customer platform of Storyo Group. E and, I'm Frank Schwartz now. I'm the global lead for the observability use case for AWS partner specialists. And yeah, let me let you, you start now, back in a second. So I've just sort of started on We'll look at first Dian Trace and Aid of Breath in that partnership together and how we sort of enable customers to innovate, move faster, and of course reduce their mean time to resolution and resolving incidents. We'll then move on to to Alex with with the interesting story around Story Group's journey with log analytics and how they've made massive changes around that. And then finally touching on actually modern log management, the sort of things that you can learn from that, how we can look at, you know, significant cost savings, a lot of the time the sort of 60 to 70% reduction in log spend, uh, whilst still getting even more significant value out of that. So start with D Dyne Trace. How many of you here, let's say are a customer of Dynatrace? OK, a reasonable amount. How many of you here work for a competitor of Dyno Trace? No? Oh, there's, there's a couple. Security? No, it's OK. Um, now as a whole, basically Di Trace, for those that aren't fully aware, focuses on observability and security. With AI at its core. Now, what that ends up meaning is that all data, all signals, whether that's the logs, whether that's metrics, traces, whether that's from your infrastructure, your applications, the real users, your AI agents or LLMs, whatever it may be, getting all of that deep insight in context. And the context is key because whenever you look at things like automation or AI. Good quality data is is called to that, you know, rubbish data, rubbish output. Good quality data obviously allows you to have far better outputs associated with that. Now, again, when you start to make decisions and data-driven decisions, again, data, context, and especially the domain context that comes into that can be very crucial to this. So really, Drees is providing that deep insight in the context of your domain, in the context of wherever it's grabbing that information from, so that as you can see here, we like to say that we'll help you understand your business like never before. Now Again, Obviously, a few quick slides first, but. We are obviously leading in this market. As you can see here, this is the Gartner Magic Quadrant, where the most recent one we've been in the highest ability to execute. And ranked number 1 in 4 of the 6 critical capabilities. And finally on there, as you can see, we are the only vendor to have been named in every single Gartner magic quadrant for APM and observability since its inception. So we've been in that lead quadrant in every single one, so that's 15 years of staying in that lead quadrant. Again, innovation and continuously striving forwards is obviously key to that. And of course it's not just the analyst side of things, you know, we have customers from every single industry. You know, whether that's, as you can see here, things like aviation, banking, retail, utilities, and all across the, across the board, we have customers in every single industry and across every single continent. So again, really driving that platform faster. Obviously we are taking the insights from our customers, expanding obviously on our awareness and ensuring that we are innovating to keep up and match what our customers actually require. Oh, Frank. That's my cue, um, so as I said before, my job at AWS is essentially just focusing on the AWS, uh, observability use case and working with both ISVs and SI partners, and, um, we have about 120,000 partners in our network, um, and of those 120,000 partners we have about 200 in our cloud operations competency which includes observability. And there's actually just 8 ISV observability partners that are specialist or deemed specialists from AWS and got the same of approval, uh, to be that observability partner. So the 10 years of partnership, um. Is a great number, but it's more important to focus on how much Diana Trace is leaning in with AWS and with the customers, uh, and that leaning in is of course represented by all of these competencies that are achieved in the bottom here, uh, which essentially are the stamp of approval from AWS where, where DynaTrace jumps through a lot of hoops that we put out there in order for, for, for you as a customer to be able to say like wow, DynaTrace, they really know what they're doing in a. Around all of these use cases, um, but one exciting part, of course, is how much Dinarace is leaning in on the, uh, on the AI use case, um, and is launching on the, you know, on a monthly basis something like, uh, all of the integrations with, uh, Bedrock Agent Core, for instance, for end to end observability for your agents or even Kiro integrations for AI assisted, um, uh, uh, troubleshooting through your IDE. Um, but also what's really important and I think what I always come across is there's very few customers that complain about DynoTrace, which is really nice to see because I think I, I, from my perspective and I see a lot of different partner engagements, a lot of different customer engagements with our partners. ETrace is really kind of moving the needle and really kind of defining of what it means to be customer obsessed, what it means to be aligned with AWS, uh, and provide that value throughout not just make migration easy but then also how does the customer get value. Out of using both AWS in the combination with DynaTrace afterwards, um, one of the cool things is also the native integrations. This is probably way over 100 at this point because the slide is a few weeks old, but, um, um, we see DynaTrace really leaning in, um, on launching services together with us and be launch partner we beta customers, uh, for stuff that you don't even know that's around the corner yet, but, um, but really kind of being tied at the hip. Um, and of course you were a global partner of the year last year, no this year 2025. You were a Technology Part of the Year in EIA last year. So I think, um, there's a, there's a big recognition, uh, from both the AWS, uh, environment as well as from our customers, um. DynaTrace leans in, um, and I can talk about each of these pillars so I hope everybody is familiar with these pillars from the well architected framework um but of course you have, um, you know, resilience, uh, you have resource optimization you have cost optimization you have performance optimization and SLA management and so on but the more important fact about this slide is that Dynarace thinks about the impact to the customer very much aligned with how AWS thinks about how to have impact with the customer, so. Um, that it's gonna be easy for, for all of us to work together and talk around the same kind of core concepts and core, um, guiding principles. I talked about AI before and how much NTrace is leaning into these integrations this number here of 500 implementations. Uh, is, um, actually just from the AIU's case, so we've already have 500, uh, wins that we have together where DynaTrace has enabled customers to really leverage, um, the best of AWS and DynaTrace to, to adopt the AIU's cases, um. And This is around or or on all different kind of layers, right? So I think everybody here in the room is pretty familiar of how to do observability or how observability should be done on the application layer and probably also pretty familiar with how it should be done on the infrastructure layer but I think the important part is sort of the wedge in between, you know, on the orchestration level where you. Prompt observability on the semantic layer where you really can look at how biased the inputs and the outputs are when you interact with your with your AI applications and then of course model health you know how do you continuously monitor the health of your models that you use for your applications. And then I mentioned this already. Dina Trace launches a lot of services with us, um, and I'm always happy when we're about to launch another service and the Dina Trace CTO is putting up their hand is like we would like to test it. I'm like perfect, uh, so that's happening continuously and it's a real joy to see how much you guys are leaning in on, on these things and giving us feedback as well and sort of co-innovate together in order to make this easy for our customers. Uh, speaking of customers, this is just a couple of metrics, um, and it's quite interesting when I, when I kind of dove deep on this one, And eventually, essentially this is all essentially working backwards to money, right? So if you're reducing the time spent um on your on your on your uh uh resolutions if you're actually reducing the overall number of uh incidents or major incidents, um, and if you just have faster issue resolving and higher up time in in the end of the day all of this is boiling down to customer happiness and money. Uh, and this is just some really, really impressive numbers, and we're glad that we're in this together, um, and I can continue in talking and talking about how much customer impact we have, but we have a customer here, uh, that we're both cousins that we're both partnering with, um, Alex, if you wanna come up and maybe tell your story. Absolutely, thank you very much, Frank. So Who here has heard of the three pillars of observability, bit of an outdated term now, but just a quick show of hands. Yeah, a few of you. I'm not gonna talk about those three pillars of observability. Instead I'm gonna talk about an alternative 3 pillars of observability. And these have come out of our adoption journey with Dynatrace. I say adoption, it's in fact a re-adoption because we'd worked with Dynatrace in a previous iteration before. I'm willing to bet most of you have never heard of Storyo Group though. So Storyo Group are Europe's biggest photo gifting company by revenue. Um, we operate in about a dozen countries under 8 different brands, um, some of which you may have heard of, um, Albelli, Photoox, Hoffman, Poster XXL. We serve over 7 million customers a year, and in 2022 we delivered over 10 million orders. Um, and we've been around for quite a long while, but not in the skies. Our oldest brand is Hoffmann, and that's been in the Spanish market for over 100 years now, so we've got some serious history. But as Storyo Group, we've only came together in 2021 as a merger of two different businesses. Albeelli, who are based in the Netherlands, and Photobox, who are a group based in the UK. When we joined together, we had a massive challenge. Immediately post-merger, we looked around our ecosystem and we had something like 5 different technology platforms providing largely speaking, the same customer experience, selling photo gifts to end users. Any organization looking to innovate and make change cannot do so when they've got to do things 5 times to roll out one feature. So we really had a huge call to action there, and that call to action was how do we move into one singular platform. We also had huge scale to deal with. Across all of those platforms, we had over 11 petabytes worth of customer data, that's customer photos or memories as we like to refer to them. We really needed to go on a massively imperative journey to reduce the duplication we had down from those 5 platforms to 1 singular platform, and that journey took up 2.5 years, that 1st 2.5 years of our journey together. An exclusive focus, exclusive and probably ruthless focus. When we got to the final point of having one singular platform, we took a breath for a moment and looked around and realized that we had made many, many trade-offs, as you have to do when you're on a journey of such complexity and such, such pressure. We previously had a really mature observability posture when we were the photo box half of the organization, and The Abeli half of the organization had had a bit of a different strategy. And when we took a moment and we had a look, we said there are some things missing here. We had a platform that was strongly human heavy. It was very, very log centric. And with that, we looked at our MTTR and our MTTD, two core metrics, and realized that they were trending in a direction that we did not like. MTTR was going massively up, and simultaneously the number of incidents we were tracking was dropping. To me, that meant that we were losing track of what was happening on our platform, we were not observing as much as we had previously been in our photo box iteration. To give you an example, Black Sunday, as we like to call it, which is yesterday, the biggest trading day of the year for photo gifting companies, we were looking at engineers manually trying to correlate performance data and understand what was going wrong across 1.1 billion individual log lines. Humans looking at that much data are going to miss not just one thing, not just 10 things, but hundreds of things. And it was intensely resource intensive in terms of the computational power needed and the human power needed, and it was fundamentally not solving the problem we had. The the challenge with this was it created a massive business risk for us. Like many businesses, Black Friday through to the holiday season is a hugely important period for us. Across those six weeks, an outage would have cost us something like, at any random point it hit, at least $274,000. But during our peak hours, yesterday as I was flying over here, I was watching the numbers come in. We trade up to $1.5 million a million euros an hour. That's a huge number. Bearing in mind that an incident takes on average, when we were looking at our observability adoption, an hour and a half to resolve, you're talking about way over €2 million worth of trading suddenly blown up by an incident. That huge risk meant that this became a massive imperative to the business. We needed to reorient what we were doing and make sure we became the first to know when trouble was brewing. So this brings me onto our first pillar, the culture pillar. The core problem we had, if you remember a minute ago, was that we had engineers who fundamentally were looking at logs to work out when things were going wrong. What that meant was our engineers couldn't answer the meaningful question, what broke? Why did it break and how do we fix it? And one of the biggest challenges we found was that was in fact a mindset that many of our engineers did not understand was a problem. They thought that this was actually quite a good way of operating. They built these amazingly huge, uh, elastic search clusters and then open search clusters, which are great for sorting through tons of information, but when you're trying to manually read all of that information, you're going to miss things. We also had a challenge around how do we deliver meaningful change to that culture. Um, our Anglo-Dutch engineering organization had a strong sense of its own identity and was very proud, quite rightly, of what they had built out in terms of the technology platform. And what that meant was people like me couldn't wander in and say, oh, this is rubbish. I would have lost all credibility by doing that, it would have been interpreted as top down. So what we needed to do was take our engineers on a journey. Something that we've jokingly started referring to as the to the 12 step program of observability. First of all, we needed to help them understand they had a problem. It wasn't just me wandering along saying that this is rubbish. Each engineer needed to look at the situation they had and say this is something that we can make better. And then we had to help them become part of the solution. So, let's talk about how we got there. First thing we did was we built out a small working group of 5 influential engineers across the organization who were strong advocates that we did not have a problem, the actual people we wanted to persuade. We asked them to start looking across the market, understanding what industry text, industry research looked like, doing some internal surveys and looking at reports like Gartner and Forrester to, to understand what the modern world of observability looked like. And then we asked them to translate all of that data into a maturity model. This maturity rather handily aligns quite nicely with Dinotrace's maturity model. It allowed our teams to grade where they were in observability, but with a particular storyo flavor to it. It was quite quick to identify that we were operating around the level 1, level 2 position, that monitoring to base level observability space, when in fact our desire was to be way up to the level 3, if not the level 4 place in that causal observability and proactive observability space. It also meant that by building this piece out, we had a framework, we had a direction, we knew we wanted to get to that level 4 position, but we knew that we were going to have to go through level 2 and level 3 to get there. So we suddenly had a clear direction to that North Star. This brings me on to our technology solve. So I mentioned we were shipping 1.1 billion logs a day. That was largely coming from a containerized workload. Um, we use ECS as our container orchestration platform. It's a fairly straightforward solve when you're looking at shipping logs from that solution. AWS provides a product called Fire Lens, which is a fantastic sidecar to your ECS stacks, um, it can simply take your existing logs and it outputs them using fluent B as a protocol, that can be ingested directly by an active gate, which is Dina Trace's technology for routing metrics, logs and other pieces into their ecosystem. And then from there it can be fed into Grail, which is Dina Tracey's unified data data lake house. Try saying that after a drink or two. We also had quite a lot of workload in lambda, about 40% of our workload lives in the lambda ecosystem. Again, quite a straightforward solve. Cloudwatch log groups can be subscribed into a Kinesis fire hose, and then that can be taken off to open pipeline, again a Dynatrace technology for pulling material in and doing transforms on it. That let us do some pre-processing and then load that into rail. So we've now got all of our logs from our ECS workload and all of our logs from our lander workload flowing nicely into Dynotrace. Last step was our more legacy ECT workload. We already had an in-place hotel implementation for that, and again that was a very straightforward solve. We were able to ship all of that hotel data, logs and the traces that we have present over into DynaTrace, directly into DynaTrace's API. And now we've got all of our 1.1 billion logs in one singular clear platform. Simple, right? This brings us on to pillar number 2. Once we got there, we started our putting our logs into context and we immediately started seeing value from the Dynotrace platform. We were suddenly seeing events happening on our platform being correlated together in an auto magic way. Brilliant if you bear in mind past pastly we've been doing that using humans and saying, well this log and this log over here correlate. We're getting that correlation out of the box and we're beginning to understand what's going on. But it also exposed to us that we were often using the wrong signal bearer, in this particular case, logs, in situations where something like another signal bearer would be a better fit. A great example is trying to piece together elements from a set of calls using logs, when in fact in reality, you may be able to display that better using tracers. Once we had those logs in context, we could start saying, right, these are the places in our ecosystem where this signal does not fit, let's replace it with a different signal, let's replace this one with some traces, let's replace these ones with some metrics. And we started to get a richer experience as well as simultaneously decreasing our log volume volume. And we started dropping that 1.1 billion down into the several 100 millions and so on downwards. We also started being able to visualize for the first time in a visual way, these connections between our distributed services. So no longer was each individual distributed service an isolated little monolith sat there doing its own thing. We could see those calls propagating from service to service. This set ourselves up with massive success, we had this foundation of logs. But we knew that we would have to build on it, and we consciously went out and made sure that that foundation of logs was not our end state. For us, this meant that. Data unification as we like to call it, moving all of our data into one place and then picking out the right signals was a non-negotiable. If we wanted to meaningfully decrease our MTTD and our MTTR we had to go on this journey. Let's talk about AI. So as we heard earlier, AI is a core part of the Dynacos platform. Even with our signals now getting into the millions rather than the billions territory. And with all of our noise filtered out and using the correct bearers, this would have still been an impossible task to look at manually using humans. We'd known that from day one, we thought it would be a Sisyphean task, we would be pushing that rock up a hill every day as we worked through our peak trading period. We knew that for any system at scale to work, we needed automation, and that's what the combination of predictive and causeal AI that is present within Dino Trace, the Davis ecosystem, brought to us. We started to see predictive bringing in auto base lining, so we were no longer having to work out what our signals looked like from a normal position, set up manual thresholds, etc. etc. That just happened magically. We also saw causal AI starting to understand the interconnected web of services we have, and we've got quite a few microservices within our ecosystem, over 200. Manually building out a web of how all these services talk to one another would have been an impossible task, but the automated nature of Dyna Trace and Davis operating started connecting all of these services together fantastically. We made a conscious decision to stop manually setting up thresholds and alerts and everything else and instead focus on making the best out of Davis and the problem cards we could do, because we knew that that would be the most impactful thing in our MTTD. To give you an example of this, yesterday at around about 8 p.m. European time, we had what would have been 3 severity one incidents start to emerge on our platform. Things we've not considered. We had no As a result of having not considered them, no ability to have instrumented and consciously set up thresholds for checking on these things, so we could have been proactive about it. But Davis was able to spot them coming, start raising performance challenges around a set of services, and we averted 3 individual incidents. Each one of those incidents alone would have been a €1.5 million outage, so in one single fell swoop. The Dynatrace platform and Davis identified over €4.5 million worth of savings to us effectively. I'm gonna talk you through a scenario we had a little while ago that also brings some light to this. Um, As you may know, in Europe, each country has different territories and different VAT rates that apply to it, in particular when you're doing territory to territory transactions. We store all of that data in a giant Dynamo DB table, not the most elegant of solutions, but we had to accept imperfections as we went on our platform journey. Um, There is also no update method nor any validation in place for that particular Dynamo TV table. Again, hold my hands up, not best engineering practice. One of our product managers was meaningfully applying some updates to our tax rates, but unfortunately, a bit of finger trouble crept in and she managed to malform one of the tax rates for our German market. Davis immediately started detecting an increase in failures of course to that dynamo Dynamo DB table and was even able to identify the lines in that dynamo DB table that were causing the failure, um. We'd just been through a fairly substantial reorganization and the team who actually owned that service had only been owning it for the spell of 2 weeks. But they were guided exactly to where the problem lay. They understood very immediately it was related to this Dynamo DB table and a particular change that had happened within the last 20 minutes, and they were able to roll back using point in time recovery to a period before that change had been made and restore service within 20 minutes. Bear in mind we'd done nothing to build out alerting for this particular scenario, so this was all automation, firing off the back and building out the meaningful impact that we needed to see to prevent any impact happening to our customers. We were able to solve this problem within 20 minutes and we were really, really glad of the investment that we'd made in this space. There were challenges though, we didn't always have some smooth sailing. We realized that our existing hotel implementation combined with Dynatrace left some things to be desired, in particular around the front end to back end space. That mixed hotel and native ecosystem is a super hard problem to solve for. We also found that some of our engineers who were still in that old school mindset of wanting to look at logs and nothing else, really struggled with our new mindset. To this day I have 2 engineers who have differing views on the value of logs versus other signals in our ecosystem. Each is right and each is valid, but as we look to move things forward, the opportunity to use other signals is one that I think is really, really valuable. And lastly, we needed a major time bound event to really bring our organization aligned around that single goal. For us, it's our peak trading period, this run up to the Black Friday weekend and then the run up to the holiday period. It laser focused everybody in on making a meaningful change, and we've seen a massive acceleration over these last couple of months since I started writing these slides to getting to a point of having massively great observability coverage. So let me talk about some of our key takeaways. First of all, our change had to be culture first, code second. Leading with the culture piece is what has enabled us to be powerful in our change. If we'd have tried to introduce a technology solution or a process-driven solution first, we would have failed on this journey. We had to change our culture before we could meaningfully make improvement. We also had to embrace the idea of using other signals. If we only stuck with our logs world, we would be. Having a better experience, but we would be missing so much value that a modern observability platform provides. And that really when you get to a certain scale, you need to use AI. You can no longer rely on the humans in your organization to do the great job that they have done because there will be so much data, even with the best will in the world, they will not be able to keep up. AI is essential to enable engineering productivity. You do not want your engineers spending their time reading logs, you want them there writing code, and for that you need AI to help you with that problem of analysis. OK, I think I am handing back over to John now. Thank you Alex. So hopefully we all found that very interesting, it was a very good story and obviously some very good examples about how log analytics and the wider sort of observability ecosystem can empower things if done in the right way. Now What we wanna sort of start delving into and looking at a little bit more here, so I just realized there's a squeaky floorboard here. um, is the traditional log management side of things really needs reinvention. It doesn't work in the the the way that it needs to nowadays. Things are scaling too much, the ecosystem getting too complex. And the more and more complex your ecosystems get, the more data they emit. And obviously Alex touched on, on various of these points here, but you see some of them with things like inflated costs. You're having that incredible amount of technical debt with these different traditional log management tooling that's out there, not just in terms of things like licensing, but more in terms of things like management and maintenance. Whether that's off the platforms themselves or whether that's the tens of thousands of dashboards that you're potentially setting up and looking at, and obviously the thousands of alerts that effectively create the, the alert storms that that you were talking about there as well. Now With all of that, it then comes onto the that correlation, that manual correlation that you're having to do to connect these different siloed signals from different, different tools to bring those things together, to try and work out where that root cause is. And again, that of course has that knock-on impact to a worse meantime to resolution. And overall worst collaboration that's taken place between the different teams because everybody has their own source of truth. And finally, of course, that brings you onto the, the high complexity side. Because overall, the traditional log management tooling obviously has been around for a very long space of time. It is built in reality for the past. It doesn't scale in the way that is needed for the current world, with microservices, with AI agents and so on, where it is dynamic, it is massively scalable, and it is cloud native. Because with all of those, you need to understand how they relate. You need to understand all the different signals, and unfortunately, logs by itself isn't effective. Logs is part of a much wider piece with all of the other signals is where things can be most effectively used. So when you bring those things together, you can start to use the appropriate signals for their appropriate means and utilize them all in the right way. So with Dyne Trace, what we're trying to do is help you scale that cost effectively. To unify and contextualize all that data, whether that's the log data with the topology, with the metrics, the events, you know again whether that's the security side of things as well, bringing everything together so you can understand that all in a single place. But more importantly, to enable that proactiveness, enable AI driven decisions to be made and automation to take place. So, really, what we're looking at here is that We all know that the applications, your cloud environments and so on are are getting bigger and more more complex. But with that explosion of data, with the petabytes of logs, metrics, traces, and so on. It then starts to be a question of what can you do with it, where is the power with that? And with automation and AI it's really trying to utilize that data, not as effectively noise or or or or a burden of you having to work on all of that to understand what's going wrong and so on, but actually trying to drive automation around it. Actually trying to drive preventative operations. Empower your teams with the automation on the back of that, so that really you can turn your data, your petabytes and petabytes of information, really into a business advantage, a competitive advantage. And so When we're trying to unify this data across the board, unify the logs with with the rest of the observability signals, with Dyne Trace we obviously try and make that as simple and as automated as possible. One of our primary ways of course is our Dynatrace one agent which will automatically detect the logs that your say operating systems and applications and so on are are writing out, be able to automatically ingest those. But of course it's not just places you've got one agents, maybe you've got open telemetry and you've got your open telemetry logs coming in. Maybe you've got log shippers like fluent Bit and fluent D and so on. And bringing all that information in, in whichever way is relevant. And of course the core one here as well being, you know, that direct integration with AWS, you know, getting all that that log information from AWS through the fire hose. So again you can understand and bring the data in no matter where it's sitting. And again, between a combination of your cloud estates and your on-premise, so really you are getting that unified view. Now what we're then doing is we're effectively storing that in our central storage, which we call Grail. So that's a effectively a specifically built for observability data lake house. And this does allow you to ingest over a petabyte of information every single day into your environments. And have that full power. Now. What you're able to do with this, as I said, is ensure that all data is stored in there. You know, whether that's your logs, metrics, traces, real user monitoring, whatever it may be, all stored in context of one another. Whether it's your log records and your traces, we know exactly which trace has written which log record. So you're not having to look through log files and just look at endless log files not knowing what connects to what. You know exactly which log record was written by which trace, which was connected to which user in their browser or mobile app. And we're trying to do this, as I said, in the most automated way possible. And giving you as much access as possible as well. So there's no data storage tiers for you to manage. There's no hot and cold storage, there's no rehydration of data. You can pause literally the data on read. So there's no there's no semantics that you have to previously set up. There's no indexing and so on and so forth that you have to set up beforehand. You can pause that on read. And what that ends up meaning. Is that no matter your question, no matter what it is, no matter whether you've thought of it beforehand, you can always ask that question. You don't have to rehydrate, rehydrate data, you don't have to index data in appropriate ways to ask those queries. You can ask whatever you want because you can pause it on read in near real time, so really. Have responsive query, even when you are looking at your terabytes or or petabytes of information. And as you can see here, you know, we are able to do that at that low ingest cost and retention. And still be able to retain data up to 10 years. All that types of data, you can retain up to 10 years. So whether that's, you know, for regulatory standpoints, you know, with things like obviously some of the AI regulations that have been coming out across the world, and you know, obviously some of the financial services, especially regulations, there is a lot of areas where things like your audit logs or, or, you know, your generative AI connections of your inputs and outputs and so on. Understanding those and storing those may be needed for, for that space of time, and again, may be quite effective and useful. But what are we doing with all that data? You know, once it's there, it, it's not just about being able to view it and show it on dashboards. Really this is where the power of the AI really comes in. So what we're looking at here is AI is built into Dyne Trace at its core and has been from the beginning. And we are automatically mapping out all of that topology information, we're automatically mapping out all your different services and dependencies and everything together. And all of those telemetry signals are all mapped against those topology components as well. Now what this means is Dintrace is then able to, on top of all of that, automatically baseline and dynamically baseline as well. Full understanding to automatically detect problems that are taking place. Because at the end of the day, you don't know what you don't know. So you can't be setting up or needing to set up alerts for every single thing that could possibly go wrong. You need to just be told when something goes wrong. And then on top of that, because of that accuracy of the data, because of that contextualization, that domain level insights, we're also able to provide precise root cause analysis. So effectively doing faultre style analysis so that you are able to get accurate root cause analysis rather than effectively trying to correlate different events together. Now what this means is you're not getting the alert storms, you're getting the full insights you need, you're getting straight to the pinpoint to be able to resolve those things. But you're also able to prioritize these on business impact. You know, how many users are impacted? How many business flows or, you know, critical business services are actually impacted by this problem. And because you've got accurate root cause, it also means you can do self-healing. The issue with self-healing on as a general correlation is always that with the inherent false positives that come in, if you try and automate or remediate on a false positive, you're going to make the situation a lot worse. Most of the time, you'll scale out your AWS instances and give ADWS a lot of money to uh to to pay for that. But. Overall, it could be a matter that you're restarting services that didn't need restarting, it could mean all sorts of different things where at the end of the day, your customers are getting a worse scenario and probably your costs are also increasing too. If you know exactly what went wrong, you can fix the root cause rather than trying to fix the various symptoms that often end up bubbling up in these alert storms. Um, and as you'll see through through these as well. There is an example here of a problem taking place where there is a failure rate increase. Now with this. We are able to have the Davis AI, the generative AI part, the Davis co-pilot, actually explain to you what happened as part of that problem. So it's not just a matter of giving you all this technical information and you working out what it is, you're actually getting a summary of that problem. What happened, what, what took place, why did it take place? And an actionable set of steps to actually go ahead and remediate that, some suggestions on how to remediate that. Now what that means of course is that you don't have to go and make those reports quickly when the, you know, when a major incident happens, you know, you've got that natural language interpretation that can easily be put out so that management and whoever else is aware of what's going on. It also allows you to obviously focus on the right areas in the right places to to delve into that. Now In this case, it was a recent deployment, um. And so the suggestions here is basically looking at reviewing the error logs and and and various things like that. But what it's then able to do with that is because we've got that direct access to the logs, we know exactly which logs are relevant to that problem. You know, it's not, here's every log that's relevant to the servers, it is exactly the logs that are relevant to this problem, the. The logs that were specifically part of that trace. Cos the thing that you'll often have is you may look at a log file that you know is relevant, lots of error messages in there. But which ones are actually relevant to the transactions that are failing, that are actually having an issue. Cos error logs happen all the time. And quite often you can get that mismatching of there are error logs, I'm assuming that they're gonna be relevant to the problem that I'm looking at. And then many times it's, it's not, and it leads you down the wrong sort of rabbit hole as such. Now what we can effectively do with that though is. As I said, you've got the visibility into all of the logs that are taking place as part of that problem. And we can start to also explain those logs as well. So again, having DynoTrace automatically tell you what that log record actually means, you know. I'm sure all of you have seen log lines before. They vary massively, some of them are the messiest things possible, um. But it is able to extract out all of the relevant information that's useful, break it all down for you, so you haven't had to pre-pause it and preconfigure it and whatever else it may be. You're able to be told exactly what the key information is. What the possible root cause is for that. And of course, you know, any actionable steps to to remediate the specifics of that, because this is where logs are really good. They're good at providing context. They're not very good from an observability point of view of creating alerts and creating problems, because there is so much noise in logs. But they are very good at providing context and providing you good insights to actually go ahead and start remediation. And finally on on this piece as well, the other thing that Dyne Trace is doing is we're also providing this idea of troubleshooting guides. So you can go ahead and you can create a troubleshooting guide based on that problem. Obviously you do all your inputs and your queries and whatever else it may be. But the benefit of what's taking place on this is that when you have a problem that occurs in the future that is similar. Dying Tricks will automatically recommend the previous troubleshooting guide that you created. Because it's looking at the similarity of those problems and basically providing you with the previous troubleshooting guide so you're not having to repeat those steps. Again, it's really, as I said, trying to maximize automation wherever possible, trying to reduce that manual effort, that time it takes you to resolve incidents and so you can get to the root cause and resolve it as quickly as possible. But it's not just about fighting those fires. You also want to look at, of course, predicting and preventing those fires from taking place in the first place. So when we look at DyneTrace's predictive AI it can forecast on any time series data at all that DyneTrace has. That that includes logs. So you can literally do prediction based on your log records directly. Now. What this means is you can start to understand if things are trending in the wrong direction. And you can start to fix issues before they actually have any impact to your end users. Now. We can effectively start to drive automation based on that from from the various logs and events and so on. So if Drace detects, in this case that there's S3 buckets that are open to the public internet, we can of course automatically remediate that. You know, we can use AWS's Action APIs effectively to auto remediate and change those S3 buckets so that you can prevent that sensitive data before it ever gets leaked. And We can also do things like proactively scaling, you know, whether you're scaling up to increase capacity of your disk, your memory, your CPU, whatever it may be, or if the logs are in in uh, indicating that there's a business specific problem or or or errors that are taking place due to maybe it's resource saturation or or something else. We can start to again. Or to remediate, or to optimize and or to improve what is taking place as part of this and overall drive, this automation and predictive insights to really, Provide the best for you, to prevent those fires from taking place in the first place. And Overall, what we're getting to here is that obviously logs quite often are very technical, utilized for a lot of different things, but one of the things we are seeing a lot more power and a lot more benefit with logs is because they have so much rich context and so much rich information, they're quite often powerful from a business data perspective. And. The ability to have that information directly connected to your IT information is, is also very powerful. cos of course it means that you can start to understand like Alex was talking about earlier, about how you can understand the revenue impact of these incidents or or problems that are taking place. You can start to understand in real time that actual visibility, to understand your entire end to end processes, you know, your end to end journeys that customers go through and where that impact is taking place, where you're having these significant issues and so on. And This has all been provided because, as I said, all this information is being topology mapped, all this information is being stored in context so that everything is relatable and everything is understood within its domain. Now Overall We're also obviously trying to really drive towards that proactive AWS operations. So. As mentioned earlier, we've got that direct AWS integration which allows us to easily capture all of the key AWS telemetry information. Now we're also able to leverage all of those cloud tags. So bringing in your actual cloud tagging strategy directly into the Dyne Trace platform so that you're not having to recreate, you're not having to build new tagging strategies and and provide additional complexity. We can inherit the tagging strategy you've already got in your cloud and bring it directly into Dyne Trace. So again. Whether you want to go and look at a log or a trace or a metric, we're literally pushing those um cloud tags to the actual signals that are actually taking place, so again you can query, filter, do whatever you want with that information. And again, Our AI can obviously utilize all that information as well, so that we can ensure that you've got that deep insights into your AWS estate for obviously these key three areas. So whether that is the sort of auto prevention side of things. So whether that's things like capacity planning and forecasting of of the events. Whether it's things like auto remediation, you know, with the root cause analysis and obviously. Putting out those fires before they hopefully impact any of your users, or whether it's auto ti optimization, where, you know, we are looking at resource optimization, we are looking at identifying misconfigurations and improvements that can be made. And I think it's always interesting to see, especially when you start to get down the route of the AI and the auto optimization side, is where, you know, the human oversight comes into these things. Because. A lot of the time, things like auto remediation aren't too difficult, you know, whether it's scaling things out, spinning up boxes, change configuration or whatever it may be. But when you start to get around optimization, you have to have goals, you have to have insights provided by a human. Cos of course if you just basically give an AI I want to make this as performant as possible, it's gonna cost a lot of money cos it's gonna obviously scale out everything. But if you're gonna, you know. Obviously go the other way and go oh I want to make it as secure as possible, it'll probably just switch it all off. So again, you know, you need to give it the realms, the goals, the boundaries that are associated with that, and optimization is that balance. And so that's always, I find a quite an interesting one to look at from an AI perspective. So Sort of finish off and summarize, really Dyne Trace is trying to help you scale your log management cost effectively. And unify and contextualize all of that information, all of the log information with all of the other signals that exist. And really to enable that proactive and AI driven insights for you to take place. Now, not only does this help reduce your mean time to resolution, but it also helps you track end to end business journeys. And obviously make those data-driven business decisions that we were talking about before as well. And obviously with AWRS we can proactively um help with the AWS operations to do the auto remediation, the auto prevention, and auto optimization. So I'm gonna invite Frank and Alex back up on stage. We're gonna have a quick sort of Q&A sort of style of things to finish this off. Awesome if you wanna sit down, I'm gonna do rapid fire because we only have a couple of minutes left. Um, awesome. Alright, um, I always like this kind of Q&A set up because it's a little bit more, you know, less, less structured, and, um, if there's any questions that you really need to be answered, you can scream them in but we unfortunately we don't have a mic to go around, but I have a couple here that I wanted to clarify, um, and the first one is on, on culture. Um, that was a big thing of one of your 33 pillars, and I just wanted to know, like, have you ever, like how did you actually manage that because culture is a, is a beautiful thing, but it's also really difficult to impact, uh, and make changes on and like how did you manage that? Transition. So for us, I think first of all it was identifying the right set of people who who could be our culture carriers across the organization and work out how we could meaningfully engage them in the process of looking at the observability problem and working out how we could move forward. And then it was a question of how do we meaningfully carry that message out to the rest of our organization, um. We did that through things like Diner Days, through support, through, through AWS running game days in combination with Dino Trace, um, and through sharing internal documentation, things like this maturity model that you saw earlier. It's hard though. Culture eats strategy for breakfast. As much as you want to build out strategies that drive observability and other pieces, unless you pay attention to that cultural piece, you will fail. How long did it take you? Oh, it was a good yearlong process, to be honest. Yeah. And did Dina Trace somehow contribute to making it a little easier? massively. So I mentioned Diner Day, this is a day that hopefully you're not all about to, to ask Dina Trace to provide, but I. They will be in certain cases where some of the Dynatrace account team come out, spend some time with your engineers, work out, uh, what their challenges are, work out what the levers are to pull, and help them understand what great can look like from an observability position. Things like the Dynatrace AWS Game Day, incredibly powerful. Running a game day is a huge thing. It's great fun, it's really engaging for our engineering community, and to work with two partners like Dynatrace and AWS, absolutely fantastic. Nice and um, are you, are you running a lot of these or are you quite, quite a few to be fair, um, I mean it, it does vary customer by customer. I think every customer is different and where the cultural changes are needed also vary as well. I think overall. A lot of it is about I guess enablement and involvement, um, so the more that people can be involved, the more people can be enabled, the, the easier that is because obviously Alex talked about through the presentation things like the return on investment was easy and and sort of insane how, how, how much could could be sort of returned very quickly. But it's all about, you know, how much is it of a benefit to that individual. And so how do we bring them in. Sometimes it is through the AWS game days and things like that where sometimes it is to an extent a more playful, fun thing to do. Um, to, to bring that in, but they all have a level of, uh, enablement sort of as part of that, yeah, and I think there's always so many little pockets of knowledge that how you can actually be enabled and everybody that's in the audience is very much leaned in and you're here and you're, you know, but how this is being translated to the larger organizations that that are behind you, I think that's always nice to kind of get the best practices from. I think that's exactly why we started with detractors, people who didn't. Believe that they had an observability problem because they were the most powerful group to convince. Once you get those people aligned, you can, you can bring the rest of the organization along. Um, those people who are enthusiastic, who are, uh, promoters of observability quite often are not the people you need to persuade to change. Yeah. Um, switching a little bit, um, 5 minutes. I wanna talk a little bit about like mixed environments. You have, of course, a lot of signals coming from hotel collectors, from the one agent from different platforms. How did you, how did you manage that because no one, no, no one comes to the table being just, you know, vanilla, and then there's a lot of complexity to it. No, absolutely, um, I spent a lot of time talking about our previous logs piece. We also had a previous observability platform provided by another vendor. And we'd gone down a strongly hotel opinionated route with that implementation. Um, hotel was spread out across all of our back-end services. Um, conversely, our front-end services had no form of observability coverage aside from the logging piece. What we really wanted was a full end to end experience, and what we learned on our journey is that with everything there is a compromise, um. Our strongly hotel opinionated pattern was great for providing a standardized interface across all of our backend services, but when we started trying to connect front-end user actions through to the back-end ecosystem, Staying pure hotel in that back end ecosystem, we were losing as much as we were gaining. So for us, we have just been through our first peak period on Dynotrace. We know we have more work to do, and I think that work will be moving towards a position where at least our first layer of of services behind our front ends are natively implemented with Dynorose one agent because we believe although that will compromise our singular hotel everywhere vision. We will get a better experience and more value from the Diner Trace platform. Yeah. And speaking of distributed or, um, you know, tool sprawl have a lot of different, different ones. How do you like, I would assume that everybody in the audience uses probably more than one observability, uh, provider. Um, how do you actually make that, you know, unified or unifying observability possible maybe for you, John. Yeah, I mean, as you said, most companies do have a lot of observability tools and and vendors out there. Um, it does range across the board. I think a lot of the time it is that that focus on, on your initial areas or, or initial places of focus where you can get the most amount of value. And then expanding from there, cos I think what you end up seeing a lot of is that it is, it's not just the, I say the, the quality of each, each vendor or each tool that that's, that's necessarily the issue. Quite often it is the siloed nature of that data which is an even bigger issue. The fact that each team has their own tool, their own source of truth, means that you end up with this whole finger pointing and war rooms that are, are taking place. And, you know, bringing that together is, is, is one key part of that, I think. And so even if that information is coming from different areas, that's why I'd say a lot of the time if you can focus on the applications, the teams where you can get the most amount of value quickly, it can then expand and sort of grow very quickly from that because you can then start to bring in the various different tools from various different teams and and grow from that rather than taking one layer like, like logs as uh as an example across the board, um, of course that can be value from a. Let's say from a licensing replacement perspective and you know cost savings on that front, but of course if you can. Let's say take everything within a particular team, you know, all the different telemetry data together, you will get significantly more value than taking one, data type across the board. Yeah, and I mean, I, I see this from my perspective as well, like the number of customers that come to AWS and say we have so many different vendors. How do we figure this out? Can you help us consolidate of one? Who, which one do you recommend? Like that's a really kind of top of mind for a lot of customers. Um, What about automation? Um, there's a lot of capability that DynaTrece brings to the table. Were, were you able to sort of walk the whole nine yards with automated and automated, um, workflows? It's a journey we've certainly started, um, so when I talked through our technology stack, I failed to mention the elephant in the room. We do have a little bit of kinetos. Um, we use cuinetos specifically for our AI workloads, and, um, this is more traditional AI where we're using our own models and we're running on our own EKS clusters, um, the actual, uh, computation around those models. Um, we have a set of challenges around that. Um, we scale up and down quite quickly as an organization, um, and when you're running on GPU bound hardware, um, we were finding that scaling up and down with the base set of Kuberne's metrics was not effective. The boot up time was just too long. Uh, EKS has a fantastic feature called warm pools which allows you to solve this problem. But equally, we didn't want a load of spare G5 instances sat around twiddling their thumbs because they're not the cheapest thing in the world, um. Dynatrace helped us solve this by predictively saying we can see pressure on your Nvidia drivers is looking like this, you're, you're getting to the point where you're going to need to scale soon. I will add one node into the warm pool and that can then scale, be elegantly scaled up onto without a long delay. Got it. Anything Too bad. I mean, we're basically out of time, um, but yeah, I mean, overall. There obviously is a lot, a lot of potential there, and I think it is, you know, as with Alex and the Storyo Group, it is quite often that staged approach. You do often start with certain scenarios, certain areas where you can see easy value and then grow it out over time as, as things evolve and as of course everybody get, gets behind it and gets involved in that and, you know, then automates and gets a lot more value. Awesome. There was a high you were emphatically raising your hand if you can scream really loudly.