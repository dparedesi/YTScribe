---
video_id: oMVRvg_8m_I
video_url: https://www.youtube.com/watch?v=oMVRvg_8m_I
is_generated: False
is_translatable: True
---

Uh, thanks for starting off your Monday with some dynamo. I'm Jason Hunter. I am a principal solution architect with a specialty in Dynamo DB. So my job is to try to know Dynamo thoroughly. And uh oh that's fun. I got the transcript uh try to know Dynamo thoroughly and if you are using Dynamo and you get stuck, you can talk to me or one of my peers and we try to help you out, you know, if you need a cost optimization, we'll look at your tables. If you say, you know, I'm, I'm having a big launch coming, am I gonna be ready? Is it gonna handle the traffic? That's what I do. Today I'm going to talk about data modeling and some of the techniques. This is a 300 level, so I assume maybe some familiarity with Dynamo but not extensive. And so one of the hardest parts of doing a talk like this is to figure out how to level set. How are we gonna start? Do I assume that you're brand new? Do I assume that you're here pretty familiar and wanna level up? And I kind of have to assume both. So what I'm gonna do is I'm gonna start with, uh, an analogy of how Dynamo is kind of like phone books in a library, and if you're new to Dynamo, this should help. And if you're an old hand, it might be fun to see just sort of an analogy of another way to think about Dynamo's data modeling. And then we'll talk about some core modeling concepts using an app that we're gonna build together and then at the end, what do you do when you need to scale bigger because it's pretty easy to write a low scale app on Dynamo, but when you get big, there's a couple of techniques you wanna know about and if there's questions, we'll do them at the end, right? So let's start with a day in the library, and this is an Alex Debris analogy originally, and so I'm kind of stealing it and expanding on it. He said Dynamo's a lot like phone books. Does anyone remember what phone books are? Are we old enough here? Yeah, a few of us? Before your cellphone knew everybody, before you could Google a name, you had these actual physical books. And on the spine was a city. And so you'd say like, alright, I'm gonna be in Denver, and you'd open up the Denver book, and in there would be a sorted list of the people in Denver. And yes, we all shared our numbers with everybody. And so you'd find they're usually sorted by last name, and you could then find by the last name and by the first name, and that's kind of a lot like what a base table is in Dynamo DB. Where the partition key, the thing that you use first to kind of find your zone, your item collection we call it in Dynamo, is the city. And that's the partition key, the PK that we call it by short. And so here we're in Las Vegas, so we're looking at the Las Vegas book. Inside of there is a sort key which is sorted by last name or maybe with a company, just their regular name and sorted alphabetically. There's a payload with each item, a phone number, a zip, maybe an entity type, you name it. This is a lot like what the dynamo base table is because there's a partition key and a sort key, and you specify the partition key, and then you specify the sort key to find an item. But what kind of queries can we do with this? Well, given a PKSK combo, I can get the payload. I can find people whose name starts with Hunter. I can find ones who start with H. Can I find Jason's? No. And so if someone is on stack overflow asking, I'd like to do my sort key but I wanna do ends with, what's the answer to that? There's two answers to that really. One is no, the other one is, well, if you reversed your sort key, then you could do it. Right? And sometimes in Dynamo you do that kind of thing like if you really want ends with and if you had another copy of it reversed, then you could do it. Uh, can I look up by phone number? Not with this data model by zip, no, by entity type, no. There's some things you can do, there's some things you can't do, and that's why Dynamo is so efficient because you go to the, when you issue your career, you go to the book, you find the sorted list, and then you either find the item or the starts with or the between or the range, you know, and that's how you find it. But if you're thinking that that's a limiting. Uh, set of queries you can do, that's why we have indexes. One type is called the local secondary index, in LSI. This is where you say, I want to have, uh, the same partition key but maybe a different sort key. And in phone books, that's what the yellow pages were. So for those of us old enough to remember phone books, in the back, the pages were yellow, that's why they called it yellow pages, and that was sorted by like plumbers and uh appliance repairmen or whatever that is. And so here I'm doing a sort key of casino or hotel, and now I've expanded my ability to query because I can go say, hey, find me casinos in Vegas. It's not hard, or hotels also not hard. So this is one way to think about what the base table is and that an LSI is an automatically updated thing when you update the one, it kind of propagates into the other one. It gives you another way to query it. Uh, what can we do and can't we do? I still can't look up by name or, or phone number or or zip here. I could do the name against the base table. There's another thing called the global secondary index. Nothing to do with global tables, by the way, common. Naming issue we have there, uh, but in this case you say, you know, I might want a different partition key. And so instead of looking by city as the book, maybe I want by zip code as the book. And that would be a smaller book probably usually, and in there would be its own sorted list of names by that uh zip code if I have name as the sort key on my GSI. And so now I can say, well, I know he's in Denver, I know he's in the zip code, and I can pick which one I want to do as my access. Makes sense? And so what can we do? We can do now a different partition key and possibly a different sort key. I've chosen here to do the name as a sort key. What else could I do? I could do a phone number as a sort key. And then I could find a certain phone number and a zip code. What if I want to look at my phone number in general in the whole database? Then I would have a GSI with the phone number as the partition key. And that would have a different that'd be very little phone books. It'd be like kind of like a little book for every phone number in the world, but then going to the partition key, I don't even need a sort key. Sort key is optional in Dynamo. I could look up by phone number. So one copy of the data and then you use these indexes to pick the different ways that you want to access it. That's the basic way that Dynamo works, uh, always with a PK and an optional sort key. Right, how are we gonna wanna update these things? Uh, the easiest way to do it is we're gonna update just the base table, and we're gonna let the other ones propagate out. They're a little bit different between LSI's and GSI's. LSI's, because they're in the same phone book, you know, the white pages and the yellow pages together, that one's done strongly consistently. When you update the base table, the LSI is updated at the same time, because it went to the same place. GSIs are in a different kind of place. They're in like a whole separate books and a whole other set of shelves. That one is eventually consistent. People want strongly consistent. I look forward to the day when we get that. We don't have it today, uh, but if you update the base table, it will usually in a millisecond, but it, it can be longer, propagate over to the global secondary index, and then you only have to update in one place, and, uh, this is how it works. If we think about storing this stuff, I picture a library holding the books, and so, you know, I'm gonna have a library and on there are gonna be a whole bunch of phone books that I have to go find, but do I want just one library? What if there's a fire? What if one of them's down, right? What if there's road construction? It's hard to get to one of them. Well, let's keep 3 copies of the phone book and let's separate them by, you know, a meaningful distance, possibly in different availability zones. And then we'll make Dynamo into a regional service. So if even one availability zone is down, you've still got two other copies and life continues. It's great that you don't have to think about this. You don't have to think about subnets. You just talk to a public endpoint, and we've got 3 copies of your data across 3 different availability zones all the time. And how do we update if we have 3 copies? Well, amongst the three nodes, they will elect a leader, and, and it'll rotate, you're the leader, then you're the leader, they come to do a vote, and one leader gets all the updates. Now when you're doing dynamo calls, have you ever seen that you can do a strongly consistent or eventually consistent read? Strongly consistent says go to the leader. You know you always have the latest data. Eventually consistent says any of the three will do. And you've got a small chance of seeing something that has not had the full propagation. A right is durable as soon as it hits one. And the other one acknowledges as well, so you have to hit the 2. The third one, we don't wait for it, we acknowledge it after the 2. And so if you do an eventually consistent read, you might get that 3rd 1 that is not necessarily as caught up, but eventually consistent reads are half price and eventually consistent reads have 3 targets instead of 1, which increases availability. Makes sense? Right, and then this is where I think it really gets interesting because most people with with Dynamo who've been doing it for a couple of months understand all that, but let's understand how partitions work. I think there's a common misconception that a partition key makes its own partition and that's not how it works. Um, think of it that in the physical library we have to have our, our books on shelves. Right, and each of these shelves acts like a partition. They're only so big, so we have to figure out how many of them we want to do. How many shall we start with? 1 seems like not enough, 10 seems like quite a lot. Let's do 4. And why did I pick 4 here? A default on-demand table actually creates 4 partitions on the back end. We had to pick a number that was enough to handle a decent amount of traffic, but not so much as to be wasteful. You don't want to have more partitions than you need. So by default it does 4. If you've heard about warm throughput. That's where you can hint to a non-demand table you're gonna be a big table start out big, and that would increase the number of partitions on the back end of a of a table, and that's a really nice feature now with cloud formation you can in an on-demand table say start large. You say that you want a lot of reads and a lot of rights. Uh, I have seen enterprises that go live with an on-demand table and they didn't do warm throughput. And as a result, the first hour, they're sending more traffic than a four-partition table would want to handle. And so it splits, as I'll talk about here, but it takes a little bit of time to do the splitting. Alright, so first thing we have to do is figure out how do we assign data to shelves? How do we assign items to partitions? I think the obvious one is alphabetically, right? If you go to a library, you're probably gonna find the cities alphabetical. That's pretty good. But we don't do that. And why not? We don't do that because city names are not evenly spread. I'm from the Bay Area. Everything starts with S. It's a Santa this or a SA that, right? So if we did that and we didn't know about what our data was, all the data would kind of go to the same partition if it was alphabetical, right? Everything would end up on the, the last one here, Q through Z. So what else can we do? Well, if you want a nice even distribution, the easiest thing to do is hash the name, hash the partition key. Now you're gonna get nice distribution, even if the partition keys are very lexically similar, they'll hash all over the place, and now you're gonna get a nice distribution where about 1 quarter of items naturally end up on each of our 4 shelves. And that's why you'll see the partition key called the hash key internally, because we hash. I kinda, I kinda like the hash key name more because the partition key is what leads people to thinking that a partition key means one partition, but it, what it is is that we hash the partition key to help determine where it goes, but it alone does, you don't get a whole uh shelf for each book. Each shelf is only so big, so at some point we're gonna fill it up. I start with a table of 4 partitions, I keep loading data, at some point we're gonna say, alright, that's enough. In Dynamo, the enough is around 10 gigabytes. So if you've loaded enough, a partition gets around 10 gig, we're gonna start thinking, hey, maybe we should do something with that. What we do is we split it. Generally when it gets big, we split it in half right in the middle, and now we have two different shelves there, we split the 40 to 5A hex prefix in half with the 5B7F being the other half. And now we can add more space and so we, we, uh, do this all the time in the background, a table that's been around a long time is gonna have a lot of partitions because as data grows we're gonna be splitting it on the back end for you. Some edge cases, what if a city or, or maybe a zip, but like especially a city, what if it gets bigger than a shelf? What do I do there? The answer is exactly what you do in New York City. If you ever saw a New York City phone book, it wasn't a book, it was several books where they had split it by the last name. This is A through H or something, and so on and so forth. They split by the sort key. And we do the same thing. We split by the sort key too. If we see that it gets really big, we can, we can split that thing right in the middle of an item collection, we can split the partition. With a caveat, this is a pro-level caveat. We can't do it if the table has an LSI. Because remember the LSI, the idea is that I have the, the white pages and the yellow pages together in the same physical place, so I can't start splitting, uh, the white pages in three because now I don't have a good place for the yellow pages to go. This is why if you have an LSI, you can't have an item collection bigger than 10 gigabytes. Because of what's going on underneath the table, we, we are like, nope, an item collection has to be contiguous. A book has to be one book, uh, which is a downside of LSIs. The upside is the strongly consistent read nature. The downside is you, uh, can never get rid of them. Once they're on a table, you can't remove them, so they're, they're table creation, and also they limit how big an item collection can be. 10 gigabytes is a pretty big item collection. It doesn't hit a lot of people, but it is there as a rule. Alright, something else that happens in the physical world is that there's only so much traffic that can go to one of these shells. Each partition has hard limits. Do we know what they are in Dynamo? Each partition gets 3000 read capacity units, 1000 write capacity units. And that's, that's a hard limit there. At some point it gets a lot of traffic. Too many people are coming to write or too many people are coming to read. So what do we do about it? We split So we can split the books across shelves if we think it would help. So before and after we've, we've said this one book is so popular that we're just gonna split it into its own item, uh, partition. This item collection, because it's a book, an item collection gets its own shelf, and now that item collection can have up to 1000 writes and 3000 read units per second. What if everyone wants the same item in that book? We can actually split one item to its own partition. It goes down to that level. So you can in Dynamo, if you're gonna really bang on one item and you're gonna update it 1000 times a second, you can, you can update it, you can read it 3000 read units with eventually consistent reads, that would be 6000 calls, so you can read the same item 6000 times a second. Uh, because of how we split it. Now, you don't get it the 1st 2nd, but we will very quickly split when we see the traffic, um, split for heat we call it internally, and if you Google up split for heat dynamo, you'll find a blog that has me, uh, testing this under all kinds of circumstances to see what's, what happens under pressure, and you see really quick expansion on the order of minutes, right? So hopefully now if you're new to Dynamo, you kind of get a sense of how Dynamo thinks and why it's a key value store, but with a sort key. As well, not just simple key value and if you've seen Dynamo before, maybe that was a little fun trip to the library. So now let's think about some real data modeling. Let's make it real. So, first thing, a table has a partition key always, you get to pick the type, string number binary. We like string a lot because string is very flexible. Uh, if you wanna do it with an account number, you can. We're there for you, if you wanna do a binary, you can. Um, fine, but we kind of like string, and we sometimes like to mangle our strings to say what they are. Like a customer ID string, we might say cust ID 123, hash being a common separator. We do that because later on this table, I might wanna store something that's not just a customer ID. We a lot of times like to use the same table and put different kinds of things in there. And if I have a partition key that's a string that self-describes what it is. I have that choice. You don't always have to, um, but a lot of times we'll name our partition key PK and our sort key SK, and that way we're very flexible of what's coming. I, I actually think if I were to create Dynamo today I might say I'm not even gonna ask you for names and types. They're gonna be PK SK, they're gonna be strings at the end, and let's just like get rid of the whole idea of, of schemas and then you tell me. But you know, SKs are optional, so I guess you'd have to at least tell me that. Uh, you can't change a key value if it is an indexed item, a partition key or que. You have to delete it and reinsert it. You can change a payload attribute, but you can't change one of the indexed attributes. All right. So, what kind of partition keys do you see? A lot of times you see a descriptive one, like zip 89109, and this says, this is a zip code. I know that because I put a zip hash in front of it. Uh, sometimes, as we'll see later when we start charting because we get so much traffic, we want to support like more than 1000 rights a second, we might add a suffix at the end 0 1234, and that way I'll have more partition keys to write to to handle an influx of data that's coming really quickly, comes a little bit later, and sometimes we see multi-value. In multi-value it's like I have a zip code and a type together and you can do zip 89109 type casino it's up to you or you just do 89109# casino um it gets awkward. Has anyone seen the announcement from about a week ago? Where we now let you do multi-attribute partition keys and sort keys on GSIs, not on base tables yet, but on GSIs. So this whole man goal to have multi-values, you don't have to do it anymore for GSIs. Really nice. Why? Because when you do a GSI sometimes you're like, you know, I want a GSI look up and I wanna do where. Uh, it's in this zip code and it's of this type, and if you didn't previously on your base table have an attribute that was those two joined together, you now have to write to all your base table items to have something to project into the global secondary index. That's a lot of rights. It's a pain on you. Now you just say, I'd like a GSI. I'd like it to be synthetically created as if these were joined together, and we're like right away, boss, no base table rights, no ugliness in your base table, no increased storage in your base table. I love a good feature. So that's a very nice feature uh that we just introduced about a week ago. Too recent for it to be on the slides. Uh, when you see sort keys, sometimes you see them typed, like name, hash, hunter, whatever. We do that because a very common convention in Dynamo is to have a partition key be the thing. It's a customer. It's a device and then the sort key prefix is like the different things that you know about it like for uh online shopping I might have address and then I might have another one that's like your credit card and another one that's like your orders and another one's your shopping cart and the sort key prefix is like a description of what we know about this so it might be like order ID 1, order ID 2, order ID 3, and I'll remember the orders that this person previously did. We call the single table design because we do different entity types in the same table. And it's popular because if I wanna learn about you in one go, I can just go to the database and make one query call and bring all of it back really fast, cheaply, because one quick call and we price by the amount of data returned or at least scanned. So, uh. That's a common convention and we see the typing here, uh, when we're doing such things. Sometimes people put a time stamp. It's a pretty common sort key. If you have a device and you have different measurements at different times, we see the, the time stamp there and sometimes you see hierarchical. Like, uh, country, state, uh, location, and that way because I'm a sort key I can do a starts with, I can say starts with USA and I can read everything in the USA, USANV and I can read everything in Nevada, USANVLAS and I can read everything in Las Vegas. So that single sort key gives me 3 different access patterns by doing this. Again, this is gonna be nice with the uh new feature because it's a little bit more expressive than you having to think through, it's a string and I'm prefixing. Now you can just say where these 3 things are like this. Oh yeah. It's syntactic sugar and I got a sweet tooth. All right. So let's put it in action, a somewhat realistic example. So I have to dream up an example of something we all know. Here's my only AI in the talk. We're gonna design a schema which is a history of a chatbot. Uh, so you, you ask your questions, you get your answers, and I need to remember this history so that I can feed it back to the, the chatbot to remember the state when you come back, and things like that. So I'm gonna try to scale this big. I'm gonna have millions of users threaded conversations, and each conversation has its own specific metadata. All right. Doable. What are we gonna do? Here's some requirements. Uh, given a user ID, I wanna pull all the threads and thread metadata. Just give me everything about the user, I'm building the interface, I'm gonna populate the left sidebar with everything. Uh, given a user and a thread, pull just that thread. OK, so not everything, but just like I expand a thread and I wanna see what's in there, OK. Given a user ID, pull a recent thread, so I pull the last stuff. OK. It's always fun when you get requirements and you have to think of a schema. I think it's pretty obvious that the partition key is going to be a user ID because every one of these requirements is given a user ID. And one of the nice things of Dynamo is that you know, if I solve it for 1 user, and that user works well, I know I can solve it for 1 trillion users. Because of how each partition key can be isolated in the database to its own partition if I have to, and even smaller sometimes. I know that it's gonna scale up. That's the scale to any level. So you can often think about your world as did I do well for one partition key and the amount of reads and writes that it accepts, and if so, it'll scale to whatever I need, and you can sleep well at night without any surprises. Because there's no dependence on what one partition key is doing with another one if they've split to different partitions. So the PK is going to be a user ID. The SK is going to be thread metadata and thread messages. So this is a pretty common convention where you have one item in the item collection that is like the metadata about it, and then a bunch of other little payload ones, maybe by time stamp here with a create time, uh, which is going to be each message and the time of that message. Pick your thing, you can do IDs, but time is easy to model. And so given this, given a user ID, I can pull everything by just saying no sort key constraint, give it everything. I can do a, a sort key starts with a create date and thread ID and I can learn everything about that thread, as long as I know the date and the thread ID together. And I can also pull recent by just saying start at the bottom and sort backward by time. Maybe pulling in batches of 50 or 100 or whatever until I've read enough, and that'll get you the last ones because it says recent threads, it isn't specific, so I'm just gonna be able to go in the sort key starting at the bottom and go back up. Uh, NoSQL Workbench is a client-site application that we often use to visualize our data, and here is a screenshot from it. This is all for one user ID 12345. We see a sort key, which is the time stamp with, uh, yep, down to the seconds, and then we have thread IDs, user IDs, all that. Notice that the bottom item is different than the first two. It is meta, so that bottom one has different attributes. It is a metadata about this. And this is weird for relational people because they're like, wait, did you just switch attribute names in the middle of a table? I sure did. I can do that. It's no sequel, it's one of the things you can do. And so what's nice is I can for a given uh thread ID just pull all this stuff in one go, even though like one's metadata and one's the details. All right? So I've satisfied the requirements, but you know how the real world works, they give me more requirements. All right, what's the new requirement? Given the user ID, I just want the thread metadata. You just want the meta objects across all threads. Alright, that's harder, right? Like I, uh, with my current model, I would have to scan all the detailed messages to get the metas. I just want the metas. I want to pluck out the metas. What do I do? I have indexes. A lot of times when you have an issue about a different access pattern, you think I have indexes. So we make a GSI. We're gonna make a GSI that's sparse, sparse meaning not every item in the base table is going into the GSI. When you do sparse ones, it's kind of nice because you pay by the storage. You don't have much storage, and you pay by the rights, and you don't have many rights. So a sparse GSI is a very affordable construct to create. I'll have the same GSI partition key, but I'm gonna have to have a sort key. You know what, anything, anything that I put on the meta-item, which only exists on the meta-item, will be my sort key. If it exists, it goes into the GSI. If it doesn't exist, it's out of the GSI, and that's how we make a sparse GSI. You do it on an attribute that's not always there. And if I do this Then I have a nice uh GSI ready to query just for you. And so it looks kind of like this. This is two different users, 12345 and 67,890, and they're metadata items. When you project into the GSI, you can choose which attributes you want to project. You can project all of them. You at least have to do the keys, which is the partition key and sort key of the base table they get projected in. And so now I can read just the, the meta. It's kind of a nice optimization when you do this kind of stuff, you think how often do I wanna do this because I could brute force it by just reading the whole item collection, you know, but if you're gonna do this a lot, then I probably wanna spend the effort doing the rights, the right cost to save my read costs later. So a lot of read versus write trade-offs. Oh, but I said GSIs were eventually consistent, didn't I? What do we do about that? Well, one thing we can do is we can just say, yep, GSI is usually, you know, tens of milliseconds, a lot of times it's 1 millisecond, 10 milliseconds. Are we OK with that? I'm gonna get the meta and how fast is this really anyway, um, a lot of times we just do that. Sometimes you say, you know what, this is my time for an LSI. I'm gonna create an LSI because I need the strong and consistent nature of an LSI. You just gotta be careful, right, because it limits your item collection size. You can't delete an LSI after creation like you can a GSI. Um, another thing you can do is you could dual write. You could say, I'm gonna manually insert into a thread table and into the main table. And you can pick which one goes first because sometimes it's OK if it's like in the meta, but it's not in the main table, and if you read it, you're like, oh, it's all right, uh, it'll come in a second, let me try again if you're kind of doing this yourself, but you have the chance that your program crashes in between the two rights, and now you've left a dangling index entry. So another thing you can do is a transact right call. We have transactions and you just say insert these into both tables and it succeeds or fails as one unit. Transactions are twice the cost of non-transactions. So if you like that, how often are you gonna do it, you know? Do I wanna pay double? Do I want the GSI that's eventually consistent? It's your choice. Alright, so up to this point, this is a pretty standard dynamo stuff. I feel like using dynamo for lower scale stuff is like weightlifting with light weights. You can really do it with bad form and still succeed, you know. But now we're gonna lift the heavy stuff. We're gonna deadlift 500 pounds, like, all right, let's talk about form exactly. So, first thing, we were successful. This was great, reduced costs. You always get this call as a database person, right? You launched, it was good. Uh, you're doing things with scans, right? Scans is where you just scan the table to find what you want, um, just like a relational database full table scan. You can do it at small tables, it gets expensive for big tables. Alright, so let's see, what can we do? First thing is I noticed that user message and bot response, which are the big text fields, are just stored plain in there. And there's a cost with Dynamo for storing things. There's the, uh, the storage unit you pay by default in US East 125 cents per gigabyte month. There's the right units because rights are charged by the kilobytes written. There's the read units which are charged by the 4 kilobytes red. There's a point in time recovery. That's a kind of backup where you keep a right ahead log and it is proportional to the size of the table and there's backups which are costly proportional to the size of the table. So the smaller your data, the smaller your bill. So if you ever get a chance to reduce the, the size, you should. So what do we do? Well, the easiest thing, we compress them. We store the data as you hand it to us. If you want to hand it to us compressed, we'll store it for you in that way, and you'll, you'll pay less. You'll pay fewer rights. You'll pay less on all the storage aspects. So GZI and LZ4 are two ones. I did a little math, uh, fake data here, and LZ4 was, was a little superior. So let me point you that way if you're gonna pick between the two. But what's the pros and cons? The pro is way less space. The con is like I can't do a filter against the text in the database now because the database doesn't know what the text is. The database is blind. I, I hold a payload. I don't know what it means. And, but a lot of times you don't care. In this application, I don't think the database needed to be able to read that string. You weren't indexing it or anything. Maybe if you moved it to another system, but then the other system would have to do the decompression. And another thing that might not be as obvious is if you have a lot of little attributes model, model version, temperature, top P, top K, max tokens, all this stuff, sometimes if it's just payload, you're not gonna index on them, you're not gonna filter by them, you don't need the database to be aware of them, they just need to be handed to the client. You can make it into a JSON, a map in Dynamo. You can compress that stored as a binary in Dynamo. And now you've got a smaller set of data. It also serializes a little faster, I think. So you might not always think about that. You can also do just a string. Sometimes people have like a lot of little attributes. You can make one attribute with a string. It's a little tighter, uh, as far as it's internal representation. It's not a huge win, but it is a win again, if you don't need the database to be aware of what it is, a way to save money. Uh, another thing you see. How much does it cost to update an item in Dynamo? The answer is it's proportional to the size of the item. If you update one little value in a 6 kilobyte item, it costs 6 right units because it's the larger of the before or after. How much does it cost to do a delete? It costs the size of the item. So, one thing that you see people do sometimes is they have like a time stamp in our meta here, which is the last update time. And the meta is 6 kilobytes, in my hypothetical example. Huh, I'm doing a frequent update here. What I could do instead is I could break it into two items, a static and a dynamic portion. I could only do my updates in the dynamic. Now my rights are 1 instead of 6. Is that good or bad? I think it feels good. It's bad if you never do the update, because the initial right needed to do two items, and maybe the first one was like 5.8, which rounds to 6 1st, and then you also have to do the other one, so that's 7. So maybe the initial right is 7. And later on you get The 111. So I'd say it's a win so long as your update is often enough to map out that I'd rather pay 7 + 111 than 6666. And this is a, a common thing. One of the fun things is when you're doing video games and you have to keep a list of uh things in your knapsack, right? Do you wanna have like one knapsack item which is just big, but every time you change your knapsack I have to rewrite the knapsack, or do I wanna have every item in the knapsack be its own item? Discuss, it's, it's a challenge. Do I wanna have little knapsacks like every unit of update is a knapsack? What is the best way to store and many items for a video game player for their payload? And you, this, this, uh, gets into it, the cost of rights and the cost of storage. Oh man, these people keep coming up with requirements. OK. Users can now delete threads. All right, And the threads might be really big. All right, so what I probably don't wanna do is have the user wait around while I'm actively deleting. What do we do? This, this is a classic soft delete situation, right? Where I'm going to mark something as deleted, act like it's deleted from the user point of view, and then later on in the background, do all the sweeping. Right? Alright, hm, I'll update an attribute on the metadata item. I'll delete the conversation later. How do I know which threads are deleted? Almost every one of these has an answer as an index, right? There's, uh, an index on the deleted threads. OK, I can do that. So what I will do is I'll create a GSI. I'll make it a sparse GSI because it'll only be the deleted thread metadata. And now if I delete. A thread, I will create a new attribute. I don't care what it's called. But it will be the attribute name that gets projected into my sparse GSI and then I can go to that GSI and I can see only items which are representing deleted threads. So maybe I will pick a GSI partition key of a thread state to say it's deleted, and a GSI sort key of a timestamp of when it was deleted, so that I could like clean up the oldest first or something. So I go here and I do a query and I say find me in this GSI partition key equals S deleted. What's the S? State I'm doing that name mingling thing you see that a lot. You don't have to, but you know I'm doing it. T time stamp T representing time stamp, um. And I can say, all right, find me all the deleted ones, forward, give me the 1st 10, and I'll go off and actually actively delete them. Is there anything wrong with this? We're in the scaling section. We're in the one where we try to say, you know, how do we do this when the, when the scale gets to enterprise level. One of the issues is that in that GSI, the partition key of deleted, it can only accept 1000 rights a second. Right? Are you gonna delete more than 1000 threads a second? Most people don't Some people do So what do I do about that? And this is actually where you get a lot of, uh, hot partitions, as we call it, is the base table has a beautiful high cardinality partition key, but the sort key has like state as the partition key. It's I, I chose this here just because it's so common. I wanted to get it out there, and the state can be an issue. So this is where you can shard. So if I was going to do 10,000 deletes a second, then instead of deleted, I would have like deleted hash random number. And now I can accept as many items as 1000 times the unique number of partition keys, so maybe I should do 20 so that none of them are close to being hot. All right, this is charting. You'll hear it all the time there. I'd love a built-in feature, but it's not there. But this is where you have to be aware that sometimes my partition, my one partition key can get more than 1000 rights a second. Now we will sometimes be able to split it for you, but if you do this, you're guaranteed it'll work. The details on when we can split hot partitions is kind of complex. Happy to talk about it, but I blogged it. If you do that split for heat Dynamo DB blog, it tests out various scenarios. All right, so what if later I have more values like archived. I mean I can do this archived 0 123456s something else 012345. Another way I wanted to show you though is that you can just pick the partition key. You don't care what the partition key is that much. So look at this now. My partition key is S. I don't know, S shard S stands for shard now, yeah, that works. Shard 012, as many as you want, and I've moved the value into the sort key. So now I can do a query which is like give me everything in S0 bucket there. S1 bucket or I could say S1 starts with Sh deleted. And now I can say, give me all the deleted ones from the bucket one, and I could now find all the deleted threads by going to my 10 buckets and for each one saying give me the sort key starts with this. I just want to show this because sometimes you put a value in the partition key and sometimes you put at the prefix of the sort key. And they both work just fine. I think this one's kind of elegant. I don't know, I pick as many shards as I want because I'm like, well, I'm gonna update this many times and I'm gonna have X many shards and then the value just becomes something that goes into the partition in, in the sort key as opposed to me having all these partition keys with all the different values there, but there's no real difference from the database performance or the cost or anything. It's just a style thing. All right, boy, these people keep coming up with requirements. I want to count the total costs per model per day, and I want to count the total costs per model for each user to do some billing. OK. And this is one of those situations where I probably don't wanna put it on the main right path because if I put it on the main right path, I slow down the user, and there's really no need for that to just do my accounting. This feels like a back end thing. And so I wanna scale to any usage and I want to not make the user wait. So have we heard about streams in Dynamo DB? Streams are Dynamos change data capture, where every mutation that happens to a table will be put into a stream. And you can then watch that stream to do something with the data and some people do this if you're doing a shopping cart checkout they'll be, oh, the order placed, and then they proceed to, to propagate it down. Sometimes people do it to propagate to a downstream system. So I can watch that and I can push it into some other database that's gonna have a different index model, or I can write it to S3, or I can check for bad behavior or whatnot, and you can also do event filters on it to say I only wanna be notified about certain things, and there's a tight integration with streams and lambda, where you can create a lambda function that says I watched this stream. And it's just called for you. You don't have to iterate streams and all. There's a lot of work to go find the latest data out of the stream, but if you hook up with lambda, it's all done for you. And so you create a lambda function that updates the count and would probably buffer internally. So a lambda will be handed not just one item at a time, but some batch of items, and you can control that so you can make a bigger batch and now your item, uh, you can have 1000 items and you have your lambda be like, all right, let me aggregate this together. And update once instead of update 1000 times. That saves right costs and it improves efficiency because probably the cloud model on one day across all users is going to be used more than 1000 times a second. So better I update with a buffered value. It kind of depends on how accurate you need to be though. Streams are really nice because they uh. Are exactly want delivery. You don't find this often in any sort of streaming event. They're exactly once in order delivery out of your database. Pretty cool. But if the lambda function has issues, what happens? If the lambda crashes, we restart it with the same payload. So you gotta be careful because the lambda might increment once and then die and then be restarted and increment once again. So you gotta be careful when you're doing the lambda stuff and there's no simple straightforward like push a button and it's all good. You basically have to have some state remembered where the lambda can know if it already did the work or not. One technique Is that you make a message ID set in the item. Of recent updates like, hey, I'm doing an update and I hashed my payload and here's my hash and let me make a note that I did the work, and here's a recent set of work so that if I crash and I come back I can say, did I already do it? Did my previous incarnation do this work? If so, I'll skip it. Um, you can also maybe use transactions with a client request token, which is a way to get item potency out of it, as long as you can get a deterministic client request token out of your payload because the lambda is always, well, not always, it's usually, uh, invoked with the same payload unless you say split on air, in which case it will split it down to different payloads to try to isolate the error out, but it's complex enough that I wrote a blog on it with I think 7 or 9 or some such different ways to do it, all of which are a little bit like, uh, OK. As a technical term, so you can look up that if you want to see ways to do this, but I want to point that out when you're doing the lambda processing, the stream is exactly 1 in order perfect delivery, but your processing of it may not be. And then something else, uh, gosh, these people, they, they, this PM, they keep making all their money giving me work. I wanna be a PM someday and make other people solve my problems. So, uh, server side search. Right now you can do a client side search, search your past, uh, chats, but it's gotten a little slow because it's unindexed. So I'd really rather have a server side search that I maintain for everybody. What do I do with that? What's the query in Dynamo that does tech search? Yeah, there isn't one. Was that, was anyone excited? Like, really? There is one. No, there's not one. Um, so what we do though is we integrate with OpenSearch. So there's ero ETL, I think this was about 2 years ago introduced, uh, and you can without having your own servers, it's all serverless and managed for you. You define the configuration of how Dynamo will propagate to OpenSearch, and I have a copy of your data in OpenSearch or a subset of your data if you want. And against OpenSearch you can do all the stuff that is advanced indexing because OpenSearch is really about. Index tools so you can do relevance ordered search. You can do an analytics type query. You can do uh rag, and we have a workshop at 3 o'clock that I'm leading where we're going to do this actual integration where we're going to use OpenSearch and Bedrock and do natural language processing with RAG driven by OpenSearch and vectors. So this is one way to do it with Dynamo and OpenSearch. Pretty straightforward. You'll see that there's just uh a definition of this goes to there and that's it. Um So one last tip, uh, and I just put this in all my talks, because it's gonna save you so much money the day that you need it. All right, so this is your money saver, this pays for your reinvent ticket. The new requirement is we want to be ready to recover from an application level corruption. So something went wrong. A bulk load happened. Uh, I deleted an item I didn't mean to. That level of corruption. Uh, it's our job as AWS to make sure that your database doesn't corrupt, but it doesn't mean that your application couldn't make a call that you didn't mean to make and actually did something bad with the data. And the thing you do with that is a push button. You can turn on point in time recovery Pitter, and Pitter maintains the change log for up to 35 days. Now you can configure it smaller if you want to forget things after a day. But we will remember up to 35 days for you. And against that you can then typically do a full table restore before the damage. That's the classic, this is you after doing the bad load, you're like, I'm gonna get fired. I can't believe it. So you just ran a bad bulk load. And the full table restore is a way to solve the problem, but it costs $150 a terabyte. 15 cents a gigabytes a way to make it sound smaller, but it's $150 a terabyte, right? You might get fired, man. How big is your table? So what else can you do? There's a cheaper way, and the cheaper way was introduced, uh, 2 years ago. Anyone remember? It is called incremental export to S3. If you have point in time recovery on, you have access then to doing an incremental export. You can do a full export too. We've had that for a long time. An incremental export though is where you say, I want the list of changes that happen between these two points. Give that to me as Jason on S3. So now, think about it. If I get this on disk, which tells me the time, the partition key, sort key, if there's a key, the, the primary key of the item, the old image and the new image. I can see that at this time I changed Mary Grace to Mary Smith. I can now undo the damage without having to go make another table, right? One is restore a table. Now I've got two tables. I gotta delta them. I gotta figure out what happened and then put it back, or I gotta point to the new table, but that's awkward, right? Because you can't rename the table to be the old name. You gotta point to the new name. But here I can just do an incremental export. I can then process it and say, all right, items for this set of item collections, these partition keys, these are the ones that are bad, roll it back. Whatever is the old image, make that the new image, basically on the table. And the cost is what? Well, it was $150 per table. Here, incremental export is 10 cents per gig of log size, and assuming that you had like a gigabyte of logs during the time window of your bad load, it changes $150 to 10 cents. There you go, um, bigger if you have a larger table and the recovery time from hours to restore a 10 terabyte table to minutes to export and process the export and, uh, be able to do it. So I'm just putting that in my various talks. If you ever hit the pitter issue, be ready. You should probably dry run this, uh, but then that's a way to undo the damage if you ever make a mistake and not get, did you wanna go back there? All right, thanks for coming.