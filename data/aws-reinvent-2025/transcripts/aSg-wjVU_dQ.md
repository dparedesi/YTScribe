---
video_id: aSg-wjVU_dQ
video_url: https://www.youtube.com/watch?v=aSg-wjVU_dQ
is_generated: False
is_translatable: True
---

My name is Justin Iravani, and I am a senior, uh, cloud infrastructure architect with AWS Professional Services, and today we're gonna be talking about, uh, the current state of PLM in terms of challenges, the perceived barriers to modernization, and some principles for addressing those challenges. And then I'll turn it over to, uh, Jim and Dan, and we'll talk through how Boeing was able to modernize their PLM operations on AWS and what's needed to operate, uh, hundreds of environments, uh, on AWS. When it comes to product, uh, data and product life cycle management use cases, we see, we AI see three key challenges raised by our customers. So the first is performance and scalability. Um, as product data and analysis solutions expand due to things like increases in 3D model fidelity, business intelligence, and metadata, and variation complexity, um, it becomes more challenging. Additionally, the needed reach of these solutions and applications to regions, organizations, and functions means more data velocity and distance. This leads to our second challenge which is data friction. So data friction has largely been an artifact of historical on-premises architectures and integrations based on legacy, infrastructure, and um. Excuse me. Uh, yeah, legacy infrastructure and applications, uh, the these functional silos also create corresponding data silos that are, uh, isolated for reasons like security and access controls. Um, due in large part to the first two challenges, global collaboration continues to be a challenge as companies engage in engineering activities which span multiple regions, span the globe, um. Additionally, these connections are becoming more and more prevalent as companies continue to accelerate mergers and acquisitions and partnerships around new technologies and innovations. For those manufacturers who whose business is dependent upon PLM today, the prospect of upgrades, let alone migrations, is very daunting. Service interruptions must be minimized. Excuse me, and new implementations have to be handled in a way that's durable and reliable. Manufacturers also have significant investment in training. Um, you know, the, the teams and engineers using these systems, and they wanna make sure that, um, they continue to maximize that knowledge into the future. There's also often multiple layers of applications built on top of these PLM systems as well as very tight integrations with other critical systems like ERP, MES, and SCP. I'm sorry, SEM. Uh, well, there's a large volume of data in these legacy systems, um, there's plenty of opportunity for design reuse, but that can often be difficult to manage. Additionally, migration projects involving PLM, uh, often last months or even years and can often be very expensive. Any PLM solution running on AWS needs to account for the core three pillars, the PLM pillars of digital twins, digital threads, and so-called digital fabric. AWS helps our customers establish various digital twins that represent many aspects of product definition. Some of these are more obvious like 3D excuse me, 3D product representations, but others may, uh, involve requirements of digital twins about, you know, how the product should behave as well. PLM digital twins and engineering twins, um, sorry, I've had a lot of coffee this morning, so a little, a little jittery. Um The um uh digital twins uh established by hosting partner solutions as well as using storage and database services to build, uh, engineering digital twins to support the various solutions that exist, um, just as an interesting sort of side note, um, Boeing was one of the, uh, pioneers in this digital twin space, so back in the early 2000s, this began with the so-called, uh, virtual airplane uh project, so they definitely continue to, uh, innovate down this path. The product data and life cycle management thread represents the connectivity between the various data sources. These threads are what delivers a richer set of information for decision making and aid overall processes for more efficient and effective outcomes, allowing for interaction with disparate data sources in real time with appropriate context. The 3rd pillar for product data, and life cycle management involves the ecosystem of the so-called digital fabric. Uh, this builds upon the development data and digital twins with interconnectivity through the threads and adds stakeholders from across the network. Global connectivity and availability is critical in leveraging and maximizing skills and resources required to deliver today's innovations. Uh, R&D is at the heart, uh, of most manufacturers. Market pressure to deliver increasingly complex products to market faster as businesses exploring new methods to keep pace and unlock new revenue streams. However, traditional R&D infrastructure, uh, and the complex integrations built on top of that infrastructure barely keep up with the existing product development work streams. This infrastructure is inflexible, poorly utilized, and can be managed in year-long, uh, cycles. Increasingly, customers are looking to AWS and our partners for best practices around R&D IT infrastructure modernization and application modernization to enable outcomes like agile engineering, multi-disciplinary optimization, concurrent engineering, and smart products. The AWS approach to modernization can help our customers overcome these perceived barriers. Uh, our, our approach is being customer obsessed, understanding, you know, what, what are the needs of our particular customers, working with the other customers, uh, in the marketplace. And um then working backwards to break down these complex problems into smaller, more manageable components. We obviously also need to focus on building solutions and architectures that scale appropriately with our customers, accounting for not only architecture but also operational complexity. So for example, facilitating things like self-service. By leveraging automation tools like cloud formation, terraform, uh, we can ensure consistency, reliability, efficient, efficiency and quality by reducing the potential for manual mistakes. Lastly, we also prioritize speed and agility over perfection. So the goal is to deliver customer value rapidly and iteratively rather than having a perfect solution, um, so this often means I'm focusing what we can do within our given span of control, uh, or as I'm known for saying, uh, we can always make this more complicated later so. So, um, with that, I'd love to hand it over to, uh, Jim. Thanks, Justin. My name is Jim Gallagher. I'm the lead architect for 3D experience at Boeing. First, maybe a blurb about what is PLM. PLM is product life cycle management. The industry concept is womb to tomb system for managing product engineering, manufacturing, and support data. It includes CAAD, CAM, CAE simulation, tooling, planning, change management, and configuration management, representing the as designed, as planned, as built, and as supported life cycle states for all systems. Mechanical structures, electrical, hydraulics, HVAC tooling, ground support equipment, etc. The goal is a digital twin of each product we manage at any given life cycle state, along with all the data and analysis leading to and supporting the management of that state. PLM systems are used across all Boeing product lines. Our next generation PLM system at Boeing is DeSo Systems 3D Experience. Boeing is investing in enhancements in 3D experience to best execute Boeing design and manufacturing processes. 3D experience is currently in production for several dozen smaller programs with several 1000 users. 3D experience will be the PLM system for the next new commercial airliner. As Justin mentioned in the introduction, Legacy on-premises infrastructure management presented common challenges. Infrastructure capacity is based on yearly budget and capital acquisition cycles. Management of limited resources requires process. Assemble a hosting package, request, review, approve, reject, provisioning. All this takes time and means competition between projects for resources. Projects may not get what they need when they need it. Different cost models for different services. Chargeback models must be developed and maintained, including which services charge back to. Oh, sorry. Different cost models for different services. Chargeback models must be developed and maintained, including which services charge back to projects and which are peanut butter spread across the enterprise. Often, there's a significant lag between approval and provisioning because of handoffs and work queues. Once in hand Legacy managed infrastructure has additional drawbacks for both the project and the enterprise. There's significant incentive for projects not to give back if they think they might be able to reuse. Chargeback models do not always reflect actual or proportional cost of the infrastructure. It's difficult to right size. Expanding often means a request approval cycle. Shrinking means a project may not get it back later. So PLM to AWS. Cloud opportunities. Boeing leadership saw opportunities in AWS to enable application teams to move faster and save money. We created an enterprise internal cloud. The enterprise worked with AWS to create network segregated regions in AWS GovCloud for Boeing, which appears to be on the Boeing network. This greatly simplifies Application access and systems integrations. Empower the team. Turn on the AWS accounts, enable the architects to design and the operations team to build. But why is it taking so long? Our app has a large and complicated footprint. It was not as easy as it seemed at first. Rehosting and reaping benefits. Requires time for analysis, learning, and iterative experimentation. We established an engagement with AWS ProServe to assist and guide our PLM team. Beware of the devil you don't know. On-prem was slow and inefficient, but the processes and patterns were well known. There were many divisions of labor and hosted services. Legacy infrastructuring. Included Uh, OS admin support, systems integrator support for server tracking and compliance, backups. Network attached storage, load balancing. OS blockpoint and patching. Our new responsibilities, skills, and AWS services, um, we needed to take care of those things ourselves. The vast majority of those legacy managed services were not available to us in AWS. There was no AWS experience on our team. Success depended on working closely with Dessau and AWS ProServe. It was clear we would, we would need to work closely with DSO infrastructure architects and AWS ProServe to establish new architectural patterns to best meet our objectives. What were the, what were the migration outcomes that Boeing was looking for? Accelerate infrastructure operations, time to provision, time to decommission, time to install and upgrade the application. Avoid bureaucracy, no approval or request cycles. AWS proficiency, ops and architecture need to understand what can be done and what should be done and be able to execute. Avoid dependencies. No waiting on other teams to provision what is already approved or needed, or troubleshooting. Reduce cost Enable elastic compute. Don't provision for the peak, provision for the baseline. Implement processes and tools to monitor and turn off VMs not being used. Network storage bill by consumption, not by provision capacity. Improve configuration management via infrastructure as code, reduce variation between servers, and simplify provisioning. The next two desired outcomes were not specific to AWS, but we use the migration as a forcing event to accelerate these additional outcomes. Follow the sun, leverage AWS account segregation to enable our colleagues in India to provision and manage environments that do not contain export controlled data. Tech insertion. Address OS obsolescence as part of the migration to AWS. On-prem, we were running Red Hat 7, which is at end of life, and so our newly provisioned systems in AWS are Red Hat 8. Lastly, The Boeing 3D Experience PLM releases are major undertakings with hundreds of people involved. We wanted to avoid impacting release schedules. So we partnered with AWS Professional Services. To bring their experience with other 38DX customers and deep AWS expertise to bear on our desired outcomes. We also engaged with DSO. We work continuously with our DSO colleagues, and this project required establishing some new priorities around how to best map the out of the box experience, 3D experience architecture to AWS infrastructure. Establish capability priorities. We had a long list of mini science projects to work for different capabilities. Infrastructure is code. Load balancing, data replication, disaster recovery, etc. Priorities and minimum viable products for different phases of the path to production were determined. Establish a schedule, which environments would migrate as part of which PLM release. Don't try to migrate everything at once. OK, now I will hand it over to Mr. Dan Meyring. Thank you, Jim. Good afternoon, everyone. My name is Dan. I'm a lead systems integrator uh at Boeing, uh, working in PLM. Here to start off with everyone's favorite topic, of course, bureaucracy. The doors are locked so you can't run at this point. That's why we put it in the middle. Um, so to set the stage, uh, in order to support 3D experience, we need a predictable stack, compute hosts, databases, load balancers, network attached storage, file systems. I got that out without stumbling, uh, DNS aliases, etc. asserts, uh, for each environment, etc. Uh, importantly, the work doesn't end once the resources are allocated. Post-convisioning steps like, you know, we have patching, troubleshooting, you know, configuration issues when it's delivered, access config, compliance, tasks, all those things are essential. Uh, and our goal has always been like anybody's would be, repeatable infrastructure that allows the application to run reliably. So before AWS, each component, the, the VMs, the network attached stors, file systems, databases, DNS, they all required requests to separate teams, and those teams not only provisioned the resources but also owned ongoing support and many configuration tasks for them. Uh, that model meant many handoffs, a lot of emails, coordination, phone calls, um, instant messaging, etc. depending on the preferences of that actual support team. Um, and on paper, you know, it worked for a long time and it looked structured. But in practice, the coordination costs and delays were significant, especially in hindsight, uh, submitting requests was a bureaucratic nightmare. Um, teams required specific forms and formats. Mistakes in a request could potentially cost weeks of delay. The images on the right of the slides here are actual example snapshots from some of my team's internal instructions. Uh, all of them were earned in blood. All of them to try and ensure the request for a specific resource was submitted correctly to minimize delays. The, the lower left hand side, that long thing is actually shorter than reality. I had to cut it there, otherwise it looked awkward. Um, that is a single request for a network attached storage file system for one region exporting to one net group. Um, typically we could, we could put a couple on one, but anything beyond that, it, it's likely to get messed up for some reason. So we, we spent a lot of time following up, clarifying requirements, and reacting to inconsistent results. That daily overhead obviously diverted engineering time away from delivery and created a cycle of frustration and rework. Our reward for that headache of all that manual model was inconsistent infrastructure, servers that were missized for the workload, network attached storage file systems hosted in the wrong region or exporting to the wrong net group, databases configured slightly improperly, and systems that required bespoke fixes. Those handoffs increased operational risk, sometimes surfacing as late night outages such as when a uh a database listener wasn't quite configured to start up automatically after a maintenance window and so we would get a prod call in the middle of the night, typically not prod, but we would get a call nevertheless to to support some environment that went down, um. And it all reduced our ability to iterate or experiment because we had to overprovision to hedge against variability. OK, so with that mess in mind, let's explore what happened when we moved to AWS. Well, a concrete early win was replacing our single Apache HPD host with Amazon's application load balancer. Um, this is kind of starting out from the user perspective, I guess, when it hits the, the link. Um, it allowed us to remove a single point of failure we had in our environments and eliminate the need to maintain complex load balancing configs in Apache across services. Uh, for example, uh, for the most part, uh, the single point of failure was that we were hosting Apache for an entire environment, usually a multi-tier environment on a single VM in the environment which also kind of shared that host with other services. So if that host went down, even if the other services were fine, the whole access to the application was, was gone. Uh, but the ALB gives us centralized health checks, visible in the console, high, highly available integration with EC2. And auto scaling, uh, that supports application cookies and sticky sessions required by 3D experience, especially for auto scaling, and simplifies the DNS and the ingress controls. For compute, the, the EC2 uh gives us API-driven right-sized instances. We can select CPU and RAM profiles uh per service and use AWS recommendations such as through compute optimizer to actually just tune the sizing. So by baking in and uh in addition, by baking our prerequisite into the custom AMIs the ECTs are built from, uh, and using instance user data for specific bootstrapping, we, we have reduced the manual setup and speed and sped up instance readiness significantly. Uh, and last but not least, the, the tags on the EC2 instances have also enabled all sorts of metadiscovery, uh, metadata and discovery that we did not have on-prem. And an additional note that's worthy of mention is, is for the AMI's for our on-prem instances, there was usually this, this forced wait period because we had a kind of push security patching model. So as a blockpoint image as time went on from whenever it was published and we consumed it. The further it went from the patching cycle, the longer it would take before the the instance was actually ready to use, sometimes up to 3 hours, and that's not great for ASG. In fact, that makes, uh, auto scaling impossible effectively. So the AMIs have completely wiped that out. Similar to EC-2s and the scaling auto scaling I just mentioned, the ASGs, the auto scaling groups, have let us, uh, scale services in and out automatically or manually. Uh, our pattern uses user data scripts and pulls the binaries from the EFS that's attached to every single host. It runs a startup bash script, starts the services, does some little finagling and, um, gripping sometimes, and, uh, just starts it up and works seamlessly. This approach supports scaling driven by application metrics. Cloud watch can trigger a scale event on either Java metrics or user accounts, and it doubles as a recovery mechanism as well because launch artifacts are stored on the EFS, which is replicated to the DR region. So speaking of EFS, it has replaced our legacy network attached storage. No more of those long, uh, request forms that always get messed up in one end or another. Uh, it's API driven, automatically scales, and supports per environment volume, so teams or environments no longer share a single file system due to scarcity, and we don't have to try to anticipate capacity needs, uh, which also means we're not paying for the storage we might have needed, though that typically wasn't the problem. We usually ran out of space. Um, on-prem, this was always a headache due to limited network chat storage sizing. Uh, anytime we went over 2 terabytes and a request for some reason, it became a slightly different request process. Anyway, all this added up to unreasonable amount of overhead to provision storage, like having to for every request having to decide repeatedly like what its purpose was, what, what region is it going to, what environment, uh, for which permissions, uh, and when we run out of space, what's going to happen, and inevitably we did run out of space, so we would then have to rob space from other environments. Uh, but now the EFS has significantly reduced that headache. Pardon me for a second. Um, it's reduced the blast radi radius of concern for that network attached storage to it's vaporized, it's gone. There's no concern anymore, and it's removed all the repetitive decision making we used to make, um, all while avoiding resource contention as well, and we're not paying for unnecessary storage. Um For databases, we now use RDS. It provides a managed database service with high availability across multiple availability zones, automated backups, point time recovery, read replicas, snapshots, you name it. Uh, we can import Oracle dump files from S3 for priming new databases without repetitive DBA involvement. Didn't really gather metrics on, on how much it saved the DBA's time as well, but there's, there's definitely been a lot less interaction with them, so they're freed up to do all sorts of stuff, as all the various teams that I've, I, I mentioned that we used to send a request to. Surely we're not interfacing with them nearly as much, so they're working on other things, no doubt. So to sum up our new infrastructure, we now have resources that are quick to provision as well as tear down. The infrastructure for each environment is consistent, right size to specific resource needs, is performance based on EC2 and RDS types being tailored to the actual workload, as well as being scalable and automatable. So how has that enabled us to accelerate our operations? Well, obviously AWS exposes infrastructure through APIs, and infrastructure as code turns those APIs into well-defined code by consolidating all those docs and instructions and manual steps into terraform code bases. We've achieved consistency across all the EC2s, EFSs, databases, DNS, load balancers, and other related services. The benefits are faster deployments, improved reliability, consistency, and repeatability. So zooming out a little bit on the infrastructure as code we're using Terraform. We started off with cloud formation, it was great, but the, the enterprise was going terraform, so we're gonna want to use their modules, so we're using Terraform. Um, our code lives in GitLab. A GitLab runner executes the pipeline to plan intended changes, and we use approval gates and validations, uh, to ensure expected outcomes before triggering the apply stage. Uh, the CICD approach enforces peer review. It creates auditable change history and replaces all sorts of manual provisioning steps with an automated repeatable flow. Um, zooming in a little bit further, uh, to standardize outcomes, we created environment templates essentially we have, uh, small templates, medium, large, and we have monolithic, which all the services are hosted on, you know, 1 EC2 essentially and, um, it's basically a death box. Uh, each environment template defines different baseline EC2 sizing, RDS classes, LDAP bootstrap settings, DNSALliases, ASG configuration, um, volume sizing, etc. and, you know, load balancing configurations. The templates have reduced, uh, the overhead that, uh, and ensured deployments match the intended performance and scale profile, stuff that we used to have to look kind of research, etc. Uh, we did not have the standardizing, standardized sizing on-prem. Instead, previously we would effectively find a close match environment, uh, then try to provision architecture based on that close match which was heavily manual and investigative process. A lot of that was kind of downstream of the, the inconsistency with some of the infrastructure we get. We, we'd have to rob stuff or, or patch things, etc. So zooming out a little bit further, um, top left there, there's an example of a small, that's kind of the, the template I was just referring to. These environment definitions provide a minimal set of parameters for res an effective, uh, an effective, uh, environment size. Um, and then, oops, let me highlight that one, poop, there we go. Our, uh, and then our terraform, uh, 3DX core, uh, module forms the kind of the scaffolding for all resources in AWS and it iterates through the definitions of, say, the small environment to deploy those subtle configurations that that environment needs. And for all required resources. And then at the build level, uh, for the environment, we source the core module using the smart default to smart defaults to remove a repetitive manual entry while still allowing for overrides. Such as for example, you know, maybe pinning an an older AMI that we may need to test, you know, some issue maybe in prod that has an older AMI and then um we just fire up a dev box with that old AMI with an older uh AMI pin to it or maybe we need to override the Route 53 DNS alias for the environment, little things like that. So now that all our infrastructure is defined by code, we can deploy multiple environments in parallel without sequential bottlenecks that we used to see. This ability dramatically shortens wait times for teams needing fresh environments and supports a more agile development cadence. The benefit of this. Beyond just automation cannot be overstated. Previously someone on the ops team would be bogged down for a day or a week deploying just one or a small handful of environments due to the manual effort and coordination, etc. that was required. And because of the pipeline and the modular nature of the code, we can easily deploy one environment. Orbs To the, to the click. Or 10 environments or conceivably hundreds of environments in parallel in only a few hours. On top of that, most of that time is passive, so someone on the ops team who's deploying these things can just go and work on other more important tasks instead of just watching it cook. Um, anecdotally, I've gone out and I've kind of pushed the limits. I tried to build a multi-tier environment as big as I can be, and I ran it at IPs, so I had to bring that out, but it was, it was not the fault of the, the terraform and the infrastructures code, which is the account ran out of IPs because it's a small test account. And then I've, I've deployed, you know, and the opposite team has deployed easily 80 dev boxes, you know, in under an hour or so, um, all at the same time. Um OK, Then click, there we go. OK, so, that said, we'll zoom out to the, the highest level we have now, uh, for, we have the AD uh the AWS3DX core terraform. Code base kind of at the lower level, um. In that we also have multiple repos to separate the concerns for the PLM infrastructure across multiple AWS accounts. We have an AMI account, uh, and a AMI repo with chef cookbooks that kind of spit out an AMI in our AMI pipeline. It dumps that AMI essentially into the AWS account for AMIs. It goes through some testing and trials, and then once it passed muster, we share it with our other accounts and then kind of pin it into the the latest version of our 3DX core infrastructure version. Um, of course we have the 3DX core infrastructure repo. We have a PLM environment repo, actually, here's the. Here's the core infrastructure. We have a PLM environment repo for per environment definitions themselves that kind of refer to the core. And we have a prerequisites repo to standardize accounts, uh, IAM profiles, roles, etc. uh, that all the accounts just have it's kind of 11 shot and done some tweaks over time as we learn things, but. Um, so we have, um, as I mentioned earlier, we also have the, uh, click forward a little, uh, we're zooming out, you know, this is the, the, the broad architecture here for our, our entire pipeline. The enterprise standard AMI feeds into our AMI pipeline, so that's in the bottom left-hand corner there. So that's loading into our AMI pipeline, ours layers on the stuff for 3D experience, and then that essentially referred to in our, our, uh, our AWS3DX core which is literally at the core of this image here and. That also refers to enterprise modules for terraform for all the resources that I just mentioned. Uh, in order to maintain, you know, IEC standards across the company. Oh, I clicked something. Here we go. OK, so that's all infrastructure. We've been talking infrastructure this whole time. Let's get to deploying 3D experience. In order to do so, we for our configuration management, we use the deployment of 3D experience. We use. Uh, sorry, I thought that was me for some reason, an automatic. Uh, we use answerable configuration management, uh. And the infants, you know, this, this image right here is actually like an actual, uh, normalized, I could say, uh, inventory document for one of our environments, what it might have looked like in yesteryear, uh, but now we actually have dynamic inventories driven by resource tags that we put on the EC2s, etc. So there's no manual copy paste of host names, etc. which obviously caused all sorts of issues. Anytime a human is interacting, there's gonna be problems. Uh, Beyond that With all the spare time we have, uh, we've kind of opened up with automating a lot of the infrastructure, uh, dynamic inventory and things, you know, since we're not laboring with that paperwork anymore to wire infrastructure together, etc. manually, we've been able to kind of refactor all of our answerable playbooks into roles, turn linear stuff into roles, and, and kind of streamline even our inventory. I think our inventory is even less than that now. It's like down to 4 lines. Um, so this makes the answerable scripts simpler. Uh, deployment's more reliable and has positioned us to integrate, uh, application deployment into CICD more tightly, and the fewer little touch points there are manually on those and in fact that dynamic inventory file is now dyna dynamically generated as well so it's not, it's, it's like one human interaction. OK, so we're, we're still working on deploying. Let's look at what the deployment pattern of 3D experience looked like on-prem. Uh, historically, to conserve the on-prem resources, we allocated multiple 3D experience services on a single host that forced serial brittle installs. Only one service could be installed at a time on a host, and a single problematic service could affect the others and often did. What's more, our answerable playbooks had to account for many deployment patterns, special cases, and the variability of the infrastructure resources, thereby becoming increasingly complex to maintain. Those conditions resulted in inconsistent deployments, uh, between environments, making each environment kind of feel like a game of Tetris, especially if there's like a deadline and we need to get it up and running soon, you're just slamming in pieces and you know by the time it gets filled to the top, you see all the little blocks you missed and you're like, oh I'll deal with that later. And we did, usually at 2 a.m. Um, in, in short, uh, or, or just as a frame of reference, the full deployment of an application under ideal conditions was about 9.5, 9.5 hours for a single, uh, environment, and that was on-prem. I did it again. I clicked the thingy. Oops, let me go back. However, now, In AWS with our right size hosts AMI's that bake in the prerequisites, we now deploy each service to its own host, very fancy, and we run the installs in parallel. Pre-baked AMIs along with parallel deployment patterns has shortened install time by over 60%. It's down to about 3.5 hours, and that was, that was simply the, the potential that was opened up by moving from off-prem onto on-prem. Um, it wasn't the deployment of the application itself that changed. Um And the combined result obviously is faster, more predictable deployments and simpler automation. Our playbooks no longer need so many special cases to handle these ad hoc configurations, robbing infrastructure from one environment or another. Um, But moving to AWS was not without its challenges, of course it did introduce a learning curve across cloud services like Jim mentioned, we didn't really have any experience with AWS, Terraform, etc. etc. um, so Terraform, GitOps, CICD systems admin, database admin, compliance, these are all roles that, uh, they were once handled by the other teams but are now our responsibility. So we did have to take that on. Uh, I learned that that's a shallow learning curve, not a steep learning curve. I've always referred to it as a steep learning curve, but. It took a little while. Uh, a major risk we encountered when another challenge was that, uh, when learning about infrastructure as code with pipelines, there was the kind of the, the blast radius. It's come up a lot. Uh, we started off adding a bunch of environments to a single main terraform file. However, when we wanted to alter one of those environments in that collection, uh, the terraform would nevertheless still need to kind of verify the plan and the state file with the other environments in that same commit, which is not what we wanted, uh, so we started kind of separating. The environments and segmenting them into their own branch and little commits. Uh, another challenge was that the terraform will do like any code, I suppose, exactly what you codify, so critical resources can be accidentally destroyed without proper guard rails, uh, hence the approval gates that we added to the terraform plan. Uh, ironically, the on-prem, the, the hurdle to creating all those resources was also the bulwark against, uh, accidentally destroying them. Uh, but We've learned a lot. Uh, one final challenge was the subtleties of the, the ASGs. Uh, auto scaling groups, we learned this the hard way. If a replacement process was not suspended and we stopped an EC-2 in that ASG, then, you know, whoopsie, the ASG would do exactly what it's configured to do and destroy that AC-2. That functionality was not what we were ready for though we deployed it with the intent of getting there. Uh, lesson learned, suspend a certain auto scaling processes that you actually don't want active unless you're ready for them. And then there was the kind of some, some bleeding edge headaches with, with 3D experience itself being on AWS on-prem we couldn't do auto scaling, so we never had to like configure the cookies in special ways for 3DX itself, uh, but now we had to tune the, the auto, the application load balance or the ALB cookie persistence to support scale out behavior for 3DX services, uh, and adjusted database practices because RDS enforces. More granular permissions instead of the on-prem grant hall that we were using a little bit wild west, but the RDS won't allow that. It's uh it's best practice not to do it and the RDS ensures you follow best practices. And then of course, all these fixes and lessons learned are, are documented in the DS 3DX knowledge base now. DS so gets that stuff in there real quick. So, um, other customers don't have to run to the same. Uh, Aaron AWS. And of course this you know with every challenge, the obverse of that coin is, uh, there are new opportunities. We have already, uh, been seeing many benefits and we continue to evolve. We're collecting metrics for components such as 3D space index to, uh, which is, uh, it basically indexes all the search data so people type stuff up and it comes up. Um, and we're, we're fine tuning things to refine trade-offs between indexing time and the database sizing and, you know, enabling increasingly granular optimizations, EC-2s and all that. Um, additionally, our team has greatly broadened its own skill set now having been freed from the shackles of bureaucracy. Uh, our time is better spent on creative tasks now rather than repetitive tasks. Uh, additionally and operationally, we're moving toward running as playbooks through CICD pipelines and plan to try out a pull model to improve deployment performance even further for disaster recovery, just going down the list of opportunities, uh, for disaster recovery, we are evaluating a smaller RDS class for the Red replica in the recovery region because the, the replica is not actually serving user traffic, so it just has to keep up with the big boss, the, the primary instance in whatever region that's in. We're also working uh uh toward enabling developers to trigger a kind of build, deploy, test, destroy workflow from a CICD pipeline or dashboard or something in order to streamline and accelerate the dev and test cycles and similarly we're working toward a build, deploy, test, destroy for our own team so that playbooks are constantly tested to failure, fixed, then merged back in for all devs to enjoy. That. There we go. OK, so what's the outcome of basically everything I've been talking about? We would cut some bar charts over here, and those are always fun. Uh, the impact of everything has been measurable and immeasurable in a lot of ways. Uh, the manual tasks and touch points, uh, the left hand bar chart there, the manual tasks and touch points for a single environment have dropped by 78%. Uh, that was 75% a few weeks ago and now it's 78%. That's a lot, uh, reducing opportunities for air and tribal knowledge. A full environment build as well, right-hand bar chart, that, that giant tower and that little sliver at the bottom there to compare. The infrastructure provisioning, the configuration, the application install now completes in roughly 5 hours versus up to 30 days. That's a 99% improvement for a single environment. Scale that out to potentially hundreds of environments and you kind of start to get to see the picture, and that was all time people were kind of spent bogged down on other kind of mundane repetitive tasks as well. They've been, they've freed up all that potential. Uh, what's more, the few remaining manual tasks that are on that like 78% of the remaining 22% that remain are just much simpler. We can now deploy potentially hundreds of environments in parallel instead of spending days or weeks in a single build. Fewer handoffs mean fewer errors and much faster time to value. Uh, and some of the benefits, uh, are more difficult to gather metrics for, but anecdotally we've, we've gone from an ops team that spent a lot of their time stuck in the Meyer paperwork, uh, and then putting out fires for, uh, or, or fielding support tickets for, uh, the rest of the, you know, kind of the rest of the time. To now vanishingly few support tickets, no 2 a.m. fires to put out in my recollection, at least in the past 18 months, and we're spending the majority of our time deploying environments or developing our tools, uh, experimenting with new services or strategies. I mean, there's a panoply of services in AWS to use, and we're just like we haven't even like scraped the surface at this point. Um, so most of our time is spent more valuably now. Uh, quite frankly, we are a whole different team than we were two years ago. Um, and, you know, this kind of feeds into the roadmap in the future, some pie in the sky stuff that is now kind of in work, uh, with this newfound spare time. We've implemented dynamic inventories on our refactoring playbooks into roles. Our current work streams include moving toward an answerable pole-based configuration model, as I mentioned, integrating an answerable into CICD so developers can self-provision environments, optimizing auto-scaling, uh, group metrics, optimizing deployments by varying the EC2 and RDS classes within an environment. Uh, that can be both used for, you know, the deployment performance, for instance, if we just want to get something deployed quickly, we can just scale all the resources up, get it deployed, get over that hump works as a catalyst, and then we just down regulate the sizing until to whatever the baseline is we expect for the user, and then those services can just scale themselves out by demand. Um, and hopefully enabling AMI rotations without data loss because not all the services can be hosted, uh, in ASGs which have otherwise, uh, which would have otherwise simplified the rotation of the AMIs, and these efforts focus on making the experience for, for devs, uh, and users kind of self-sufficient or self-serve and resilient. And more next steps include quantifying the improvements in support load and continuing to expand self-service. OK, so getting to setting the sun on my portion here, um. In order to support our follow the sun operating model, we have established distinct accounts and IAM boundaries so teams have the privileges they need while limiting exposure. This enables distributed teams to work around the clock and respond faster to incidents and equally important, dev environments are now cheap and disposable. Uh, developers can rebuild instead of hoarding. They can shut down, start up, uh, restart, save money, uh, when not in use. Uh, changes like those reduce support calls, late night wake-ups, and improves developer velocity. Uh, overall we're getting closer to a complete cattle, not pets operational model for both our infrastructure as well as our dev environments and quite frankly, uh, on a personal note like. We would not be here if it weren't for the professional services team. They came on board. Aaron Brown, Justin Iravani, who I have the privilege of sharing the stage with, they came in and they, they walked our poor souls through the valley of death. We have no idea what it would have looked like, uh, without them. It it definitely would have been more expensive, messier, and not, not the high quality that it actually is now and keeps getting better. Uh, and I'll, uh, I'll end it there. Uh, I'll hand it back to Jim now. Thank you for your time. Extent, yeah. All right. PLM operations controlling costs. Uh, migrating to AWS services enabled us to Uh, get precise usage-based cost. Now we only pay for what we use. Compute, network, storage, bill by consumption, not bill by capacity. Operating expenses. We got out of the capital budget cycles. We're no longer purchasing infrastructure annually, but instead we get a monthly bill for our infrastructure. Infrastructure stopped when not used. If we're not using it, we just turn it off, which simultaneously stops the cost. The team also developed by leveraging EC2 APIs a self-service webpage where developers Can start stop their VMs without AWS privileges. Infrastructure can be No, I gotta bullet out of order here. Right sized infrastructure because of all Because of all the various EC2 instance sizing, we're able to provision EC2 sizes that are right for each app service and the right quantity for the environment size. We no longer provision for the peak, we provision for the baseline. But the plan for the peak, but plan for the peak via auto scaling, which Dan talked about. Infrastructure can be divested. Super easy for us to decommission things we're not using, just terminate it. We don't need to worry if we need it again because we have the power to provision. Latest infrastructure. We update to the latest EC2 and RDS types. Which are usually less expensive than the old ones, just by shutdown, change type, and restart as the new types become available. Time for improving. Time for improvement We have moved away from the heavy processes that Dan talked about, enabling more time for continuous improvement. At the end of the day, real real savings vary depending on environment type. Monolithic developer sandbox servers with all the services on one host. are approximately the same cost between on-prem and AWS and, you know, that's taking into account the Boeing chargeback model. However, disaster recovery enabled production environments are less than half of, half as expensive because of right sizing and auto scaling. The more servers associated with the environment, the more savings we were able to realize with the move to AWS. Now, let's talk about the actual business outcomes that all this new technology enables. Accelerate operations. Major improvements in time to revision and decommission realized. Avoiding bureaucracy. 90% reduction in requests and approval is needed. Some requests are still needed, mainly around networking, subnet provisioning, routing, information security, managed firewalls. AWS proficiency. Our operations team has done a tremendous job assembling the skills to operate and automate infrastructure in AWS. Follow the sun. We got a couple of bullets out of order here. I'm going to come back to that one. Avoid dependencies. No waiting on other teams to provision what is already approved or needed. Still, some dependencies exist for troubleshooting the areas that belong to other teams, such as networking and firewalls. Cost reduction. We have realized significant savings. Improving configuration management via infrastructure as code. As Dan described, we are fully invested in infrastructure as code and reaping the benefits. The next two outcomes were not specific to AWS, but we use the migration as a forcing event. Follow the sun. We successfully enabled Follow the sun for PLM operations. Tech insertion OS and CPU obsolescence, both realized. And lastly, maintain release schedule. Early on, we established that our PLM release dates anchored our AWS migration activities. We respond to the PLM schedule, not the other way around. We maintain this principle and it has worked well for us. And now, back to Justin. Oh, lost my Audio here. Oh yeah, great. So, uh, thanks Jim. So let's talk about, uh, some of the takeaways. So as you just heard, by leveraging AWS services and partnering with AWS ProServe as well as using the AWS ways of working, uh, the PLM, the Boeing PLM team, um, was able to greatly increase their speed to execute. As well as their operating flexibility, having time to do more things, uh, we heard they were able to reduce the application deployment time by more than 99%, um, you know, kind of end to end, which is, you know, really I think quite, quite a tremendous accomplishment, um. They were able to gain those consistent environments, so developers obviously love that consistency um because of the automation infrastructures code they were able to cut down on that tribal knowledge and so uh the the time to first commit for a new team member is basically within a day you can be deploying these environments. Uh, also there's no longer handoffs between teams, between the DBA team, the storage team, etc. etc. Um, by right sizing their resources they were able to, um, you know, starting and stopping those instances, being able to, uh, right size things they were able to reduce their overall infrastructure costs by more than 40%. So I'm sure all of your CFOs, uh, would love that as well, um, by leveraging the latest in, uh, compute and networking AWS services, the, uh, AWS3DX implementation is snappier, definitely snappier, uh, as described, and, um. Uh, Again, a lot more consistent. So in terms of the overall performance, it's a lot been a lot more stable for the developers to get in and, uh, you know, use their environments. Um, Boeing also gained new features by enabling AWS services such as, uh, easy data replication, uh, across region for disaster recovery and auto scaling. Uh, uh, wise man once told me that's the holy grail for the, uh, 3D, the 3D experience application. So, um, now that we've talked about, um, how modernization led to real business outcomes for, uh, Boeing, let's talk about what does the future of PLM look like. So, uh, PLM systems are about building high quality products. The more time teams are spending on, uh, focusing on problem solving as compared to, you know, being down in minutia and doing a lot of tasks, the better. Um, this is really where generative AI energetic AI, uh, comes into the picture. So using these cutting edge tools, uh, we'll be, you know, we are able to do things like ask the PLM natural language questions, right? Hey, why is, you know, what material is Part A made of? Um, hey, why is Part B 5 millimeters and not 6 millimeters, right? Um, who are the approved suppliers for Part C? So, um, again that you know, being able to ask the PLM, there's a lot of benefits, um, for the developers there, uh, furthermore, by integrating agentic AI into the day to day workflows. We can do things like put an agent in the publishing workflow, uh, and check for say compatible materials, right? Cutting down on or I'm sorry, incompatible materials, uh, cutting down on rework. Additionally, we can also do real-time, uh, so-called bomb or bill of materials, uh, analysis and do real-time change recommendations. Um, we can also do natural language operational activities. So for example, if I get a new vendor rather than, you know, having kind of heavyweight process, I can just in Slack, hey, PLM, add this user to this, uh, to this service. Um, one of the device teams within AWS has been piloting an AI powered, uh, PLM, uh, platform which leverages AWS services such as AWS Bedrock and Amazon Q. Uh, some of the initial feedback is very, very promising. So the feedback from a variety of roles, from sustainability scientists to product design engineers, is that the generative, uh, PLM platform lets them innovate by cutting down on the amount of time, um, that they need to look for things. So really stream streamlining that knowledge is discovery, um, and that, that just saves tons and tons of time for them. It accelerates work flows by automating repetitive information gathering tasks so you can set up a little agent every day to aggregate information on your behalf. This allows the teams to focus on productivity and higher value, uh, higher value activities like innovating that problem solving we talked about, and this leads to increased productivity. Um, it also, uh, enhances decision making by providing relevant contextual information grounded in organizational knowledge resulting in more informed decisions that align with business, uh, goals and priorities. Um, it also improves collaboration by making knowledge more accessible and discoverable across teams. Uh, obviously, you know, uh, a, a big deal to have that information flow happen. This fosters a culture of knowledge sharing and drives continuous learning and, um, success outcomes. So as AI services improve these types of tooling will become more impactful, and we're just getting started. So, um, if you're interested in exploring the future of PLM with AWS, please reach out to your account team. Thank you all for coming. It's been my honor to present our journey, um, to you here today, and I look forward to exploring the future with you so. Thank you all so much. And um please remember to fill out your session survey so that Dan and Jim can get invited back next year and we can, we can see what they did. So, great. So thank you all.