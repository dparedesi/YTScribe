---
video_id: VSDEuFu1sro
video_url: https://www.youtube.com/watch?v=VSDEuFu1sro
is_generated: False
is_translatable: True
summary: "Darshit Pandya, Senior Principal Engineer at Serco and AWS Community Builder, addresses the common problem of \"context paralysis\" during incident responseâ€”where engineers are overwhelmed by a flood of disconnected alerts (alerts from Cognito, API Gateway, Lambda, RDS) without seeing the full story. He introduces a new metric, MTTC (Mean Time To Context), arguing it is crucial for improving MTTR (Mean Time To Resolution).\n\nThe solution presented involves moving from chaos to \"platform intelligence\" using the Model Context Protocol (MCP) and Amazon Bedrock. Darshit details a blueprint using three specialized MCP servers:\n1.  **Auth Flow Context:** Uses Claude 3.5 Sonnet to correlate events, analyze authentication health, and detect bottlenecks (distinguishing between proactive and reactive responses).\n2.  **Service Dependency:** Uses Claude 3 Haiku (for speed) to map service dependencies, predict cascading failures, and suggest isolation strategies (e.g., implementing a circuit breaker).\n3.  **Recovery Coordination:** Uses Claude 3.5 Sonnet to suggest remediation actions categorized by risk (low, medium, high). Low-risk actions can optionally be automated, while medium/high-risk actions require human approval, keeping the \"human in the loop.\"\n\nThe architecture involves Claude Desktop sending requests to these MCP servers, which then act as clients to AWS services (CloudWatch, DynamoDB, EventBridge) to gather real-time data. This data is analyzed by Bedrock models to provide actionable intelligence rather than raw metrics. The session emphasizes the philosophy of \"engineering clarity,\" not just automating chaos, and using AI to amplify human judgment rather than replace it."
keywords: Incident Response, MCP, MTTC, Bedrock, Context Paralysis
---

All right, all right. How are you all? Hope you all had a great time at the reunion so far. Raise your hand if you've ever been paged by. At midnight or at 3:00 a.m. or after office hours. Quite a few, then you are in the right room. You know, most challenging part. In an incident response isn't missing the data, it's missing a context. To achieve better MTTR means meantime to resolution. I'm coining a new term, MTTC is very important. MTTC means mean time to context. But before we dive in, let me show you why this matters. Picture this, it's almost midnight and your authentication service is going down. Your pager goes off, slack alerts are lighting up. Within 1 minute, you are staring at a wall of alerts with tens of cloud watch tabs open on your laptop. And this is the ride begin of chasing the symptoms of your incident. Everything looks critical. Nothing tells you what actually started and. We have data, but no clarity. You can see we have tons of different alerts telling all the details, but not telling the complete connected story. This is the chaos we all know, and this is the context paralysis. Sounds familiar, right? But before we dive further into Let me introduce myself. I'm Darshit Pandya, senior principal Engineer for Platform at Serco. I'm from Auckland, New Zealand. Originally from India, since last decade, Kiwiland is my home. I'm AWS community builder since 2021 in Servalles. Also, I'm organizer of SurOS Meetup Auckland, Platform Engineering New Zealand Meetup and co-organizing ANZ SurS Days and AWS Community Days New Zealand. Feel free to scan this QR code and connect with me. So Here is what we are going to cover today. These are the four key areas. First, the context paralysis. Wide traditional approach, creating chaos during the complex incidents. Second, from chaos to platform intelligence, a better intelligent approach. Using model context protocol and AWS services. Third, from chaos to clarity, the MCB blueprint. A technical implementation which you all can use as well. And last, beyond MTTR building the calm into the cloud means the philosophy that changed everything. By the end, you will have a blueprint of this implementation as well. Let's start with the problem. So what exactly this context paralysis means? Let me show you. Here is what chaos looks like. Alert slams slack. Within 1 minute, tons of alerts are flooding up. Cloud watch, error, RDS throttling, dynamo DB going down, your lambda timeout. Many things we are keeping a story within minutes of the incident as well. The data isn't missing here. It's overflowing. Now, the gassing begins in the war room. Is this lambda chalking the dynamo DB or Dynamo DB slowing down the lambda? Is this the last deployment causing the issue, or is this the cascade, uh, problem? Or is this the upstream problem? Something is not right, but in between all of these things, a single statement cuts through the noise, is, is this fixed yet? All the stakeholders are asking the same questions. Customers are waiting for that one. And in this time, everyone is staring at the signals, but no one can see the story. That's a context paralysis. Too much data, no shared map. Let me show you what this looks like in the architecture. This is an e-commerce service architecture, standard, robust, and surveillance pattern. But during the incident, it becomes a chain of cascading symptoms. First, we get an alert from Cognito. Immediately it's trigger 4 to 9 or 5 XX error. For Amazon API gateway. This causing lambda functions, error rates spike, cold starts starts increasing, the system crashes, and finally, Lambda is in a retrial loop hammering RDS instance. The problem. These alerts are separate. Whenever we are receiving the alerts from our microservices or authentications lambda. We are receiving alert, but in a separate. In the war room situation when you have an incident. Under pressure The human brain struggles to reconstruct this picture or this sequence to identify the root cause. And human miss obvious pattern when they are stressed. What if AI could organize this chaos rather than adding on top of that? That's exactly what we will see in the next. So, how do we move from chaos to platform intelligence? The solution isn't another dashboard. Not another alerts or not another metrics. On the left, you have a chaos, and on the right, there's the actions. That's the traditional approach everyone is using till now. But when you add MCB server, that's model context protocol, sits in the middle, providing the context, they correlate signals, apply AI analysis with Amazon backdrop. And present you with a meaningful actionable intelligence instead of raw data. I have built 3 specialized MCP servers. First, off flow context, which analyze your incident. Second, service dependency is going to isolate your incident. And 3rd, is going to provide you recovery, coordination to execute as well. Let me show you each one in detail. Outflow context is your authentication intelligence engine. It has 3 core tools, correlate your off events, get authentication health to seeing that what is cascading impact happening, and detect the bottlenecks in your incident. This pulling the data from Amazon Cloud Watch, Dynamo DB and Even Bridge. It's talking to the Amazon Bedrock with cloud Sonnet 3.5. We are using this uh cloud sonnet 3.5 for complex pattern recognition. This is the difference between proactive and reactive response. Next, service dependency mapping. Service dependency is your cascade failure prevention systems. Four key capabilities are there. It map your service dependency, predicting the cascade, suggests isolation, and create dependency visual. Here, we are using cloud Haiku 3V1. Because it's required to generate a faster response which can show you where is the problem in your entire system. Thus containinginate before the. Any problem in the entire complex architecture. And finally, recovery coordination. So recovery coordination. Is going to keep humans in the loop as well. It's not going to provide direct resolution or executive steps as well. Suggest the remediation actions. Execute low risk action items. It will categorize low, medium and high risk action items as well, while medium and high risk, it will require a human approval so humans can check what suggestion is coming out. Shall I execute? Is it going to increase my bill? Is it going to increase my insurance, so many things it can provide you information as well. And for this one, we are using Cloudstone 3.5 for remediation actions, generation, risk assessment, human approval, and recovery process prediction. Now, let me show you how all these 3 MCP server works together. So, this is for chaos to clarity, the MCB blueprint. This is complete surveillance architecture, MCP intelligence blueprints. Cloud desktop. Sends a request to MCP server. That appropriate MCP server calls AWS client like CloudWatch, Dynamo AB, AWS Lambda, and EventBridge. Gathering real-time data is then sent back to the Amazon Bedrock. Amazon Bedrock will pick associated cloud model. And it will return a proper analysis from your traces, metrics, and, uh, logs as well. This isn't a theoretical. This is the production ready which you can use in your AWS environment right now as well. Let me show you these inner actions with some real snippets when I, I was working on this one as well. This is the outflow context. When the incident happened, you guys have seen that the slack alerts flooding it with the incident happened. When you are asking to, you know, like analyze my current incident and provide the context, first outflow context will kicks in. It will start analyze. It will provide you immediate suggestions as well that what is the current status, which services are impacted, because when microservice architecture, you cannot cover everything, but here is the thing, it will tell you that which service has the most impact authorizer, order service, and payment service. Those are the critical services is having an impact right now. And In between, we can see that lambda cold start detect in the auth pipeline that causing 800 millisecond latency. That's the predicted cascade to users and other services as well. That's the dependency. That's the context, not just metrics, that's the actionable intelligence. Service dependency immediately maps the blast radius. So, instead of guessing which service might be affected, we had a clear visual cascade path for isolation and remediation. That is a circuit breaker. Cascade failure path. Here you can see that in a different color with amber, green, or red, that shows that where is the actual problem and what is about to start as well. Means cascade failure path, authentication to user to the order service, estimate the impact, suggest the isolation, circuit breaker on a user service. That's important. This predictive incident response, we are getting ahead of the problem, not to chasing it. And recovery coordination provide the action plan. This is the 3rd MCP server, which is going to provide you recovery coordination and suggest this specific action with a risk assessment. Low risk actions enable here user can just click the execute button. It will take executions. Those actions will be fine and executed into the production, medium and high, it's required a human approval. And once you're like, you know, like the line manager or on-call incident manager is approving that request because of the sensitivity around, uh, it will also going to execute in your production environment as well. This is the production ready incident intelligence. This brings us the philosophy behind all this, that context matters to achieve right MTTR so focus on context. But this isn't just about a faster incident response. It's about beyond MTTR. So This shows the paradigm shift, the traditional approach, and number of smack trikes, threshold breaches, slack alert floods, human stress. But with MCP and Amazon bedrock, real-time correlation, Amazon bedrock analysis, MCP informed clear context, and human in the loop, we are not replacing human, but we are empowering human. Intelligence amplification through contextual correlation, not metric multiplication. As the Varal Wagas reminds us. Everything fails all the time. And this is the fundamental truth of distributed system. The question isn't about how to prevent a failure. The question is, how calmly does your systems help you to understand and respond to what happens when this happens. That's what we are building here, not the failure prevention. But calm and intelligent response in the production. Here are the key lessons. Three key lessons, context with metrics. That's a reality. AI organize and human decide. Still, humans have a power, but to decide if this suggestion is right to execute right now or I can wait for a while, and this will help you in the post-incident action item as well. And designed for calm. AWS and lambda, AWS Lambda and MCP server is helping you to building those things into your production environment. Intelligent amplification through contextual correlation, not metric multiplication, and let me leave you with one final thought. Don't automate your chaos. Engineer clarity. This is the most important lesson that most of the engineers try to automate their existing chaos, faster alerts, more dashboards, more metrics, bigger on-call rotations. This is just automating the problem. Instead, engineer, engineer the clarity into the system itself, build the context of our agents that understand your architecture, use AI to amplify human judgment, not to replace it. Create calm interface that works consistently under the pressure, pressure. Your next incident is inevitable. Your response to that doesn't have to be chaotic. Thank you I would love to hear it from all of you, how you're going to solve your incident response, and would love to connect with all of you as well. Please scan this QR code, and thank you for attending these sessions.