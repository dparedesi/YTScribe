---
video_id: YxkmvQxjWFk
video_url: https://www.youtube.com/watch?v=YxkmvQxjWFk
is_generated: False
is_translatable: True
---

So, we wanna, I wanna start by asking a question. Who here, uh, whose title like or job description officially like has the word FinOps in it somewhere? How many hands? Raise your hands. You got like 4 or 5. OK, maybe like 10. Probably about what, a quarter of the room or so. Uh, who's in the second group where, uh, your, your title has absolutely nothing to do with FinOps and you've kind of accidentally stumbled into having to do cost allocation or cost reporting? Yeah, my hands up for this one. Awesome. Um, so I'm Justin Marks. I'm a senior technical account manager with AWS Support. Uh, Jason, so this is Jason, product manager for AWS cost and usage report and the focus. Cool. And, uh, and we're gonna talk to you today about advanced multi-cloud cost reporting with Focus. Um, but before we get into the focus topic, I wanna take everybody back through probably a, a common scenario you've dealt with, right, uh, over your career of dealing with cost and usage data in the cloud, um, so. You had a team who started using AWS and you had to figure out 4 things about uh cost and usage data, how you're gonna collect that data, right? Getting the reports turned on, where you're gonna deliver them, um, and then you're gonna have to figure out how to normalize it, get it into formats that the finance team cares about, and get it into formats that your engineering teams or the teams that want visualize, like how to get it into those formats. Um, you're gonna need to figure out how to analyze it. What tools are you gonna use? Are you gonna go pick one off the shelf? Are you're gonna try to wing it with, uh, you know, some of the native tooling, uh, depending on which cloud provider you're with. Um, and then how you're gonna derive insights out of that data. And finally, how to visualize it, right? Ultimately, folks end up building dashboards and uh trying to find ways to build reports for their executives on how this is going. Um, and it stops there, right? Um, it wouldn't be a multi-cloud conversation if it didn't stop there. So, then you get another team who comes and says, hey, we use the SAS platform, and this SAS platform has costs and usage, and we need the data from there too. And we'd like to integrate that in our reporting pipeline. So you have to go figure out these four things for that provider too. Um, and then when you know it, you hear about this other team that's running in another cloud, and now you gotta go figure it out again. You can see where I'm going with this, right? Um, so, Having to relearn these things every time you adopt a new provider, you use a new SAS platform, um, it can be hard. And Jason, what do we, uh, what do we have to help folks manage this complexity? Sure. So, the thing of practitioners spend countless hours every day to normalize the data. And this is because different providers out there define the data differently, and we want to solve that problem for our customers and we want to solve it at the root. So organized by the Final Foundation under the antitrust compliance that we work with different field of practitioners from different industries and created this new open specification called Focus. So Focus stands for field of Open cost and usage Specification. It provides consistent cost and usage data, such as billing, pricing and cost matrix. So, let, let me take you through a quick uh journey, how the focus journey would look like when you start with the focus. So when you start Focus, you will first get the data from your data generators. This can be your cloud providers, can be a SAS providers, or even your data center providers. So Focus is an open specification anyone can create. And as of today, the majority of the large cloud providers has adopted Focus, and we are seeing increasing trend for the SAS providers to adopt Focus. This will give you a universal view of your cloud cost cost and use data across all your technology stack, not just limiting to cloud. So once you get your data from your, your providers, and you injecting that data to your FinOps two platforms, this can be Amazon Athena, which we will be demoing today. And for to query the data directly. And you can also visualize your focal data using the Amazon Quick site or even plug those data into Excel for uh for different insights. And once you have the data injected to your FinOs data platforms, you can analyze the data for different use cases such as demonstrating your business values, finding optimization opportunities across the the vendors that you have with. So now you understand how the focus in look like in high level. So let us do a quick exercise. Uh, Justin, can you help us to run a quick exercise to see where people are with the focus journey? Yeah, we're just trying to see where folks are. Um, and I think I may have asked a couple of you outside, so I have a little bit of a sample to base this off of, but hands up if you've, uh, seen the focus spec, like you've read some of the documentation out there, it's about half the room. Um, OK, so for the folks with their hands down. You've now seen part of the focus back. Um, it is a, it's a big document. Uh, it covers things like, uh, normative requirements, uh, different columns, things about metadata and attributes for the cost of usage data set. Um, this is an example of availability zone, right? You get a nice definition here. Um, it explains the intention. It calls out, uh, you want, in the bullet points here, the first bullet point says it is recommended column, right? So it's not mandatory. It may or may not be populated. Um, that'll be important later. Uh, it's type string, things like that, right? So we can scratch this one off. Everybody in this room has seen part of the focus spec. Uh, so let's do hands up if you have focus reports like created and you're playing around with that data today. A couple of hands up. So about half the room, OK. How about if you have analyzed or visualized this focus data before, so it's not just there, you're, you're actually like using it, your teams are looking at it, hands up if you've. OK, and, uh, how about if you've joined AWS focused data with other providers, the show quick show of hands, who's pulled in multiple providers, including the SAS providers, including SAS and other, OK, we got 4 or 5 hands. OK, great. So I mean it's a pretty good understanding of, uh, we have a good understanding now of the experience of focus in the room, so. Uh, Jason, do you want to talk to us about how, for the folks that have their hands down who haven't created the reports yet, how they can create data, uh, create focus for AWS? Sure. So, AWSS data exports platform is the centralized data hub for you to export all your FinOs related data set. As of today, we support 4 different, uh, table types. The first one is AWS cost and usage report 2.0, which is our native supported cost and usage data, and it provides the most granular Abase cost and usage data. And the second is the focus. We currently supporting 1.0 and 1.2 that we just launched two weeks ago. And third is the cost optimization recommendations which pulls the data from cost optimization hub to help you to identify the cost optimization opportunities like right sizing or savings plan recommendations. And lastly is the carbon emissions exports which provides you the carbon footprint data associated with your database usage to help you to better track your sustainability data. So, let's dive a little bit deeper to your focus data, to the focus data set with database columns. So we launched Focus 1.0 last year during the ring event, and Focus 1.0 includes the 48 columns total, 43 columns from the Focus 1.0 spec itself, and we added 5 additional AWS columns. This would help the customers to better navigate the AWS resources with Focus itself. And there are 8 columns with performance gaps, and we have published those in the performance gap report, and we also did that in the 1.2. So, as mentioned, Focus 1.2, we have just launched two weeks ago, Definitely encouraged to have a try for this uh enhanced version of a Focus specification. So Focus 1.2 provides 14 new additional columns on top of Focus 1.0. It includes capabilities like capacity reservation ID and the status columns help you to better manage your capacity reservations. We'll break down your capacity reservation use and unused status. It's included the invoice ID column, which help you to better reconcile your cost and use data with the uh with the uh physical invoices. It also includes the capabilities with for SAS providers to generate the um focus data set due to the newly increased, newly introduced the pricing currency columns. So enable SAS providers to integrate their uh pricing uh in the terms of tokens and virtual currencies. And it also added more granularity for skill usage and uh for skill usage analysis. Uh, this includes your usage type and also product attributes. And last, uh, last one is the account structure classification. It helps you to identify whether your usage is coming from your management account or from the member account. And one more thing not highlighted here is Focus 1.2 also start start supporting hourly, daily, and monthly time granularative which Focus 1.0 today we are only supporting hourly. OK, Justin, you gave a great presentation last year about CR 2.0, and this year you are giving a presentation about focus. What are the differences? Yeah, so who here feels, uh, it's we had half the room have like played around with focus or turned it on. Uh, how many folks feel comfortable with like cost and usage reports, they've like played around in Kerr? All right, a couple more. Um, so I, I think it's valuable to just take a minute here and frame up. Um, some of the, uh, relations and stuff between the two data sets. Uh, one of the most common questions I've got about focus up to this point is like, do these columns map to each other, right? And how do we map curtude values to focus values. So here's a quick example, uh, non-exhaustive list. This is 8 columns that I picked out that map 1 to 1. Um, so this is things like your, you know, account information, uh, the charge periods. Um, things like the service names, uh, resource IDs, and tags, right? So if you look at usage, rows and cur 2, they map directly to us, the values that you would see in focus one. There are also columns like blended cost, um, and line item type that are related, but the values aren't going to map 1 to 1. Sometimes things will, uh, will map, and sometimes, uh, there are requirements in focus one. Uh, for specific values that you're not gonna see incurve. Um, so they're, they're related, uh, one informs the other, the Curve two values, uh, inform what you see in Focus, um, but they're not gonna map 1 to 1. Um, there's also some net new columns that you get from adopting focus. Um, these are things like effective cost. Um, who here is a fan of using net amortized View and Cost Explorer? Hands up if you've used net amortized View. Uh, if you wished, that was in, uh, incur. Uh, we don't have a column in there today, but there are ways. I'll show you, uh, in a few minutes how to get this value out of the cost and usage reports. But, um, effective cost is, uh, essentially the net advertised cost from Cost Explorer. Um, so if you adopt Focus, you can expect those values to align. Um, we also have things like service category. Um, so if you, uh, were using Kerr to analyze all of your storage usage, let's say, or all of your compute usage, you would have to build a big ware statement, uh, add all the services, usage types, etc. that would, uh, that you'd need to track there, um, and report on, but you can use service category and focus, and, uh, and it's already categorized for you. Cool. And, uh, and like Jason had called out, we do, uh, ship 5 custom columns into our focus reports, uh, via data exports. Um, in the spec, they call out that, you know, we can prefix, any provider can prefix custom columns with an X underscore. Um, these are valuable if you used some of these curved two auto values. Right, if you use cost categories today, um, or if you're taking a look for, uh, like operations or usage type, um, you can pull those values using these X_ columns, um, and then you'll see in our examples, whenever you're joining these data sets from multiple providers, you're gonna want to leave these out, um, unless you want a bunch of nulls everywhere. So. All right. And then finally, before we get to the demo, I want to take a minute, uh, a few minutes here to, to just talk about the architecture of like what we're working with today. Um, so we have this FinOs account, right? Uh, this is where we're gonna do all of our analysis. Um, we're not gonna run this in a master pair because best practices say we shouldn't run anything there if we can help it. Um, and whenever you're trying to work with your AWS focused data, um, we're gonna pull that data across, right? We're gonna replicate it from the bucket, uh, in your master payer account, we're gonna move it into, uh, the spinoffs account for analysis. Um, and then you're gonna tie glue together. Glue's going to, uh, crawl it, add it to the data catalog. And, uh, and then we'll use Amazon Athena to query that data. Um, and if we were just doing AWS data today, this is where this architecture diagram would stop, but we want to show multiple clouds. So depending on, you know, the setup, you can use something like the uh AWS Glue connectors to reach out to various data sources and pull other focused data in, put it in an S3 bucket. Um, you can use database lambda to go out and grab things out of, uh, more custom kind of places if needed. Um, and some providers may provide options to push that focus data into an S3 bucket. So, um, a couple of these examples, uh, your use case may differ depending on which one, but essentially the, the common thing is get the data. And S3, uh, we're gonna use glue to do any transformations if needed, uh, and then do the cataloging so that it's ready to go. Uh, and then finally, to tie it all together, uh, today we're using a consolidated view in Athena, um, that unions all of this data together to give us one place to query, um, and we'll show you what that view looks like too. And it doesn't have to be Athena, um, you know, uh, I jumped ahead. Uh, you can visualize on top of this too. So Athena can be a data source for services like Amazon Managedrifana, uh, for Amazon Quicksight, um, for any other BI tools that can tie into Athena. Uh, as a data source. And then once you get this consolidated view, you can build whatever graphics, visualizations you need. And again, it doesn't need to be, um, it doesn't need to be Athena. Uh, customers regularly use Redshift here too, right? They may do scheduled data loads or may run lambda functions to get things loaded into Redshift, uh. Um, and then use things like the, um, the Redshift query, uh, editor V2 to query that data, um, and then same kind of deal, any, uh, visualization, um, solutions out there or services like Grana and Quicksight that can use Redshift as a data source, um, would work just as fine too. So, let's, uh, let's see focus in action. Um, so, the first example we'll have on our demo is talking through that Athena setup. OK. Quick spot check, can everybody see this? Is this big enough? OK, cool. Thumbs up. Great. So, uh, we're here in our Athena console, and, uh, just a quick tours, you know, we have our own, we have a database set up just for focus samples. We've pulled in a number of tables, uh, of example data that we've been able to pull together, uh, from the Azure FinOps toolkit, um, from the FinOpps Foundation, um, and then our focus reports. Um, and then we built this consolidation view down here, which is what we're going to be taking a look at now. So This, uh, this view is, is an example view that I've actually borrowed from the, uh, cloud intelligence dashboards. Hopefully you've heard of them before, um, but if you haven't, um, you know, it is a, a solution out here and they have a focus dashboard, um, available. And Uh, inside of here, you can see a lot of very similar architecture diagrams and, uh, out here in their docks, they actually have a published focused consolidation view. So this is the same way that they, uh, have built, uh, built their dashboards. So we've borrowed some inspiration from them, um, and then just tuned it kind of as we needed for the demo. Um. This, uh, line for here is pretty important. So, um, remember how I had called out the availability zone, uh, example we had shown from the spec, um, It's not a mandatory field, so it may not be there. So there are times where, uh, we use nulls to, uh, to add these columns to the unions work. Um, so our first one here is just pulling in our focusv1.0 report. Um, and you can see we haven't changed much. It's, uh, we've selected just about every column except for the custom columns. Um, the one thing to call out is that our tags field is, uh, it's a map. Map type map stream. It's map stream. So, um, and the other examples that we had were, were in JSON, um, and that would be a little tough to work with. So we did, uh, cast our tags to JSON and do a format, uh, just so that things are nice and consistent today, um, not necessary, but it's gonna make the demo a little bit easier. Um, and then we do a union hall here where we pulled in the Azure sample from the FinOps toolkit. Um, and you can see places like availability zone here, we've nulled out because this sample data set didn't even have that column, but for the union to work, we needed to have something there. So it's filled with nulls for now. Um, and then there are some places where we've done some timestamp formatting, and that's just again for consistency. Um, so like casting from, uh, ISO 8601 to something a little bit more friendly for us to work with today. All right. And then finally, we did one more union all here with the Finns Foundation sample. Um, and this includes lots of sample data from multiple providers, um, very similar story, just dealing with, uh, some formatting for, for date times. Um, for this one, we did have to use coalesce because, um, some of the samples had null values, and I don't know if you've ever tried to multiply or divide by null, but it doesn't work out very well. Um, so we used coalesce here to replace any of those null values with zeros. Um, And that's, I mean, that's it. So this view, and I can give you a quick preview of what kind of what we're dealing with here. Again, this is um just a quick sample of like the first, you know, couple of, um, first couple of columns that we see and I It's everything that I guess we would have expected. So let's, uh, let's go ahead and close out of this. Now that we've seen how to set this up. Jason, do you want to walk us through, um, some of the cost columns and how to derive savings and how to think about like how the different cost columns work? Sure. So as the person who manages cloud costs and usage data, you, you will likely to be asked the questions like how much have we spent on the invoice last month, or how much money would our usage cost going to be look like if we do not have any commitment purchases, or how much would be the amortized cost if we purchase everything upfront in with the savings plans. So this can be explained with the focal data set via the 4 standardized cost counts today. Let me just quickly show you that. So in the focus data set you can uh you can sum up your build cost which is very simple as a bill cost and this gives you the the cost basis of your invoice. And for running this query, and you select them. And just hit run and this will give you the exact amount that matches to your invoice. With Focus 1.2 invoice ID columns, you can actually add it in your invoice ID column and will show the invoice that you have paid for each of the, the invoice ID. So this is to help you to expand the first question. How much money have we, have you spent with the invoice across different providers? So let me remove that and answer the second question. How much would we spend if we do not have any discounts or commitment purchases with database? So, for this question, you can use the list cost column, which is the equivalent to the on-demand cost that showed up in the data set. And for this column will show your usage cost without any discounts and commitment purchases. For correlated content, you can use a contracted cost, which represent the cost with the, the discounts but without the commitment purchases. And third, to answer the questions is what would be the, what would be my amortized cost going to look like if I purchase everything upfront with the same spec. So this where Justin will give a more deep dive here, but we introduce an effective cost column. Which shows your amortized costs with all the discounts and with all the commitment purchases, reduced rate impact. So after knowing those three cost columns, you can do a simple math by using your list cost to minus your contract cost. Do you know how much savings you have achieved on the discounts? By using your contracted cost. And to minus your. Effective cost, you will know how much commitment purchase savings have achieved with your. With your account. And you can also even do the, the percentage of savings by using your sum of this costs minus your sum of effective cost to get your total savings and divided by the sum of this cost. And times 100 to get the percentage. Discount So in here, we have to make a note that. You probably already heard I keep emphasizing usage. In focus data set, the usage record and the purchase record are both represented in the 90s. In order to understand different hypothetical scenarios with discount without the commitment, you will need to apply one more filter, which is called charge category, and you want to navigate to the usage record only. This is because for the situations like when you have a savings plan purchase up from with $100. And if you have spent that $100 on your cover usages already, if you don't apply this filter, you will end up uh getting your list cost of $200. So we want to make sure when you do the analysis for the usage, make sure you apply this filter into your record. OK, let's want to query. OK, so you may, you may notice that we've been highlighting, uh, subsections here. Uh, that's a way in Athena to, uh, to run like just sections of your query, uh, with a bunch of the other stuff that we have further down at my break. So just highlight everything that you want to run there. Oh yes. Oh, in line 7, yes, contracted costs. There we go. Let's catch you guys already focused, satisfied. If we make typos, please yell them at us. OK, it's me, that's right. OK, let's run it again. Perfect. So this shows your usage record as showing if you do not have commitment purchases or discounts, you will end up paying for the same usage for $5130 and with the discount it's going to be less and your amortized cost is going to be $4300 and your discount savings $537 and you will also know your total savings discount of 16%. So, let me hand over to Justin to do a more advanced techniques for the focus queries. And one thing you need to know is what the queries that we are showing today, you can also run against the focus 1.2 data set. So it's backwards compatible. All right, so let's do a super quick demo here of uh advertized costs and effective costs. Um, we talked a little bit about how this is possible with Kerr. Um, so I wanna give you just a quick, uh, glance at what that would look like. Um, so if you're familiar with Kerr 2, You'll know that this may be a little harder than, uh, you know. Than it would be if you used focus. So let's see, we want to grab billing, start date. Let's grab the, uh, we're doing a multi. Cloud talk today. So let's grab the billing entity and the payer. And uh we'll add this amortized cost column, right? But that's not a column here. So, um, If you've, uh, has anyone ever used the Kerr query library, cost and usage query library? Hands up if you've ever heard of our library out there. Um, so we have this, uh, it's part of Well Architected Labs, uh chock full of, uh, Kerr and focus queries if you're looking to get started with some of this analysis. Um, and one of the ones that we have out here, um, under our library help section is, um, how to calculate amortized cost with, with Kerr. Um, so we've already done the hard work here. I'm just gonna borrow this so I don't have to take it all back out. And uh we'll paste it in here. And then we're gonna focus in on our sample data set, uh, has all this data from September. Uh, I'm gonna borrow Jason's filter. All right. And then we're gonna see what our advertised costs look like in this payer that we're running it against. And I didn't add my table. I'm not working in the same space on this one because this is our focus sample, so I'm gonna grab it out of a different database. And you know what, I did this earlier, um, I already used the focus column and I shouldn't have. There we go. OK. So you can see here to get advertised costs, um, we had to copy and paste this like nine-line, um, case statement where we're evaluating different uh parts of the, the values here to determine what number we should use. And you can see for this payer account, we had 22835 as amortized. Um, but we said effective cost was net advertised, right? Uh, so how do we figure out net amortized? So we have to pull in. Those net fields, uh, those net columns, um, and unfortunately for this demo account, there's no discounts. Um, so a lot of those columns are null. Uh, we're using coalesce here to make sure that we handle those nulls correctly. If you've never used coalesce, um, we have, I have another example in here and I'll show it the next time, but on the next query. Uh, but we're just gonna copy and paste this, uh, net advertised one over our advertised costs. And what it should do is any place where there's a value, um, in our related net columns, it will use those first, else it will, uh, It'll grab our non-advertised ones, so this should give us the same values. All right, there we go. So this payer, 22835, great. So what would this look like if we were just using focus today? Um, so the same exact query and focus is just selecting, um, you know, billing period start, provider name. Billing account ID and uh an effective cost instead of uh a nine-line um case statement. And we're gonna grab this from our Focus Consolidated view that we set up earlier that should Give us not just this payer account, um. Oh, some effective cost, yeah. Gotta sum this up. Great. And what we're getting now is not just our 22835 here, right? Same value uh from this payer, but now we've pulled in our data uh from other providers, other accounts. Um, Cool. So that's uh one of the ways that you can save some time, not having to build out your own uh effective adverts, net advertised cost um field. So, let's talk about the last, uh, the last example we have here where um we've been tasked to figure out where workloads may be running across accounts and or providers. Um, let's say our, uh, our boss has come to us, you know, and they, um, They've asked us to track down um where like our FU and uh and and bar uh applications are running and where they cross. um. Cross, so an example of the data might look something like this, right? They show up with a crude drawing that says, show me where like uh production foo is 50% in one account and 50% in another account, uh, and maybe where things uh span different providers as well. Um, so this is kind of like what we're working backwards from today. So the first thing we want to do is pull in our cost data, right? Let's figure out um Where all of this is. And this should give us, uh, again, very similar, uh, view to what we saw in our, uh, In our effective cost when we just did, but we brought in the account name now so we can kind of track down providers' account names and how much that effective cost was. Um, So now that we have that. Where, uh, we need to pull this across apps and environments, right? We had production, we had development. Where are we gonna pull this from? Anybody have an idea where we pull those kinds of values from? Tags, exactly. So, uh, what do the tags look like? So let's, uh, let's query our, our tags column here. We're gonna use distinct so that we can just grab, uh, unique values, uh, so we can have an understanding of like what we're working with. Um, and great, you can see it's all JSON, thankfully for the, the JSON format we did in the view. Um, but you can see here that Uh, in some places we've used, uh, ENV for the environment key. Uh, some other teams used environment. And, those teams, sometimes they don't follow the tags exactly as we hope, right? Uh, and then sometimes we have things like application, uh, in other places we have, um, app or something else, right? So we have to be able to like deal with all of these different values. Um, so, How do we extract some of these values, uh, using Athena? Um, so, I've come up with two different ways that I think we could work this. One of them is using JSON extract, um, one of them is using redjack. Um, which one, does anyone have a preference? Hands up if you think we should do the JSON extract one first. Any idea? Any preference? How about the Red Jacks? Anybody interested to see the redx one? We'll start with the JSON one. So, the way JSON extract works, um, this is the general pattern you'll follow. You're gonna call this function, we're gonna run this over tags, and then you'd want to run the, run this against the path. So if it's, you know, tv, um, for the key, that should work just fine. Uh, but like we saw, if we run this, it's only gonna give us a subset of them. Um, so we have like a rough idea of what some of the environments are if they use the ENV key. Um, So in this example, we used another coalesce here, um, which again will give us the first non-null value in a, in a set of values that you feed it. Um, you can use, there's other ways to do this too, but we're gonna stick with coalesce today where we can, uh, like copy and paste this and grab like, you know, EMV, we'll grab like capital EMV, um, we'll grab environment. And this should give us An expanded list of values here using JSON extract. Again, this is gonna be dependent on getting everything into like a nice consistent JSON format, um. And we can see here that we have a couple more values in here. Great. So, um, So what if we don't have everything in a nice pretty JSON format? Well, we can still, uh, use things like regex to uh just run it over a string, right? And JSON is also just a string, so it works just as fine for that. Um. Same kind of syntax, where we're gonna call regx extract. We're gonna run this against tags. Um, and I am not gonna type a regex pattern out in front of all of you, unfortunately. Um, so, and I'm gonna, these are all gonna be shared in the code examples at the end. Um, but, you know, here are some examples of, uh, different ways that we can use reg-ex extract to get things like environment. Um, so we're gonna Drop this here and talk through it real quick. We're gonna run red jacks against tags. This is our pattern. We're looking for any keys that have one of these four environment patterns, uh, and then we're gonna be, uh, finding anything that's, uh, after the space colon and double quotes, and we're gonna grab that value. So that's what this should do for us. Um, and let's see what that looks like. See if it looks any different than. Our Jason extract. OK. So, you can see, uh, with our regex pattern, we actually captured a little bit more values. So, um, a little bit more flexibility here, um, and I think it's a lot nicer than having, you know, a coalesce or something with 4 or 5 different values in there. Um, And the other difference here is that you can see Jason extract wraps things in double quotes. Uh, the regex, the way that I wrote it won't, uh, won't wrap it. So let's, uh, we, we got environment. We still need applications. So let's go grab our pattern here for application. And you can see I've accounted for uh capitalization changes. I've accounted for some of these teams using things like project and the FinOps team use their own FinOs tag. Um, so we've, we've pulled in the data for all of these, and, uh, we'll go ahead and just take the JSON extract out of here for now. This should give us, um, A list of distinct combinations between uh environment and application. So, now we can see, uh, these are all the environment and tag uh application combinations uh in our focus data. Um, cool. So this is how we're gonna need to pull this data and to join it with this cost data now to start figuring out that distribution. Um, So we figured out costs, we figured this extraction, we're gonna join them now, and we're gonna try to figure out if this Foo app or whichever one of these apps is split like fifty-fifty between accounts, 60/40 across providers, something like that. Um, so the first thing we're gonna do is take that, uh, that first query we typed up and we wrapped it in a width statement. So that lets us take these results down here that we've been seeing the whole time we're running them and put them in like another temporary table so we can do more work. Um, so you can see here, um, we'll run this subsection again, just so folks can see we've added some effective cost into the mix here. So this should give us, um, our provider, our account name, the environment and the app tags, and then this effective cost. So you can see we have, um, you know, a couple of production apps here costing a couple of dollars in a couple of different places. Cool, so let's. Now that we've wrapped that in the width, we can go ahead and run, uh, run queries against this result set down here. Um, what this is gonna let us do is we're gonna use window functions. I don't know if anyone's ever used a Window function, but we're going to, um, for the sake of calculation, uh, add each like total cost to the row for either that environment or that app so that we can do more, um, more calculations here. So we're gonna do some effective costs and we're gonna use over here. That's gonna give us our window function. We're gonna partition this by uh the regex environment tag. And What this is gonna give us is, I'll show you that column that it adds. So what this is saying now is um. We have this last column here that shows us the environment cost, right? So if we scroll over here, this is saying like our, our tag for non-production environments costs a total of $7938 which doesn't seem particularly interesting when it's the same thing as effective costs, but as we scroll down and take a look at like prod, this is our prod environment costs us $314. Um, that lets us do some calculations uh for like how much this row lines up with, uh, with its aggregate here. Um. We're gonna call this total and effective cost. And then what we'll do is we're actually gonna add a second calculation because we're not doing this just for environments. We want to do this for apps and environments. So we're gonna add the, uh, the app tag in here as well. And what this will do is this will show us our environment and application effective cost over the whole data set. All right, so in this example, we now have some dev stuff up here. Uh, we can see that um development costs us about 1391 a month uh for that particular month. And that, uh, this particular row, uh, this particular, um, app and environment tag, so active bolt matrix and dev, actually didn't cost us anything that month, but there'll be examples further in the data set here that uh where that's not the same. So these zeros aren't particularly helpful. Let's go ahead and filter those out. So we're gonna take some effective cost that's greater than 0, and now this should give us the more interesting. Set to poke around in. OK. So now we can see, um, we have a couple of apps here. Prod F, right, has, uh, had different costs here in different accounts. Um, and the total uh app in here cost was like 38 bucks. And this is where we're gonna do this math. We're gonna take like this rows effective cost, and we're gonna divide it by, uh, this combination cost to start figuring out where, uh, where things are distributed. So, we're gonna wrap everything in another, uh, width here. Um, and we're gonna call it percentages, right? So we have our first query that we wrote, uh, we have the second one where we've, uh, added our, our, uh, our window functions. Um, we filtered out these zeros. We're also gonna filter down the production, right? We don't need, uh, we're just trying to track down production costs right now. OK. And then we're going to filter out usage uh with no app tag because we saw a couple of those. So we don't need to see ones where there's no app tag. And finally, um, we're gonna run one final query over like all of that so that we only see what we need. Um, so we're gonna, we're gonna select things like provider name, account name, um, and then, oh, we still need to calculate the percentage. So, um, so what we're gonna do here is, um, we took our sum effective costs for the row, right? Which is here, uh, we're gonna divide it by, um, the aggregate like this window function, right, for that function, and we're gonna multiply by 100 to get the percentages, um, and we're gonna select what we need out of this set. So this should Give us provider name, the accounts, the environment app, the cost, and then this, this like split, right? And the other piece here is we only want to see places where it's not 100%, right? If, uh, if Foo and Dev only runs in one place, that's not interesting. So we're gonna filter out um places where this app or the percentage is less than 100. And this should give us that report that we were kind of chasing. From here. What did I miss here? Oh, I have 2 from. Copy and pasted it wrong. OK, so OK. And what this is showing us is we have uh two apps here, uh, one called Foo, one called Data Bricks, uh, that run in production, and they're split, um, both across providers and accounts, and we can track down, um, that, you know, Prod Foo runs, you know, 2, 2.5% in, uh, this AWS account, and it runs about 50/50, uh, 50/46, and these two Microsoft accounts. Uh, and then finally for data bricks, we can see, you know, a similar distribution where it's like 99% in this one AWS account. Uh, and then there are, there's some more codes out there that are tagged as proud data bricks, uh, running elsewhere, so. Cool. Let's sweep back to the presentation. Yeah, so finally, uh, we wanna talk about visualization real quick. Um, you know, we poked around in Athena, uh, looking at SQL results can be nice, but it's not as pretty as some of the graphs that I'm sure a lot of folks, uh, would like to have. So, uh, I wanna call out, uh, again to the cloud intelligence dashboards like that team has done wonderful work. Um, it is an out of the box solution, uh, visualizing on top of all the. Data that we, uh, kind of presented to you guys today, um, it has the, uh, that consolidation view, uh, they have integrations documented for how to pull in data from Azure and Google Cloud and Oracle Cloud. Um, today they support Focus 1.0, uh, but they have plans, uh, to add 1.2 here soon since we just recently, uh, announced, uh, 2 weeks, 2-3 weeks ago that it was out. Um, so keep an eye on their, uh, their change logs that's coming soon. Um, Jason, let's, uh, sure, wrap things up. Yeah, thanks, Justin. The cost allocation and cost attribution is always a very hard problem to solve in the cloud cost management, especially in. Of multi-cloud or SARS vendors. So Justin gave a good demo to solve that problem with the Focal data set, and that data set is also published in the, in the our query libraries that you guys can grab today. And let's recap what we have learned since from the, the question we asked in the beginning. Have you seen the Focal data set? We have seen this all together. Have you analyzed this, the focal data set, which we have done this all together? And have you joined Focus dataset, and in this practice, we joined Focus data set with other providers in Athena. And what's remaining is put Focus into action. So, after this section, very encourage you to create a focus exports from the database data export platform and visualize your focus data and get your hands with the focus. Yeah, and getting hands-on, uh, again, hopefully we can make this easy. We have a whole bunch of resources here, um, that will link you out to the code examples that we had today, um, the Kerr query library where you can get started, uh, either with Kerr or Focus, um, the cloud intelligence dashboards, uh, a whole another library of links if you feel like diving into, uh, cost allocation strategies on AWS, uh, and then both mine and Jason's, uh, socials, if you want to connect. And of course, uh, you know, reinvent is all week. Uh, there's lots of other sessions focused on Fin FinOs practitioners like yourself, um, whether it's, uh, AI for FinOps, FinOs for AI, uh, unit cost, or, uh, cost allocation strategies, uh, multi-tenant cost allocation strategies, a whole lot going on. Uh, so check these other sessions out. Um, and come check out the, the Cloud Ops kiosk. We do have a kiosk in the AWS Village, uh, in the Venetian Expo, um, where, uh, folks like myself and Jason will be hanging out to give one on one demos, uh, with their swag and stickers. Um, feel free to come and say, hey. And that's it. Thanks for, uh, thanks for your time today. Uh, I hope the rest of Reinvent is awesome, and, uh, Jason and I will hang out for a couple of minutes. If you have questions or you wanna come up and talk. I think we have a couple of stickers that, uh, we'd be more than happy to share with folks. Um, and please, if you could, uh, complete the session survey in the AWS Events app, uh, we are a data-driven company and we love all feedback, um, so. That's it. I hope everyone has a great rest of your, your week here.