---
video_id: WiPnxN1Xl-I
video_url: https://www.youtube.com/watch?v=WiPnxN1Xl-I
summary: "In this deep-dive technical session, \"NVIDIA Run:ai & Amazon SageMaker HyperPod Integration for Distributed Training,\" Rob Bagno from Nvidia presents a comprehensive solution for maximizing the efficiency of expensive GPU infrastructure on Amazon EKS. Bagno highlights a pervasive problem in the industry: the \"Allocation vs. Utilization\" gap. Organizations often incorrectly assume that because their GPU clusters are fully \"allocated\" to users, they are being used efficiently. In reality, developers hoard resources for idle notebooks or inefficient jobs, leaving millions of dollars in compute capacity wasted. To address this, Run:ai provides a specialized orchestration layer that sits atop Amazon SageMaker HyperPod, bringing high-performance computing (HPC) disciplines—such as dynamic queuing, quota management, and preemption—to cloud-native Kubernetes environments. Bagno details three architectural pillars that define the platform. First, Resource Optimization features allow for granular control over hardware. This includes Fractional GPU technology, which enables multiple lightweight workloads (like Llama 3 8B inference or VS Code sessions) to securely share a single physical GPU, effectively multiplying the number of users a cluster can support. For large-scale training, the platform employs Topology-Aware Scheduling. Unlike standard Kubernetes schedulers that place pods randomly, Run:ai understands the physical network topology of advanced hardware like GB200 NVL72 racks. It intelligently places interdependent workloads on the same high-bandwidth NVLink domains, drastically reducing latency for distributed training jobs. The session is anchored by two powerful demos illustrating resilience and fairness. In the Fault Tolerance demo, Bagno simulates a critical hardware failure (a \"GPU XID error\") during a distributed PyTorch training run. The integration shines here: SageMaker HyperPod detects the hardware fault and automatically provisions a replacement node. Simultaneously, Run:ai’s elastic controller detects the lost capacity, momentarily scales the training job down to fewer workers to prevent a crash, and then automatically scales it back up once the new hardware is online. This self-healing capability turns catastrophic failures into minor, automated hiccups. The second demo focuses on Dynamic Preemption. Bagno shows a \"Team A\" utilizing 100% of the cluster for low-priority training. When a \"Team B\" researcher logs in to launch a high-priority Jupyter notebook, the scheduler instantly preempts and pauses a fraction of Team A's workload to grant immediate access, ensuring that interactive development is never blocked by batch processing. Concluding with a look at advanced features, Bagno introduces GPU Memory Swap, a technology that automatically offloads idle processes from GPU memory to host CPU RAM, allowing for even higher density of \"cold\" workloads. By combining SageMaker HyperPod’s robust infrastructure health checks with Run:ai’s intelligent scheduling, organizations can move from static, inefficient allocation to a dynamic, highly utilized AI factory that serves both massive training runs and agile experimentation."
keywords: NVIDIA Run:ai, Amazon SageMaker HyperPod, GPU Orchestration, EKS, Distributed Training, Fractional GPUs, Topology-Aware Scheduling, Fault Tolerance, Dynamic Preemption
is_generated: False
is_translatable: True
---

All right, welcome everyone. um, thank you AWS for, for hosting reevent in Vegas again this year. It's always a, a fun time being out in Vegas. I hope everybody got some, some good food for lunch and, uh, ready to learn a little bit more about Nvidia Run AI and Amazon Sage Maker Hyperpod. So we've had a lot of really good conversations this week with customers, our, our partners AWS and our other partners in the ecosystem, right? And we see a lot of customers consuming Nvidia GPUs and Nvidia hardware in different ways, right? I'll talk about one of them today. There's a lot of great ways to consume Nvidia hardware through AWS's services, right? You can use Sage Maker. You can use Bedrock, some, some great announcements in the keynotes this week. Um, I'm gonna. Typically focused on Nvidia run AI and how it works all across EKS within AWS, so we'll focus a lot on EKS and Kubernetes as one way that customers can really optimize the GPU infrastructure that they have under management and really get the most throughput through that that hardware. OK, so a little introduction about myself. My name is Rob Bagno. I've been at Nvidia for about 1 year now. So previously I, I came from the Run AI team. I was a solution architect there for 3.5 years before the acquisition, uh, 1 year ago. Uh, I worked with all of our, our largest customers across the US and worked primarily with a lot of customers in AWS as well. So we'll start by going over a little bit of Run AI. So after maybe like 15-20 minutes you'll all be experts in terms of what the solution can do and you know really the focus of how it drives value for a lot of our customers today and drives the best utilization of their hardware footprint. We'll talk about how this is complemented between Run AI and uh EKS. So Run AI basically it's going to be a Kubernetes based service, but the idea here, there's a lot of unique capabilities that we can do within EKS that drive value for customers. And then we'll spend a lot of time and our demos talking about the integration and the cohesion between Run AI and Sage Maker Hyperpod. So we have uh quite a few customers on both products today, and I'll talk about how customers are, are using the product. Products some of the value and how they kind of work seamlessly together to drive the best kind of compute experience when you're consuming GPUs within AWS and then I'll wrap it up with a few summaries, some other things that the, the Run AI and AWS teams are working on. OK, so if you're not familiar with Nvidia Run AI, the, the quick rundown, it's basically a, a GPU orchestration tool and, uh, a full scheduling platform. So I'll talk about some of the unique capabilities that it brings. You also have policy driven. Governance. So the, the concept here we wanna drive the best utilization of your hardware. So there's a lot of different techniques that we employ and a a lot of different ways that we work with the administrators of these clusters so they can drive the highest utilization of their compute. It's oftentimes even that we work with customers that they're not really aware of what their underlying hardware utilization is, right? So we often have misnomers between allocation and utilization. That's one big bucket where customers think, hey, all of my GPUs are currently allocated, you know, all of my users are are sitting and using their compute. But that utilization of the actual hardware may not be at its optimal level, and Nvidia Run AI really introduces a lot of techniques that the administrators of the cluster can use to drive the utilization to sometimes get 2 to 3x throughput on the amount of workloads being run and products being brought to market. The other pieces of seamless user experience, right? So now you have great visualizations, dashboards, the ability as an end user to come in, uh, and get on-demand access to compute, all core pieces of the platform that we'll talk about in a little bit more detail. The final piece here is like a a very open uh and API first architecture, right? So the idea of the platform is we come into a lot of different enterprises that are consuming their compute in different ways and we work with data scientists and research teams that all have their bespoke tools, frameworks and capabilities that they want to deploy and that they're comfortable using. So we have to provide basically a middle layer here that ensures that all of those researchers, developers, data scientists can get on-demand access and they can still use those tools and frameworks that they want and that they've been comfortable using in the past even though they're they're going to adopt the Run AI platform. OK, so a quick note on the architecture diagram here. I'm not gonna spend a ton of time on this, but just to give you an idea of some of the deployment, uh, capabilities of Run AI here, there's two core components. On the left of the screen you have the control plane. This is more the administrative piece of the platform. This is where your administrators are going to come in, deploy your clusters, configure your clusters, uh, provide kind of that fine-grained access. It ties into your identity management so you can tie right into different identity providers and identity systems within AWS as well. And that single control plane can be delivered as a SAS product. It can also be delivered self-hosted or air gapped. So we have customers in the federal space that basically consume the full stack in a fully air gapped model. That single control plane that you see on the left there can then manage multiple clusters on the right. So every time I say cluster, just think in the context of a Kubernetes cluster or an EKS cluster, right? So if you have, you know, a cluster such as a hyperpod, that can be one of your clusters. We also have customers that have EKS zones or or EKS clusters in, in different regions, different geos for. Um, data gravity and, uh, compliance reasons, so that single control plane will manage those multiple distributed clusters and provide that same end user experience regardless of where their compute is and, and regardless of where they're launching their workloads. OK, so here's a little bit of a breakdown, uh, in terms of the, the Run AI architecture diagram. I, I typically talk about this one from the bottom up, right? So from a, a bottom up kind of conversation about the architecture here, you have your infrastructure, right? And when we're talking, you know, high performance GPU clusters, you have your GPUs, your storage, your. Data your networking, right, that's all kind of that that bottom layer there where run AI slots in is in that Kubernetes era, uh, a layer basically, right? So you have that Kubernetes layer that's going to sit on top of all of that compute, aggregate all of that compute and, and GPUs, and what we'll do is we'll create a large pool of GPU resources then. And then Run AI will get installed into that cluster and it kind of brings three pillars of capabilities and I'll focus on a few of those capabilities as we as we go through the presentation, but the core is centered around AI life cycle integration. So I, and I kind of started the talk talking about, you know, there's multiple ways to consume compute, there's multiple ways to run different workloads whether you own your own infrastructure or whether you're consuming it through services like Sage Maker and Bedrock, right? This is kind of focused on teams uh and enterprises that have large EKS clusters and they still wanna run the full AI life cycle. They wanna provide development tools to their researchers and data scientists. They wanna train, uh, it could be single node. We also support a multitude of different frameworks for multi-node distributed training and fine tuning. And then you can also do model serving within the platform. So when we talk model serving from Nvidia, you know, obviously Triton's the trusted go to, but now we're seeing really unique advancements in terms of model serving. Uh, Dynamo is a cool example that I have a slide on at the end, and I'll talk about how Run AI integrates with uh with Dynamo as an offering. But the concept there is now you have this large pool of compute and you can basically orchestrate that full AI life cycle. The second piece that we'll talk about is around resource management, right? So now you have this large pool of compute. How do you take this pool and start to share it between different developers, different research teams, and different business units, right? So the core of that resource management is driven around our quota system, which I'm gonna spend a little bit more talking about. And then we also have a policy engine that's, that's something that we can talk about as well to get really fine grained in terms of driving the best utilization of the hardware. My favorite part about the stack to talk about though is the actual workload orchestration. So I have a few slides on this, but there's some really cool things that Run AI and Nvidia Run AI can do from a GPU perspective that really help you get the most out of all of your GPUs within the cluster, and all of this is kind of centered around uh a scheduling capability. So if we look at the, the things that we'll go into detail next, right, you have kind of that Run AI middle layer, it brings a lot of different capabilities, right? And we'll focus on a few of those capabilities so you can start to understand how it interoperates now with Sage Maker Hyperpod. But what all of this allows basically now is you have this uh scheduling and orchestration layer that sits on top of all of your compute, but what it drives really is the ability for those data scientists and researchers to bring their top of stack tools, right? So I mentioned Dynamo, uh, there's a full suite of AI. Enterprise tools from Nvidia that are focused on different verticals, different industries, it's incredibly easy to deploy any of those tools now on top of the platform. So whether you wanna use NIMs, Dynamo, blueprints, uh, or something from the medical space like Clara or Monai for image processing, right now you have a single platform that sits on top of your compute and your different developers that are using different tools can now consume that compute in a pretty efficient fashion. And the great part for your IT teams is they don't have to go build bespoke clusters for every tool and framework that a data scientist and researcher wants to use. This single platform will allow all of those different tools and allow all of the compute that sits on the bottom layer to be shared much more efficiently than if you were to build those bespoke clusters for each kind of solution. So the first way as we get into that infrastructure pooling that we make it very simple for administrators. So think of yourself as an administrator who's, you know, keeping watch over a really large GPU cluster. It's a pretty big investment for your enterprise. You basically can go and install Run AI into that EKS cluster and then this is kind of like the the next steps that you'll take as an administrator to start to operationalize that cluster to onboard your developers and and users. The first piece is around infrastructure pooling. So we have a core concept in the platform called Node pools which basically assists customers in managing heterogeneous environments. So if you have a mix of GPU types H100s, A100s, if you're leveraging, you know, the, the B-200s or starting to bring B300s online, it's oftentimes we have customers with these heterogeneous mix of GPUs. We also have customers, uh, if they're operating on-prem, maybe they have a subset of nodes that have Infiniband so they can create a scheduling pool for all of their Infiniband connected nodes and you can start to align workloads to go into these different compute types and compute pools. So these group nodes, they typically share a common feature or property, you know, common ways to do it basically are the underlying GPU type. The other way that we see it leveraged pretty heavily within EKS as well is if you have underlying node auto scaling groups. So if you have an underlying node auto scaling group for a particular machine type or a particular GPU type, it can be aligned with a node pool and as you scale up additional nodes of that type. It will sit right into the the node pool. So long story short, right, these node pools provide that contextual layer that make it very easy to take a heterogeneous mix of compute and simplify it down, distill it so it's easier for end users to basically come in and consume it. So I know the screenshot there might be a little bit small, so let me talk through uh what that screenshot is basically covering. It's basically looking at the, the node pool configuration page. On the top you see the, the key and the label basically. So everything from a node pool perspective is driven by uh node labels that are defined with. Than Kubernetes, so basically when you define a node pool, it's going to look across all of your actual compute nodes in your cluster and then provide that classification of hey, if it, if it has an H100 or if it has some type of label that we wanna include for our node pool, it then gets obviously grouped into that node pool, right? This is incredibly important as well as we get into our GB 200 and GB 300 platforms because of, you know, the, the addition of NVLink into these systems. So as you bring in NVLink into the picture now you. You have to be aware of your rack apology. You have to be aware of your other racks in the data center, and you have to provide scheduling that ensures that you know you're making the, the most efficient use of this underlying infrastructure and I'll talk about how we do that as well. So the core of how that's done is driven by network topology. So this is a concept that we actually first developed for EKS, um, and the idea here basically is to improve performance for distributed workloads or workloads that need to intercommunicate within a cluster. The other core is optimizing that GPU utilization, right? So if you remove a lot of those networking bottlenecks or or speed slowdowns on the back end, you're obviously going to be able to feed the GPUs at a better rate and get better GPU utilization. And it's also very important for multi-level topology aware scheduling, and I have some, some screenshots that we'll talk about there as well. But the idea of networking topology as you schedule a workload, right, uh, basically comes into, uh, the, the run AI scheduler in this case, and the run AI scheduler is going to make a placement decision in terms of, hey, which node or which rack or, you know, which host am I going to actually put this workload or this group of workloads on. And the concept of our network topology here is you can kind of have a a failover or a hierarchy of different labels that are taken into account by the scheduler for placement. So the core concept is we is we built this basically around zones and regions within AWS. So if customers had very big EKS clusters and those clusters were kind of spread out across, you know, different zones, different regions, now when we take that scheduling decision into account, we'll look at that network topology and, and match these labels as we go to make the scheduling decision. So the core of this capability coupled with the node pools ensures that you're going to get that like for like architecture for a really large distributed workload. It's also going to make sure that that network backbone and that compute kind of grid that the workload is running on is best optimized and the workload is placed in that best optimized place. What does this look like in a GB 200, a GB 300 system as you bring some of these online? So let's look at the, the screenshot on the right here. So the left part of the screenshot is kind of traditional Kubernete's pod affinity scheduling, right? You say, uh, in your pod spec and your YAL spec, hey, I need a GB 200, I need multiple GB 200s or a GB 300, right? If you have that type of scheduling scenario, you could end up in use cases where you're not taking advantage of Nvy link. So you see on both screenshots, right, on the bottom left you have rack A, and on the bottom right you have rack B. And this is basically the, the rack systems that you see at the booth and, and see at, you know, GTC and when Jensen presents on stage, right? These are basically all of your rack level systems that are connected via NV link. So if you have pot affinity, it would be very common for the Kubernetes scheduler to place our, uh, our red or our pink workload across these two different racks, right, because the, the scheduler in Kubernetes is. Gonna say oh I'll put one here as my next workload comes in. I'll put it on this server and it kind of goes back and forth to spread out workloads across these different servers and these different racks. But if you have topology aware scheduling, you'll be very aware of those NV linked domains and as you get this workload coming in on the right, the scenario says, hey, we're aware of our topology, we're actually going to put, you know, this request for. 6 GB 200s on a full rack, right? So that core concept of what we built for, you know, EKS to drive optimizations from a zone and and region kind of topology flowed right over to how we started to build the scheduling for our GB 200 and our GB 300 systems again driving the the best efficiency of that underlying hardware. OK, so those are kind of the core things that we do at a macro level, right? To, to help drive the, the best fitting, uh, of a workload to your underlying compute. Uh, it's also coupled with things that we do on the micro level. So let's talk a little bit about what we can do that's really unique from a GPU perspective. The first will be fractional GPU technologies, right? Uh, I don't think anybody in the room doesn't know this, but GPUs are only getting bigger and bigger when it comes to memory footprints, right? Um, what we see with a lot of our custom our, our customers, right, they can be pretty heavily underutilized from a memory and compute perspective if they can only run one workload on a GPU. So when it comes to Nvidia in general, we have a few capabilities that allow you to do fractional GPUs or introduce the ability to put multiple workloads all on a physical GPU. The first is VGPU, so we, we don't see this too often in, uh, AI workloads, but we see it very common for desktop virtualization and simulation. The other capability here is MIG or multi-instance GPU, and multi-instance GPU is basically taking a hardware GPU and then providing hardware partitions within that full GPU. So there's only certain GPU cards that have MIG built in, and those profiles are already predefined, and you can pick and choose which profiles that you want to spin up on the GPU. The third option here as we get to Run AI and Kubernetes is Run AI's own kind of proprietary fractional technology that we built, and the concept of this is it's purely basically driven off of CUDA, so it's done purely at a software level and it allows that single GPU to be shared between multiple containers and what we're going to do is protect against memory overflows and processing clashes between those multiple workloads that all can be running on the same GPU. The cool part about this technology, it's, it's totally transparent. There's no code changes required to to make use of this fractional technology, right? You can use any container that you're using in Kubernetes or in EKS today and start to use this Run AI fractional technology from from day one. The other capability we also have that's that's starting to be adopted even more now is dynamic GPU memory. So if you're familiar with Kubernetes in the audience, there's a concept of request and limits within Kubernetes. So whether it's CPU or memory, you have kind of what the pod is going to request when it's deployed and then kind of that upper limit of of what it actually may need if it's functioning, you know, at full kind of throughput within whatever container it is. And we took that concept basically and applied it to our GPU memory fractions. So now you can basically take any container that you want to run and set a dynamic GPU memory range where you can set, hey, at any point in time I want 5 gigs of GPU memory. Uh, maybe you start a user in a Jupiter notebook, you know, debugging some GPU code, and 5 gigs is great for them, right? But now they've validated that code and they actually want to feed in more data and feed in a bigger data set. Instead of totally restarting that job and and having them kind of make sure that code is saved and and move it over to a different workload, that dynamic memory capability allows them to expand from that 5 gig memory footprint all the way up to whatever full GPU that they're sitting on, so it's a really unique capability that allows you to shift that memory footprint below the container and allow you to, to again drive user density on the hardware. So kind of the core capabilities of where we see customers leveraging our fractional GPU technology is is kind of both ends of the AI life cycle. We see it used very heavily in development work uh when we see customers launching Jupiter notebooks, VS code sessions right where they may not need a full GPU. So what it allows basically is instead of giving every developer a full GPU, now you can basically sit 5 to 10 developers all on a single GPU to drive that user and workload density, and what that unlocks is now the rest of your cluster is free to run, you know, more training workloads that are typically more intensive and typically more compute intensive with their requirements. And then as we get to the other end of the AI load life cycle, you also have imprints workloads, right? So if you look at a lot of Nvidia's blueprints, if as you look at some of our NIMs as well, not all of these particular components that require GPUs really need a full memory footprint of an H100 or B1 B200, right? So the core concept there is we can take those workloads, those imprints workloads, regardless of their names or blueprints or something custom that you've built as well, and set the workload on the exact amount of GPU memory that's required and it'll still function at full capacity, right? So now you can drive the, the density of your development sessions and now you can deploy and serve your imprints models on the exact amount of compute that's required for them. And the good concept here, right, all of those fractional GPU technologies that are listed above, they're all supported by the, the Run AI platform and through EKS, right? So the, the core piece of, um, hey, I still wanna use MIG because I like that hardware isolation that's fully supported as well within Run AI, uh, but maybe you have use cases where Run AI fractions are a better fit for, for what you wanna serve. OK, so what does this look like in terms of, you know, real world scenarios and performance here? I wanted to give a quick example here of some benchmarking that, you know, a third party team at Nvidia did. We basically took uh LAMA 3B or LAMA 3.1, uh, the 8 billion parameter model, uh, and Deep Seek, the, the 8 billion parameter model. If you look at kind of the memory requirements for those models, it's, it's not a full GPU. I think LAMA 8B sits around like 17 or 20 gigs of of GPU memory that's required. Deepeek is about the same. So what we did basically is we, we took these two models, uh, we deployed them on an H100, and what we did is we benchmarked them on a full GPU and then we benchmarked them as they were running on half of a GPU. So now you have uh Lma and Deepeek running on half of a GPU, the same physical GPU. And we saw no difference in terms of the metrics in terms of the concurrent users that we could support in terms of the throughput uh and tokens per second as well as that time to first token. So the, the core of this is, hey, it's it's not just we can do it, we can slice it, we can fit multiple workloads on it, but it actually doesn't really have too much of an impact in terms of real world performance depending on what's on the GPU. So you can choose to, you know, not use Run AI, not use fractional technologies, and, you know, sit LA on its own GPU, sit Deepeek on its own GPU and get the same performance, but you'll be using, you know, more GPUs at the end of the day. And what we see a lot of our customers doing is actually, you know, they're not just deploying one replica, they're, they're scaling these replicas. Up and down as they see spikes in traffic, so the core of if we sit it on the exact compute requirement needed now we can actually scale more replicas and then we'll be significantly more efficient as we grow more replicas of these models. OK. Next piece that I'll talk about here is a a really advanced technology we only have a few customers kind of actively using this in production today, uh, but it's it's really powerful in what it can actually do so this concept is one of our GPU capabilities and software that we call. GPU memory swap and it's basically you know if you're familiar with the concept of swap and technology it's it's not something that we invented by any means but we kind of took the cue of hey you know what what can swap do for GPUs and GPU workloads so what we basically did is, you know, we, we looked at a lot of these GPU servers that were being built, uh, by a lot of our OAM vendors, right? And we realized that you know there's a a huge amount of RAM being put into these servers and a lot of our customers weren't seeing a huge amount of utilization when it came to the, the RAM that was included, so we, we took that as a cue and we took the other challenge that our customers have a lot of inference processes that may be kind of stagnant or idle because they're not receiving requests at any point in time. And what we decided to do basically is is see if we could take those unused resources on the on the actual nodes and take the concept that they may have workloads that become idle whether it's a development process, whether it's an inference process and what we basically did is developed the technology that allows us to deploy anything on the GPU memory if that workload becomes idle on the GPU at any point in time. And you have another workload that could make use of that GPU. what we actually do is we swap that GPU memory content into your host memory, and you think, oh, there may be like a a pretty big overhead with that. There actually isn't because it's going over the bus speed over the server itself, so you can actually dump that GPU memory onto host memory. And kind of maintain it in host memory if somebody comes back in and then continues to work in their Jupiter notebook and calls the GPU or if you get a query in that imprints process and that's really what that graph on the right is showing, right? So you see that performance of scale from zero versus CPU memory swap. Using different models, so obviously the, the bigger green bars, uh, or sorry, the bigger blue bar, right, that's going to be kind of your, your cold start challenge of hey I don't have a model deployed and I'm basically gonna put it on the, the GPU for the first time that's how long it's gonna take to load everything into GPU memory. But with that swap capability you can load that model even before you have requests coming in and then suspend it onto your your CPU memory or or your host RAM, right? And then as requests come into that model we'll swap the memory context back into the the GPU memory and we can swap multiple models all at the same time, right? and iterate through the the different requests that are coming into that GPU. So if you wanna talk about, you know, our customers that are worried about, you know, 90% utilization not being good enough on their clusters, they're looking at technology like this that's gonna drive them, you know, from 85-90% even higher on their, on their hardware footprint, so it's. Really cool technology that really drives the the best utilization and helps you really cut down on those idle GPU times if you have a lot of imprint processes being deployed on the GPUs that may not be called, you know, 24/7 or uh at the same time throughout the day. OK So we had our node pools, our, our network topology to provide the contextual access to all of the compute. We talked about everything that can be done on a GPU level and a software level that, you know, provides those resource enhancements and user density enhancements. Now let's talk about kind of the, the core of how this ties into um how workloads get onto the cluster, and this will be the, the scheduling component of the platform. So Run AI, uh, basically took a lot of concepts from HPC. So if you're an HPC person in the room, you're looking at this list and you're saying, yeah, this has been around for a long time and it's true, right? But what we did basically when we built the scheduler is we saw that a lot of this AI life cycle basically. Was being orchestrated and deployed on Kubernetes, but if you look at the Kubernetes scheduler, it's really not a great fit for, uh, GPU workloads. The Kubernetes scheduler was built for, you know, database scaling, uh, you know, web server scaling and, and deployment. It wasn't necessarily built to manage workloads that are. Intercommunicating across different nodes. So the core of what we did is we took a lot of those concepts from HBC, including, you know, multiple cues, queuing and DQing, preemption and reclamation, right? And we built a purpose-built scheduler within Kubernetes that aligned with the AI life cycle and was a better fit for scheduling GPU workloads. So kind of the core of how this operates, I, I mentioned guaranteed quotas a few times. This is essentially what you're looking at on the the screenshot on the right. A lot of our customers will set up these projects, uh, and these projects are given a guaranteed GPU quota. So if you look at that middle column on the screenshot, that's their GPU quota. So as a developer, as a researcher who's consuming this compute, you always know that you have compute available to you within the cluster up to your GPU quota, right? And this changes a few things. We all know developers are worried about getting access to GPUs, right? And they know if they need to run some code on GPUs at any point in time they're going to go and try and grab whatever GPU they have available, and they may not be ready to even run their code at that point. So what the scheduling and guaranteed quota kind of does is it changes that mindset instead of them being nervous that they can't get a GPU to to run their code for whatever sprint or cycle that they're working on now they know that there's GPUs available to them. Uh, within a matter of seconds that they can go in, request, uh, and basically be able to allocate for whatever they're working on, so it changes that dynamic for an end user and developer and ensures that, hey, now they have GPUs available to them. So it's great for a developer, it's great for a researcher, it's also great for the business and the enterprise too because these quotas can be shifted on demand on the fly if you have a shift in business priority. So we have a lot of customers that are developing different models in. The space and maybe you have a competitor launch a model in the space and and need to catch up a little bit, right? We've had real world scenarios of some of our customers saying hey we're gonna shift a good amount of our quota and capacity to this team so we can release this product basically uh and catch up to uh our competitors in the market so it allows the the business and the enterprise to start to align the compute with uh the the different initiatives that they're working on while still ensuring the developers get the access, right? OK, so I talked a lot about Run AI. I wanted to set the stage in terms of the products so you understand it, um, and some of its core capabilities. Now I wanna talk about some of the unique things that we've done with AWS and SageMaker Hyperpod. So there's a, a few ways that we look at, you know, SageMaker Hyperpod in terms of it's really unique capabilities and what it brings to market. So the probably the coolest thing that I've seen Hyperpod do basically is health checking of the underlying infrastructure, right? So as you build these, these, you know, training clusters and large compute clusters. At scale you're going to run into some some hardware issues, some network issues, and the kind of the core concept of, of how we see hyperpod right is the ability to provide hardware replacement if you come across any of those issues. So the, the, the idea here is you can drive that increased ROI and better. APU optimization by leveraging uh Nvidia run AI all of those scheduling and orchestration capabilities, and they kind of tie into how Sagemaker Hyperpod is going to deploy the infrastructure, monitor the infrastructure, and also replace the infrastructure if it comes across any problems and that's actually one of the demo videos that we'll walk through as well. What does this allow really since you're getting this great you know compute cluster, it's going to allow faster time to market you're gonna get you know pretty good elasticity in terms of how that compute gets allocated and shared between the different users and teams and you're gonna get, you know, top of the line, uh, performance. The other piece that you know I'll mention and and talk about at the end, now you have centralized control and visibility, right? I mentioned that that challenge that a lot of our customers have and the misnomer between allocation and actual utilization of the hardware. Basically what we're able to do is monitor even down to an individual workload load level how it's using the GPUs and how it's using each individual GPU that may be allocated to it. So I'll talk a little bit about that, uh, monitoring. How it can be actually also integrated into the the AWS stack for for visualization too um and how that helps drive the business to you know start to change its allocation patterns of the the actual hardware. OK, so now we get to the fun part. Let's go through some of the demos here. And the first one is the job resumption on the, the hardware fault, right? So this first demo, uh, I'll kind of set the stage here as some of it plays, but the, the concept here is you have some of your AWS Sage Maker Hyperpod documentation on the left. On the right you're seeing the Run AI UI. And what we have here basically is a hyperpod with 4 nodes. Uh, it's H100 nodes, 8 GPUs on each one of those nodes, and on the left we're looking at some of the annotations that you can add to a PiTorch workload. So in this case we're running an elastic Pie Torch workload. Um, it's basically an elastic workload that allows you to grow and shrink based off of the compute that it's deployed on. So we go through a little bit of the YAML set up so you can see that as well and the the core of it basically is you have these annotations to basically retry and and rerun the job in case you come across any hardware issues and that's kind of coupled with the the other components that are built into your elastic Pie Torch workload. So with all these workloads you have some type of database that all of the workers connect into. And this is actually what's running actively on screen so we can go into a little more detail of those YAML files if you want, but the idea here, it's a pie torch job that's elastic, uh, and it includes the annotation so it can work transparently with Sagemaker. What we're doing on the terminal here on the bottom actually is part of the pieces that you know I mentioned were, were very great about SageMaker Hyperpod, right? You have the fault tolerance and the hardware monitoring. So what I actually did there within the terminal is we inject a GPU XID error code onto the the hardware itself. It's just a simulated one in this scenario, but you see, as I injected that our distributed Pie Torch job has already gone back into a pending state, right? So you can see that on the right. And on the bottom we're looking across the different scheduling taints and labels that are on the node. So as we injected that error code, what Hyperpod does basically it labels the node on a Kubernete's level. We see that label that Hyperpod added that's saying, hey, we have a. Hardware issue on this node. Please don't schedule anything else on this node and what I'm actually gonna do is take this node because I'm going to replace it. So the, the whole health checking of Hyperpod here is looking at this infrastructure, looking at the health of the GPUs and and the other components within the servers, and it's going to automate the redeployment of this node that we detected a GPU issue on. As that's happening transparently with Run AI, what we've done basically is scale down one of our workers in our elastic job. So we had 4 workers running before we actually scaled down to 3 workers that you can see running on screen. That allows the job to continue running even though we just came across this hardware fault. So if we talk about providing one resilient infrastructure, that's excellent, right? So, so Hyperpod is gonna say I found some type of networking issue. I found some type of hardware issue. I'm going to drop a new note in place. But coupling that with Nvidia Run AI, which is gonna say, hey, you know, I, I no longer have this node available for scheduling. I'm actually gonna remove this worker the job is going to continue to run and that's really gonna help you be, you know, very resilient in terms of what's happening within the cluster. So what we see on the left now are one node is back into a pending state that's the one that's going to be removed from the cluster. The other one on the bottom is the new node that AWS is is sliding into place in the cluster. So it takes a few seconds here. uh, I kind of condensed the demo a little bit, but in terms of like what does this look like in a real world time scenario, it's about 1 minute to 10 minutes to remove that uh node that we saw the issue on and have that additional node placed into the cluster just to give you some idea of of how long it takes. So what you see on the left right now is everything is up and running. These are all of our nodes in Kubernetes, so we have that additional H100 node added. And as you deploy that node, another piece from Nvidia's software stack comes in and installs the drivers, the container tool kit, basically everything that will initialize that node and allow the GPUs to be consumed within the Kubernetes layer. So it does take a little bit of additional time there that's within that 10 minute span that I was speaking of, and that's basically going to install the Nvidia stack on that node that gets spun up. So once that Nvidia stack is installed, we'll come back to the nodes page. You can see, oh, now we have our H100 available and all of our GPUs available. We go back to our workloads that are running and within a few seconds we'll scale up that fourth worker of that elastic job. So within the span of 1 to 10 minutes, right, we've experienced some type of node failure. We've experienced some type of hardware issue. Sage Maker Hyperpod basically took use of the unique capabilities where it can swap in a new node. It brought that new node into the cluster, brought it online, and then from the run AI perspective we kept the workload running. We ensured that as that additional node was added to the cluster, we would add this worker to the new node that got spun out so it could continue running. So all of that, you know, on the scale of 32 GPUs is maybe not that impressive, but we have customers doing this at, you know, much larger scales, and any of these transient hardware failures you really don't wanna affect, uh, the training of these large jobs. So that's really. One of the core capabilities that uh you know how SageMaker Hyperpod and and run AI work together to ensure you know you drive that best utilization of the hardware and ensure that you know your your hardware has really great up time to to run these workloads. Alright, so two more demos, actually, actually one more demo that we'll go through here. So this particular example is that that same Pie Torch workload, and I'll, I'll kind of condense this one and, and talk about uh, a few different things within it as well. But long story short, this is the same elastic Pie Torch workload that we showed in the previous example that survived the hardware fault. But this time we're gonna talk. About it in a little, uh, in terms of like a scheduling scenario, right? So we mentioned those projects, those guaranteed quotas, right? So we have our 32 GPUs within the cluster. What we're actually gonna show in this scenario is how that compute can be shared between different users and different teams. So we have our nodes, we have our node pool created with our H100s. And we're gonna come down to now our projects tab, and our projects are we're gonna provide our, our guaranteed quotas, right? So if you look at the projects that we have on screen, if the text is small, I'll kinda walk through it, but the idea is we're guaranteeing Team A 16 GPUs, we're guaranteeing Team B 16 GPUs, Team C, you know, they don't get any guaranteed access. They can use GPUs kind of when there's anything idle within the cluster. And the core of the scheduler, right, is to provide that guarantee, but we also allow that bursting, right? So Team A wants to run a big job, you know, we're all at reinvent, we're not on our GPUs this week, so we're gonna allow Team A to use the full cluster. But what happens when we come back from reinvent and we wanna, wanna run a workload, we saw a cool example we saw, you know, some type of agentic thing that we wanna test out and deploy in the cluster. You know, same scenario that we can walk through here. So let's say we're all Team B. So what it looks like from Team B's perspective, they know that they can come in and and get up to 16 GPUs within the cluster. So again, we're just going through the YAL file for the the workload that is running. Uh, but we'll, we'll go through and, and talk about what it looks like for Team B to deploy their workload. So a few other things that I wanted to show actually quick within this YAML file too. It's the same one that we ran previously, but we go a little bit more into the worker spec, and within that worker spec you could see the resource request. So you could see the AG. Use for each worker you can also see those EFA devices. So if you've worked in EKS at all, if you run a lot of distributed workloads, EFA is gonna, uh, kind of be your back end communication channel. It's kind of like an analog to Infiniband if you have like a, a super pod or base pod on prem or Spectrum X, right? And then this is basically gonna be the the torture and command that runs that distributed workload. So this is the job again that's currently running for Team A. But now we're in the scenario of Team B, right? We wanna run one of the examples that we saw at Reinvent this week. So as a user from Team B, you can basically, oops sorry, you can basically now come into the platform now and launch your own workload. So we'll go through what it looks like for a user to actually submit a workload to the platform. We'll come in, we'll choose our project as Team B. Again, as a user you can be assigned to one or multiple. Projects depending on what you're working on, we have templatized workload submission if you wanna kind of deploy uh Jupiter as a service for end users, but I'm gonna come in, just deploy my simple Jupiter notebook here. I'm gonna request two GPUs. You see some of our GPU memory fractions on there as well. And then at the bottom, data sources as well, we can integrate with everything from, you know, FSX within AWS to to S3 based storage. And as Team B I'm gonna come in, launch my Jupiter notebook, and within a few seconds, basically what you're gonna see is we're going to preempt one of the workers from our elastic pie torch workload and we're going to reallocate that compute to Team B so now they can come in and within a few seconds their Jupiter notebook is up and running and they can connect to it right within the UI. So those, those 4 workers from our elastic job, we scaled them back down to 3 so we could reclaim some of that compute. And within a few seconds our our user from Team B is able to get their Jupiter notebook that's GPU accelerated so they can start running their code or whatever example that they saw this week that they wanted to uh to try out. OK. I'm not gonna go through the full example here of our our hybrid and multi-cluster capabilities, but the the core concept here I talked about it in the beginning that single control plane can manage multiple clusters, multiple distributed clusters if needed. But I will talk a little bit about the monitoring and consumption, right? You saw maybe briefly there in the demo, uh, as we went through the overview page we're tracking kind of that, um, holistic GPU allocation and, and utilization. We track it cluster wide we track it on a node level. We track it on an individual job level, and then those job level metrics can be aggregated into your different projects. So, uh, as you look at that top left screen shot, basically this is going to be your GPU allocation and utilization for your cluster over time. As you look at the, the screenshot on the bottom right now you have your different projects, how they've allocated that compute over that given time frame. And then you also tie that allocation number now into a utilization number, right? So we always have that misnomer of customers between, yeah, I don't have any free GPUs we're we're really using them really well, but they may not really know how well they're being utilized. So this dashboard helps to point out not only how the GPUs are being allocated to users and teams, how efficient they're being, uh, with the GPUs that they've requested and and allocated. And then the, the pie chart on the right basically is talking about your usage patterns. So the, the general breakdown that we see with customers is about uh a fifty-fifty breakdown of in quota hours versus over quota hours. So that, that over quota capability was highlighted by Team A who wanted to use the full cluster, run their full elastic pie torch workload when when nobody else was running. Um, and in the real world scenario if you're locking users to a static quota or a static upper limit, you know that that's really gonna be highlighted here where you can see those, those over quota hours become pretty significant for our customers and how their end users get access to GPUs. And then final piece from the the monitoring and consumption perspective, you know, we'll track this on an individual job level, how individual pods and workloads are are utilizing each GPU that they're allocated to. OK, last piece that we'll go through, last slide, uh, and then maybe open it up to a few questions if there's any in the room. Uh, there's a few things that we do beyond kind of like the, the enterprise version of the product that, uh, you know, really help out the, the full ecosystem here. The first one on the top left is the KAI scheduler. Everybody calls it Kai. Uh, it stands for Kubernete's AI scheduler, but the concept here is we took that core scheduling capability that we built for the platform, and now this is fully open sourced. So if you wanna make use of scheduling and quotas, aligning those with with node pools and heterogeneous, uh, GPU or compute types, that's, uh, a fully open source component of the product that you can test and try out. We also have actually a lot of customers on AWS2 using our model streamer, so the model streamer capability is a component of VLLM, uh, or, or a model serving kind of engine, and the, the core piece of the model streamer is, is streams in your safe tensor files. So this can significantly reduce that cold start issue that you, you know, see in a lot of larger language models, um, and by. Kind of combining and providing a method of loading those tensors in in a more efficient fashion, you know, it, it really helps to reduce the challenges that we see for for customers who have cold start problems and again that that component is is fully open source as well. And then the final piece is is Dynamo, right? So obviously we integrate with uh a lot of the other Nvidia software that's being deployed NIMS, Blueprints, Dynamo, uh, and the really the approach of the platform is to be open in terms of other tools and frameworks that we see our customers using. So we make it very easy and and we build a lot of the scheduling logic so as new tools and frameworks come out it makes it very easy for you to consume them within the platform and support them in a in a more enterprise supported way. So yeah, I, I hope everybody got a good understanding of Run AI, how it integrates with Sage Maker Hyperpod, some of the quick examples here of, you know, how they function much better together to drive the, the best in class GPU utilization for, for some of these really advanced clusters and, uh, yeah, happy to open it up to maybe a few questions since we have a few minutes left here but thank you everyone for the, the time. Good.