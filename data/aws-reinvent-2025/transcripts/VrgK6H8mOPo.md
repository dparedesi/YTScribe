---
video_id: VrgK6H8mOPo
video_url: https://www.youtube.com/watch?v=VrgK6H8mOPo
is_generated: False
is_translatable: True
---

Hello everyone. Welcome to day one at AWS Reinvent 2025. In the next hour, we will explore how to, how BMW accelerates the connected future by using an event-driven architecture for their unpredictable workload using AWS serverless services. At the time we engaged in 2023, BMW of course had a working solution for their remote services service. Is what it was functioning. It met requirements. It solved the business need. But having a working solution was not satisfying us, and we both asked ourselves what can we do to make a good solution great by insisting on our highest standards. And this is what we want to share with you today. My name is Christian. I'm a principal solutions architect with AWS and I have the pleasure of working with BMW for the last 5 years. And today, I'm excited to co-present with Ludwig. Thanks, Christian. I'm also very delighted to share this breakout session with you today about one of the most widely used BMW connected drive services. My name is Ludwig, and I oversee the developments of the remote services back end as a product manager for BMW since a couple of years now. So I'd say, let's dive right into it, shall we? Sounds good. So today we will share the significant improvements BMW Connector Drive could realize by moving or modernizing their remote services application to a fully event-driven serverless solution on AWS by reimagining their current solution and taking all the advantages by running in the AWS cloud. So Ludwig will start with providing you with an overview of what BMW Connected Drive is all about and what remote services is offering to BMW customers. I will continue with diving into the remote service architecture which was in place in 2023 when we engaged. And then I will dive into the new architecture, the decisions we made, and also the learnings we could get out of it. Then Ludwig will continue with Explaining how BMW iterated over this new initial architecture and improved it even further and then at the end Ludwig will close with the benefits BMW could realize after moving this new architecture into production earlier this year. So let's start by establishing some context. Across the entire product portfolio, so from BMW, Mini, and even Rolls-Royce, the BMW Group is providing compelling and industry leading connected services. We call them connected drive. So from interacting with your vehicle via the my BMW smartphone app to in-car gaming, in-car streaming, all the way to your intelligent personal companion based on large language models. The list is very long, and these are just a few examples. These features, they all transform your vehicle from being a simple object, moving you from A to B into an intelligent, connected companion. Over 20 years ago, BMW connected its first vehicle. Today we have the largest fleet of connected vehicles with over 24.5 million. Out of those, we regularly update more than 10.5 over the air. And managing this scale requires handling more than 16.5 billion requests and processing more than 184 terabytes of data each day. However, With the growing fleet and more and more data queD products and features, we expect those numbers, the requests and the traffic process during each day to triple within the next two days. So as you can see, we require a very strong, resilient and scalable backend, and that's one of the reasons we partnered with AWS for 10 years already. One of the most prominent products of connected drive is the my BMW app. It's the ultimate indispensable vehicle companion that fuses our customers and their vehicles together. We recently hit the milestone of 50 million active users. While having an A star rating of 4.8 stars on iOS and Android. The app provides a magnitude of different features, from in-vehicle life data to information about recommended services and repairs. The option to book appointments at your local dealerships. And of course the remote services. That's the the topic um this breakout session is about. The remote services empower our customers to remote control their vehicles from anywhere at any time. Right, so they aim nothing less than making the lives of our customers easier. So imagine you're getting ready for your commute to work and you could pre-climatize your vehicle to your desire while still being in bed. Or picture another scenario that you can see on the 2nd screen where you leave and lock your vehicle. And you forgot, and after a couple of minutes, you receive a push notification that you forgot to close one of the windows, and you can do so with a simple tap on the screen. So there's all sorts of different use cases. These are just a few examples. On the far right you see another, another one that's actually events from within the vehicle. So if someone is trying to steal your car, you receive a push notification and if you opt in, you will also receive um videos from the inside and the outside of the vehicle. There's also quite the traffic on our back end. Um, currently we're handling 2.5 million events every day, um, and that involves processing more than 100 million API calls every single day. Before we now have a look how the remote services looked back in 2023. I want to shortly recap how remote services evolved over time. So as you can see in 2006, the very first um use case door unlock was built on top of the communication standard CDMA here in the US. And how do we communicate with a vehicle, you might wonder. So let's travel back in time and listen to one of the very first remote services executions. Some of you might recognize that. It was actually done with simple voice calls. And additionally, we needed a way to remotely wake up the vehicles. So if you park your vehicle and leave it for hours, days or weeks, we cannot cannot have it always on. So we need to remotely wake it up. Back in 2006, this was actually not possible because there were not always on modems. So maybe as a little historical detour, what we did instead, the vehicle regularly booted its modem by itself, and the BMW call centers, which were the only ones triggering remote services back then, were constantly calling the vehicle until they reached it. So this is quite interesting from a technological point of view, it doesn't scale. And that's why in 2009, with the first always on modems, we introduced the remote wake up with an SMS. And we also added another layer of communication which was SMS to talk to the vehicle. What we didn't do is we didn't sundown the older generation, because if you buy a vehicle, you want it to be connected for decades. So that's always the pattern with each new vehicle generation, we keep adding new technology. whilst not being able to sun down any of the older ones. So for example, in 2013, we added an HTTP based communication protocol, and then in 2018, MQTT was added. And then in 2021, we're now also able to remotely wake up the vehicles with UDPIP triggers. Actually, the only two weak generations that have been sundowns are the first ones simply because the communication standard is not supported anymore. And we didn't stop there. It's not 2021 anymore. I think most of you have heard that BMW is reinventing the vehicle once again with its Neulasse technology. So let's stop right here at the M BMW app. That's the front end to our remote services back end. And what we've just seen is the brand new BMW iX-3, and it's not just any new electric vehicle that pushes boundaries once again. It's actually the beginning of a new era because between now and 2027, BMW will release more than 40 new and updated models that will benefit from more cluster technology. And as Neulasser challenged everything inside of the vehicle, We as a remote services backend team ask us the same. We do have a working solution, but is it good enough? Can we improve something? Are we future ready? And to ask, to to answer all these questions, I'll give back to Christian to have a look how they looked back in 2023. Thank you, Ludwig. So, now that we know what connector drive is and what services remote services is offering, let's take a look at the current, uh, the architecture which was in place in 2023. So you have to know that BMW remote services was migrated to AWS in 2021 in a lift and shift approach. So previously, it was running in BMW data centers, containerized in OpenShift. And In most cases, as we have seen, a BMW customer is triggering a request to the remote service backend, for example, to locate their car. And to fulfill such a request. Usually the request is enriched with additional data from within BMW, and of course the request is also validated. Afterwards, the request is forwarded to the MQTT broker, which is responsible for the communication with the vehicle. And what you can see here is at this time. Remote services was mainly leveraging Amazon Kuberittier and Amazon relational database service. And while this architecture looks quite simple, it's not telling the truth. So this nice little container icon you can see here is hiding the fact that there's a large monolith behind it. So if we zoom into this container. You can see The stack was running on top of Linux and the Java runtime inside the Java EE server. Sounds familiar, right? So back in this time, so in 2006, we developed applications in this way. But over the last years we also learned that for a large application which needs to scale to handle millions of requests, that's probably not the best architecture. And also managing the complexity became a problem, so Ludwig just mentioned there was no real opportunity to send down legacy code because they have to support functions for decades. And so over time, technical debt grew. While remote services was serving BMW customers well for decades, the team, as Ludwig mentioned, felt the service was aging and presented the team with challenges. So Ludwig just mentioned, BMW expects to triple the number of requests in the next 2 years and probably it will not stop there. Ludwig also mentioned that until 2027, BMW plans to release more than 40 new models. So time to market is very critical also for remote services to offer new services or to adapt services to new vehicles. And as a BMW customer, you expect a premium product. And this is also true for remote services. So remote servers have to make sure that even under an increased load, they have to make sure that the system is reliable and provides the same low latency as of today or even lower, and this was already a challenge in 2023 at high demand. And last but not least, with the increased number of requests, people also were asking, OK, what does this mean to my AWS infrastructure cost? Will it increase in the same way? What can we do about it? So optimizing remote services for cost efficiency is also an important topic. And with this unique opportunity to rebuild remote services from scratch, we also took the opportunity to talk to our key stakeholders and asking them if you can just improve one thing in remote services, what would it be? So we talked to the customer and for the customer having a reliable service which is responding fast was their most important thing. We also talked to to the DevOps engineer and the DevOps engineer was. Focusing on velocity, so how can I increase my velocity? The senior management was telling us, hey, we would like our engineers to focus on new features and not spending so much time on maintenance tasks. Ludwig was requesting, hey, how can we get our AWS bill down? And the The reliability engineer we talked to, he was mostly interested in having a robust system which can scale quickly. And we know that running a POC is a considerable investment and can be quite costly. And therefore we agreed upfront running only an intense two weeks pilot and intentionally we are calling it a pilot because we want to use this as an accelerator if this pilot is successful and not throw away the code we have developed. And we selected two representative use cases from remote services, which was remote Hornblow and remote light flash, and the pilot was intended to give us the data point and the confidence that the new architecture will solve the challenges the team was facing and also achieving the goals our key stakeholders were asking for. So for our pilot, we started on a blank sheet. There are only 2 hard requirements we were told. First, we must support the current interfaces that my BMW app is using, so we were of course not able to change this. And second, we also have to support the same bidirectional interface to the MQTT broker. And happily BMW already had a vehicle mock which we are using in this pilot to stimulate a real vehicle because we also want to do some load tests and therefore everything needs to be automated. And then we analyzed the key building blocks in remote services, we identified 4 main building blocks. So starting on the top left, we have this event creation component which is responsible for receiving the remote remote service requests coming from the My BMW app. We have this outgoing vehicle communication service which is responsible for the communication with the MQTT broker. On the bottom right, the incoming vehicle communications service, which is responsible for taking the requests, the asynchronous requests coming back from the vehicle and process them. And last but not least, at the top right, um, the event processing component which is responsible for event distribution, persistence, and also the life cycle management. And in addition, we also decided to implement one subscription mock so that we can measure end to end latency of a remote service execution. So from the time when we have seen this request the first time. Until we have replied with a push notification to the my BMW app. And if you take a look under the hood, this is how these surfaces look like. And because we have broken down these services in these independent components. This helped us to start these or to work on these services independently after we have agreed on. The event format, which of course is the same for for all of these components. And the best way to understand how these services work together is walking you through one example, use case, for example, the remote horn blower. And it starts at the top left in the my BMW app where a customer is requesting the remote horn blower, and this request is received by an API gateway. This API gateway is integrated with our web application firewall, which is validating the request, and a valid request is then directly forwarded to an AWS step function express. And here we are leveraging the direct service integration from API gateway to step functional. And this helps us to keep latency low and also cost low because we don't have an additional lambda function in between. And if this is new to you, um, you have a QR code here and also the link to our documentation to learn a little bit more about this. When this event is in our step function express, we start with a parallel task step which allows us to process an event. Within two or more components at the same time. While at the left side is where the real business logic is happening, I would like to focus on, on the right part first because this message is then forwarded to the event processing service and this is a component which is called multiple times during the execution of one remote service request. Therefore, I would like to go through this first. So. This message is or this event is forwarded directly from step function via the direct service integration to an SNS topic. And as you can see, we have 3 SQS ques subscribed to this SNS topic. You may also recognize this little red filter symbol right next to a SMS topic. So we use um subscription filter here because not all of these components are interested in all events. And by using subscription filters you can avoid unnecessary work in your downstream components, which means just consuming a message and then figuring out, OK, I'm not interested in this and throwing it away, especially with a lambda function. This is just compute you have to pay where you don't get any value out of it. OK, now let's focus into the middle box, the event persistent component. This event persistent component is responsible to store every event in Dynamody B in this case, so that we have a history of all events we have processed. So this process is quite straightforward and after this event is processed, this request is in the status created in our database. The only thing to mention here is to keep the latency and cost low, we use AWS batching um in our integration from our lambda function to SQS. We don't use uh a batch window. But using a batch size of 10 provided us with the best balance between speed and cost efficiency. At the same time, in the top right, uh, event distribution layer. This event was also processed. And in this lambda function we transform this event to the format an external subscriber is expecting. And in this pilot we only subscribed to final events because our Subscription mock here is only interested in final events because in this component we calculate the entire time which takes from seeing the requests the first time until we have pushed the notification about the successful execution of a request to the my BMW. And we do this by leveraging the embedded metric format. This is an easy way where you just lock your a predefined adjacent structure to a system out, and cloudwatch locks and cloud watch events will take care of creating a custom metric for you. And in our case this was a metric of the duration when we have seen the message or the request the first time in API gateway. We store this data in Dynamo DB and the time when we see this final request here. OK, at the bottom of this uh event processing box, we have this event life cycle component. And this is a component we discussed quite heavily what the best architecture will be. So for this you have to know that within remote services at a given time there can be only one. Active or in-flight remote service per vehicle, which means for every request which is triggered by the my BMW app, we have to make sure that this request will end in a final state, whether it's a successful process or a failure. And if you imagine you park your car in a parking space where you don't have connectivity, this wouldn't be the case. So the request would stay forever in the pending state because the car is not receiving this message and also not responding. And therefore in this component we have this lambda function which calculates the maximum expected time for an event status change depending on the remote service type and the status. And then our solution We put a new message into the second queue, which is an SQS delay queue. So for example, if you expect a status change within the next let's say 10 seconds, then we would add this message in the second queue with a delay of 10 seconds. So the message is in the queue but cannot be read from subscribed lambda functions, for example, until this time is over. And after the 10 seconds are over, This message comes visible to the lambda function which is consuming this event, checking a dynamo DB whether there is a state change in between or not. If yes, everything is good, we can drop the message, but if there was no state change in between, this lambda function just incurs a new event into the event processing component indicating that this request execution failed. OK. Let us go back to the event creation service on the left side where the real business logic is happening. So parallel to whatever I have explained before. We are processing the incoming request and here again we have, we are using the parallel state which allows us to validate. And enrich the event just to save time. BMW is heavily using Java and also our lambda functions are implemented in Java. And to keep the lambda cold start agency low and also costs low. We are using RVM to compile these lambda functions to native code, which means you don't need a Java runtime to run it. It's really just a native binary you can execute. And it helped us a lot to lower the The latency And after these lambda functions were executed successfully, So as the event is enriched, it goes into another parall layer step. On the right side again. We send this event to the event processing component and eventually the status is changing to pending. On the left side We are putting the message into an SQSQ which belongs to the outgoing vehicle communications service. This message is consumed by an AWS Fargate service, which is then translating this event into an MQTT message and is forwarding the message to the MQTT program. And When the vehicle is connected to the MQTT broker, it will receive the message and then start working on this request. Asynchronously, the vehicle is sending back a message to the MQTT broker acknowledging the receipt of this message. So this message is consumed by an AWS Fargate service running on our incoming vehicle communications service. So this Fargate service is also only simply forwarding this message to our second step function. Which then again in parallel is forwarding this event to our event processing service which will then update the status in our database to running so we know that this request was received by the car and the car has acknowledged the receipt. And then depending on the remote service type and also the status or event type which we received, we have to execute a more complex or a much easier workflow to to work on this response from the vehicle. In our use case, there is nothing else to do and we have a very easy workflow here where we don't have to do anything else. So And when the vehicle Has finished The execution of this request. It will send another asynchronous message in the exact same way as I have explained to the MQTT broker, which is then processed in the same way, and the status in the event processing component is changed to execute it. So this is how remote services works in in our new architecture. And now let's take a look at the result of this pilot and yeah, what did we achieve, Ludwig. So, let's have a look what we did achieve and we want to do this by iterating over the goals we set ourselves up from the pilot and see how we performed. So as one of the first goals, we wanted to significantly increase the time to market. And if you recall the high level architecture Christian just explained, you see the four main components, and then you see how they are broken down again into these little microsurfaces. And by breaking down our monolithic architecture from before into these little building blocks, each responsible of a certain business logic, um, we were establishing a very strong separation of concern. And what this allowed us to do was to, to split up and to parallellyze the work. So during the pilot, we had several two-person teams working on completely independent parts of the application and of course, of course this um was speeding up the entire development quite, quite well. Um Additionally, what we introduced with this architecture was a very high extensibility. So for example, if you were to add a new remote service, use case. Um It's mainly not code that we have to write, it's mainly going through all these little modules and deciding if it's relevant for the use case or not. So mainly it's adjusting subscriptions. Of course we have to serialize and de-serialize the payloads towards and from the vehicle, but other than this, we can already introduce a new command. Additionally, if you have a look at the step function in the event creation box, um, we can easily extend it, right? So if, if there's a requirement to do additional pre-checks, we can fetch another API and we can do another conditional check over there. Also in the event processing, as you can see, these modules are all subscribed to that event status topic. This can be easily extended. So one example, for example, is we are running in more than 100 different countries worldwide. They all have different requirements from authorities and sometimes we need to disclose certain information when we fetch data from vehicles. Well, we can just add this regulatory disclosure module, have it subscribe to the relevant topics, right, the relevant um vehicles and relevant commands, format the data and push it out to the authorities. So it's very extensible as well, and we've quite significantly reached that goal. A 2nd goal of ours was to reduce the maintenance effort for the team to run remote services. And I'm going to do this by comparing the legacy architecture and the surveillance pilot side by side. And first off, at the top, let's compare the code complexity, the code code maintenance. It's identical, right? We, we, in both worlds, we had to take care of our framework and the Java JDK and also the, all of our dependencies. But way more interesting now is the infrastructure maintenance, because in the legacy world we were using a proprietary API gateway where we had to update its versions and install security patches. Um, we are running on Cupernitas, so we had to take care of all these versions, check for braking changes. Then the containers inside of the cluster were responsible for the operating system and the Java runtime. And we're using Aposcris RDS database. It also comes with versioning. And as we're not supposed to persist the events forever, we had another no JS script edit that would regularly delete some of the jobs as Posco doesn't have anything like this built. So if you compare this now with the new um surless world, well, we use AWS API gateway and it's fully managed by AWS for us. We have, we don't have to take care of anything. Then instead of Kubernitis, we run serus and lumbar functions, step functions, queues and topics. And they're also all managed by AWS for us, so we don't have to take care of them. And you might say, hey, but still, the Java runtime is attached to the lambda function. But as Christian just explained, as we are building natively with the GVM and we're only pushing binaries to the lambda, um, there's also no Java runtime to be taken care of. Then instead of the Posco RDS we were using a Dynamo DB also fully managed, and it brings this time to live functionality on document level. So A AWS would delete the events after a certain amount of time for us. So as you can see with the new surveillance pilot, we almost stripped down all maintenance. Um, effort, um, the only little, little bit of effort left was that vehicle connector that is running at Fargate. Um, in this new architecture, we also have to make sure That we can scale to to the required scale BMW needs and Also under a sustained load, and for this we set up a load test to verify, to demonstrate this. And here on the left side you can see our artillery configuration, so we are using artillery, well known load testing tool. And in the 1st 2 minutes we were slowly increasing from 1 request per second to 5. In the next 5 minutes we increased the load from 5 to 100 requests per second, and then over the next almost 3 hours we were running a sustained load of 100 requests per second. You can also see that we have configured two types of remote services here remote horn blow and remote light flash. And the result of this new architecture was we could run this sustained load in almost 3 hours, and in this time we processed more than 1 million events or requests without a single error. And because we are moving from a monolithic application to a distributed application. People, um, had concerns that increased latency will be not acceptable and that that we might not be able to process P99 in London in less than 1 2nd. And during the low test we also measured the end to end latency with our custom metric I just explained before. And yeah, as you can see, our P99 metric was way below 1 2nd. And if you look at the P50 metric, we could demonstrate that every second message could be processed in less than 400 milliseconds. And if you may be wondering what this latency spike is, so we also took a look after our pilot and this and solved this. So this was because in our pilot setup we are not setting HTTP request timeouts in a proper way, but now when the request is not coming back within a second, we just drop the request and retry and this helped us to improve the P100 metric as well. And as I mentioned, uh, cost is an important aspect for BMW as well. And after we have defined the architecture, we are using the AWS calculator to calculate, calculate the expected AWS infrastructure cost. And the quick rough estimate already indicated that we have or we can substantially reduce AWS infrastructure costs with this new architecture. And the AWS Coast Explorer provided us with the proof after running the low test. So when we are provisioning the pilot architecture, Every AWS service was tagged, and this helped us to attribute the AWS cost to each and every service. And after we have done this low test, we looked up in costs explorers, the AWS cost broken down on an hourly basis. And for services like lambda, SNS, SQS, step function, API gateway, which comes with a pay as you go model, we could simply take the cost of executing 1 million requests and extrapolate this to the scale BMW would typically sees in a month. For other services like Fargate and Dynamo DB storage cost and cloudWatch storage cost. We also took the cost from running this low test 3 hours and extrapolate the cost of running it a full month. And the result of this low test showed us that we were able to. Decrease AWS infrastructure costs by 20%. So after we had successfully finished the the pilot um and uh having all the goals reached, we went on and decided to, to build the full blown enterprise solution. And during the implementation, we went on a couple of iterations, and I wanna take you along um to the most interesting ones. So for the first optimization, let's have a look at the communication towards the vehicle via MPTT. So, so far in the presentation we've always abstracted that away by saying, well, the vehicle is connected to the back end, doing some, doing a um MQTT broker um residing in another account. So let's zoom in a bit further here. In our legacy architecture, we connected to the broker inside of our monolith running on the Cupernitas by implementing the BMW libraries wrapped around the broker. And So the first step when we implemented the the pilot was we moved that into a container running on Fargate. That would still implement the libraries and would still directly connect to the broker and it did work. But we were not completely happy with this. So we reached out to the broker team and discussed some ideas for improvements and luckily they were already onto something they would actually build um on the bottom right they would build an API that would abs abstract away how we would place messages on the broker, right? So they would offer an API. And we could simply invoke that API from within our new account. Pass the payload we want to send to the vehicle and reference a topic, and they would then go on and place that message onto the broker for us. And there's a lots of, there's lots of benefits for this because with different vehicle generations, we have different um framework versions, library versions, it's all abstracted away now, um we just have to call an API. And for the topics, for the MQTT topics we're telling the broker to subscribe to, they would receive that message and place it into our incoming vehicle message queue. So just like that, we would be able to remove that, that connector on Fargate, which was directly connected to the MQTT broker, um. With that serverless components, right, just invoking an API and receiving the results in our queue. And if you recall that maintenance slide from before and I said well there's some maintenance left actually now having that serverless connection towards the MQTT broker we almost stripped down all of the um the maintenance efforts. There's still some certificates to handle and some secrets, but um no more software maintenance. Then optimization number 2, the vehicle simulator. So in order to validate new vehicle generations, um, new commands, new use cases, we cannot always rely on proper hardware. Right? Sometimes the prototypes are built very late in the stage, um, so we need some sort of vehicle mock. So we have to take that car out of the picture. And At the top you see the legacy um implementation and we had back then already built um a very high level um vehicle simulator that was running also on Fargate connecting to the broker as a vehicle um this implied having all different security aspects in place, certificate handling so it was, it was quite the the complex project. And we could only mock some, uh, a few use cases and just the, uh the good case, right? Like everything is working. So now with the serverless connection to the broker, you see at the bottom, well, we went on and thought, hey, we can do something else. We can just mock that API from the broker in our own account. And use a lambda function to place mocked messages, vehicle messages into our incoming vehicle queue. And so just like this we could remove all the broker and all the legacy stuff out of the picture and we can simulate thousands and thousands of use cases, edge cases, error cases, um, and mainly it's just configuration, um, what type of messages we're placing into our own queue. And what this what this allowed us to do was to actually shift left the entire validation phase because at BMW there's entire departments that would later on at a later stage test all the use cases manually with a real vehicle and by having this um uh simulator in place now we can shift left all that validation. So while having the connector and the simulator in place, we went on um to the blue and green um deployments because so far when we were updating our infrastructure with terraform there was mostly just downtime involved, right? So like if you would um decommission and commission some new lambda functions and edit something at the step function or gateway. So we went on and experimented with lambda analysis and API gateway stages. But we realized we couldn't completely separate the traffic, the customer traffic from the new deployments. So here's what we did instead. We added an application load balancer in the very front. And we gave them, we gave it target groups and based on weights it would route the traffic to either a blue or a green execution environment. So we had duplicated all our relevant components and we can easily do so because it's pay as you go. We don't pay any extra for it. Surely there's also some shared components like the persistence layer, um, but all the main components have been duplicated. And what we also did, we added a custom header to the application load balancer, um, and this would allow us to override the weights, and we could have all the customers run on blue, for example, deploy a new version on green, and then use that server simulator I've just explained from before and run it against the green environment and only if all these test cases are 100% successful, we would then switch over the traffic canary style from blue to green. This was the outbound traffic, right, so like customers triggering the app, API gateway is invoked, and we send a message to the vehicle. Now the interesting question is what do we do on the way back? Because when the vehicle then answers us, acknowledges us, it received one of the messages. Um, we get the message on our queue, we use a lambda function to deserialize the payload, but then we actually don't know which environment it was triggered on. Was it blue or was it green? And during the deployment when we have half the traffic on either environment, we want to make sure that it's running in the same environment it was triggered on. And when we send messages to the vehicle, we lose all context and in our stateless system we don't know which environment it was. So what we did is we invoked another step function. And within within that step function we would fetch data from um Dynamo DB and actually it would be the um event ID that we had created during event creation and where we persisted the execution environment next to it. So we'd then find that event ID that we received from the vehicle and we say OK, it was triggered on blue. So we found the result. Going the right path of the step function and then would place that message onto the um blue topic. But also if you recall, there's some events from the vehicle where the user didn't trigger it at all, like a theft event when someone is trying to steal your car. So when we fetch um that event ID we won't find anything in the database. So we go on and follow the left path of the step function and fetch another table that reflects the weights of the application load balancer and that way we know whether it's fifty-fifty or 80/20 and then we then also know where to process and where to put the message onto. Just like this, we have increased our resilience tremendously um again. So the very last optimization I do want to share is developer decoupling. So in order to provide um quality releases, um what we of course did was setting up different stages in our accounts, so like the production environment and the integration in the test environment. And developers, of course, want to be able to quickly smoke test their new implementations locally. And with the legacy approach this was quite easily um doable by just running a local docker container and spin up that uh that uh implementation um well in the serverless world it became a bit harder because people could um invoke their lambda functions locally but for the end to end testing you need the whole chain, right? You need step functions and cues and you wanna check the subscriptions on your topics. So the developers need to deploy to our test stage and then run the tests. And with several developers in place, this of course became a bottleneck and because there were conflicts we had to sequence the deployments and we had to sequence the testing and this of course was a big bottleneck and so we thought of our activities we had done for the blue and green deployment where we had duplicated all components. And we thought, hell, when we can duplicate them, why not splitting them into more parts and why not giving each developer its own set of components. So we have a CI pipeline and each dev can now just commission or decommission his own set of components again since it's all pay as you go, we don't pay anything extra here. So, Um, to summarize the presentation, um, I think most of you have guessed it already. It was a full success story for us. Um, we successfully completed the pilots. We went on and implemented the enterprise, um, solution. We then went on and migrated all the traffic onto it, and I wanna quickly summarize, um, all the benefits we're receiving from it. So first off, we have now almost limitless scaling because A AWS is taking care of that for us. Um, our site reliable site reliability engineers, they don't have to worry anymore in the middle of the night, um. And additionally, we could increase the time to market significantly by 60%. Um, We reduced the maintenance effort quite a bit, um, and therefore have quite the efficiency gains. Um, that's also one of the reasons half of our team can now focus on other innovations because we can, um, uh, extend and also run the stack with less people. We could reduce our costs. Um, there's no idle non-profit environments that are producing a lot of costs anymore. Um, also with that serverless simulator in place and the shifting left of all the validation phase that saves tens of thousands of euros because these entire BMW departments have to test less later on. And we could do all that um by keeping the latency the same, right? So usually if you come from a monolithic architecture and go to a distributed um event driven system and all these components are connected via the network, usually it adds latency and we did some iterations and some tweaking and we could actually keep the latency um identical um and still be able to benefit from all these surless innovations. And with this, I'd like to thank you all for coming. And um Yes.