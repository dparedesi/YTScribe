---
video_id: 9SV_rIFrFh8
video_url: https://www.youtube.com/watch?v=9SV_rIFrFh8
is_generated: False
is_translatable: True
---

Alright, uh, good afternoon, um, I know, like for the past two days you guys are probably hearing, uh, pretty much a lot about agentic AI and generative AI. Uh, we're gonna talk about it as well. Uh, well, all of this artificial intelligence is sort of promising to solve humanity's greatest challenges. Um, it is coming with, uh, one of our more severe environmental crises. So in this talk today. We're going to be talking about optimizing generative AI workloads for sustainability and cost. Um, I'm Isha Dua. I'm a senior solutions architect at AWS. Uh, I've been here just like 6 years or so. I'm looking forward to the talk today. So let's get right into it. So, uh, as of the past decade or so, uh, the data center, electricity demand and the energy consumption has kept pretty stable at around 100 terawatt hour, 100 terawatt hours or so, uh, and we were able to, you know, we had some offsets that we were able to incorporate to actually balance the growing demand of electricity and energy, uh, but in 2021 we saw the generative AI boom, we saw that, you know, foundation models flooded the market. And since then, the energy consumption, resource consumption, and like data center electricity consumption has increased at a drastic rate. The model training size itself has grown about 350,000 times over in the last 10 years, so it's a huge number, and there are also, uh, you'll see there are a lot of studies that are indicating this as well. So in August 2025, uh, Goldman Sachs Research, they forecasted that about 60% of this increasing electricity demand. For the data centers from the data centers, it's actually going to be met by burning fossil fuels, and that's going to amount to around 215 to 220 million tons of carbon dioxide now in the atmosphere. And to give you some perspective, like if you were to drive around like a gas-powered vehicle for let's say like 5000 miles, it would produce a ton of carbon dioxide. So it's a very staggering number. And there are other studies as well. If you go around and look up, you know, you'll find lots of them now talking about environmental crisis that we're entering because of this increasing demand in electricity. There's a 2024 article from the World Economic Forum that also notes that the computational power for these workloads is doubling every 100 days or so. So yeah, uh, like I said, the numbers are sta these statistics and these numbers are staggering. We as developers, as architects, as hyper scales, uh, we have to do our bit to sort of like make innovate more responsibly, uh, build interventions into the system so that we're able to mitigate this ballooning carbon footprint. You know, not just like us as developers, even the hyper scales, they have to look into how do you optimize the building of data centers, how do you actually use efficient algorithms, how do you build efficient silicon for these model training and model inference workloads. So that's where we come to sustainability at AWS. At AWS back in 2019, we took the climate pledge, the climate where one of our core tenets of the climate pledge was to power our operations with 100% renewable energy, and we did meet that goal in 2023. But just like 11 recommendation here would be that if you were to build a generative AI workload or an agentic AI workload, or if you were to consume these services, um, building it on AWS is automatically going to be a huge carbon reduction opportunity. AWS is about 4. 4.1 times more energy efficient than doing this on premises, and this is coming because of. Lots of efficiencies that we've built, like for example, I was talking about powering our operations with 100% renewable energy. We have hardware efficiencies, all of our managed services that are in place, the service teams actually making sure that there are sustainability opttimisations that are built into the infrastructure for these hardware for these services, schools. Cooling efficiency, the way we cool our data centers, we have evaporative cooling. We also use lower carbon concrete in our data centers. So that's another way that we are sort of helping in this cause. Again, we have optimized silicon for model training, model inference, and we're going to talk about it later as well. But what I want to say is that basically just optimizing these workloads on AWS, building them on AWS, can automatically reduce your carbon footprint by up to 90%. So, let's sort of uh jump a little bit into some of the recommendations we have when you are actually building these workloads out. The first recommendation I will have here is using a managed service. When you use a managed service, it lets you operate a little more efficiently because you've shifted the responsibility of high utilization and sustainability optimization on AWS. We, our service teams, our teams are going to manage the capacity underneath. We're going to manage how we're going to optimize the infrastructure. We're going to manage the sustainability of the infrastructure. Uh, whereas you can only, you don't have to do all of that undifferentiated heavy lifting, and you can focus on just building your workloads. Um, some of the, some of the services that you see here are managed services. I'm sure you guys have already heard a lot about Bedrock. Uh, it's a serverless, fully managed service that gives you access to over 200+ foundational models, uh, through a single API. Uh, we also have other services here that you can see on the screen. Uh, if you're very comfortable with the Sagemaker ecosystem, for example, then using SageMaker for model training is a good approach. Um, if you, if you come from the HPC world and you're very comfortable with SLRM orchestration. And you want to do model training, but you want to use SLM as an orchestrator, then we also have Sagemaker Hyperpod which actually can help. But if you're more like, I love, love Kuberneti's orchestration, then we do have EKS as well. That is a managed service, um, that is Kuberneti's orchestration. Similarly, we're building agents in gentic systems and multi-agent systems. We have a service called Bedrock Agent Corp. What I'm the, the main takeaway here would be, uh, using a managed service can help you make more sustainable choices because of the optimizations that we have done on these services in the background. Now, you know, one could ask me like, OK, Bedrock, for if I, let's say if I choose Bedrock, and Bedrock has 200+ models, how am I supposed to make that model selection? How do I know which is the more sustainable model, or how do I know that this is the model that's the right fit for my use case? Uh, now here is where you would, uh, ask yourself certain questions like what, what is my use case? What's the business outcome I'm trying to achieve? Do I need an open source model or do I need. Proprietary model, do I need do I just need it to be like, do the outputs only need to be in English, or does the model need to be multilingual? Does the model need to be very general, or does it have to produce outputs that are very specific to, let's say the healthcare domain or like the financial services domain? So is it a general model or is it a domain specific model? So there are some questions that I would recommend that you ask yourself here. And Bedrock does have a capability which is called Bedrock evaluations. This is, it comes, it comes pretty handy. It lets you assess and compare different models. You have 3 techniques here. You can use LLM as a judge. That means you can judge the output of these models based on correctness and complete. even potential harm. But if you're more used to, let's say traditional metrics like birth scores or F1 scores, you can do that as well. And you have the third method which is you use human in the loop evaluation. You can use your own private workforce. You can use Amazon provided workforce. But it's a fairly easy process. You define the task type, you give it your custom prompts, you define some evaluation criteria, and then you sort of assess the results. Uh, Bedrock evaluation is going to automatically give you the more sustainable model choice, uh, and I think the one key takeaway here is, again, you do not need the best and the brightest and the biggest model out there for your use case. Sometimes a smaller model can just do the trick. Remember, the bigger the model is, the more inference, the more resources it's going to need during inference, the more it's going to cost you, the more the carbon footprint is going to be. So just because it's the shiniest object out there, uh, that's not like the way or the approach to actually choose a model. And one example here would be like chat GPT 3.5. It has 175 billion parameters, but there's an alpaca model which is 7 billion parameters, and they both behave qualitatively very similar because alpaca is a distilled chat GPT 3.5, so. Again, like the use case, the business outcome, uh, make sure that you like, uh, select the model based on what you're trying to do. Now, you know, once you select a model, uh, there are chances that you may have to customize the model a little bit. You may have to adapt it to your use case because it's not generating the output exactly the way you want it or exactly how it fits your use case or your scenarios. So here we have like this sort of. photographic and if you look at this, it goes up in the order of increasing energy consumption and carbon emissions. So the first recommendation here would be that whenever you're trying to first actually let me back up first, never think about training from scratch unless you've tried all of these other techniques. If you have a model, if you have a base model. to customize it. The first thing that you can do is try prompt engineering. Try some simple prompts. Check if this is something that can actually help you, if it's producing adequate results, if you're happy with the accuracy, then this is the simplest approach minimal investment, low cost, lower carbon footprint. It's a good technique which is on the bottom, which is PE. Let's say there's a scenario prompt engineering did not work, and you may have to augment the model with some proprietary information. You may want to add your own internal documentation that could provide the model a little more context to produce more tailored outputs. In that case, retrieval augmented generation can actually help. So that would be the second approach that we would recommend. Now there could be scenarios where let's say the the model is very domain specific. Um, in those cases you may have to actually resort to fine tuning. It could be parameter efficient fine tuning or if that doesn't work, it could be full fine tuning, but fine tuning is another approach that you can try if rag and prompt engineering don't work. Uh, parameter efficient fine tuning is, uh, is again more sustainable because it does only uh. It only tunes a subset of parameters and not all the parameters, so like Laura or prefix or P-tuning, these are techniques that are out there that can help. And finally, like, you know, you have the full fine tuning and the continued pre-training and training from scratch approach, uh, that's something we recommend if nothing else worked out, you know, uh, because that is the most costliest, that has the maximum resource consumption. It's heaviest on the carbon emissions, uh, so, and like time and investment and cost, so make sure that, you know, your use case actually, uh, you, you're able to justify training from scratch for the use case that you're trying to meet. Now, um, another like quick recommendation would be choosing the right silicon or whether you're doing model training or you're doing model inference, um, choosing the right silicon really matters. Uh, there are, uh, EC2 instances in silicon that we produce in-house that are, um, more efficient than comparable EC2 instances. For example, we have the ranium family which you can. Used for model training, which is about 25% more energy efficient than the comparable EC2 instances. This was the ranium 1 family. Now we have ranium 2, which is 3 times more energy efficient. We're also coming out with tranium 3, which is likely to be more energy efficient, so making sure that you're using the right silicon for training when you use, coupled with the managed services that we spoke about previously. is actually going to be good, sometimes better price performance, better cost, and obviously better in terms of resource consumption and energy consumption. Um, and again, for inference as well, um, we do have like the influential instances which are more energy efficient. They have better, 50% better performance per watt. You can use those for inference, but there are scenarios where you may not need GPU instances for inference. There are smaller models, for example, like classifier models that you can, you can actually run on non-GPU instances. So think about like graviton. Instances, uh, those are 60% again more energy efficient than the comparable EC2 instances, and they've like the performance is like I think become 4 times better since we started, since Graviton was launched in 2018. So, um, yeah, so just choosing the right silicon really matters here as well, um. Now if I go to another thing, another technique, uh, sort of like, um, you know, like if you're deploying the model or if you're, uh, if you've actually sort of, you want to generate inference and prediction responses, once you've trained the model, you've adapted the model, you've customized it, you've done all of that work. Now you're at that stage where you're producing inference responses. There are many techniques that are out there that can reduce the model size, that can compress the model, that can optimize the memory usage. And I would definitely recommend that you sort of think about these. There are libraries like Deep Speed and Hugging Face Accelerate and Faster Transformer, and these libraries do offer these capabilities wherein you can compress a model. For example, you have the ability to optimize the model for distributed processing. You can prune unnecessary weights. You can sometimes distill or maybe transfer the knowledge of a larger model to a smaller model that. Like with almost as much with very little loss in accuracy, so model distillation is also a very good process here. Again, try different precision types for efficiency. So quantization is another approach that you can consider. And obviously you're optimizing the models for hardware for the silicon that you're deploying on, so there are techniques during the inference stage that you can implement to reduce the model size, and if you reduce the model size, you're optimizing the resource consumption, you're reducing the cost, and you're also reducing the carbon emissions in the process. And finally, I'd like to also point out that During this entire life cycle, all the way from, you know, you thought of a use case, you built a model or customized a model, you deployed the model throughout this entire phase, we recommend uh building in observability at every stage, having a way to continuously monitor and optimize these models. Once these models are deployed to production or like wherever they're deployed actually. Monitoring is essential, you know, you need to check for like data drift and model drift. You need to check if the outputs are actually relevant in the real world scenario or not. You also need to check for potential harm sometimes, like is there any bias in the outputs or like is there any harmful content that's being generated. So we also need to build responsible AI practices throughout the entire life cycle. We do have many tools that are available for monitoring and optimization. I'm sure a lot of you are already familiar with CloudWatch. You can monitor your CPU, your memory, uh, your disk, and your other metrics, um, but there's also SageMaker Profiler and neuron monitor. These are very suited for like when you are doing model training. And you want to look at training metrics like they also sometimes give you more information about like you know what the data distribution looks like, what's the training, what, what are different training metrics, the loss and the accuracy, it lets you sort of look into the black box of training and actually be able to optimize certain parts that that don't look quite right. And finally, we have the Nvidia Systems Management Interface. For those of you who are using the Nvidia family of instances, like let's say you're using the PE or the, yeah, the P family of instances for training or for inference, uh, you can use the Nvidia Systems Management interface for like looking into, let's say, GPU usage and other metrics that can actually help you optimize further. Um, so, yeah, all I'm like, I think that again, the key takeaway here is that, um, build observability across the entire life cycle, because that's going to help you optimize different paths and different phases of this life cycle. Um, with that, I just want to sort of wrap it up, uh, and maybe like, uh, leave you with this slide, which are the key things that we talked about, um, user managed services. We want to make sure that the base model that we select has been selected, keeping our business outcome and our use case in mind. The biggest and the brightest is not always the best. Um, model customization techniques go in that order of, uh, you know, like progressively think about them progressively all the way from prompt engineering to rag to PEFT and then full fine tuning and then finally training from scratch. Uh, there are lots of inference optimization techniques that are, that we spoke about like pruning and distillation and quantization that can, that these, there are libraries out there that can help you, uh, implement those and. Silicon choice for, um, there are silicons that are out there for training for infants that are more energy efficient, uh, that are, that offer better performance, price performance and performance per watt. Uh, and finally, you know, make sure you're continuously improving this entire process. You're monitoring, you're building observability at every stage, and uh with that, these are some links that I'm going to leave you with. Uh, they sort of talk about the same thing that we talked about during the presentation. How do you like. Optimize different phases of the generative AI life cycle, um, and there's also guidance for optimizing the ML Ops, uh, ecosystem for sustainability on AWS and, uh, uh, with that, yeah, thank you very much. um, thank you for your time. I'm available if anybody has any questions.