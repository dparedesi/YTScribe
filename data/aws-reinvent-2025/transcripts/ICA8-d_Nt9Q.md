---
video_id: ICA8-d_Nt9Q
video_url: https://www.youtube.com/watch?v=ICA8-d_Nt9Q
is_generated: False
is_translatable: True
---

Uh, good afternoon, everyone, and welcome to our session. I'm Ravi, senior Solutions architect at AWS, and today I'm joined by Brian Lloyd Newberry, Associate Vice President, Enterprise Architecture, and Tabari Gon, lead architect, Cox Automotive. Just a quick question before we dive in. How many of you are experimenting with AI agents in your organization? And keep those hands up if you have launched one to production and operating at scale. You see the drop, that's what we're going to talk about today. And you'll see Cox Automotive's blueprint for addressing this challenge. So here's the agenda. I'm gonna do a quick intro of uh Amazon Bedrock agent Cour, then hand it over to BLN who'll talk about Cox Soto's Asian T AI journey and their success blueprint. Tabari, then we'll walk you through a reference implementation, key considerations, and lesson learned from the trenches. We'll stick around for any Q&A, so let's dive in. So the biggest challenge we see getting your agents from prototype to production. What are AI agents anyway? They're autonomous systems that can reason, make decisions, use tools to take actions in order to perform goals autonomously. Your POC works great, but there are 5 key challenges that you must address run time. You need scalable infrastructure to run, run long running workflows with complex orchestration. Without the performance overhead and the cost overruns, agents need to maintain context and, uh, to, to have a meaningful conversation with the end customer across the sessions. So you need to have a several, uh, managed memory offering. Agents, you need to secure access to agents, the tools they access, and the data. Agents need a mechanism to discover your existing systems through agent ready tools and talk to other agents seamlessly. And because agents are non-deterministic in nature, you need complete visibility into their execution so that you can trace, debug, and evaluate their behavior. Let me use an example to drive this better. So picture a service technician assistant. A customer walks in with a check engine light. The technician takes pictures of the dashboard, perhaps records the engine noise, and keys in customer concern. The agent uses this data, looks at your knowledge, looks at the knowledge sources like OEM manuals, technical service bulletins, recall notices, and so on in order to come up with the proper diagnostic plan. The technician starts working on it and the technician finds something else. Then the agent has to adapt and make a different recommendation. So you see this, this back and forth that goes on to adapt and, you know, diagnose the car. The agent determine service and technician, uh, technician assistant determines that they need parts to fix the car. Then the agent goes and checks the internal system pricing, inventory, availability. Uh, and keeps customer preferences in mind. This customer prefers quality OEM parts or this customer is budget conscious. And then recommend the parts. And if parts are not in stock, perhaps agent has to log into legacy supplier applications, web applications, and place order. So this repair can take hours and the agent maintains the session throughout. And when the customer returns weeks later, the agent must remember everything to resume the diagnostic process again. So now imagine this whole infrastructure scaling up, supporting multiple technicians, addressing multiple customer problems. Right, and with full session isolation and you need to trace every single step how an agent recommended a particular diagnostic step and what sources it used for complete debugging and evaluation. So without these capabilities we believe you're stuck in the prototype mode and that's the gap agent core fills. So Agent Core offers 5 fully managed services with 2 special purpose tools. Runtime is secure, scalable, and serverless. It supports multi-modal inputs up to 100 megabytes of payload with long running sessions up to 8 hours. Uh, you can use any framework to build the agent, and you can use it with any model. Memory offers a short-term memory for storing the conversational state and long-term memory to extract um uh uh to, to learn from customer interactions and adapt. Uh, there are out of the box strategies such as user preferences, semantic facts, and summarization. You can use those strategies, overwrite them, or bring your own strategy. Identity provides secure authentication. And credential management, it supports OA and I am out of the box. And Gateway converts your existing APIs and lambda functions as agent ready MCP tools and provides a built-in semantic search mechanism for intelligent discovery. Observability gives you complete visibility into agent's path trajectory with out of the box cloud watch dashboards and integrates with your existing observability stacks via open telemetry format. Browser enables the agent to perform complex web automation tasks, and code interpreter allows agents to run ad hoc, uh, complex calculations. In any language in a secure sandbox. So we believe this is the infrastructure that gets your agents into production. With that, I'll hand over to BLN to walk through Cox Auto's agency care blueprint. Thank you. Y'all hear me OK? Good. So my name is Brian Lloyd Newberry and everybody calls me BLN if you want to know why, I come up after the talk and happy to tell you. I have the privilege of leading the Cox Automotive Product and Technology Innovation lab. Me and a small team have for the last 3 years been completely embedded in generative AI and in this journey to move toward genic software. I wanna start by just introducing you to Cox Automotive because you may not understand the brand, you may not understand Coxomotive as an entity, but if who here is like shopped for a car or bought a car any time, right? Not everybody, right? But if you've heard of brands like AutoTrader.com and Kelley Blue Book.com, you're running into some of the largest consumer portals in the automotive industry. People use them to buy vehicles every single day, and so we get a lot of traffic there. We do websites for the majority of the OEMs, the automotive manufacturers, uh, in, in the United States, uh, so we see a lot of web traffic there. So in the consumer space we see a ton of consumer information coming together. We also provide liquidity in the automotive market if you've ever traded a car in. You probably have had that car, uh, liquidated. The, the dealer took it through Mannheim Auctions, sold it, and bought other cars from Mannaheim Auctions to sell to other people. We also offer an entire suite of uh dealer software things like ERPs, custom CRMs, inventory management, and, and the like, as well as a suite of finance applications, and we're moving into mobility and other other spaces as well. We're the dominant player in the automotive automotive industry for software providers. Because we have that much visibility in the industry, we have. A view of everything happening, we see the auto automotive industry every single day. Just, just as a round number, we got about 5.1 trillion insights about vehicles in the market. Um, we see. Hundreds of millions of customer interactions as they as people look at vehicles, they get their vehicles serviced and and move through that life cycle. And we provide all of, we take all of that information, which is the lifeblood of AI and we wrap AI models around it. We've been doing AI for quite a while. We have hundreds of AI models in production. I think 150 something was a threshold we, we crossed last month. Some of those might be aritic. All right, so what I wanna do today is tell you how as an organization you can get moving. One of the things we see as we work with product teams across the organization or and engineering teams is there's a lot of playing with agenic AI. Although most of the time people are just playing with assistants, not agenic AI, and so there's a lot of generative AI and sort of like noodling around the edges, but people don't really know what is an agenic application and how do you get moving and how do you build it. Uh, our, our chief product officer has this saying, start with crazy and work backwards. And so we're gonna, I'm gonna tell you about a crazy experiment we ran this past August, uh, and then Tabari's gonna show you, uh, how one of the specific, uh, projects that we kicked off went, and then we'll wrap it up. As I said, we've been in this journey a long time. Uh, Mannheim, our, our, our, uh, auction provider, has been around for probably 80 years at this point. I should know that number off the top of my head. Sorry, Roger. Um, we've been, uh, in in 2018, we went all in with AWS and migrated from 50 data centers down to 3 and the rest running in AWS regions, uh, both east and west. Uh, we've been doing predictive AI for a very long time using traditional machine learning models, um, and Kelley Blue Book, that little book that people used to literally buy to look at the price of used vehicles, was a mathematical compilation of vehicle evaluations, uh, you know, that's going back 100 years. In 2023, something happened. So November 30th, 2022, chat GPT was released by OpenAI. The, the, the chat chat GPT 3, model went live. Uh, that created a ton of buzz again we knew that these models were out there. We've been looking at them, created a tons of ton of buzz, and what happened is in March of 2023. We went all in on generative AI. We took this team that I'm leading right now. We threw us in, uh, and within a month had access to multiple, uh, providers, models in our environment. We built a sandbox. We started to build products, and we, and in the following year we had 3 major products live, one of which had saved about $750,000 a year, um, by just eliminating the need to pay for content to be generated someplace else, and our content was better and faster. And then we went in with products for dealers that increased response rates from customers by 50%. And we did other products as well, um. On top of, on top of to power that generative AI model, we really focused on our data and bringing all of our data together and in 2024 on the way back from Amazon reinvent. We launched a new initiative. We set up a couple of teams to go in and dive into the genic space and get ahead of the industry. Uh, there was a lot of. Fud and excitement there but there wasn't a lot of of meat behind it um and we did a ton of work we're really successful driving through buildings internal apps, uh, doing proof of concepts but we. We were kind of stuck. We didn't really get a lot of pull through on our products. We had some. We had 4 or 5 products using generative AI in production, uh, what, a couple of them internal, the rest external, but we weren't seeing the, the pull through from our product teams. People had things they needed to deliver. They didn't wanna disrupt what they were doing. Uh, people were scared. I bet you if you went, if we all sat down and asked what an agent is, there would actually be some debate. Uh, agenic applications people think about as a chatbot with a, with an AI, a, a generative AI interface, uh, strapped on, and that's a very naive version of agentic AI. Google Deep Research being or deep research in all of the deep research tools being one of the, the first examples I saw of the true power of this kind of system. But going back to Newton's principles for from, you know, high school physics. You've got inertia, momentum, and, and reaction, so, um, what you need to do is you need to figure out how to get moving. If you're not changing anything, nothing is going to change. You have to get moving. No one is an expert at building agenic AI systems. Like it's been around for for a year, right? Uh, and having a PhD in math doesn't help you. Because it's a soft skill, it's looking at systems, doing systems modeling, uh, writing text that helps guide these LLMs to do things. Just pick something and get your teams moving. So let's talk a little bit about the craziness. So, uh, the 2nd end of the 2nd week of July, MJ said, I wanna launch 5 products by Labor Day. And there are going to be genic AI products, brand new product capabilities and market. And we said OK, now we've been building things we knew we could do that, but, but we had 5 weeks to do it and the teams that we had had never built Adriic Iops before. Well, Tabari and team had built a little bit, but, but they were a little ahead of the curve but not very far, so we were starting from ground zero. What do you do? Your chief product officer and EVP of product and technology is saying we're launching 5 products by Labor Day. You're starting from ground zero with 5 teams. You've got the blank page problem, so you panic. Kinda like when you have to put together a deck in a hurry, just as an example, um, you, you, you don't know where to go, you don't know how to move, you're stuck. So here's what's necessary to accelerate, have a goal, set a deadline, drive excitement. We had a goal, 5 projects, 5 teams in production. We had a deadline, Labor Day, which was about 5.5 weeks from that point. And we had excitement because what we did is we brought those 5 teams together in a room. We made a lot of noise around it. It was a big event and we said that we're doing this hard thing. We're gonna launch 5 products and here's what the products are. They weren't fully flushed out, um, but they, they had a good core around them and we're gonna use Agenic AI to deliver them. And to in the solutions that we're bringing to market. So you've got the excitement. Now, what do you do next? What are our assets? What are our gaps, and then let's get moving. So what do we have? We were all in our ABS. We're close partner with them. We, we understood what their product roadmap was, um, and, and we had been deeply engaged with Bedrock. We were one of the first customers running Bedrock models in production, um, uh, and first with access to, to, to Cloud. Um, we had a small team of internal expertise, a couple of teams that have built these types of apps, and we had some ideas whether they were good or not, you know, sort of we didn't know yet. We needed to get them into production. And we had to launch 5 things. That's our inventory. So what do we need? I'm an architect. So that's a kind of blank pages are scary to architects, especially in new spaces. So you start out with the reference architecture placemat, right? So this is what are all of we, we, we sat down and the most of the gray in this is stuff that we already had solved for, but there's a lot of white boxes here that we had to figure out things like cost management and model monitoring and orchestration agent workflows and tools and API management and orchestration and how to connect it to our data and how to see what's going on. You know where the story is going because Ravi talked to you about the products that AWS has put together in Bedrock and in in the Agent Core. But this is a really scary problem for a lot of teams because it's a blank slate they don't know what to do so what do we do? We, we were on Amazon Bedrock, we said we're gonna stay here. We're gonna take a risk and we're gonna dive into Agent Core. Agent Core hadn't been released. It wasn't released until after we were done with these projects. We never released stuff into the market and again it wasn't hyper scaled at that point, but we never put stuff on first generation products, uh, uh, technology products into market. We did here because we'd partnered with AWS. We saw the roadmap. We understood what was there, and we knew how silly it would be for us to build it ourselves and more silly for our teams to have to figure out how to build them themselves. You heard some of what Robbie was talking about earlier. So the next decision we made is you, you get into this conversation about which agenic framework to, to, uh, use, and I make the joke, and it's really not a joke that there's like a new genic framework every 2 or 3 days, and there has been for the last 1 year plus just like there's new models every day. Pick one and go What you're doing here we'll, we'll talk about in a second. You have the ability to pivot and change, especially if you're using Agenic AI to help you do that work, pick things and go, and Strands is an AWS backed open source product that runs natively and, and at, at the point we started was, was, uh, you know, best option to go and, and run on, uh, Agent Core, uh, and it met all of the needs we had. Um, the other thing is Agent Core, uh, sorry, yeah, Agent Core could run other, other frameworks, and we could have given one framework to each of the five teams, but. With that maybe we would have learned things, but we really wanted to get a market in 5 weeks so let's focus all of our energy learning one thing and getting really good at it and enabling the teams so focus is an important part of this. So we talked a little bit about strands at the high-level architecture level. Building, uh, we talk about, uh, I think when as Robbie was talking, we're talking about agents and how magical they are and all the things you have to maintain, but building an agent yourself as a developer, there's only a couple of core things you do in the agent one. You write some text, which is the prompt. 2, you make some configuration, which is usually also written in text. Uh, and 3, you attach it to a model with some configuration parameters, and then you attach it to some tools that allow it to do things. Conceptually, that's all you have to do with strands, all you really have to think about is what the text you write is and the tools you use are, and how you take that very abstract system and make it real. You're literally just writing Python is configuration. In this case, super simple case from the Strands website, you're bringing in an agent, you're bringing in a calculator tool that already exists. I don't have time today to show you the implementation of that tool, but if you looked at it, you'd see the Python documentation describing what the tool is and when to use it and what the parameter values mean just like you should be doing with any Python or any source code you write. It's well documented, and the, the strands framework can bring it in and, and wire it in automatically. Um, and then you say to that agent, hey. You know what's the square root of, you know, whatever that number is, uh. This is how LLMs actually solve math problems, by the way, uh, or how they should, you should rely on a deterministic process to do that. Um, it really is that simple, and you can get started in strands running on your desktop today or, uh, running an AWS infrastructure in minutes. So the key message here is get started and and focus your teams in on the features and functionalities, not the frameworks and infrastructure. The language doesn't matter, Jenna. I can write it for you. Uh, the frameworks don't matter. There's 100 of them, and they all do pretty much the same thing. Um, find one that's really well supported and run it. And since you're all here at the AWS conference, start with Agent Core and strands. You can use the other frameworks as necessary. So I'd like to invite Tabari up and Tabari, uh, was the lead architect for one of the projects we launched, probably the most successful project, the most customers using it today. He's gonna tell you about how that like what what he learned and what, what the journey was for that, and then we'll come together and wrap up on maybe we'll tell you how the rest of the projects went. All right, am I on? Great. OK, thank you, BLN and good afternoon, everyone. My name is Tabari Gowen, and as BLN said, I'm a lead architect at Cox Automotive. And so just a moment ago, BLN shared with you our agentic journey, our company's overall agentic strategy, and where we're planning to go with these principles. Now, I'd like to walk you through what that's actually looked like on a product team at Cox Auto and hopefully leave you with some of the patterns that can guide you to building your own agentic solutions. But of course I'm gonna have to tell you how we got here. So let's start at the top. As BLN said earlier, it's around 2022, 2023, and the LLMs are hitting the mainstream, chat GPT, etc. If you were like our team, you were probably thinking around this time. This tech is actually pretty cool. I mean, how can we use it? How can we put it here? How could we put it there, etc. And so our team at the end of 2023 started our first Gen AI POC and oh great, at the beginning of this year we actually launched that product, um, called Predictive Insights. It's a human in the loop, uh, Gen AI message generator for our company's CRM. So with Predictive insights. As you click a button, as our dealers click a button, they can craft personalized messages to their customers using all of the data that BLN mentioned earlier and since we launched in earlier this year, we've got a lot of good praise. Our dealers are loving it because they can move their customers right alongside their journey, uh, with just a simple click. But Here we have a problem. Predictive insights requires you to actually click a button and generate a message to the customer, but the reality is at the dealership, more than half of their leads come in after hours where there isn't anyone to click the button. So we thought to ourselves, all right, this is working out pretty well. How can we take what we just built and move fully autonomous? And in doing that process we realized the actual question was how do we build. Automated AI solutions that our users will trust because it's one thing to say yes we need to build a solution that knows a dealer's voice and knows their brand and can meet their customer at any time of the day. But at the same time, we need to do that while maintaining safety and brand reputation just for us and the dealer, all without human oversight. So that became the challenge, and here's what we built. So this here is our fully automated virtual assistant for customer conversations at the dealership. It's built on Amazon Agent Core and the Strands agent framework. If we start at the top, we have an orchestrator. When a customer sends in a new message, the orchestrator understands that intent and routes that message down to one of several sub agents sales, service, and so on. Each sub agent now understands its own domain and it can handle its part of the conversation independently. So then once all the sub agents are complete, the orchestrator now grabs all of that resulting data and crafts a message back to the customer and continues the conversation, and this goes back and forth for as long as the conversation um is necessary. Of course, as you can see in this diagram, there's more to this story. And sure, we can give you all of the, the technical details, but the reality is when you're starting one of these projects, where do you actually start? You start with your foundation, and that foundation is Agent Core. So When you're building conversational assistants, you know one thing you need is to keep your data separated. And as Belen said earlier. Many of you who had your hand up earlier, uh, you know that these AI frameworks, they change every day, so the tools that you build your solutions today might not be the tools that you use tomorrow, so you're gonna need to build your infrastructure on things that are both safe and that could evolve as your business evolves, and this is where Agent Core comes in. Uh, it keeps your data sessions separated and it allows you to swap frameworks without rearchitecting your entire stack. When my team started our process, the, uh, the guidance that we received from several sources was to start on, uh, agent squad. I think this is AWS Labs, don't quote, but the reality is by the time we got into mid-project, the new guidance was use strands agents. This is, this is a more better solution. Go down that route. Now normally in the old days, this could have taken several weeks to migrate the project, but because we had started with Solid Foundation, it only took us 2. So when you're building your solutions, also start with a foundation that can evolve with your business, and that will leave you the room to think about the problems that actually matter to your domain. Like safety. How do you keep these agentic solutions safe? Anyone? For us, we red team them. So you might be asking, alright, what exactly is red teaming. Red teaming now is when you actively try to make the system go off the rails, and you can do this in a number of ways. For our, our assistant, for example, uh, you could try asking the assistant a question in a foreign language. Um, another option is maybe you can feed the assistant bits and bytes of unreadable characters, or if you're trying to be clever, you could try to sweet talk the assistant to maybe just maybe give it its system prompt or tool definitions. You're basically trying to find your system's failure modes. And now, as you hear this, you're probably thinking, OK, isn't this just testing? No Because testing checks what works and red teaming actually tries to break it and you can't leave this type of solution to the end for us, we red teamed before our alpha phase, our beta phase, and we continue to red team um as we move to production. We, we catalog every exploit and then we fix them and then we iterate to the next thing and we do this after every code deployment. And every prompt change, because realistically, your stakeholders are gonna ask you, how does this thing break? And if you do this, you'll have both the answers and the examples of what could go wrong. And then of course you're gonna have to fix it. How exactly do you fix these solutions? You use your guardrails. So Because we're at this conference, you've probably heard about bedrock guardrails. I'm here to say that most of you know that you need something to prevent AI from going off the rails. That's a given. But when you're dealing with customer service, if you only focus on blocking, you're gonna provide a poor customer experience. So in our opinion, you have to consider your guardrails from at least two perspectives something to completely block the system. As well as something to just gently nudge the conversation into a different direction that, you know, you actually wanna to do something with, we call these hard guard rails and soft guard rails. Now with hard guard rails, uh, the system doesn't even talk really to the LLM usually these guard rails sit right on top, and they're the things that respond with things like, I can't help that, and the conversation stops. These are the things that you can configure with bedrock guard rails. On the other hand, soft guard rails do use the LOM, but you configure them with your workflow or your prompt design. So for example, let's say this is really specific for us, but I think you'll understand, let's say you created a virtual assistant for the automotive industry. And in the process of a customer having a conversation, the customer says, yeah, you know, I really do like this car, but I have pretty bad credit, so I was hoping I could get into this for maybe 100, 150 a month. And you've designed your virtual assistant to solve a lot of problems, but you don't want it to handle pricing negotiation. When you use soft guardrails, you tune the system so that it responds with, that's a great question for our finance team, let me schedule an appointment. And these kind of guardrails allow you to continue to be helpful while staying safe. So think about your guardrails from both of these uh perspectives. All right, so I've given you a lot already. But let's, let's just pause for a second. Let's say that many of you here are trying to hit that orange, ready to launch, and after this conversation you've decided, alright. Let's use the gent core, that will be our foundation. And what this guy just said sounded about right. I'm gonna go home and red team my application. And because we're, we are safe, uh. Practitioners of this agentic landscape, I will also configure my guardrails, so that things don't go off the rail rails, and I'll use the soft guard rails. This point, you should probably be ready to launch, correct? No. Because the reality is, even if you red team your system to see if it breaks, you still actually have to test that it works, right? But traditional testing can only take you so far because these LLMs are probabilistic. So you're going to need to find something that's more dynamic, all right? We know this, so what are our options? Well, we could just manually review every conversation, and that will work for a couple dozen conversations, maybe a couple 100 conversations, but when you're dealing with tens of thousands of transactions daily, manual review just won't scale, so you're gonna have to look for some other option, and this is where automated evaluation comes in. So What do I mean by automate your evaluation? Just as you can use an LLM to generate these responses, you can use another LLM to judge whether or not these responses are correct. This approach is called LLM as a judge, and the process is pretty straightforward. First, you generate a bunch of test conversations, then you run them through your system. Then you evaluate. How did it do? and you track this over time. Once you have actual data, you can use the same evaluation framework to test whether your system is working for the things that matter. So this type of testing actually checks what's right. For us, we track metrics like relevancy, completeness, and tone, things that matter for customer conversations, but of course yours will be different, so find the metrics that matter most for your business. All right, so we're gonna go back to that previous slide one more time. We're still here. We know that Agent Core is where we're gonna go. Red teaming is the right thing to do. Sure, Be our guardrails and the soft guardrails, we got it. And now you know that not only are you trying to test what breaks, but you gotta make sure that it works. So we're gonna automate our evaluation so we can scale up for our actual production needs. At this point, now you're ready to launch, right? I think you already know where this is going. No, because like I said just a moment ago, LLMs are probabilistic, and you could do all of this work and you still will have the LLM do something that you least expected, and the last thing you want is to wake up to a whopping LLM bill. So now what do you do? Well, You need to put something to stop the system before it goes off the rails while you're not around, and that's where circuit breakers come into the mix. So When I talk about circuit breakers. I'm talking about the hard limits that stop the assistant. When things go wrong. In our case We consider two metrics as our hard limits, cost limits and turn limits. So for example, let's say that in the process of a conversation, we've identified that the conversation has hit our P95 threshold in terms of cost. At that point, the assistant will stop. Likewise, let's say that we're having a conversation, and we've gone back and forth and back and forth and back and forth and back and forth, and all of a sudden, I don't know why there's a 2 and a 0 underneath, but let's say that we've hit 20 turns. At the same time, our assistant will stop. In either case, the P99 cost or the turn, we hand off the conversation to someone at the dealership, so now they can decide, does this conversation continue? So these type of metrics, cost and turn, those are things that Bedrock tracks for you, but it's your job to analyze that data and set the thresholds that make the most sense for your business. So set these limits from day one and know where your boundaries are so that when you fail, you can fail gracefully and you can sleep well at night. All right, so. Those are some of the uh the patterns that we have learned through the process of um getting our our project into beta and beyond and let me, let me share with you where we are today. So, as I just said. We're right now in beta and our dealers are seeing the the results that they need. Um, we're hearing that their customers are getting the answers that they need both during the day and after hours. We plan to uh to launch at the beginning of Q1, so stay tuned. Um And that's really all I have right now. Your system, however, is gonna look different because your requirements are understandably different. But if you start with the right foundation, like Agent Core and focus on the patterns that matter, you also can create your automated AI that your users trust. Thank you. Stanford. So I promised you we'd come up and tell you how we did, but before you do that, who thinks we can, you know, you saw one, who thinks that's the only success we had? Oh come on, someone's gotta be, be negative, right? Who thinks we knocked all 5 out of the park? Um, so thank you, I appreciate your faith, um. Let's talk a little bit about it. So we had 5 projects today. We have 3 apps in production in a beta, uh, format. Uh, the first one to be going, uh, full live is the one that Tabare is talking to you about. Uh, we have an app going into production that's actually being used in dealerships that, uh, essentially there's a, when you, when you price inventory, there's a lot of work you have to do, um, and we remind people to do it and they don't do it. Uh, and so we built an genic solution to do it for them, and instead of nagging them to do the work, we said, hey, I did the work for you. Are you good with it? And then we'll move that into full genic moving forward. So that's another type of product we built a couple, we built a couple others that are that, so 3 are, are in production today. One's about to launch a little bit later and let's just say 1 we took back to the drawing board and reimagined. That actually is probably some of the most valuable data we got is what didn't work, right? Uh, what I'll tell you is agenic systems are hard. The hardest thing that we ran into is, is in this is a soft way of programming. It's not deterministic. This whole thing is non-deterministic and so the way you think about products and the way you think about capabilities here. Agents have agency. They can do what they want. You have to put the guardrails around, but that's a hard thing for people to wrap their mind around from a product point of view. So what were our lessons? Let's talk really quickly about our lessons today. So the first lesson is get moving. Nobody's an expert. There's some great Amazon has some great offerings here in Bedrock and Asian Core Strands is an option, but there are a bunch of, like, as you heard, any genic framework can run on it if you, if you prefer something different. Um, the other is to think disruptively. There many people would not have said we're taking 5 projects and putting them with live production traffic with customers in 5 weeks. That seems insane, right? Start with crazy and work backwards. Agenic AI lets you fundamentally change your assumptions. You can use it to build systems faster than you can ever imagine before, and you can have it build things that you could never imagine. And, and so the, the embrace of Agenic is key, and, and, and you need to be using Agenic every day, not just as engineers but as product leaders, as engineering leaders, as architects, because until you get past the fundamental understanding that agenic is different, you can't understand how to build agenic products. I talked a little bit earlier about chatbots. Tabari's application is far more than a chatbot. You have those multiple genic layers. There's a chatbot agent with other agents back in the background doing work. You can build tools like deep research. You can build tools that essentially invert the shopping funnel so that you give customers just click on this button and, and it, and you can buy it. There's all kinds of things you can do that are not just chatbots. Think beyond chatbots, but you've gotta use it every day because it's gonna change the way you work. So Tabare. Yeah. So when you're thinking about those things. And you start building What can you start with right after this? Number one. Start red teaming Today. I know it says tomorrow, but start today. Learn how your system breaks. Learn how to break your system and do these things before your customers get to it first so that you can set up your guard rails and prevent some of the things, some of those things happening down the line. Second, design for your worst case. If you already have an idea of what kind of keeps you up at night with the uh with what your agentic solution could be, build your evaluation framework around that, so that over time you can see whether or not that was truly the worst case and whether or not you've mitigated the crisis from occurring. And then finally, as I said before. Even if you do all these other things. Things will still go wrong, just, just let, let you know, things will probably still go wrong. So set your hard limits, understand what your thresholds are, understand what your boundaries are, so that when you do have these failures, you can fail gracefully. And if you do those things. You'll be all right. So to wrap this up, uh, we're at, we're at, um, you know, AWS reinvent and so I, I was thinking about like what's the right message to land this on. Today is your day one. This is a Bezos quote. Day one is a core Amazon philosophy. We work very closely with him. We talk about day one and we adopt it in our, in our practices and try to, try to model it. Day 2 is stasis followed by irrelevance. Followed by excruciating painful decline followed by death and that's why it's always day one get moving, don't be stuck in waiting for someone else to do this or the right answer, um, get moving and, and build something in 5 weeks or 1. So, uh, I want you, there's a couple of resources here, uh, there's a QR code for leveling up skills, uh, and there's a QR code for the Asian Corps Deep dive hands-on workshops. I, I highly recommend that it's a great way to get your, your hands on and, and, and see how easy it is to, to build these things and so definitely scan those with QR codes, um. And please, please, please complete the session survey in the mobile app. Uh, we'd love to hear your feedback and, uh, how we could do better and, um, and thank you very much. Uh, if you have questions, we'll be up here, uh, for a little while. We've got a few more minutes before we need to relinquish the room, so thank you very much for being in attendance today.