---
video_id: v9i5L37PmtE
video_url: https://www.youtube.com/watch?v=v9i5L37PmtE
is_generated: False
is_translatable: True
---

Alright folks, hello and welcome. Uh, today we're gonna be talking about Amazon EKS auto mode. I know a lot of you are probably familiar with reinvent sessions where they spend 45 minutes telling you how great something is and then 5 minutes showing you how it works. We're gonna flip the script a little bit and we'll start with the demo showing you what's possible with Amazon EKS auto mode. My name is Cy Venom. I'm a principal solutions architect at AWS. I'm joined by Alex Kessner, who's the principal product manager, uh, and the lead PM for Amazon EKS Auto mode, and we also have a guest who'll join us on stage here in a little bit, uh, from Capital One as well. So stay tuned for that, folks. And like I said, I do wanna go ahead and start with a demo. I wanna show you folks what's actually possible with Amazon EKS Auto mode. And we're gonna start here with our familiar Amazon EKS dashboard, but don't worry folks, we're gonna go to the terminal here pretty soon. I just wanna show you what we're starting with. We're gonna make this just a little bit bigger first. Uh, we're gonna go to clusters. I have a cluster that I created not too long ago, um, just to show you folks, there's nothing in the cluster right now. It's completely empty, so 0 pods, 0 nodes. But I promise you it's production ready and I think the interesting thing about this is that, uh, you know, generally with any sort of EKS cluster, uh, when you run get pods and all name spaces you expect to see some things in there, anything for, you know, things like pod to pod networking, maybe coup proxy, maybe the CNI, the container networking interface, but with an auto mode cluster you're gonna see nothing in there but I promise you it is production ready. And I think the first hint for knowing whether it's production ready is uh by looking at the custom resource definitions in the cluster. And here's kind of that hint of you'll see things in here for like node claims and node pools that's the underlying open source carpenter project that gives us dynamic node auto scaling. You'll see some indication here that we can create ingresses and load balancers. Essentially our cluster has these components managed in the control plane to enable it to be ready to serve real workloads. But they're not running in our data plan, so no nodes, no pods, nothing that we're responsible for, nothing that we're paying for, and most importantly, nothing that we need to manage when it comes to things like cluster upgrades. Let's do something a bit more interesting. I'm gonna clear this out and then in the bottom terminal load up EKS node viewer. This is just a tool to let me see what nodes are in my cluster again, no nodes and over here I wanna apply a very simple retail, uh, store sample application. And just the UI component and you know there's no nodes, right, but in just a matter of a few seconds you saw Carpenter pick it up and we saw that it picked a C6A large instance um it's not ready yet it's starting the node any second now it's at 16 seconds, 17 seconds you can see on the bottom right no started networked into our Kubernetes cluster. It picked a C6A large as you can see, and we have one pod still pending to start but scheduled on that note, right? In the second here it should switch to running boom, there we go. Uh, let's do something a bit more interesting. We're gonna scale this deployment up to 10 replicas now. I don't think all 10 replicas are gonna fit in that first node. Nope, see, so it fits 3 in that first node. And then to fit the remaining 7 it's gonna spin up a C6AX large. By the way folks, it's deciding the nodes for me. I didn't script this ahead of time. I didn't tell it what to do. It's gonna pick the right node for the right set of workloads. Boom, there we go. The node is ready. The pods are scheduled, and in a few seconds here we should see them all flip to running. So there you go. You can see a 10 running. Bound on the two nodes and the the deployment scaled up by the way, it's also working on spinning up an application load bouncer for me, but that takes a couple of minutes and I'm trying to show you the full demo in 5 minutes. So let's clear this out and poured forward, uh, the service that we've created here. And then once that's ready, switch to a browser. Here we go and we'll go to local host 8080 and there we go. This is our retail store sample application. Pretty basic. Just, uh, front end UI application. We did scale it 10 times and so you saw with the fresh EKS auto mode cluster we're able to start serving traffic. We were able to dynamically scale the nodes where the workloads work in a matter of minutes and I think that's really the idea here, uh, is we wanna show you the, the rapid pace of, uh, how quickly you can start getting workloads going. So that's my quick 5 minute demo. I have a deeper dive demo coming here in a little bit. This was the what of how things work, but for us to start understanding the why, I wanna pass it to Alex, our lead PM on EKS auto mode. He's gonna talk a little bit about how we got here. Take us back to the beginning. Off to you, Alex. Awesome, thanks. Thanks everyone for joining us today. Really excited to be with you. Um, you know, I think that, uh. It's so exciting to see how quickly you can get started up and running with EKS Autoode today, but it's also important to understand why customers choose Kubernetes and how EKS evolved to get where we are today with Auto mode. Stepping back, Kubernetes was created to help customers manage the increasing complexity of the cloud. Today it's the leading cloud operating model, and its popularity is only increasing. In 2024, 93% of organizations, up from 84% last year, surveyed by the Cloud Native Computing Foundation, are either using Kubernetes in production or actively evaluating it for their organization. Without a doubt, Kubernetes is the de facto standard for operating in the cloud. This is because Kubernetes is incredibly useful. It has a relatively simple set of APIs for managing large groups of servers and coordinating how your applications run across them. Because it was both born in the era and from the complexities of the cloud, it also prioritized consistency. After all, what good is Simplicity if there's different versions of Simple for different applications, let alone all the different places your applications need to run. And while Kubernetes itself covers the vast majority of the functionality that you need to build, deploy, scale, and operate any kind of exciting new application you can think of, it's also extensible, which makes it incredibly powerful. There are currently 195 open source projects managed under the Cloud Native Computing Foundation and hundreds more landscape projects which run on, integrate with, or extend Kubernetes. You can even write your own. OK, so Kubernetes is great. I think you all know that, that's why you're here. Actually, using Kubernetes is great, but running and operating Kuberne's clusters can be quite hard. So this is why 8 years ago, right here at Reinvent In Fact in 2017, we launched Amazon EKS. This was in response to feedback we got from customers that managing Kubernetes at scale was hard. They had to monitor, scale and manage the Kubernetes control plane to meet their requirements for security and resiliency. Let alone find ways to integrate their clusters with other AWS services that their applications needed. Since then, Amazon EKS has emerged as the most trusted way to run Kubernetes, offloading the undifferentiated heavy lifting of hosting the Kubernetes control plane to AWS while remaining fully Kubernetes conformant. This allows customers to focus on how to run on running their applications instead of managing their cluster's control plane. And we didn't stop there. Over the last 8 years, we've been consistently busy delivering new features and enhancements for not only EKS, the AWS service, but also the broader Kubernetes open source community. We started with this basic managed control plane and have steadily added capabilities for compute management, auxiliary software, security, scalability, networking, observability, troubleshooting, and even recently this year, EKS capabilities itself, a new launch that you should certainly check out as it's very exciting for managing operational software on top of the cluster. With more than 250 launches over the last 7-8 years, from expanding into additional AWB regions, reducing pricing, introducing open source projects, etc. we've been very busy. We've even created brand new open source projects like Carpenter that powers Auto mode, Crow, and ACK that have become industry standards. We've gone deep into operating into every aspect of operating Kubernetes in what we think might be the largest scale anywhere. Which is why we now run 10s of millions of clusters for customers every year, and this keeps growing. But when we spoke with some of our customers running these tens of millions of clusters, they told us that while EKS made managing clusters easier, there was still more that we could do to make deploying and operating Kuberna's applications easier, which is why last year at Reinvent. We launched EKS Auto mode. Um, Auto mode is not only a major evolution for easily running, production ready Kubernetes clusters, it fulfills our vision of how we think Kubernete should operate in the cloud. AutoMode provides Kubernete's conformant and AWS managed compute, networking and storage for any new or existing EKS cluster. This makes it easier for you to leverage the security, availability, scalability and operational excellence of AWS for your Kubernetes applications. Auto mode allows you to create application ready clusters like the one that you just saw from Psy at the beginning of this talk, uh, preconfigured with the essential capabilities and best practices from our 7 years, 8 years, excuse me, um, of running tens of millions of clusters. Dynamically scales and cost optimizes cluster compute based on your application's needs. It selects provisions, secures, and upgrades AWS managed EC2 instances within your account using AWS controlled access and life life cycle management. And it handles operating system patches, health monitoring, updates, and limits security risks with ephemeral compute, strengthening your strengthening your security posture by default. Autoode also meaningfully simplifies cluster upgrades, reducing the operational overhead of running Kubernete's clusters, enabling you to focus on the activities that are critical to your business instead of maintaining infrastructure. All of these features help reduce the time it takes to launch new applications and allows your organization to get new products or modernized applications to market faster. Because Automotive Kubernetes conformant, it means you're able to do so while continuing to take advantage of that vast ecosystem of open source tools and software. Auto mode helps improve your application's availability as well by managing the infrastructure where your applications are running, you're better able to meet the needs of your users and organization. Finally, it does all of this while automatically optimizing for cost efficiency both of the compute and your team's time. Reducing the operational overhead of running Kuberne's applications enables you to focus on again those things that are critical to your business, so. I've talked a lot about the work that it takes to manage all of these Kubernes clusters and the infrastructure behind them. Let's take a look at what that actually looked like practically speaking, before EKS Auto mode. Customers had to provision a cluster, install all the essential Kubernetes plug-ins required to run production grade applications, select, configure, and launch the best compute for those applications, and finally with those pieces in place, they could finally deploy the applications into their cluster. But this is just the beginning of actually running these applications. With the applications deployed, you now have to you have to continuously monitor all of this infrastructure and software, repairing it when unexpected issues inevitably arise, and upgrading it as new Kubernetes versions or operating system patches are released so they can remain up to date, compliant and secure. All of this was operational work that you had to take on, which was reflected in the architecture of a non-auto mode cluster. So let's come back and look at, see what this looks like with EKS auto mode. With just one click or API call, Auto mode provides a fully automated AWS managed Kuberras conformant compute, storage and networking for any EKS cluster. Once you've created a cluster with Auto mode enabled, there's nothing else you need to do before you can begin deploying applications like you saw from Psy. Auto mode automatically provides the infrastructure your applications need. Uh, scales resources to meet your application's changing demand, automatically optimizes for cost, and monitors and repairs nodes as needed. All of this eliminates a ton of operational work so that you and your teams can focus on building applications that drive innovation. So the other way you can think about this is what is the architecture of a cluster look like both without auto mode and with auto mode. So here's what it looks like for a cluster without auto mode. Um, the classic AWS managed cluster control plan is on the left in an AWS account, and all of the other Kubernetes plug-ins, software, infrastructure provision, and other AWS resources that your applications need are on the right in your account. With auto mode, this changes quite a bit. You can see that not only does AWS run the cluster control plan as we always have, we also now run a set of essential and integrated Kubernetes capabilities for compute, storage and networking. Straddling our and our customers' accounts, Autoode also launches EC2 Managed Instances, a new operating model for EC2 shared by similar features in ECS and Lambda, just announced even this year. This allows you to delegate operational responsibility for the instances themselves to an AWS service like EKS with auto mode while they continue to reside in your account and function otherwise like normal EC2 instances. Other supporting infrastructure resources like load balancers and EBS volumes continue to exist in your account as they have since EKS launched. You know, one other way that we like to think about how the balance of what each of us is responsible for changes with auto mode is with the shared responsibility model. This shows what part of an architecture is our responsibility and what part a customer is responsible for. With standard EKS clusters, we're responsible for all of the AWS global infrastructure, foundational services, as well as the cluster control plane. Well, everything above that in this diagram from cluster capabilities to compute and compute life cycle management, operating system patching, monitoring, repairing, all of these are fundamentally our customers' responsibility. While some of the 250+ features we've launched over the last 8 years have helped make managing parts of this easier, they're ultimately in your sphere of responsibility to manage. With auto mode, we move this boundary of the shared responsibility model considerably. We take on far more of the undifferentiated heavy lifting, including all cluster capabilities for compute, storage and networking, but also operating system patching, monitoring, health, and repairing the EC2 instances where applications actually run. This lets you focus on your application, your application security, how it's monitored, how it's operating, and any other essential plug-ins or add-ons that you might need. This lets you focus on the things that are essential to your business as opposed to managing infrastructure, of course. It's one thing for me to tell you about how great Automotde is, but it's another to see it yourself, which is why I wanna hand it back over to Cy to go a little bit deeper in his next demo. Alright, let's do it, folks. Thanks for that, Alex, and I, by the way, these last two slides, I just like flipping back and forth between them really fast, and then you can see just how much we'll manage for you with auto mode, right? I, I, it's just, uh, something I like, but the operational overhead that I couldn't offload was figuring out how to do that in PowerPoint. There you go. Uh, that's why I'm here. That's why I'm here, Alex. Alright, awesome. Well, it's time to flip into a demo. Let's see auto mode in action, folks. I gave you a quick 5 minute, just very quick flow and how all of this stuff works, but how about we jump back to where we were. So essentially we had deployed the UI application. We had scaled it up, but I wanna do something a bit more interesting. I don't think any of you folks have seen any other speakers talk about JN AI at this conference, right? No. Well, let me be your first. Uh, we wanna show you folks a real meaty LLM model deployed in EKS auto mode from scratch on an Nvidia-based instance. We wanna show how accelerated compute and this kind of GPU support comes batteries included in an auto mode cluster and hopefully. It'll give you a good sense for how much we're actually trying and helping you manage on the operational side of the world. So first I'm gonna downscale these. So let's go ahead and run a um scale command, and I'll just take this deployment down to 0 replicas. Uh, by the way, that autocomplete is courtesy of Quiro CLI. Check it out. Um, OK, and, and just like that you can see in a matter of seconds, uh, honestly instantly those nodes are now at 0% utilization. They're underutilized, so what do we expect to happen? Well, you'll see for yourself. Um, next what we wanna do here is apply a node pool that is gonna be our kind of accelerated compute here. OK, what do I mean? Uh, essentially, well, if we wanna run an LLM, we can't just run it on basic CPU memory-based instances. We need a proper GPU attached, and we want an Nvidia GPU. So let me open up Quiro again, my favorite code editor. Of course, and uh we're gonna, we're gonna go to the GPU node pull. Let me scroll down and just let me show you folks how you configure a node pull before we kind of took it for granted, there's a general purpose node pull that comes with every EKS auto mode cluster that's gonna let us deploy general purpose workloads. But if we wanna do something more interesting and most of the time you're gonna wanna do something custom for your workloads, you're gonna wanna customize the type of nodes you deploy. And you may remember with managed node groups you would have to kind of explicitly list out all the type of instances you wanna use and it configures an ASG an auto scaling group, and, uh, you know, it integrates with cluster auto scaler and there's just a lot going on there with Carpenter, uh, the underlying open source project that powers the dynamic node auto scaling in EKS auto mode. Instead of, you know, building up the list of instances, we filter down based on our requirements. We tell it we want, um, instances that are either spot or on demand, and if we're lucky today, we'll get a spot instance which is major cost savings on uh G6 G5 instances. Um, we also want it to be in the G class, so G5, G6, uh, greater than instance generation 4 again, so we get those two that we're looking for, and of course within those instances there's large X 4X large, so on and so forth, um, and we give it an architecture as well. So we're telling Carpenter, here's my requirements, figure out what I want, right? All right, let's, let's keep going here. I'm gonna go back to the CLI and actually apply the deployment. So recently OpenAI open source their GPT models they call the GPT OSS. I'm gonna deploy the 20 billion parameter model which does require a a pretty good Nvidia instance GPU. So as soon as I created that, you can see here about 8 seconds ago it decided to spin up a. 54 X large on oh hey, by the way, take a look at these two instances uh that we had scaled down from earlier you can see they're already scaling down those nodes are being deleted because we're consolidating and by the way, if we scale to like 3 instances rather than 0, it would probably have removed one of the instances, OK. This 20 billion parameter model is 14 gigs, so while it decides to pull the image and take some time, we're gonna switch gears here and take you to the console. We're gonna go back to that in a minute here and earlier I cheated a bit and we started with an existing auto mode cluster. It was version 1.33. I do wanna show you how we create a cluster, but over here if we go down to the version of the cluster itself, uh, we can see we can upgrade it to 1.34 and also there's a create backup button here. So one important thing to understand here as a customer of, um, uh, of, of, of consumer of auto mode, you get to decide when to upgrade the control plane. We won't take that over for you. This is a major operation and you should be in control of that. But we do control the node upgrades, all the components that, uh, Alex showed earlier, right? Maybe I can just quickly jump back to that slide, um, and show you folks all those components that might be running inside a cluster. We'll upgrade those for you as well. Uh, you don't have to worry about the worker nodes next time they're restarted, they'll be in the latest version as well. This makes your life easier and you know, take some of that undifferentiated work and gives it to us. I do wanna quickly call out this create backup button. Uh, essentially, um, uh, just, uh, uh, a few weeks ago, maybe a month ago, we announced support for, um, Amazon EKS and AWS backup. So now with the click of a button you can back up your clusters, which we tell you it might be a good idea to do before you upgrade your cluster. It's up to you, but it's an irreversible action to upgrade the control plane version of your cluster. So you might wanna create a backup and with AWS backup it's that much easier. Just a quick call out. OK, let's get back into. It so this is our existing cluster, but I wanna show you folks how to create a fresh cluster. So we'll go to clusters, hit create cluster, and we have this relatively new, uh, quick configuration flow with EKS auto mode clusters. You can go to custom configuration and set everything as well, but we wanted to show you, yes, buzzword incoming, a one click deploy option for auto mode. Um, let's zoom in here a little bit and just go through the options. We'll scroll down. We can, uh, randomize a name for the cluster, so serious pop crab. I like it. Let's scroll down, uh, pick a version, and then look, I know Kubernetes SRE experts that still stumble around creating cluster roles and node roles. We made this easy for you. Hit create recommended role. You click, click, click, and we'll make one for you. In this case, I happen to have a couple of roles in the uh account that I can already use. Makes life easier. I know a lot of you probably use EKS Cuddle out there. This does that for you. Well, now we do that for you in the console as well. Um, OK, let's scroll down here. We have the VPC that's preloaded and the subnets as well, but I actually wanna dive into something, um, a bit more interesting here. If we go to this quick configuration defaults, there's a section here. Let me just zoom in here so you can see it a little bit better. Um, OK, so essentially with any auto mode cluster these are the components you get out of the box and again I know we've talked about this before, but I really wanna drive this in. So application load balancing, when we create, uh, a, a service and we need load balancers, application load bouncers or network load balancers, we'll look at the Kubernetes objects you deploy and create the kind of, uh, corresponding resources in the AWS back end that comes out of the box, block storage, the EBS CSI driver. Container storage interface just the way that Kubernetes is able to help you create back-end storage for staple workloads that comes out of the box as well. Compute auto scaling, I think you all know what this is. I said it like 5 times already. It's Carpenter. It's an underlying, uh, open source project that we open sourced a few years ago. Um, we'll handle that for you as well. And folks at Carpenter, if you've ever set it up before, there's some roles you gotta set up. There's, you know, you have to deploy maybe a managed node group for it. You don't have to. We're gonna run that in our control plane. GPU support. That means batteries included, right? Again, I, I said that before. Essentially any time, you know, if you didn't leverage auto mode, you're, you're doing this thing where you have to line up the kernel version of your Amazon machine image and the and the instance, um, to the device, uh, device driver version for Nvidia. You also have to install the Nvidia device plug-in. And then maybe you, yeah, you want tools like the DCGM Nvidia exporter if you're familiar with that. It gives like in Nvidia logs to Prometheus, I think that's data center GPU manager, DCGM exporter that comes out of the box for you. And then these last couple of bullet points for Q proxy, VPCCNI, core DNS. I know it's like a ton of acronyms. Just know it's like pot to pod networking. It's the networking that any Kubernetes cluster needs. We handle that for you as well. That means not just that you don't have to worry about the nodes they run on, also that we'll upgrade them for you, right? So, uh, those are all the components that come with it, and we'll scroll back down here and we can hit create. And so now you see, you know, what I had to do to prepare. The cluster for the 5 minute demo that I had done earlier. You hit create and you're good to go. OK, now hopefully I've given it enough time. It looks like that pod has been scheduled, um, on that G54X large from earlier. Remember this is the GPT OSS 20 billion parameter model. We're just gonna describe pods. There's only one, so it'll come back very quickly, and I wanna call something out here. Um, we successfully pulled the GPT OSS model from my private ECR. I put it in the ECR before this talk. Um, it's 14 gigs, right? You can see that. I think it's 14 billion bytes, and it was able to pull that in just over a minute. Quick tech recap, couple of years ago, uh, AWS open sourced a project called Sochi. Back then it was for lazy loading, but later on we released a capability called Parallel Pull. Essentially it allows you to pull multiple layers of a Docker image concurrently. Now with generative AI it's extremely important, you know, that time to first token should be as low as possible, so pulling a 14 billion byte model, right? 20 billion parameter, 14 gigs in a minute, uh, well, before Sochi that would take 2 to 3 minutes. So that's already such a dramatic improvement. With auto mode we can figure that for you out of the box, uh, on these uh on these G instances for example we get that parallelpo offered by Sochi out of the box ready to go and this really shows you kind of the things that I didn't have to do to make this demo work. I, I mean you guys saw it. It was a fresh auto mode cluster. I applied, uh, the, the configuration. Uh, essentially, uh, a deployment that had one deployment and one service, and it integrated with the Nvidia GPU drivers. It gives me the logs that are streaming to Prometheus. I have Sochi parallel pole support out of the box, all of these things that I didn't have to do that are handled for me, um, and that's, that's what I mean by, you know, batteries included. Another way to think about it is the infrastructure for auto mode is GPU aware. OK, I think we've given enough time, by the way, after the model pulls it then needs to start it, which takes a couple of minutes. I think we've given it enough time. Let's switch to another tab here. We're just gonna port forward, um, that service that we created earlier, the GPTOSS service. Boom, there we go. Now let's ask it a simple question. I have a copy paste here. It's gonna curl that local host 8000. As it what is machine learning, and I piped it to JQ just to make the pretty output. OK, taking a look here we can see it's a thinking model. So the reasoning here is what is machine learning. They expect a concise explanation and there we go. There's a response. So essentially what did we show here? Essentially it's uh infrastructure that moves at the speed of your ideas, right? It's we, we wanted to run an LLM inference workload. We have a deployment. We wanted GPU based instances. We first created a node pool. We told the node pool the type of. Uh, nodes that could support this workload and then we created the deployment itself. And really when I was thinking about how to make an exciting demo for Auto mode, it, it, it was really showcasing all the things I didn't have to do. It was the, the streamlined experience and so hopefully I got the point across of it, it, it really is that easy, uh, to, to deploy even the most, you know, meaty like, uh, production ready, like a 14 gigabyte LLM model, um, in a matter of minutes, OK. So I know a lot of you have the question on your mind right now this is all great for a demo, but what about for a regulated a high scale environment? Well, to talk about that I would love to introduce. Dan Levine from Capital One. To talk a little bit about how they started using Amazon Ecas Auto mode and how they got to where they are today. Here you go, Dan. Hey folks, thank you very much, Cy, for the introduction. My name's Dan Levine. I've been working at Capital One for about 8.5 years now, and my main focus has been the implementation and maintainability of Kubernete's at scale. Like Cy said, Capital One is a highly compliant, highly regulated company that needs things to be risk averse. In order to do that, we need certain assurances to use a technology like EKS Auto. So, I'm gonna talk about a few things today. I think the thing that might be most poignant for all of you is a little bit about our journey. We didn't get here overnight. There's been 10 years of build-up since Kubernetes has been released that led us to the decisions to adopt EKS Auto mode. I also wanna talk about some of the solutions that we've come up along the way to make it more palatable to be used in an enterprise such as ours. Of course this is an EKS Auto talk. I wanna talk a little bit about EKS Auto. I wanna talk about the benefits that we've received from EKS Auto and our enterprise and a little bit about how our users are using EKS Auto. So, like all good speeches, let me take you back 10 years to when Kubernetes was first born, and a lot of different platform teams within Capital One were trying to implement Kubernetes. We were bootstrapping EtsyDs, we were managing control planes. We were doing all of this in several different areas of the business. All of these platform teams that thought that Kubernetes was a right use case for them had different opinions about how to do all of these things. It was a challenge, created a lot of arbitrary uniqueness within our enterprise, and we weren't able to solve a lot of problems quickly. Now, the three main challenges that we hit because of this were a high SRE effort, constant churn, and scalability issues. High SRE effort because everyone's doing something a little bit differently. They're spending a lot of hours trying to figure out how do I do this correctly. High churn because in Capital One, we have a set of compliance software that needs to run on Kubernetes within the enterprise. This software needs maintained, updated, and controlled. Scalability because frankly Kubernays was a little young at that point in time but also nobody was working together on a centralized solution. So what do we do? We moved to a centralized solution. We called it the federated model. Now within Capital One, we decided that Kubernetes and at the time EKS should be managed by centralized tooling that a team should carefully curate for all of the platform teams within the enterprise. These platform teams would then be able to contribute more to the business value that they were actually creating with their platforms now. In order to do that, we had some tenants. We had some core principles that we really wanted to enforce to make sure it could work at the scale that we needed to. We had several platform teams that wanted to use Kubernetes. Those tenants were to be able to meet our platform teams where they were. We wanted to do it with them and not for them. So we created a sense of shared ownership and empowerment across the enterprise with our platform teams that use Kubernetes to ensure that they were getting the product that not only they wanted. But they liked But how did we do that? We looked upstream. We looked at Kubernetes to see how it was kind of solving this problem within the community, and we used a concept that they've invented the special interest group, or SIG. When we had critical challenges within our enterprise, we would create a SIG that involved some subject matter, and that SIG would solve the problem. We needed to ship metrics off to somewhere. You'd get the engineers that had the pain and the people that were passionate about the problems in the same room, in the same SIG, to solve these problems. All of their solutions would then come right back to the centralized tooling that we maintained and pushed out to the platform teams. This was great. We were operating at a, at a higher scale, we were innovating, we were doing Kubernetes in a mature manner, but. We still had some pain. Two areas primarily that we experienced pain was infrastructure management. And container management. Infrastructure, when you get to the level of scale that we did. You need to make sure that you're patching OS's, you're, you're using the right instances. You're not overprovisioning and spending hundreds of thousands of dollars on dead air CPU cycles. Container management because like I said before we had compliance software that needed to be pushed out to every single cluster. That software needed to be maintained. It needed to be updated. We were spending hundreds and hundreds of hours maintaining all of this and making sure that it was doing the right thing in our enterprise. And it caused a lot of friction. And that's where we get to today. That's where we get to EKS auto mode. EKS Auto mode, like Sy and Alex have been saying, manages a lot of that for us. AWS is in the business of taking away that management pain because frankly, I've used Kubernetes for a long time. I don't wanna manage it anymore. I've been doing it enough. Now I know what you're thinking before I kind of say how EKS Auto addresses the pain, I do wanna acknowledge you don't take a massive financial institution, switch on auto mode on a Monday and say, great, that's terrifying. There's several things that you need to do in order to adopt, and I wanna talk about that adoption really quickly. We decided that to dog food EKS auto mode, we would use it with our central delivery platforms. The centralized team that created the tooling. Well, let's throw EKS Auto at that. Once that was working, once the control plane was behaving like we expected with all the software and compliance containers that we were handing it. We didn't have promises, we had data. We had actual proof that EKS Auto was solving the problem that we believed it to. Once we did that, the conversation became a lot easier. And that data was addressing the two pain points that I talked about earlier. Automated infrastructure management. Carpenter backs EKS auto mode. With Carpenter in EKS auto mode, we no longer have to worry about am I provisioning the right instances, am I using CPU that I don't need, we give it a deployment, we let it run, Carpenter does the rest. It's managed for us. Also Simplified container management. My team is no longer spending hours and hours focused on upgrading containers like EBS CSI driver or the load balancer controller that are table stakes to run a Kubernetes cluster at scale, within a massive enterprise. Right now we're looking at some other use cases for this. The two primary ones for us are machine learning. In a multi-tenant platform, these make great use cases for EKS Auto mode because of the scale that they have, especially in a multi-tenant platform that has several different applications, a platform team won't have to worry about what they're throwing at the cluster again. They deploy it, schedules, the workload and infrastructure is handled for us. Now, Just to close here, there's been a lot of key metrics that we have deemed valuable in our usage of EKS A, but I think one of the best ones is one that's a little more intangible. It's silence And what do I mean by that? Well, before EKS auto mode, a lot of our customers would come to us and they'd say, hey, why is my managed node group not updating? Why is EBS CSI driver stuck in a crashback loop? Why isn't it on the version that I need it to be on? And now with that. Our Slack channels are silent. Nobody's asking any more questions around these problems that we were dealing and spending massive amounts of hours troubleshooting with our platform team partners, and that silence is golden. So with that I'd like to close and hand it back over to Alex to talk a little bit about some of the great features that they've delivered over the last year with EKS Auto mode. Thank you very much. Thanks, Dan. Appreciate it, Dan. Dan, stay on stage with us. It's all good, um. You know, so one of those features, for example, is this Sochi parallel poll and unpack feature. We just launched that this fall, dynamically drastically reduces container startup times and just one of the many things that we've been working on, you know, just like those 250+ features that EKS itself has been launching over the last 8 years, EKS Audible has been busy responding to customer feedback and building new capabilities that help delight and make customers' lives easier, so. We've been making continuous improvements and while all the details are available in our our AWS user guide for EKS and there's a release notes page specifically, I want to highlight some of the kind of most significant additions that we've made over the last year and there's 6 key areas that we've been focusing on, each in response to specific feedback that we've been getting from our customers as they've looked at auto mode, thought about if they could use it and what they might need to adopt. And so first I want to talk about expanded availability. You know how we brought auto mode to new regions including the US GovCloud and local zones, making it accessible for public sector workloads and edge computing use cases. Next, we'll explore the advanced configuration options that give you more control over networking and security while maintaining the operational simplicity that's core to EKS Auto mode. Then we'll dive a little bit deeper into that Sochi Parallel pole feature and explain how it automatically is optimized for GPU instances and why it makes it such a game changer for AIML workloads. We'll also cover our enhanced security and compliance capabilities that make it easier for customers like Capital One to adopt auto mode across large enterprises and highly regulated industries. After that, we'll discuss a couple of capacity management features and then finally we'll wrap up with thoughts about how all these work together to continue to evolve Kubernete's operations. So first, one of the most requested features for Auto mode has been broader regional availability, especially for government and regulated workloads. I'm excited to share that EKAS Automode is now available in all the commercial regions where EKS is available with the exception of the Abis China regions in Beijing and Ningxia. And in 2025 we expanded that to include the AWS GovCloud regions, enabling agencies and organizations with sensitive workloads to take advantage of AutoMode's operational benefits while meeting their compliance requirements. It's a natural fit for AutoMode's high operational and security bar. Additionally, this year we've brought Auto mode to AWS local zones, allowing you to place Kuberne's applications closer to your end users or on-premise systems, reducing latency while maintaining the operational simplicity that Automode provides. This has been particularly valuable for customers running latency sensitive applications like real-time analytics or perhaps games. We've been significantly expanding the configurations that are available in auto mode without affecting its simplicity, and to be honest, this has been one of the largest areas that we've been investing in over the last year. We took a very deliberate approach when we launched Auto mode to have it have a relatively simple set of configurations. That were available and we wanted customers to come to us and tell us what specific other knobs and buttons they may need for it to be a good fit for their use case and so with that feedback we've been busy answering and delivering those kinds of customizations that customers need. So first, and one of the most highly requested features that we had for auto mode was the ability to use separate subnets and security groups for pods as opposed to nodes, which you can now do through pod subnet selector terms and pod security group selector terms. This may not be as familiar because when you're running when you're trying to do this in a standard EKS cluster through the VPCCNI, you do this through a mode called custom networking. This approach achieves the same thing but does so in I think a little bit of a cleaner user experience that's very similar to other kinds of configurations that you may be familiar with in auto mode. We've also added a bunch of other advanced networking configurations, things like the ability to disable or enable the ability to associate a public IP address with the instances that Auto mode launches. And being able to specify whether or not the traffic on leading nodes that auto mode launches should be routed through a proxy so you can specify that certain destinations should be going through a forward proxy, whereas others shouldn't like local host destinations. This is essential to support environments that require this kind of proxy for outbound connectivity or compliance reasons. For customers, we've heard a lot that customers needed to provide various private certificate authority material to the nodes that Auto mode runs to be able to authenticate with other systems in their environment. So you can now do this through the certificate bundles parameter. This lets you provide sort of a set of public key material for auto mode instances to use as they communicate with other services in your network. This is particularly valuable for organizations with internal PKI systems or custom cert authority requirements. These additions make it so that Autode's operational simplicity is paramount while still giving you the configuration flexibility that you need to meet your specific organization's requirements. And I just wanna add one thing here folks. It's like when we launched Auto mode, uh, we started listening to how customers were using it and so I just wanna stress on something you said, Alex, uh, you know, talk to us. We will share with you our, our public road map on Git GitHub. I know Alex probably looks over the, the, the issues on our road map every day. It's probably in his morning routine. So we, we really are listening and, uh, tell us how you wanna use auto mode and. He'll make it happen. Uh yeah, and to be, that's exactly right. Like we, there's a number of other things that we've already heard, things that I think we'll hear in the future about if only Auto mode would let me set X, Y, or Z, it'd be a great fit for my use case. That's exactly what we want to hear, and that kind of feedback is invaluable for me as a product manager to figure out what we should be doing next and, and how we can help enable our customers to offload all of this kind of operational burden that that AutoMode allows you to um to delegate delegate to us. So please keep it coming. Um We saw this live in size demo, the Sochi Parallel image poll and unpacked, and this is a common pain with containerized workloads, especially those using for AIML use cases where the time it takes to download that image and unpack it can delay really meaningful metrics for these kinds of scenarios like time to first token, and these are increasingly critical as companies adopt more and more AIML and inference use cases. We've addressed this, as Cy showed and mentioned, by implementing the Sochi parallel container pole and unpack capability, dramatically reducing the startup time by parallellyzing these operations for different layers in the container. The best part, as Cy mentioned, is that this is also automatically enabled for every GPU enabled instance in auto mode, as well as those that don't have GPUs but have local NVME storage. So these are things like G series instances from Size demo or the P series with you know other like Nvidia hardware, ranium instances, as well as a variety of others that just happen to have that really fast local local MDME storage. There's zero configuration required for this. We've we've done a bunch of work to figure out what the right settings are because there's a fair amount of them for Sochi and applied them automatically for the given instance that happens to be launched at that moment. This drastically reduces the container startup time as you saw in Size demo like what, 2 to 3 minutes down to like 1 minute and 15 seconds or something. Yeah, we've seen up to 60 and even sometimes higher percentage reduction in the total pull time. Yeah, um, and you know as your organizations start to think about how they can, they can leverage AI, uh, this will become, I think, an increasingly important factor in, um, you know, how to, how to serve those workloads quickly and efficiently. So you know Dan Capital One works in a very highly regulated industry, um, a large, you know, a large requirement for compliance and security for us is our top priority. We've added several important capabilities over the last year to make sure that Auto mode can still can meet even the highest security standards. So first and foremost, you can now encrypt both the root volume and the data volume of instances launched by auto mode using custom encryption keys that you provide. So these are like KMS keys that you you you you provide to AWS and that will then allow you to encrypt all of the data that resides on those instances for customers in regulated industries and as part of the launch into the US GovCloud regions. We've also enabled PIPS validated cryptographic modules for EKS Auto mode's controllers and the instances that that we launch. So this means that you can activate these PIPS crypto cryptographic modules using a setting in the node class for auto mode, so advanced Security. Phipps, um, and this will ensure that your clusters comply with federal information processing standards. So a key thing for a lot of customers in the public. Sector or that have customers of their own that are in the public sector and run in these gov cloud regions is to be able to attest that their infrastructure meets these requirements. This is one piece in how you can help enable that with just a single setting in a YAML. One of my favorite new features addresses a common IAM challenge. So you know Cy was alluding to how even seasoned operators will struggle with getting the right sets of IM resources stood up, and you know often it's a really sensitive operation to delegate out to large swaths of an organization. It may not be something that every user in a given team should be able to do, that is creating new IAM roles or attaching various policies to them. So you can now enable teams to use auto mode without having to grant them those additional permissions. They just need an instance profile. This simplifies the access management and maintains your security posture. I think that this is kind of a great example of a sort of a real world example of the principal lease privilege. You don't really need someone to be able to create IM roles to be able to launch auto mode instances, just that instance profile. And so if you, if you're in an organization where there is very careful sort of attention paid to the permissions that different users or personas get. Um, this will be a great feature that will make it that much easier to get started with auto mode. You won't have to go ask your security team for permissions they're gonna wanna, you know, maybe think twice about giving you. Instead, the very simple instance profile is a great way to to work around that. These features are, I think, fundamentally a reflection of our commitment to security by default, which Auto mode has had since its launch through the choice of bottle rocket, its operating system and various other sort of security enhancements that Autoode has, and we hope that it helps give you the tools that you need to meet your own security and compliance requirements. So the last group of features that we have here are around the capacity itself, so like the instances in your cluster. While Auto mode's dynamic capacity management powered by Carpenter is perfect for a lot of workloads, there are absolutely scenarios that require more predictable or reserved capacity. So that's why we've added these two important features around how Automode handles capacity management. So first, Automo can now prioritize reserve capacity, so on-demand capacity reservations or capacity blocks for ML, which will help increase the cost efficiency of your existing investments in these these kind of like pre-purchased options. You can configure this using this new sort of stanza in the you'll recognize a theme here around terms, new stanza in the node class capacity reservation selector terms. This lets you target very flexibly reservations by ID or via tag and will let auto mode say, hey, if I can fulfill the compute needs for a workload using reserve capacity, it'll always try to do that first rather than spinning up an on-demand or spot instance, for example. So I mentioned auto mode by default has this very kind of dynamic and flexible compute management model powered by Carpenter, but not all workloads need that kind of dynamism, and for that we've introduced a feature we're calling static capacity. So this is a very different style feature. This makes capacity management in automode work very differently than it does by default in that you can tell us that you want to tell Aomode that you want to have a set number of instances always running. Regardless of how many pods are in the cluster or how many pods are pending, which is typically how new instances come into an auto mode cluster, this allows you to pre-provision capacity independently from the workloads in your cluster and will ensure that you always have the resources you need for mission critical applications or those that simply just don't need to auto scale. Um, I think these features give you a lot more control over how auto mode manages capacity and maintains all of the operational benefits that you get from auto mode, you know, obviously helps you optimize for both performance costs and your team's time, which is really critical. So You know, as we wrap up, I, I wanna leave you with a few key takeaways from our journey today and uh through EKS Auto mode and, and kind of what it means for how we think Kuberne's operations looks in the future, so. First, Auto mode is a fundamental shift in how we think about operating Kubernetes. While 93% of organizations are using or evaluating Kubernetes, the operational complexity can be a significant challenge. Um, EKS Autoode allows you to move from manual, complex cluster management, that that pain on the the the the maze slide in in dance section, um, to a fully automated AWS managed cluster infrastructure. Second, this isn't just about reducing operational overhead, it's about enabling innovation. As Dan told us, when your platform teams aren't constantly firefighting infrastructure issues, they can focus on what really matters building features that drive value for your business and help, help you and the rest of your organization innovate. Third, Automa maintains the power and flexibility of Kubernetes while drastically simplifying its complexity. You still get full access to the entire Kubernes ecosystem. Autoode's Kubernetes conform it. Anything that you can do in Kubernetes, you can do in auto mode, um. But you just simply don't have to bear the burden of that heavy operational lift because it's Kubernis is important, all of those tools continue to work seamlessly. Finally, enterprise requirements don't have to mean operational complexity. Automode is ready to meet your security, compliance, and performance needs while keeping things simple. Whether you're just starting on your Kubernetes journey or looking to modernize your existing platform, Autode, give Automode a shot and tell us how you think how it works out for you, and I think you'll experience what we've been discussing here firsthand. Thank you all so much. Thank you, Cy, Dan, uh, and enjoy the rest of your reinvent. Thank you. Thanks.