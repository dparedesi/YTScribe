---
video_id: 8vy_UGHKKyc
video_url: https://www.youtube.com/watch?v=8vy_UGHKKyc
title: AWS re:Invent 2025 - Kiro and Amazon Bedrock: Unlock AI Agents for Your Legacy Apps (MAM403)
author: AWS Events
published_date: 2025-12-03
length_minutes: 52.32
views: 922
description: "Are your legacy applications holding you back from AI-driven workflows? In this live-coding session, discover how Kiro and Amazon Bedrock AgentCore Gateway bridge the gap between traditional business systems and modern AI agents without touching existing code. We'll demonstrate how to make REST APIs instantly accessible to AI agents using Kiro's vibe coding approach to naturally understand your intent alongside AgentCore Gateway's integration framework and Model Context Protocol (MCP). Watch pra..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Just before this session I was actually out in the hallway chatting with some folks and found out that this was their very first reinvent and it's so exciting to hear and I love seeing people come here for the first time, just the, the grandeur of it, the excitement of it all, and it reminded me of, of the first time that I attended Reinvent. It was actually back in 2014 and I was sitting in the keynote and Andy Jazzy took the stage and he announced AWS Lambda. And that was quickly followed by Amazon ECS so it was an incredible year of new releases, and it created a, a new dawn of application modernization and for the next decade I would spend working with customers talking about cloud native architecture and application modernization. My name's Ryan Peterson. I'm the worldwide tech leader for modernization here at AWS. So if the last decade of application modernization were concepts like serverless, containers, cloud native, microservices, distributed architecture. We believe that the next 10 years is going to be about agentic AI. I'm gonna walk through a bit of a primer of AI agents for the first half of this session. The second half is going to be spent, uh, doing a demo, so very much code walk through, uh, actually seeing all of this, uh, in action. Uh, wanted to start with just giving a little bit of a history. Of how we got here because AI is a very broad topic and you're gonna hear about it a lot this week and so I wanted to just define kind of what makes now what makes 2025 the turning point to where. Agentic AI systems are going to see a launch within just about every organization that we're going to encounter. So we're all familiar with the idea of Gen AI of conversational agents of answering questions, but how did this come to be? It starts back in 2017 with Transformers, and what this did is it enabled text to be processed in a fundamentally different way. Traditionally using things like bag of words and n-grams, you would have to process text sequentially, and this had great limits into the scalability of how, how big we can build our models. Transformers changes by allowing parallel execution, allowing us a much greater amount of scale. Able to process large corpus of text in a much more efficient way and allowed us to build out models. This was followed by the discovery of scaling laws. So as these models were built and we increased compute parameters and data, we began to see predictable results, increase in efficacy, increase in accuracy of these models as we scaled up these three aspects. What made this. Really fundamental in the change and the path to agentic AI is that it gave license to organizations to invest heavily in these to really increase models based on this predictable path of increased performance. Then we got into this few shot reasoning like type of behavior and what's important here to realize is we're traveling along from just next token prediction into actual reasoning and reasoning came about not because it was specifically programmed or designed into these models, but it was actually emergent based on the bigger models, the more parameters, the more data that was being trained on and compute capacity. Next we got into the chain of thought. So if you remember early models, they were very bad at things like math, but if you can explain the reasoning of dissecting a word problem into its component parts and how it solved it and give an example of that to the LLM, it got very good at it. And this wasn't additional training, this wasn't fine tuning, this was actually just adding this as part of the prompt. Then came tool use and this is where agents began the ability to actually interface with the outside world. This fundamentally changed the role of agents and how they can interact with disparate systems, get access to real-time data, and then actually act out in, uh, the real world and that gave way to the react framework. So how can an agent reason, act, interpret, and then react. And finally, uh, almost a year ago to the day, I think it was the end of November last year, um, Anthropic released the standardization model context protocol, and this gave way to really a large step advancement in agent development because we now had a standard that we can all build towards so going from left to right we really increased the autonomy and business impact that AI agents could actually deliver. At AWS we see billions of agents existing in the next few in the next few years, fundamentally changing how we build, deploy, and really interact with systems across the board. And the analysts agree and see that by 2028 33% of enterprise software will include agentic AI and that's up by just 1% that was observed last year. So we really are at the beginning of the hockey stick of agentic AI adoption. This next stat I think is even more um uh telling is that 15% of our day to day work decisions will be made autonomously through AI agents. So what are AI agents exactly? We throw the term around a lot. And so here's our definition. They're autonomous or semi-autonomous. They can reason, plan, or act with both digital and physical. Uh, physical becomes interesting. There's actually a colleague of mine that took a off the shelf LLM as well as off the shelf, uh, humanoid robotic robotics, and we're actually able to make it work and work and interact with its environment that traditionally took very specialized models and training, but LLMs are actually able to reason and adapt and respond to these things in a physical environment and actually navigate and operate robotics even. We're gonna go a little deeper now and talk about the fundamental components that work in harmony together to actually deliver the functionality of an AI agent. So at the center of this all is really the brain of the agent, the LLM. This is what you hear a lot about. This is where the, the reasoning actually takes place. The goals and objectives, instructions, you can think of this as the mission statement of the agent. This is what defines what it's overarching goals are and guardrails and instructions. Next is tools. These, as I mentioned, are access to immediate real-time data or to um interface with other applications to interface with existing legacy applications. That's really why we're here today and we're gonna spend more time on tools in just a bit. Next is context. This is both short term and long term in a conversational interface which is fundamentally different from the user experience that we're all used to. You need to have context what transpired before, not just in that conversation, but perhaps 10 conversations back when you discovered insights into this particular user. This is where it really changes and this is where Agentic becomes so exciting. It's the ability of agents to take actions on their environments. Observe the response and the results, make adjustments, develop new hypotheses, and then create and deploy new actions thereafter and have an actual loop in this. If you've done any kind of playing around with agentic systems, the one that I actually love to use is like QCLI, for example. You can actually see this in actions where it actually develops hypothesis, might even deploy and create a, um, uh, an on the fly application that executes against data, figure out that wasn't quite right, make an adjustment, and it keeps going until it gets right. This is what makes agentic AI. Um, the, the, the future for many of our organizations to work autonomously to make decisions in a non-deterministic way. Now I need to have a bit of a reality check here because over the past year we've worked with a lot of customers who have attempted implementation of not just agentic AI but just AI in general and the stat here is one that I've seen proven out in the field and that 40% of agentic AI projects will be canceled at the end of 2017. The reality is is that this is actually hard stuff. It's hard because the fundamental infrastructure and architecture is vastly different than what we are used to and a lot of it just really hasn't existed yet to date. These are new standards. These are our, our new capabilities, and it's been incredibly difficult to, uh, to produce and so we call this the prototype to production chasm. And so you start off with this idea of a proof of concept and I'll be honest, like 90% of the time it's some kind of chatbot, right? Um, I urge you all to think beyond the chatbot. There's a lot of great implementations for agentic AI, but you have this idea and it's just so much excitement and potential and you get out there and you're starting to develop your, uh, your agent, and then you try to deploy the production and you realize that there's huge performance issues with your implementation. There's problems with scalability because it behaves differently from regular applications. Security is a concern all of a sudden now you're opening up new, uh, attack vectors that you were not even aware of and governance, you're violating particular regulations um or policies within your organization. And we really fail to get any meaningful business value again because needing to build everything from from scratch is incredibly challenging. And that's why at AWS we really want to be the best place to build the world's most useful AI agents. We wanna empower you to deploy these agents reliably. And at scale And we're gonna do this with 4 fundamental pillars. The first is building the state of the art science, and this is in our own first party models such as Nova frameworks such as Nova Act or Partners and giving you model choice from companies such as Anthropic. Next, we want to provide the best in class infrastructure for running these agents and these aren't just repurposed hardware to run agents. These are purpose built to run agents for you in the best, most cost optimal way. We wanna deliver the best specialized agents because not every agent needs to be built from scratch. There's a lot of agents that can be reused from organization to organization, and we wanna make sure that we're providing those to you in the easiest way to implement as possible. And finally, we want to provide every experience to be as intuitive and easy to use as possible. In the world of AI it should reduce the friction for adoption of new technology. Not make it harder One of the key service features that I saw that got me as excited as that day in 2014 when Lambda and ECS were announced was this past summer when I was sitting at the New York summit and the keynote was being delivered and Amazon Bedrock Agent Corp was announced. And if you've been in application development, uh, for many years such as myself, many decades in fact, you start to see certain patterns emerge and so just like Lambda and ECS provided compute platforms for modern cloud native architectures, you can see similarities and patterns in what Amazon Bedrock Agent Core is delivering and it immediately became clear to me that. This will be the future of application modernization. Bedrock Agent Core, however, Exists in a much more full complete AWS AI stack. And so I talked about those purpose built agents up at the top, and these are things like AWBS Transform Connect, uh, Amazon Que, um, our first party models such as Nova our partner models. Agent Core kind of sits there in the middle with Amazon Bedrock where we give you model of choice, allowing you to pick the right model for the specific application that you're designing it for. All the way down to the compute layer and ML science layer that we've had for many years if you want to go and actually build your own models deploy on self-inference on ranium uh or influentia under our custom silicon for example. So Agent Core exists as part of this broader AWS AI stack. We wanna give you choice and we wanna give you the primitives so that you can build in a way that best fits your organization and your business needs. Go fully managed with something like Agent Core. Or go and control inference yourself and deploy on EKS. So let's walk through Agent Corre just a bit because we're gonna spend a demo going over some of the detail. Uh, first up is the Agent Corps runtime, and this provides the secure serverless environment for complete session isolation, supporting multi-modal workloads, and long running agents. Next up is Agent Core Gateway, and this will be the service that we're gonna demo here in just a bit, uh, but this is gonna provide a connection from the agent into the tools that are made available through existing legacy applications. Next is Agent Core browser that provides you that uh browser runtime uh instance so that you can um perform those browser-based events at scale um and then Agent core code interpreter where you can deploy codes in an isolated protected environment that your agents run. This is all supported from a security perspective with agent core identity and agent core memory helps you to manage that context I spoke of earlier, both the short term as well as the long term. And agent core observability gives you insights, logging, performance metrics across your entire AI agent framework. Let's now dive deep into Agent Core Gateway, which is the tool that will bridge agents to your legacy applications. So when we are working with customers that are starting to deploy agents. It first sees a lot of value just in working with the general knowledge of the LLM and doing things like text summarization or categorization and there is value there but what one quickly realizes is that agents to be effective need access to existing enterprise APIs existing databases, existing knowledge bases, and agents are developed with different agent runtime, different frameworks. And we were doing this internally in Amazon and we were developing tens of thousands of agents internally. And it took months to set up each and every individual agent to interface with all of our other tools and resources we needed to do something different to connect all of these. Agent Core gateway is the answer for that. It's, it simplifies the tool development and integration so it gives you the ability to transform your existing APIs or lambda functions into accessible tools that are based on the MCP standard. We take security as the highest priority here at AWS and that is why Agent Corp Gateway. Provides not just security and authorization for inbound transactions for access to the actual gateway but outbound to your existing API agents as well supporting your existing uh your existing authentication and authorization frameworks. Finally, is a problem that is a fairly new problem as we actually roll out and deploy agents and tools is the amount of tools that you wanna give your agents access to begins to grow exponentially and this causes extreme latency and unpredictable results in that nondeterministic environment. And so Agent Core Gateway integrates semantic search across your tools supported through the SDK. And so we'll go into an example of exactly how that works as well. So here's what the overlying architecture looks like in its most simplistic way. So you have agents that are running and they're implementing the MCP protocol. They are the MCP client. Agent Core gateway presents an MCP endpoint that your agent points to. Again, this is all based on the MCP standard. Agent Core Gateway then is configured to interface with all of your API tools and resources, and those APIs and tools can be rest endpoints, which is a very common architectural pattern for deploying APIs for years, or they can be AWS Lambda functions. So we have a lot of customers that have invested heavily in serverless architectures and deployed function as a service with AWS Lambda. All that investment can be reused and made accessible to AI agents. Let's take a look at how it works in terms of setting up this new agent core gateway. So exposing MCP tools for existing rest service. So we have a rest service that's out there. It's existing. It's running based on the open API standard. We have an existing identity provider and that could be OCTA, uh, Intra, or Amazon Cognito. We'll go into the console and we'll actually set up the gateway. We'll give it a name, a description, and we'll set up the inbound configuration. Using the existing identity provider of the application. We'll then set up a specific target to that REST API again using the existing identity provider and the existing authentication and authorization mechanisms of that API so we're not creating any end run around security we're integrating and making security in a built-in environment. And then finally we'll actually use this from the agent where we can list, invoke, or search with the um search functionality that's provided by Amazon um agent core gateway. Let's go deep into the security. When the client invokes a request to go to the MCPM point for Agent Core Gateway, it's going to need to generate a no-off token that it's going to send along. Agent Corre Gateway is going to then consult the identity provider and determine if it has access inbound to that Agent Corps gateway. It will then check and see which resources it has access to on the outbound side accessing those existing API and tools as lambda functions or REST API endpoints. It then integrates with the existing authentication and authorization using either API keys, IM, or OAF tokens to access those existing API endpoints. All of this is supported from an observability standpoint using cloud trail, Agent Core observability and of course integration with Agent core identity. Let's take a look at the semantic search, and I mentioned that tools we found can explode exponentially, and so it's not uncommon to see customers that begin a path developing these agentic systems and providing access to tools having tens, hundreds, if not thousands of different tools, and the way an LLM looks at these tools to determine which ones to use can increase with each of those tools that you provide. And so this is a big problem with conversational AI. Users have an expectation that responses and answers are given quickly. We had a little bit of a time period there where the newness of it was so amazing that if it took a few seconds and spun around we were OK thinking right for 42 seconds. Uh, that's not gonna last longer. People are gonna want to have that immediate response. It won't be that much longer where it's going to really parallel what we've seen in web environments where users lose interest within what is it, like 500 milliseconds or something ridiculous. Semantic search helps that and so what it does is it actually takes a look at the context of the request and the context that the LM is operating in and allows you to use that to execute a semantic search on the tools that are available within the gateway and so instead of getting hundreds of tools, you get a small subsection of tools that are actually applicable in the context that you're working with them. This improves the accuracy, speed, and the cost. Let's the agent focus on tools that are only relevant for the given task. All of the indexing and the search and the tokenization of all of the details of each and every tool is handled for you automatically by Agent Core Gateway. And all of the search infrastructure is provided serverlessly, so no additional servers or infrastructure to manage to provide that semantic search for your tools. So this was great for us, right? We have the ability now to use Agent Core Gateway provided the framework, the infrastructure, purpose built that we can deploy all of our agents. We still struggled though. We struggled from the developer experience perspective. We still were constantly repeating the same undifferentiated heavy lifting within code to get our agents working across our organization. We started to build some tools to abstract that complexity, to remove that friction for our developers that were building these agents because we needed to move quickly. Last year we released all of this to open source as Strands Agents SDK. This creates a framework that makes it incredibly easy to build agents. What used to take weeks or months can now be done in a few lines of code in minutes. This is open source. This is open use and so we wanted to make sure that we're supporting all of the different A uh AI agent runtime, giving you choice of which models. As well as integration tightly with Agent Core and Amazon Bedrock. So this provided us with a lot of freedom to abstract away the complexity of agent communications uh to the developer to build these agents quickly, but as we're retrofitting these applications and making them use in agents, we still wanted to really leverage AI for the actual coding aspect of it. And so we launched Q Developer followed by QCLI. But last year we released Kira. Kairo is the AIA IDE for bringing prototype to production. Introduced in Kira was a concept of spec-based development. So you start with the specifications. The specifications can be generated in a conversational format. The agent helps to develop those, ask questions about those. And builds them into something that's human readable so that you can review and validate that the specifications match what you want to produce in your application. You'll then go through a design phase and then ultimately developing um tasks to actually automate and execute. Quiro is now general availability. If you haven't already used it, I strongly urge you to download and give it a try. Uh, I'm introducing it here. We're gonna use it in the demo, but we have a lot of great sessions that do a deep dive into all the features and functionality of Quiro, so strongly encourage you to go and, and check those out. OK, so that concludes kind of the presentation portion gonna launch right into the the demo now, um, want to kind of display how you would integrate agents into legacy applications. So I started thinking about the demo. I wanted to take something that was fairly well known and something that, um, uh, is, is ready to go and open source and available for you to actually download as well. So if you've been in application development. And you have developed REST API endpoints, you're probably very familiar with OpenAPI, formerly known as as Swagger. So OpenAPI, uh, developed an application that they use for demonstrating REST APIs and, and kind of showing you how they all work in a, um, in a pristine way, and that is the Swagger pet store. So in this demo I actually downloaded the Swagger pet store. Uh, container and just deployed it to ECS on Fargate. I made no changes to this application, just downloaded it from the GitHub repo, launched it up into the, uh, container, uh, and it's up and running, and so it was responding to API calls, um, passes all the tests and everything, but absolutely no code changes whatsoever. So swagger pet stores. Oh, That's weird, uh, so this is the. Open a got some AV problems going on here. The. Is it just that slight? Let me go to the next one. OK, apologies, I'm not sure what's going on there. Well. Imagine this is showing an open API specification. Um, if you're not familiar with open API, we're basically defining each of the endpoints. We give it a, uh, description, um, what the operation is, what the parameters in, and one thing I wanna really emphasize here is agent core gateway, the way that it, it operates with the open API spec and the way that it makes itself available to agents, it's highly dependent. On your specification being very well documented because it's going to use that specification to define the functionality and tasks of the actual agent. OK. That's weird. All right, let's see if I can get this to work from over here. And we'll start the demo. Here we go. All right, so first up is we are going to actually take a look at the um settings for uh Quiro. The agent settings have this concept of steering files. So the steering files are what defines um how Quiro operates. um, so I'm actually just gonna go ahead and set this up with the strands agent SDK read me as the steering file. Um, so I'm gonna go ahead and make a directory for. Um, for Quiro in the, uh, in the steering, and we'll go into dotkiro steering. That's where the steering files for Quiro exist. And now I'm gonna go to the strands agent SDK website here and we'll just click on the GitHub, uh, repo. GitHub repo has a readme file um for strands SDK. This is where it defines what the Quick start is, installation and example implementation. Let's grab the raw URL here and we're gonna drop this into the Quiro steering directory. OK, so now we have a steering file for Carro that's gonna explain Strand's, uh, SDK. This is right from the repo. I didn't change anything. Um, let's go ahead and open it up and take a look. Under the Cro directory, there we go. And Read me file is uh all there. Let me clean this up a little bit. OK, read me file is all there. It's in kind of an HTML format. You can see it now in the Karo settings. There's the agent steering, and we have this refine button, and the refine button is what is going to actually re-output the read me file in a format that the AI agent actually understands. So you can see it's analyzing the RMI file. It's now converting it into a steering document that's a better guide for the AI assistant. You can see all of it changed. It dropped all the HTML stuff, um, put it into a markup down file. A read me and it kind of gives a description of exactly all the changes um that were done so structured around core concepts made it easier to read you can see the differences there um from what I had and all I needed to do was click on that refine button. Now if I open it up I can go into kind of a preview so we can actually see that in a little prettier way, um, but this isn't a format that the AI agent now can fully understand. Um, this is a Kira steering file and so this is going to help in all of the subsequent interaction that I have with, uh, with Kira. OK. So clean that up a little bit, um, and Cool, so now let's go ahead and start with um kind of this vibe or spec. I talked a lot about spec, but for this purpose we're gonna go ahead and do vibe and I'm just gonna say hey let's let's create a minimal uh AI agent in Python, uh, that is terminal based. And I'm going to specify that we're going to want to use Strand's agent SDK. That's it. And so you can see that we're including the steering file that's the read me document that I downloaded and um uh I refined so that the uh agent within Quiro can understand it. So it's referencing that it's creating the requirements for the application. It's creating the actual Python file. It's creating a read me for this application. So this is already so much better than the developers I've worked with where documentation actually exists, um, while the coding, the code is actually being written. Once it's done it actually gives a full definition of everything that has been created project structure features, um, and all of that so let's go ahead and take a look at the actual. Um, the actual agent that was, uh, developed in the actual code, so that's all an agent.pi. So what you can see here is in just a single prompt and following the steering file it's created an AI agent and this is a fully functioning agent, but look at the code that got created, it's incredibly minimal, so. A bunch of comments. You have two lines of, of, uh, kind of import statements there and by line 23 it looks like the agent is already well defined. So a few lines of code and strands SDK abstracts a lot of the complexity of, of doing the streaming, doing the IM authentication, connecting up to the Bedrock SDK. All I had to do is say, hey, I want a bedrock model and I specified Nova Pro here. Actually Kiiro picked that. Um, I'll kind of go with it for now. OK, so looking down, um, beyond that, the rest of the code is really just kind of doing terminal management, handing the inputs and outputs and the prompt because remember I said I just wanted this to be terminal based. Now you could do something like open web UI if you want, but for this purpose, uh, I'm just gonna go ahead and, and do this in the, in the console opening up the ReadM, it not only says what the application does, but it says how to actually set it up. Uh, requirements are all the libraries that need to be installed and it's Python compliant. OK, so let's go ahead and give this thing a try and actually run this agent. So I could follow those instructions that it gave me, but I'm actually gonna use Quiro here and I'm gonna say, hey, run, run this agent. And so this is in a new session, so Quiro's looking at the read me of the application, um, how do I run this thing? What's the code? Let me validate it. Let me see the requirements here and already it ran into its first problem, right? The libraries aren't actually installed already, so it saw the error, it interpreted the error and said, hey, I need to actually install these libraries. So it goes in and it's going to install the libraries, and then it and then it finds another error, and that is that, hey, everything's running here in a virtual environment, so you gotta set up a virtual environment first. And then it wants to execute that. In the virtual environment but it's saying hey I haven't ran this first so you have to actually give me permission so I'm gonna give it permission and we're gonna execute the um library installation uh or Kiro is gonna execute the library installation for our Python application. So there we go. All the requirements are actually installed now. It's looking at everything saying, hey, it looks like I'm ready to go. I wanna run this thing, but again it says, hey, this is new code to run validate. I do. Here we see initializing strand's agent and looks like everything is running, but Quiro's still waiting because I'm kind of in a terminal wait here so let me exit. Now Carro sees everything is running successfully. There's no errors. Congratulations, it looks like everything's good to go. So let me actually run the agent now and remember that this is a pet store application, right? So what I've done so far though is I've just created a new agent using Strads SDK. I'm not connecting to any legacy pet store at all. I am connecting to Bedrock though, so this is an AI agent that is using Bedrock, the Nova Pro model that I can interact with. So I'm just gonna simulate kind of what I would in a pet store and let's just go ahead and say, hey, you know what? I want to um uh let me clean this up here. There we go, so I'll say, hey, I want to um uh I wanna adopt uh a pet here, but I want the pets to make sure that they're good with kids. So that's gonna actually send the request to the LLM with Bedrock and it's going to answer back using the LLM's general knowledge. So I just said, hey, I wanna adopt a pet and the LM is more than happy to comply. It says, hey, sounds good to me if you wanna adopt pets, dogs are great. Who doesn't love a golden retriever, right? Labradors, beagles, these are all answers from the general knowledge of what a good pet to adopt for kids actually is. And then down at the bottom you can even see that it has some tips again this is the agent uh actually taking uh freedom here to add additional information. OK. Let's now connect the legacy applications. So I'm gonna go to the AWS console. I'm gonna search here for Agent Corp gateway or excuse me, Agent Corps, and I'm gonna go into under Build and deploy we have gateways. And under gateways, I'm just going to go to create uh a brand new gateway and here I'll give it a name so we'll call this um pet store gateway. Uh, I've got some additional description if I want instructions. Here's where I enable the semantic search that I talked about, uh, but for this demo and purpose we're gonna go ahead and leave that off for now. The inbound off configuration I can connect to existingcognito, but for now I'm just gonna use the create a new one. same with the service role you can connect the existing service role, but I'm gonna allow the, uh, console to create this for me. Um, and then here's where I define the actual target. So we have Gateway and Target. So I'm gonna define the pet store. API here And we're gonna define this as the REST API. That's the one you had to imagine earlier, right? Um, so we can upload the REST API or we can actually paste it in here. I'm gonna paste it in and this is just the JSON open API document that I showed you, uh, earlier, so I just copy and pasted that right into the console earlier I created an API key to access the, uh, API, so I'm just selecting it here. Uh, and clicking create gateway, and that's it. All the definition was in the open API, the endpoint, all the rest APIs were all defined, uh, within that open API spec. So the gateway was created successfully. You can see some ARNs and some endpoints there. It says that the status of that gateway is, is ready to go, um, so we can actually start using it. And the target is there and defined and that's listed as ready as well so that target is the actual uh rest API that I defined in that open uh API spec there. The console also shows some invocation code. So here it actually supports strands MCP client uh example code that you can go through and scroll and take a look at and you can actually copy it for implementation if you want to. I'm not gonna actually copy all that code and try to modify it and implement it. Um, what I'm gonna do instead is I'm gonna go back to, uh, Quiro here. So I'm gonna copy the MCP uh URL endpoint here, uh, straight out of the, um, the console. And go back to Quiro and I'm just gonna tell Quiro that I now wanna update this existing agent with the new MCP endpoint that I copied from that console there. So update the agent to use the MCP tools at paste in the endpoint, and that's it. So it's gonna pull in the steering document. The steering document is gonna instruct, hey, here's how you actually implement, uh, MCP with agent SDK. And so it's updating all of that code now. And so it's working through the agent Python file making those updates. There we go. We can see the two different files that it was, or excuse me, 2 edits that it made to agent. Pi. It added additional uh libraries and requirements to work with the MCP and it even updated the documentation. There we go, so all the code is done. It actually even goes through and does a compilation to make sure that there's no errors, validates it, and then it's gonna report all of the changes, um, that were made. There we go. Let's take a look at all the changes. You can see these new code editions, uh, increased uh import libraries that have been added. This is the access token, the OA token that now needs to get created. To actually interface with the MCP and then the actual code to create the agent uh using the MCP URL. So again these are all code changes that I didn't need to make, but Kira was able to make just based on the instructions that I gave to it here. OK, so let's go ahead and set up the final piece which is the actual OAth token generation, the, uh, authentication piece. So here's the token URL that I'm gonna need, but I'm actually gonna need the client ID in secret, and I grabbed that over, uh, from the Cognito console. So remember I said I wanted to create a new Cognito user pool, um, so that got created over here, um, and then I grabbed the client ID, uh, for that you go over on the left hand side under. I think it's applications and then uh yeah applications app clients. So there's the app client that was created by the gateway console definition and my client ID and my client secret are uh available there so that's what I need to actually update. Remember Caro told me I needed to update these to the environment variables. And we can see that in the code that got generated, I have these 3 environment variables that want to get set. So I'll start with the MCP client ID. And we'll grab that from the console. We'll just copy that from Cognito. And we're gonna paste that in here. Next, the MCP client secret, and we get that from Cognito as well. And then finally is the MCP token URL and this is the one that we saw earlier that we can get from the actual code sample in Agent Core gateway so we'll go back there. Paste that final one in. All right, so now we have all of the credentials that we need for the environment variables and so now we're ready to run this. New agent that now has access to our entire um pet store. Let's clean this up a little bit. Make this a little bigger. OK, now we're gonna run the agent. You can see here that we see setting up our MCP connection, and it says connected to MCP endpoint with 18 tools available. So this is all done with the gateway, and I'm gonna actually pose the exact same question. I'd like to adopt a pet that is good with kids, and here's where you'll notice something different. It's actually executing tools. It's using the pet store find pet by status API endpoint. It's finding a bunch of pets that are currently available for adoption. It then goes in and says it wants something that's friendly with kids. It makes a determination of what tags might be, then it does find pets by tags, and it goes through and it finds all of these pets that are available in the running pet store, the legacy application that I did not need to change. And can find all of these different pets by status and by tags based on friendly um and other uh different tags that matches the context of I wanna adopt a pet that's good with kids and I find specific pets here so I have a dog named Buddy. I have a dog named Happy Test Pet uh looks like another dog named Max um and then even presents some additional options there. So let's look at how this works in context. So I'm gonna say give me more details about Buddy. Buddy was defined in a return value before, but it already knows what that specific pet ID was for Buddy and so it goes back to the tool to get details about Buddy and gives different photo URLs, um, specific information about Buddy, how long it's been available for adoption. And then integrates that with the agent general knowledge to give more information about learning about the adoption process, what to expect from a dog, and all of all of these things. So what we're demonstrating what you're seeing here is an agent that is using an existing application that has not been changed. In a nondeterministic way, so imagine this pet store that has a UI that you have to define, you have to think about how your user is going to interact with your system. So you might have a list of pets that are available, but you might not have encountered this idea that people are gonna want to look for pets with certain behavioral traits. But because you have the API that can function this, the agent designs this agent this this access in a way of using the tools to satisfy the requirement, combines that with general knowledge, and gives very meaningful results and allows you to bridge legacy to AI agents and use them in meaningful ways. So let's take this, um, let's take this just a step further here and let's say I wanted any pets that would not be good for kids. So I've been talking about good with kids and friendly so let's throw the agent a curveball and let's take a look at pets that. Would not be good for pet for her kids and so what it's doing is remembering that it found these traits by using these fine pets by tags and so it's iterating on different behavioral traits and personalities and executing that using tools with the fine pets by tag endpoint. With the existing API using the existing application that's running up in ECS. So it found a few pets it looks like and now it remembered that I wanted to get details before so it's actually executing those get pets by ID and it found a few. So here's um looks like a dog and here's an exotic animal and it's even explaining why those specific pets would not make for uh friend would not make good pets were uh that were friendly with kids. So this one that's shy for example says shy temperament may be overwhelmed by children's energy that's combining the general knowledge with the details of this specific pet here, right? OK finally I'm going to say hey, are there any cats because you said a better option was cats here so now I'm gonna actually say let me see if there's any pets by a specific um breed here right? and. It goes through and executes find pet by status again and now it has a a a different objective. It's gonna actually go through and find specific cats and it looks like it found one. It found a cat named Max. It remembers that I was looking for a pet that's good with kids and so it's going to integrate there and using the general knowledge explain why cats are good pets with kids. So that's the end of the demo, but I wanted to just take an existing off the shelf application and show how you can integrate it with the agent. What I strongly urge everybody to do is continue this this journey and dive deep into agent core. You can use the URL. Here we have a lot of sessions that are based around Agent Corp, Agent Corre Gateway, as well as strands, but more than anything, not just deep diving, I really encourage you to start actual using. Don't wait till tomorrow or wait until next week, uh, or even next month as soon as you get back. Think about where agents would make a difference in your organization. Develop that use case. Develop the business value, prove it out. And then repeat. I wanted to thank everybody. I apologize for a little bit of the, uh, the AV issues there, but hopefully you're able to, to imagine the open, uh, API. Um, I'll be around for a few minutes afterwards if you had any questions at all. Um, please enjoy the rest of Reinvent. Thank you so much.