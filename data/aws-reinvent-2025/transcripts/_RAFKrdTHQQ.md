---
video_id: _RAFKrdTHQQ
video_url: https://www.youtube.com/watch?v=_RAFKrdTHQQ
is_generated: False
is_translatable: True
---

Hey guys, welcome, uh, good afternoon. This is really awesome. Um, we are super excited with this, uh, customer panel. Uh, we have an exciting session for you, uh, if I can advance the slide, um, and it's very germane to, uh, what we have been, uh, listening in at the keynote. Uh, how many of you were able to watch Matt's keynote this morning? So you, oh wow, this is more than I thought. So you, you heard about, uh, AI infrastructure and how we're optimizing. You heard about AI factory. You, there was a Tranium 3 launch. It's now GA, uh, we, uh, that he talked about ultra clusters and ultra servers. So, uh, so we have a distinguished panel of speakers. We have Andrea Klein from, uh, Director of Engineering at ARM. We have Andre Dwyer, uh, senior director of engineering at Genentech, and we have, uh, Shawna Gbole, who is a field CTO at Fireworks, and, uh, what we really like you to take away from this session is, uh, when you go back home, you should be armed with some tips and tricks and mechanisms in which, in how you can scale, uh, your AI applications using AWSS infrastructure and our stack, um, and so we have a framework, uh, that seems to work, um. Uh, the agenda is really that I'll set up the framework. We think about scaling AI in terms of scaling up in terms of T flops and, uh, memory and memory bandwidth, scaling out in terms of, uh, increasing the number of GPUs and servers and ultra clusters and, uh, clusters, and then scaling across the infer stack, and each of the speakers will actually, uh, index on that. uh, Andrea will talk about her experiences scaling out, uh, she'll, um. Talk about fault detection and observability tools and how, um, you know, uh, what, what you need to pay attention to when you scale out to thousands of GPUs and thousands of clusters. Uh, Andre will talk about scaling up. He has a very interesting application around molecular dynamics, so a lot of uh LLMs, but also a lot of spatial coding. And he'll end with agentic AI. The genetic is actually using agentic AI for what they call lab in the loop. Uh, so this will be a very interesting thing to hear from, and that's actually a good segue to Seanno's, um, uh, session where he's going to talk about the principles of building, uh, agentic AI and how they have been able to optimize across each layer of the stack at the hardware layer, model frameworks, and runtime layer, uh, to get what, 1010 + trillion tokens per day. So he will talk about. Uh, more on that, um, I'll let them introduce themselves. Uh, fireworks also has a booth, uh, so, uh, uh, Seanak will have more details. All right, so without further ado, so when I think of scaling up and scaling out, there are two workloads that come up, uh, front and center and almost spend, I spend most of my days working with accelerated compute and uh understanding customers' needs around AI and around HPC. So AI as we think of frontier models, 100 billion plus models, they of course need a lot of compute, but even smaller models that are more network latency sensitive like video and image models, they require scaling up and scaling out in terms of inferencing, especially around aggregated inferencing if you're serving batch mode inferencing, but also real time where latency is critical, this becomes important. And the reason it becomes important is these models are processing. Huge massive amounts of petabytes of data and doing massive amounts of parallel computing and scientific calculations. So that's why scaling on infrastructure becomes important and where AWS spends like our team spends every single day talking to customers. And that reminds me, I forgot to introduce myself, so I'm a principal specialist, uh, in our Gen AI accelerated compute team. So everything to do with accelerated compute, uh, one of the hardware layers that I'll talk about, um, uh, comes through our team, uh, train them in furniture, of course, but NVDia GPUs and other GPUs, uh, that's what we deal with, uh, day in and day out. And then HPC also has some interesting quirks. They have uh both tightly coupled and loosely coupled workloads, and I've had the fortune of working in AVADAS. Andre will talk about his protein folding use case. So these use cases also lead to large amount of scale requirements. Um, and so if I, if I dive deep into one of the workloads, uh, if you think of training, these are typically episodic, uh, workloads. They take a few weeks to a few months, uh, but they are tightly coupled. Often customers ask for, when they ask for GPUs from us, they ask for a single AZ. In fact, they ask for a single spine, uh, and one of the reasons why we. Introduce ultra servers that you heard at the keynote is because uh even a single spine was not enough, uh, so we needed to create a high performance domain for them, um, but customization is a little bit different. Uh, these runs are short but very frequent, so the runs could run anywhere from 1 to 4 weeks, and you could have anywhere between 4 and 10 cycles a year. Um, and there you often could use heterogeneous computes, so you might have P series, GPUs, and G series with CPUs. So you have to think about how to scale up using containers, Andrea will have a word or two about that and how to use tools like Carpenter and EKS to sort of scale up a heterogeneous compute cluster. And inference is um completely different. It's often spiky and sporadic. Even batch mode inference goes on for a few hours, let's say document processing or summarization, uh, and these are often loosely coupled workloads, uh, but depending on whether it's real time or batch mode, you could have different latency requirements. So let's, let me think of a, if you think of a mock example of a chatbot, this is a very simplistic example of chatbot producing one output token at a time. Of course now chatbots are much more sophisticated, but by and large input token length still happens to be higher. So the initial portion, the encoding portion or the prefill portion is compute bound, and the intuition there is you load the hyperparametters from the HBM memory. But depending on the length of the prompt, you could essentially be using the entire prompt computation using those hyperparameters, so you're essentially amortizing the one-time load operation across a large context length. But now if you go to the decoding portion, uh, it's the same intuition. You load the hyperparameterss again from HBM memory, but now you're processing output one token at a time. So you're amortizing that load operation across one token at a time. Of course it's simplistic. Now you can have many more tokens at a time and in fact Seanak will talk about how they optimize that the runtime layer, um. And uh we're not done. So think of memory capacity as inference, of course some of the bigger models like LAMA 4 and Deep Seek require at least a P5E, uh, if not P5EN, which is our H200s, uh, so memory capacity becomes important. But even for training, if you think of pipeline parallelism, if you're trying to, uh, compress a model into a single GPU. Uh, and the memory is not enough, you start getting into pipeline parallelism, and the more pipeline parallelism you have, the less efficient memory usage you have, so that starts hurting performance as well. But we're not done. Some of the models, especially mixture of expert models, MOA models, mixture of agents, uh, some of the multi-modal models, uh, video processing, of course, uh, some of the large frontier models, they are network bound. So we introduced a couple of years ago 10P10U networking, and as the name suggests, this is our, our, our goal is to deliver tens of petabytes of non-blocking bandwidth. Across tens of thousands of servers in under 10 microseconds. So this is something we use ultra clusters for, uh, I keep pressing the wrong button, uh, ultra clusters for, and we'll talk about that in a second. So this is the framework I want to present. When we think of scaling up, uh, scaling on infrastructure, we can either scale out so you can add more and more servers and clusters and ultra clusters, uh, which is Andreas's topic, uh, or you can scale up, which is if you have smaller models or less than 10 billion parameter models, but even if you have bigger models and you want to kind of homogenize your procurement and you have a certain cluster that you're purchasing, uh, uh, which is uh Andre's domain. You can scale up so you can now go from A100s to H100s to H200s and now uh uh uh Blackwell's, uh, or you can scale across which is the inference stack and that depends on what kind of models you have. Is it distributed high performance influencing on the right or single node, uh, GPUs, um, on the left. So I mentioned ultra clusters. So this is, uh, in fact, today when ranium 3 was announced, announced, uh, we also announced ultracluster 3.0. Even Ultra Cluster 2.0 delivers 60 the bandwidth. This is our way of managing tens of thousands of GPUs, or tridium accelerators, in one high-speed domain. So this becomes really important when you're scaling out to hundreds or even thousands of GPUs. We have another paradigm called UltraServer. Last year you heard how we were able to use UltraServer in a single high-speed domain of either 72 GPUs or 72 or 64 cranium accelerators, and this morning Matt announced ranium 3, which is our way of packing 144 cranium chips in one single high speed domain that gives you 5 times the performance per megawatt. Um, in terms of scaling up, we have had a 15 year plus partnership with Nvidia. We have been launching instances almost every year, and over the last 6 months, the pace has increased, as you can see. A couple of weeks ago we announced a B300 base P6, uh, which is, uh, which delivers a whopping 6.4 terabits per second networking speed, 270 gigabytes of memory per GPU. So this is the biggest, baddest GPU that we have, uh, but you can see, you can read the specs on others as well. We have GP 200 base P6E. We, a number of you should be using, uh, hopper-based instances as well. And finally, um. The stack, we have the hardware layer which is what our team primarily focuses on at AWS, but on top of the hardware layer we have many other abstraction layers right from security hypervisor using nitro to storage to EFA networking and then managed services like Sagemaker. Uh, so this is kind of the framework, uh, I wanted to set, and now each speaker will speak about their experiences on kind of optimizing this and in fact that this would be the best part of the session where you can, uh, hear from them. Um, we'll have Q&A, uh, we would love to have devote as much, uh, time as possible for the Q&A if you can hold on, hold on to the questions till then. It'll help us go through the narrative first and then we are, it's uh we are open session after that and we'll pass the mics around for your questions. Uh, next up is Andrea. Uh, she's a director at ARM, and she'll speak about her experiences. OK, hi, my name is Andrea. I'm here from ARM where I deal with some GPUs, and I have prior experience dealing with a lot more GPUs at once. Uh, the following advice is really targeted at someone who's dealing with, uh, a large number of GPUs for the first time. And so maybe you've had a few servers, you've run a few experiments, but 1000 GPUs kind of sounds like a lot, and you don't really know, uh, what to do, um, and if you're, if you have not really budgeted for more than a few machines, most of this you do not need to worry about. If on the other hand, you have raised like, let's say 1 to 10 to $100 billion seed round and you're making a pre-training cluster and this kind of thing. Um, also, this advice is not for you. I'm gonna have a lot of, uh, different suggestions for you. Um, but the basic idea is that there are certain problems that emerge predictably at scale for GPUs, and 1000 is about the line where you cannot ignore them anymore, and somewhere between 1000 and 10,000 you will have all of these problems and then exciting new problems emerge beyond that that we're going to ignore, um, but do not worry, we're gonna solve all of these problems in about 10 minutes and so just pay attention if this is you. So for my sake, let's assume you are my target audience and uh you have money. The money is not the problem. You need to buy some GPUs and you need to manage them. And so this is great. Congratulations, you now have a GPU farm, um, but you do need to do some planning in advance and think about how you're gonna handle these things. And it's a little bit harder than it looks, especially if you've dealt with a few machines before and you think, I know how to run GPUs and everything is, is fine. Um, I think there are a few things that are really important to pay attention to at this scale. Uh, the first thing is you're adding more GPUs and so problems that happen with GPUs happen more often, but you're probably also adding more jobs and larger jobs. Larger jobs are big jobs that use more GPUs at once. And so when you have, um, Bigger jobs and longer running jobs, any problem is going to occur more often. It's more likely that your, your workload gets disrupted and fails, and you need to be very good at, at spotting this and recovering from it. And so at this scale, you've probably got a few different use cases going on. You may have multiple tenants or customers who are sharing the capacity. You may have different types of jobs, maybe some are training and some are inference and some are doing something else, and you think you've bought a lot of GPUs, but if you ask your users, you have not really bought that many. And so very quickly, all of the workloads pile up and are kind of demanding GPUs all at once and you get yourself into some trouble. Uh, the second thing is you're probably with this type of machine and this type of scale doing some kind of distributed workloads, probably distributed training, maybe even distributed or, or multi-node inference since you kind of have to pay attention to networking and you have to pay attention to distributed job optimization and failures. And, and finally, as I mentioned, everything fails more often at scale, rare failures become constant failures, and you have to worry about this. And so don't, don't lose all hope. It's not that bad. There are established ways of dealing with these machines at this scale, um, but I think before I talk about what to do with your machines, I'm going to, uh, consider the possibility that you have not yet made any purchasing decisions. Maybe you just have money, but you don't have GPUs yet. And you, if you're in that, uh, scenario and you're seeing many nice slides with many, many different types of GPUs and accelerated instances, it can get overwhelming trying to decide what to pick, but I think there are a couple of rules of thumb that are, are helpful. Uh, one thing, if it's not obvious, is to try and get all of your machines in one place, and this is a little bit hard to do, especially with constrained availability, and maybe someone can offer you a few on one coast right away and a few in another place and a few in another place, and that's fine. Um, that's OK, but then you're gonna do a lot more work managing them and your utilization is probably going to be a lot lower if you have a fragmented system. And so given the option, especially for newer GPUs, um, I recommend either at the beginning or kind of pre-negotiating over time to consolidate your capacity into a single location. And increasingly for modern GPUs, this does not just mean one region or one zone. Um, which you can run with one control plane, it means one physical ultra cluster build or equivalently one networking spine. And so try and get your machines of one type in the same physical cluster with very fast interconnects, kind of all 11 place. Um, we may discuss this, uh, later, but you, you have a broad selection of instance types, and it can be tempting to hyper optimize your machines to your workloads because maybe you have many different workloads and you benchmark them all and one is faster on one machine. And one is faster on another machine, but you run a bit the same fragmentation risk trying to get greedy and get capacity wherever you can get it right away. If you try and get a super optimized, uh, machine for each mach uh for each workload, you may not have consistent traffic to keep all your machines busy all the time. And so I tend toward what I call workhorse machines, machines that are fast enough and, and you have a lot of the same thing that they can handle most of your traffic and, and remain consistently busy. And so if you, if you manage this, you make a choice, maybe one or two accelerated instances, and all in one place. There are a lot of ways to manage capacity, but I think it's a fairly safe recommendation that at this scale, you should think about running your own clusters and that you should do so with Kuber 80s. I think it's also fairly safe to say that for the majority of use cases, there's no, um, major downside to using a managed Kuber80 service like EKS. I think there are exceptions to that, but you kind of have to have a reason not to use it as opposed to a reason to, to justify it. Um, and so this will simplify your cluster management a lot. And I think there are some uh good practices about how to set up your cluster that we'll look at in a moment with an example diagram, um, but you should kind of standardize your cluster. All your machines should be on the same versions of things, so all your jobs can run on all your machines and tend to have the same dependencies and same failures, um, and you should do something to deal with, uh, GPU failures, which we'll talk about more as well. So here's the diagram I promised, and it's just an example. You may have a slightly different setup, but there are a couple of things that I want to draw your attention to. The first thing is that you have somehow got your machines all in one place and so there is only one account in this diagram. There's only one cluster, and all the machines are in one cluster, which is very nice. Um, the second thing to notice is that there's a, a mysterious scheduling or orchestration component implied in the cluster other than the built-in Kubernetes scheduler. And again, there's a lot of selection and we'll look at some options for this in a moment, but especially for batch computing, I think it's, it's broadly accepted that Kubernas is not really going to have all the scheduling properties that you want for AI workloads, and you're probably going to end up layering something on top, uh, to, to manage the way your workloads are scheduled to machines. The third thing to notice is the node pools. This is deliberately vague. It could be auto scaling groups. It could be, uh, managed node groups of some kind, but they're, they're divided up. The machines are not all in one giant pool somehow. So probably in Kuber 80s, they've got different labels or taints and workloads are kind of pointed at one node pool or another. Uh, there's some CPU only capacity in there. Don't forget to budget for that. You will probably need it. There's a couple different types of GPU. They're in different node pools with maybe different driver versions, um, and you can see that in this scenario, there's been more than one GPU procured for the batch use case. There's some P5s and P6Es, and those are in different node pools and kept logically separate. Workloads are either going to one or they're going to the other. And if you do end up combining some kind of real-time infringe use case with offline batch processing, which there can also be reasons not to do, but if you do do that, I think at least have some kind of logical barrier between the real-time capacity and the offline processing capacity. And finally, I'm not gonna make a sales pitch for any particular storage technology, but planning out your storage hierarchy is gonna be a really important part of planning your cluster. Uh, it does help that the newer GPU instances increasingly come with a lot of built-in SSD in the form of NVME, uh, especially P5 and definitely P6E. And so, uh, that is your first bet and best bet for storage during things like training. It's very fast and it is very free and it is built into the machines. But inevitably that is not a durable place to store anything that you care about long term. And so probably there's some other storage in the picture, um, that's keeping longer term records of, of training data and checkpoints or, or data that your workloads are processing while they run in the cluster. And so finally, it is just a bit implied by this that you need to budget for things other than GPUs. So I pointed out there's some CPU only machines in there, so do not forget these. There's some storage, do not forget to budget for storage. And then finally, these things talk to each other and so there is definitely some networking traffic likely to be happening here unless all of your data is inside the ultra cluster and never leaving. Probably there's some data flow in and out and so don't forget to plan for and budget for networking. So I mentioned scheduling an orchestration. You probably need to make some choices beyond just turning up Kuber 80s and deciding that you're done. Um, for batch jobs, there, there's a longer list of, of quality tools than I can list, and so I, I don't want to sell you on any particular one, but I think anyone who tells you that there is a perfect scheduler for all workloads is a little bit suspicious. Um, you, you should really evaluate a range, uh, for what you're doing. And if you're looking at, uh, hosting serving workloads, you're probably looking at a different set of tools than someone who's doing mostly, mostly batch processing. And so when you're thinking through the options. Um, especially at this scale, 1000 GPUs is not as much as it sounds, and if the size of your large jobs is anywhere within an order of magnitude of the size of your cluster, if you do not have something that does gang scheduling on top of Kuber 80s, your jobs are gonna deadlock very fast, and you will have 2 or 3 that are half started and nothing goes, and the whole system locks up. And so you definitely cannot ignore this at the scale for this type of workload, um. In addition to scheduling, you, you need to worry a little bit about resource management. You are definitely going to have a range of workloads and probably a range of tenants fighting over this capacity, and all you can really do is have some kind of a policy that everyone has agreed to that the system enforces on their behalf. And whatever, uh, tool that ends up making your scheduling decisions probably also has something to say about your resource management decisions. And so you need to look at, at your interest in both of those properties when you choose that tool. And then finally, um, most of these things are fairly solid, but you, you, you should look not just in principle how the scheduling works and what promises it makes in the docks, but also make sure that it can handle the, the load and the traffic that you expect in the system when it's live, because the scheduling will be a single point of failure for your system. And then I will give a special shout out to, to, to GB 200. And so if you're dealing with any rack scale capacity like GB 200 or upcoming GB 300, uh, at this point, you really cannot um avoid dealing with topology in your choice of scheduler. Most likely some workloads have some constraints, maybe multi-node workload needs to go to the same rack or big training workload needs to use several whole racks in training, and so you also need to, to choose a scheduling component that, that can express that the way you want. Uh, finally, observability. If you have spent all this money and bought all these GPUs, you have better know what is going on with them, and this is partially for your own sanity, so you can tell if the system is working. It's partially for your users' sanity, so they can tell if it's working and they can also tell how busy it is. And finally, it is also for what I will refer to as the powers that be, people who maybe have a financial interest in what you are doing with all the money and all of the GPUs, and by the time they come asking what your utilization looks like, ideally, you should already know. And so there's certain, uh, these, these are kind of made up diagrams just to give a sense of things, um, but there are a couple of things I think you should really monitor in real time and for long periods of time so that you always have this information. You should know how much of your capacity appears to be working, uh, and you should know how much of it is used by end user workloads, what kind of scheduling utilization you're getting. And then depending on your use case, you should have some notion of live traffic to the system. The graph shown is for something like a batch system that has queued jobs and running jobs, um, and this also is very entertaining to people who want to know why their job is queued and whether the queue is very long. Um, over longer periods of time, not necessarily in real time, but maybe on a daily or weekly basis, I, I really recommend at this scale having some notion of how responsible your end users are being. And so some notion of workload efficiency that catches if workloads are requesting GPUs that they never use or running with a really low average utilization or have really a wonky, uh, resource configurations is, is a great idea. We'll let you catch these things out quicker. Um, before they tank your overall metrics. And then finally, you should know how the computing time or the cost in the system is being spent between your, your tenants or your use cases and have some kind of uh monitoring or, or showbacks. And with this, I'm going to pass it off to Henri, who's going to talk about his lived experience in this domain. Hi, my name's Henri and uh. I wish I had heard Andrea's talk a few years ago because I was the target audience, so what I'm gonna do is walk you through some of our sort of learned experience scaling AI compute for Lab in the loop and uh. I'll start by giving you a brief overview of what I'm talking about. So, I work in uh AI for drug discovery, and what we do is make machine learning models to design molecules. And when we talk about lab in the loop, what we mean is essentially you start with some experimental data and train machine learning models on it, use those models to come up with new potential molecules in our case, and then make those run experiments, get new data that hopefully feeds back into the models, and eventually, uh, by iterating, you reach the level of quality of your molecules that you're hoping for, or at least that's, that's the dream. And uh on the right this is just an example of um one class of molecules that we worked on called antibodies we were optimizing across expression how easy they are to make an affinity and sort of you can see over subsequent rounds we're kind of pushing the Pareto frontier forward which is what you're hoping to do uh in this kind of process and so. In order for this to work, one of the things that you need is good machine learning models, and nowadays, in order to have state of the art models, what that means is training large models on multiple nodes and, uh, in particular, I'll focus on. Um, one of the kinds of models that we've been working on, which are these LLMs in the like 7 billion, 70 billion parameter range, I guess they still count as large, um, and very briefly on, on our infrastructure, so we have our GPUs on, uh, parallel cluster, which is one of the schedulers that Andrea was talking about, and most of our long-term data storage is in S3 and so. By scaling up our model size and compute, we ran into. All of the following challenges. The first one are hardware faults. When you're working on one machine, one node, a few GPUs, typically you don't run into this, but at scale they're inevitable. I think we ran into these as soon as we were scaling across tens of nodes, um, and these are typically seen as, um, you know, the training job crashing with unclear reasons, uh. And so In order to sort of solve this, what we did was test the nodes. There are a variety of different ways you can do this. In the end, we sort of run a very simple Pytorch script on a new node, make sure that it's able to use the GPUs, load data on the GPU, uh, and run simple calculations, and then remove bad ones. A trick here that took us a little while to figure out is just because you evict a node you might actually just get it back, especially when you start worrying about challenge number 2, which is networking, so. With networking these are typically really hard to debug errors. Um, you don't often have clear reasons why some some networking error happened or can see it in the logs we usually saw these as nickel timeouts and. What we found was really helpful is really try to avoid hops. So if you can avoid cross zone traffic or even cross spine, uh, networking, you can get much better, uh, stability and also better performance. But in one spine there's only so much capacity. That's why this issue of getting back the same node can happen. And the final challenge is around the model implementation, and this is something that's really fully under your control, uh, one. One strategy that really helped us was carefully testing the various libraries used, so CUDA, your EFA kernel, um, if you're using, you know, P instances, as well as, uh, you know, nickel OFI plug-ins and things like that, all of these are really brittle, and making sure that you test all of the versions that you're gonna run together and sort of upgrade them all together will lead to tremendous improvements in your, in your stability. On the data loading side. The common challenge is really feeding data fast enough to the GPUs so that you can get that good utilization, and this is sort of the most common source of bugs that I see when machine learning scientists come to me with bad GPU efficiency. This is also sort of a Python problem because in order to load data quickly you need to have multiple threads, multiple processes. This is notoriously finicky in Python and uh spending a lot of time stress testing your data loaders is really useful. On the checkpointing side, no matter what you do, there will be failures for your models and so being able to checkpoint and resume is, is critical and uh that will really help, help you save sort of wasted compute. And lastly, we found that as you start training larger models, moving away from abstractions and libraries that sort of wrap Pytorch to sort of native Pytorch helped us improve the efficiency, uh, and the stability. And so putting these in practice. Here's some charts of our GP utilization. So if you don't have the pleasure of in your day to day job staring at GPU utilization charts, um, I, I feel sorry for you. They're pretty fun, but also what you're looking at here essentially on the, on the Y axis is the percent average. GPU utilization everyone OK there uh average GPU utilization percent for an LLM training job and you can see a couple issues with the one on the left. The first one is sort of low GPU utilization. We're around mid-60 to a mid 60%. That means, you know, if you're running on 100 GPUs, sort of 40 or 35 of them are not being utilized, um, on average. The other issue we see here are these sort of pauses every couple uh tens of minutes where the GPUization drops to zero and this is often a symptom of sort of checkpointing where everything is stopping, stop the world, you're writing to disk all of the weights and then, um, resuming and the other thing we can see here is sort of relatively short training runs on the order of several hours. This is because we had some stability issues, um. And so by implementing some of the improvements that uh and, and addressing some of the challenges I mentioned in the previous slide, distributed checkpointing, really investing in our data loading and dropping down to fully sharded data parallel, we're able to really bring up our GPU utilization to sort of where you want to be, you know, mid 90%. This is the chart I want to show to the powers that be, how good we are at using our GPUs and not the previous one. Of course using your GPUs is not enough. Uh, it's really important to use them in ways that are effective and so for us we run varying types of compute, but I showed here two major ones. So on the left it's molecular dynamic simulation, how long it takes to generate a one nanosecond trajectory. And here less is better, so you wanna take less time to generate that one nanosecond and what you can see it's kind of interesting an L40S takes about as long as a B-200 to run this compute. This is just one GPU, um, which, you know, L40S is relatively old or very old I guess in GPU generation time. It's, uh, 3 years old versus the B200 which is pretty recent. And uh so that means, you know, if you have a lot of specific kinds of workloads, you can, you should really benchmark them to see if it's worth considering having dedicated compute and uh tuning the architecture for that on the LLM training side, this is sort of what you see in all of the releases of new, of new chips. We saw very similar results, uh, where you get more tokens per second, which is good on newer GPUs. Yeah, so, tying it all together, you know, thanks to some of our efforts in um. Optimizing our LLM training, we're really now we're working on. Having intention aware agents that are trained on all of our internal data so they understand drug discovery, they understand our models, and what we're working towards is having these agents, you know, be able to translate. Some requirements. So in this case, the example I'm giving you is this phase in drug discovery that's called lead optimization where you have a molecule and you're trying to improve certain properties. And so the idea is you ask your agents, can you improve these properties and the agent can translate that into a workflow, you know, run these generative models that we have, these property prediction models that we have that are able to identify. Specific aspects of the molecule and then rank and filter those maybe run through a few loops of that and ultimately then we can send that for synthesis to the wet lab and look at the predicted and real output and sort of compare and so in order for the agentic loop to work what you need is to have all of your models deployed so we use Kubernetes to run those and sort of some agentic framework to tie it together and. That's how we've really been working on speeding drug development process and trying to improve the success rate of our R&D, yeah, and with that, I'll hand it off to Shani for more information about agents, yeah. Uh, thank you, Andre. Uh, so as Henri was describing, the most successful, uh, agents that are built are built using models that are trained on the data that your enterprises have. And that's what I'm going to talk a little bit about. We call it, um, artificial autonomous intelligence, which basically essentially means You own your data, so you should essent essentially own your models and own your deployments as well. I'm Shane. Uh, I'm the field CTO at Fireworks. Uh, we specialize in, uh, influencing as well as, uh, training, uh, for a lot of open models. So I think um 2025 is the year of agents, I should say, or the year of agents. Um. I think almost all of you must have used one of these agents, uh, specifically cursor, uh, or Source graph. They're all are in like the coding space. You have a bunch of them in like the document, uh, processing space. You're seeing a lot of like enterprises, uh, show up that are doing, uh, legal processing, healthcare processing, FSNI, and those types of stuff. Everybody's building one or these types of agents. And so the, the agents on the top are some of the ones that have like really picked up this year, but we feel and we have observed that there are many, many more that are actually being uh being built out and we feel that these are the agents that will sort of be more pervasive this year as well. Um, so when you, when you think about like agents, I think there are two main principles, uh, that we think about, um, and both of these principles are based on the fact that your data is the most precious commodity that you have and the models that are generally available do not align with the data that, that exists in your enterprises, especially because they have been trained on uh public data or they have been trained on data. That is synthetically generated by a few companies. Uh, and so what we have realized is the first principle is don't treat your model as a commodity. Your model is your product. It needs to be built using data flywheel. Um, I think Andre gave a great example of how they built out their models by iteratively training and are like constantly improving the model using the data flywheel that they have. Um, and then the second one is in, in the same lines, your model is your IP, right? Your model and your product needs to work together to provide your users with the best user experience that they, they can have. Um, so the overall principles, um, your model and your product should coexist and your model is your IP. Do not treat it as a commodity. Now, when you think about models, when you think about um this infrastructure, when you think about like where LLMs are, there are so many choices. So I'll give you a little bit of overview over the next couple of slides about the different types of choices you can make, uh, whether they are regards to models or whether they are regards to infrastructure. Um, so models now come in different types of modalities. Um, you have text. Uh, you have image, you have image generation, you have video generation, you have, uh, embeddings, etc. You have different types of model providers, all of them have like different expertise, right? Uh, so you have model, uh, oh sorry, you have Amazon. You have Meta, uh, you have Deepeek, uh, you have Gwen, Gwen's from Alibaba, and they have all been showing signs of catching up with the, uh, the proprietary or the frontier models that you, that you see today. And of course like there are lots of model sizes. Um, I, I'm specifically talking here about LLMs or large language models. So they range anywhere from 1B, even smaller for some of the embedding models all the way to 1 trillion, which is the latest model that you saw from this company called Kimmi, which is roughly 1 trillion parameters long. After you choose the model, the next one is model alignment based on the data that you have. Again, many, many choices here. Uh, the first one is supervised fine tuning, where you give your models a few set of examples to sort of align the model. Then you have DPO where you give it positive and negative samples, and then you have reinforcement fine-tuning, where you have a reward function that tries to align your model, uh, based on the data that you have. The last one is, is sort of the one that now almost all the Frontier Labs and these open model companies actually use to improve the quality of the model because it's cheaper and it gives you like really good results. And then the last one is uh talking a little bit about uh serving those models. Uh, so we talk about like the inference infrastructure. Um, there are lots of trade-offs here. Typically you have 3 things your inference can be better, it can be faster, it can be cheaper, and most of the companies have like 1 choice and then uh or 2 choices. And we can also talk about a little bit after this that can you actually make trade-off on all the 3 choices. Um, then you have, uh, security constraints, right? Uh, whether this model and deployments should be SAS-based, Should they run in your own private PPC? Should they run in on-prem? Do you have legal approvals to use models? Are these models properly penetrated, tested, and so on and so forth. Uh, so with that, um, uh, let's move to the next one which is, I'll talk a little bit about like the inference infrastructure. Uh, Andrea covered, uh, that you have for influence infrastructure, you have different types of, uh, serving stack for bad jobs, different types of serving stack for, uh, real-time jobs, and so in the next few slides, we'll talk a little bit about when you, when you have your own, uh, deployments, how do you think about which things to use? So the first one is pick the hardware. There are so many of them, right? Uh, there's accelerated compute, you have AWS, you have Nvidia, uh, providing you GPUs, uh, you have, uh, what we call here Ampere series, Hopper series, Blackwell series, uh, and, uh, as of today, Tranium has ranium 3 as well. Um, all of these hardwares have like different trade-offs to make. Um, I think, uh, Ani covered a lot of them as part of his talk. Some, uh, some have huge amounts of flops, some have lots of memory per GPU, uh, some have really good inter inter, uh, GPU connects, etc. Um So once you sort of pick the hardware, you pick the hardware provider, and then you have to start thinking about optimizing your inference engine, um. You typically have a few choices that I have listed out. Uh, these are the most common ones, uh, but you have many more. Uh, you typically think about model charting. If your model is very large, it might not fit on a single GPU. You have to shard it across different GPU. Uh, you have disaggregated serving. Uh, this is where you separate out the two phases of generation, which is the prefill, um, and the, uh, and the generation step. Um, this type of techniques are very, very common, uh, especially in gentic AI systems where your inputs are extremely long, but your outputs are structured and very short JSON outputs. Uh, then you think about which inference kernel to use. Um, there are many of them. I think TRTLLM has done a great job, uh, or NVDA has done a great job of like providing a bunch of like inference kernels. Um, VLLM, SG Lang have all, um, create, created like flash infer, flash attention, etc. which are like doing really well. Um, then you think about crossour serving. So what if your model is so large that it doesn't fit on. A single GPU machine, then you actually have to shard the model across like different types of hosts. When you do that, you need like really good communication to happen. Uh, so, uh, Amazon here does a great job of providing us with EFA where the, the speed of transfer of KV cash from like one machine to another is like really, really quick, and that essentially helps with faster generation, faster throughputs. Um, speculative decoding is one of the most commonly used techniques that allows you to train a very, very small model that just speculates, and the larger model, uh, instead of generating just realizes or just checks whether the token that's generated is actually correct or not. So once you've finalized the infferencing engine options, you then have to move to serving stack. Um, Andrea already covered a lot of this in her, uh, Kuberetti's, uh, talk around like horizontal scaling, uh, but I'll talk a little bit more deeply about specifics to, uh, inference. So you have prompt caching, uh, which is one of the most common techniques now that's used if you have like the same system prompts that get used, uh, what kind of, uh, System that is provided by like the inference engine, etc. so you can use like prompt caching. Second one is session affinity, uh, very, very useful if you have coding types of agents that you're building or that you're using so that the same request goes to the same machine, um. In these types of cases, the reliability of machines is extremely, extremely important, and so using something like EKS that's actually managed using some, using Amazon, which is extremely reliable and extremely available, is super important because then the failures around uh session affinity or your prompt sky dropping reduces significantly. Then you have request failover, load shedding. These are typically the cases when you sort of get like spikes of traffic, uh, or you have, um, or you want to fail over to like another region. So once you've picked out your serving stack, um, you then have to move to, um, global secure serving, right? So Andrea talked a little bit about what if try to get your GPUs in one region as much as possible, but sometimes it is not possible or sometimes you might have, you might need to have cross-region traffic or you might need to have regionality. And so you use global secure serving in that case, you need to have some sort of global scheduling. You need to have private VPCs uh for secure surveying, uh, you might even want to do, uh, things like private link so that your data never leaves your VPC boundary, and all of these are provided by, uh, Amazon's uh ELB systems, uh, or through Amazon's private link, uh, and that's where we are seeing a lot of prevalence of secure and, and compliant infrastructure. Um, so that that was a very high level of like what the inference stacks are, um, so this is how we actually built our inference stack, taking all of these into account, um, and so today, uh, Fireworks serves about, um, I think this is a little old, uh, maybe like a few weeks old, um, about 150,000 requests per second, uh, we process more than, um, 13 trillion tokens a day. Uh, we have a bunch of models. Uh, across different modalities and we host on private secure cloud. Um, and we have a booth, uh, uh, our booth number is 1588, so please come and talk to us if you're interested in learning about any of this. Thank you. Please, please do prepare a lot of questions. We really hope, um, you guys ask the best questions of all. Uh, please do, uh, fill out a survey from your app. This really helps us raise the bar, uh, at future events, and the more number of people, uh, sign up the survey, the better, um, uh, better data we get in terms of, uh, figuring out what worked, what didn't work, and what needs to improve. So any, any questions, any burning questions on how to scale AI?