---
video_id: RkdPAFJEPSA
video_url: https://www.youtube.com/watch?v=RkdPAFJEPSA
title: AWS re:Invent 2025 - The power of cloud network innovation (INV213)
author: AWS Events
published_date: 2025-12-03
length_minutes: 60.4
views: 869
description: "Journey through an evolution of AWS networking and discover how it continues to serve as the foundation for how AWS delivers on the promise of cloud computing. Join Robert Kennedy, VP of Network Services, as he unveils innovative breakthroughs powering today's cloud infrastructure. Through customer examples and exclusive behind-the-scenes insights, learn how AWS transforms networking across every layer - from our global backbone to AI/ML-optimized data centers, advanced content delivery, enhance..."
keywords: AWS, Amazon Web Services, AWS Cloud, Amazon Cloud, AWS re:Invent, AWS Summit, AWS re:Inforce, AWS reInforce, AWS reInvent, AWS Events
is_generated: False
is_translatable: True
---

Please welcome to the stage vice president of network services at AWS Robert Kennedy. All right, hey folks. They're all here for networking, right? Great. OK. I'm glad to see there's as many passionate people about networking as I am. Folks, I'm, uh, Rob Kennedy. I'm the vice president of network services. Uh, so over the next hour, I'll walk you through the highlights from the past year. I'll introduce you to a few exciting innovations we're unveiling at this reinvent and share some insights, hopefully you'll find valuable for your own business. I have been with AWS for over 16 years. I have been in the networking industry, uh, since I graduated college. I have a Strange fascination with uh with networking, uh, like, uh, how far it's come has just been absolutely amazing for me. Uh, it has been the heart and soul, I think in terms of our, our innovations and allowing us to, to move forward and, uh, you know, going from, uh, what I thought was basic network connectivity back when I graduated to some of the sophisticated intelligent infrastructure that we have power in our modern applications today is, is just fundamentally amazing. So, we've also over the time, uh, back over those 16 years, really seen some amazing workload transformation, you know, simple web applications, the massive artificial intelligent models and global streaming events that reach millions of users simultaneously. From personal devices to orbiting satellites, network infrastructure underpins about every aspect of modern day life, accelerates breakthroughs in medicine, scientific research, and next generation large language model development and use. At AWS we build networks that keep things running, while also pushing the boundary of what's possible. And at AWS that starts with owning the entire stack from top to bottom, and bottom to top, and the fiber in the ground. Under the sea, physical hardware, the operating systems that sit on top of that uh that hardware, we control the infrastructure, the data centers, we build them from the ground up, power, cooling, uh every aspect, all the way out to the internet edge uh is owned and operated by AWS and really nobody else can say that. Uh, and we've also built our own software defined network that sits on top of all the physical foundation and it gives us unprecedented control and flexibility. And we don't just build networks, we engineer them from the silicon to the software. Full stack control means we deliver reliability while simultaneously pushing the boundaries of what networks can do next. Networking supports workloads of every scale, from the smallest application to the most demanding high performance computing environments. Right now, no technology is pushing those boundaries more than AI. Models are getting bigger, more complex, and more demanding by today, rapidly approaching the very limits of what traditional networks can handle. Models have already crossed the 1 trillion parameter threshold, but it's not just about size. Customers need to continuously train these models to improve performance and then deploy them globally to serve businesses and customers around the world. The network is no longer just plumbing in the background. It's the backbone that makes it all possible. So let's start with that simple question. Why does networking matter? So think about atoms on their own, they can be interesting, but when they connect and combine into different molecular combinations, they make amazing things. Water. Air And my favorite one, which I may have had a little bit of before I came on stage here, uh, being Irish is whiskey. Um, so. And hopefully you all know that the Irish invented whiskey, and if you ever hear any Scottish person tell you differently, it's a lie, they have no evidence whatsoever. So Same is true in AI. Single AI accelerator, such as an AWS trainium chip or GPU, it's quite powerful by itself. But to train a large ML model or run inference at scale, you need something more powerful. You need a distributed system comprised of over a million GPUs, all connected through low latency, high performance networking. Think of it this way, those accelerators are atoms, the network that's the bond that connects them and transforms them into something far more powerful. At AWS we work to strengthen those bonds at every level, from the smallest chip to chip link, all the way up to hyperscale clusters. So let's zoom in and look at the basics. Let's start with the simplest use case. Two AI accelerators working together to train a large language model, sharing learned knowledge, adjusting, passing data back and forth, like it's dance partners in perfect sync. I'm not gonna do any dance moves cause I'll probably fall over. But when these AI accelerators sit in the same server, connected by PCIE or NVyLink, they communicate in nanoseconds. It's a beautiful partnership, but they're, here's where our story gets complicated. Today's AI models are massive. We're talking about 1 trillion parameters are growing, so we have no choice. We must break up the perfect partnership and scale out across hundreds of thousands of accelerators. Won't we do that? Well, the instant we step outside that single server, physics becomes our greatest enemy. We move to the next server, latency penalty. Move to the next rack, bigger penalty. Move to the data center, even bigger pen penalty. Once at a region, now we're talking big challenges. They help to reduce that latency. Remember we go all the way down, even to the physical fiber itself that we put in the ground. So we're constantly innovating, even at that level. And that's where technologies like holocore fiber, which we talked about last year, as we just launched, this technology is a revolution in material science and manufacture. Imagine the precision it takes to fabricate a long strand of fiber with a perfectly hollow core. The benefits are worth it. Instead of traveling through grass more slowly, the information travels through the holocore. Since last year, we have deployed holocore live on our network, and we've continued to deploy thousands of kilometers across our data centers. And it ends up with a 30% improvement in latency, which is seriously meaningful. But still, every boundary you cross, every step you take away from that perfect accelerator to accelerator connection, physics comes into play. At scale, every connection matters exponentially. The bigger the system, the more latency compounds. Trading a large language model with over 11 trillion parameters and growing may require billions of messages to move between AI accelerators. Why billions? Because every microstep of the process, exchanging parameters, gradient updates, intermediate matrix multiplication and transformer layer may be divided across 8 to 16 accelerators, and each of those accelerators must share partial results hundreds of times per second. Delays, even on the smallest scale of microseconds can impact job performance. That's why optimizing communication isn't just important, it's essential as clusters grow. That brings to the $1 trillion question. How do you build AI infrastructure that's fast enough, reliable enough, and scalable enough to train the models that we will define for the next decade. So, let's start with a concrete example of how we think about scale at AWS. First, let's take our ranium 2 ultra Server. It's not just a server, it's a new category of compute. We pack 64 ranium 2 chips into a single ultra server, connected with high bandwidth, low latency neuron link interconnects. That's 512 neuron cores working together and here's the key insight. Those 64 chips aren't just randomly connected. They're arranged in a carefully designed topology, a 2D torus within each instance with corresponding cores linked in circular pathways that bridge across instances. This isn't just accidental. The network topology directly determines what's computationally possible. The depth of interconnectedness is what makes 1 trillion parameter models feasible. Whether you're training or running inference on models, the ultra servers interconnect design ensures 64 chips can work together seamlessly, maintaining the performance characteristics you'd expect from a single massive instance. Ultra servers are great for those models that need scaled up infrastructure. And when you need to scale beyond an ultra server. When you're connecting hundreds of these units together, the network has one job, make it all feel local. Every AI accelerator should think it's talking to its neighbor, even if it's sitting racks away. That means microsecond latency, not milliseconds, and massive bandwidth across every connection. That's exactly why we build EC2 ultra clusters. Pettabit scale, non-blocking fabrics that connects over a million training chips or GPUs with predictable ultra low latency. These clusters have already powered some of the largest AI training runs in the world. And the same architecture that drives training performance, it delivers rock solid inference too, same network fabric that synchronizes thousands of AI accelerators during training, seamlessly coordinates inference requests across models, model replicas. Deployed fleets of ranium or GPU instances or real-time apps, massive models, heavy traffic, and you'll still get consistent response times without congestion or slowdown. But scaling to these massive clusters isn't just about connecting more hardware, it's about choosing the right network topology. So let me explain the fundamental trade-offs we face. So ML servers have multiple accelerators, each with an index, and ML clusters often use wired rails, connectivity that creates mini networks to connect accelerators of the same index to improve performance. This approach is great when all is working well, but here's the reality, any single link can affect your entire training run, causing work to restart from checkpoints. And in clusters with a quarter of a million or more links, failures are gonna happen. The conventional alternative is to connect all accelerators using a topper rack switch. This gives you the flexibility to rat around failures, but it decreases bandwidth efficiency and increases accelerator interference. The trade-off raw performance versus resiliency. Training job utilization is driven by network stability, not just raw latency. ML training jobs create periodic checkpoints that pick up in the face of failures. However, recovering from checkpoint failures is relatively costly. But can we develop technology that gives us the best of both performance and resiliency? Well, of course you know the answer. We're AWS of course we can. This is exactly why we developed Ultra Switch, a new first hop networking device in our data centers, homegrown at AWS. UltraSwitch uses a variety of information such as accelerator index, cluster topology information and network capacity to align AI accelerator traffic consistently down dedicated adaptive rails. We allow for optimal rails oriented data flow when possible, and when faced with failures, we quickly recover to provide the network stability. In the event of failures or periods of congestion, Ultra Switch can adaptively route traffic around an impaired area while keeping flow collisions to a minimum, enabling the job to continue without disruption. We also examined their shorter links, uh, connections between devices in a rack and between racks, we typically have used copper interconnects for their reliability, but the cabling density and the length requirements make copper impractical when you need to route hundreds of thousands of thick cables through limited rack space. So traditional optics solve the cable size and reach challenges, but they're more expensive, and they're less reliable than copper. We needed to reach the reach and size benefits of optics with the reliability and cost effectiveness of copper. So again, thinking about all of the things that we own across the stack, this allows us to get into even the optics. And so ultra short read optics delivers exactly that. By limiting cable lengths, minimizing connectors, we enable innovations in optics design and manufacturing that are not possible with traditional optics. The result, reliability that surpasses copper with 3x improvement in service to network link reliability plus enhanced power efficiency. But networking isn't just about the hardware innovation. When we're training models at massive scale, hundreds of thousands of AI accelerators exchanging gradients every millisecond, the network can't afford to pause. But traditional routing protocols, they were never built for this. Taking seconds to detect failures, propagate routing information and recomute best paths for traffic flows. And at AI scale, a few seconds of delay could still stall an entire training run, wasting compute time and money. To fix it, we built Cidder, scalable intent-driven routing, a part of our intent-driven network where Ultra Switch handles deterministic forwarding at the switch level, making sure packets never collide. Cider operates at the network control plane. It continuously monitors the network, detects congestion and failures in milliseconds, and adapts paths instantly, so the ML layer does not see any network interruption. The result, rapid convergence, with the ability to converge ML fabrics consistent of a quarter of a million physical links in less than 500 milliseconds. No job restarts, no idle accelerators, just smooth, uninterrupted training, even when the unexpected happens. Think of it this way. Ultra Switch gives you predictable paths through each switch. Cider makes the network itself adaptive and self-healing. Together, they form a fabric that's fast, predictable and resilient. Exactly what large scale AI training demands. And in just 3 years, AWS has deployed over 300,000 switches and more than 40 million physical ports dedicated to ML traffic. That's growing faster than our core network itself, and it reflects the extraordinary demand for AI. And just recently, we activated Project Rainier, one of the world's largest AI compute clusters, is now fully operational. AWS collaborated with Anthropic on Project Rainier, which features nearly 500,000 training Q-chips, and actually we've just gone up to a million. And it provides more than 5 times the compute power Anthropic used to train its previous AI models. We're at tropic scaling to more than 1 million ships already, they are actively using Project Rainier to build and deploy its industry leading AI model, Cloud. All right, we built one of the largest world's largest purpose-built ML networks, delivering the scale, performance, and reliability our customers need to train and run the most advanced models. These networks don't just accelerate training, they also power inference the scale from serving interactive chatbot chatbots to running recommender systems in real time. Whether you're building or serving your models, AWS networks are designed to keep them fast, reliable, and cost efficient. And everything we do here passes on to our entire network as well. These clusters don't exist in isolation. They must connect securely and seamlessly to the rest of your environment, your data pipelines, your applications, your users, and that's where VPC services come in. So now let's shift from the inside of the cluster to the outside and look at how we extend these innovations into VPC, the network foundation that every AWS customer builds on. So I've mentioned before, everything starts with an atom. The atoms bond into molecules and then further connect, they scale, extractted to more complex structures like DNL, cells, organisms, whiskey. You can think about networking services in a very similar fashion, allowing to build complex applications from simple foundations, with each component building upon the one below while hiding complexity. We handle all the physical networking we just discussed through GPU communication, through PCIA and NVLink to ultra clusters and connecting across data centers across the world, so you don't have to. When you move from prototype to production, you need networking that can isolate different workloads from each other, enforce security policy, and scale globally. Whether you're training in running AI training, inference applications, high frequency trading, or retail platforms, you should focus on the application logic, not cable management or switch configurations. While you see many providers offering racks of GPUs, you need a a solid network that can isolate your workloads from each other. At AWS we've spent nearly 2 decades perfecting software defined networking. We provide you with a software abstraction that gives you programmable access to the network without worrying about the underlying infrastructure. We built this from the ground up to deliver microsecond level latency and deterministic performance while hiding all the physical complexity. And that's exactly what VPC represents, the programmable foundation that every production workload on AWS builds upon. So what does that mean for you? It means you get your own private section of the AWS cloud. Completely isolated from everyone else. Within that space, VPC gives you secure boundaries with complete traffic control. You decide what connects to what, when and how. Whether you're running AI workloads or traditional application, production environments require strict separation between workloads, compliance domains, and security zones. Under the hood, AWS builds VPCs as a software defined network with focus on security, high availability, and resiliency. Customer packets never leave AWS controlled infrastructure. We could do this because of another innovation that we've talked about many times, the AWS Nitro System. Combination of dedicated hardware and lightweight hypervisor, enabling more performance, enhanced security, and faster innovation. BPC control planning programs, nitro-based network virtualization, creating millions of independent routing and security domains. Every one of them isolated, programmable, elastic, all built on the same foundation. It scales without redesign. We operate the largest VPC deployments in the industry with hundreds of thousands of resources in each handling billions of route updates and petabits per second of throughput daily. Need a new environment for AI training with a strict blast radius? One API call. Need regulated workloads with compliance isolation, same building blocks, same controls, instantly available. Whether you're running a single workload or managing more than 50,000 VPCs, the same foundational technology scales with you. When we first introduced VPC, the world was a lot simpler. Most customers ran in a single network, one VPC, one region, one set of subnets, everything lived in a in a single place. Fast forward to today, startups launch an AWS of microservices spread across multiple VPCs from the start, front-end services in one VPC, APIs in another, data processing in a third. Meanwhile, some of the world's largest enterprises operate tens of thousands of VPCs, each mapped to a specific team, application, or environment. Your VPC Foundation provides isolation and security, but true power emerges when we connect them intelligently. VPC peering creates direct private links between networks. Transit gateway scales and architecture of hundreds of VPCs through a single managed hub. Internet gateways open to your applications to the world, and that gateways enable private resources to reach out securely without any inbound exposure. But here's what's fundamentally shifted. Developers have moved beyond infrastructure thinking. They no longer think in terms of VPCs and subnets. They think in services, APIs and connections. This is how applications work today. The code doesn't care about network topology. It thinks in business logic. And that the architecture reflects this reality. A single modern application might span multiple VPCs, stretch across regions, and live in different AWS's accounts entirely. If from the developer's perspective, the ask remains beautifully simple, simple. Connect securely to a database, reach a machine learning model, or access object storage. That's why we built a comprehensive application networking suite that puts your applications at the center, like molecule bonds that hold complex structures together. Let's explore how we're making this vision a reality. So the real magic happens when we build application centric networking on top of this VPC foundation. That brings us to BPC lattice, the ultimate expression of intent-driven networking. You'll see this intent-driven networking, like, even in our foundational, physical network, and you see it at our software-defined level. Lattice is an application networking service that eliminates the complexity of routing tables, peering configurations, IP overlap challenges. Instead of configuring network paths across tens of thousands of EPCs, you simply say, this service is allowed to talk to that service in a secure manner and define the trust level. Lattice automatically discovers endpoints, enforces zero trust policies using IM and continuously monitors connectivity. No service meshes to deploy, no gateways to manage, no complex addressing plans, just application centric networking that matches how you build software. With Lattice, you get networking for your applications without needing extensive networking expertise. Your application connectivity requirements don't stop here. As you scale your applications don't stay in one region. They follow your users, your data, your business requirements all across the globe. However, multi-region architecture shouldn't mean multi-region complexity for your applications. Last year we launched cross-region private link for SAS services. Private connectivity to third-party services across any region without internet exposure or routing complexity. So today I'm excited to announce that we have the full extension of this feature to meet your needs. Cross-region private link for AWS services. Now your applications can access S3, Dynamo DB and other AWS services across the region using the same private connectivity model. The abstraction handles the complexity while your application simply connects to the endpoints it needs. This is the power of layered abstractions. Each new capability builds upon proven foundations, creating increasingly, increasingly sophisticated possibilities while maintaining simplicity for developers. But private connectivity is just a foundation. Next challenge becomes ensuring optimal performance when distributing requests to those services. So we've learned something critical, infrastructure patterns that work for traditional applications can hurt AI workload performance. Our customers running large language models experience inconsistent response times and poor resource utilization despite sophisticated load balancing. They'd have idle servers while others were overwhelmed, not from CPU or memory issues, but something more nuanced. When you're running large language models, server capacity isn't just about CPU. It's about model state, memory utilization, token processing complexity, and the specific type of inference request. A server might be technically available but completely unsuitable for the next request based on its current workload characteristics. Traditional load balancing algorithms based on CPU or connection myths. These real bottlenecks in AI workloads. We needed something entirely new. That's why we built Application load balancer target optimizer. A load balancing solution designed specifically for the unique demands of AI and high performance compute workloads. So you install the agent on the target, and the agent can you configure the max number of concurrent requests that you want that target to receive from the load bouncer. The agent tracks the number of requests, the target's processing. If the number goes below the max request number, the agent sends a signal to one of the load bouncer nodes. The node registers that signal, and when the new request arrives, it knows that it can send the request to that target. This also brings us to a critical realization. In this world of distributed services and cross-region connectivity, every application becomes an ecosystem of APIs. And managing that ecosystem requires intelligent coordination. That's where API gateway comes in, not just another networking service, but as the intelligent conductor, managing how your APIs are documented, exposed, updated, secured, and accessed. Here's what we've learned from processing over 140 trillion requests in 2024. It's a 40% increase year over year across 400,000 accounts. Most successful applications aren't just well coded, they're well connected. So let me share a story that helps illustrate this evolution. IA, the largest private sector bank in Brazil, started what seemed, it started with what seemed like a simple architecture, a mobile app connecting the three microservices, front-end authentication, and data processing. Each in their own VPC using the patterns we just discussed. But applications never stay simple. Within months, they were integrating machine learning models for personalization, adding real-time analytics, connecting third-party payment services, and building partner APIs. Today, I operates over 5000 APIs across 4000 lines of businesses, each with different security requirements, performance characteristics, and scaling patterns. This distributed architecture now handles more than 160 billion API calls per month across 13,000 AWS accounts, a scale that transforms simple networking decisions into critical infrastructure challenges. This is exactly why we've been innovating rapidly in API gateway. In the application centric world, your APIs aren't just interfaces, they're the control plane for your business logic. That's why I'm excited to introduce 3 new major capabilities in API gateway. First, API gateway portal transforms how you scale API ecosystems across your organizations and with external partners. In today's interconnected world, your APIs aren't just technical interfaces, they're the business enablers that drive revenue, partnerships and innovation. Traditional approaches create thick friction through fragmented document documentation, manual API key management and inconsistent developer experiences directly impacting adoption and business outcomes. API gateway portals deliver a unified self-service solution. Developers get searchable API discovery, auto-generated documentation, and immediate interactive testing. Providers gain valuable usage analytics and automated governance for security and compliance. This reduces API management overhead from weeks to minutes, so teams can focus on building great APIs instead of managing infrastructure. Another exciting launch is response streaming for API gateway, purpose built for the AI era. Traditional APIs follow a request response pattern where the entire payload must be generated, buffered and transmitted as a complete unit. This works for small JSON responses but creates significant problems for AI driven applications. When serving large language model responses or processing complex analytics, buffering entire responses creates suboptimal user experiences. Users stare at loaded screens while your background, our backend generates a complete 2000 word response, even though the first paragraph could be delivered immediately. This also ties up valuable compute resources and memory. So response streaming support enables real-time streaming of API responses as they're generated, delivering content immediately rather than waiting for a complete generation. This dramatically reduces perceived latency and improves resource utilization by freeing up memory and compute as data transforms and transmits. It enables entirely new user experience patterns, real-time collaborative editing, progressive data visualization, and interactive AI conversations that feel natural and responsive. For API providers, you can handle more concurrent requests with the same infrastructure, reduce memory pressure, and provide instantaneous user experience instead of batch process delay. And this is critical for the AI era. As applications become more conversational and interactive, users are gonna expect to flow for things to naturally flow. Right, I'm also pleased to announce that API Gateway now supports MCP proxy functionality, enabling seamless transformation of existing REST APIs into MCP compatible endpoints through integration with Amazon Bedrock's, Agent Cores, gateway service. So organizations face significant challenges making existing APIs accessible to AI agents. Currently you must build custom MCP servers for each API, implement complex security translations, manage data across protocols, handle capability discovery, and monitor AI agent access separately, all while maintaining enterprise security and governance. Asian Corp Gateway's MCP capabilities resolve this. With automatic protocol translation between MCP and REST, including context management and capability advertisement, it provides API key and IAM based authentication. Supports both public and private APIs and includes enterprise controls for governance and compliance. Now your AI agents can seamlessly connect with existing and new REST APIs through MCP powering agentic workflows. With built-in support for observability to troubleshoot and optimize your applications. Now that we've established the foundation of application centric networking, let's address what makes this all possible, security. Intelligent applications require security that's equally intelligent and adaptive as your network security must adapt and respond without manual intervention. The more connected your applications become, the more sophisticated your security must be. The AWS protection spans every single layer of the network stack. Begins in the silicon, with the AWS Nitro system enforcing hardware level isolation, extends through the physical network with encrypt encrypted transit. Continues through software defined boundaries of VPC with network hackles and security groups, reaches the application layer with load balancer security policy and DDoS protection at the edge, which extends to every public API. Security is the foundation that enables every abstraction that we build. Security must be deployable anywhere, effective at scale and intelligent enough to adapt. That's why AWS security is programmable and integrated into every workflow. It must be as intent driven as networking itself. From physical hardware isolation to logical network isolation, VPC carries the same isolation principle into software defined boundaries. We're extending that integrated security approach to solve one of our customers' most persistent operational challenges. Organizations across financial services, healthcare, government and retail face operational complexity in maintaining encryption compliance across their cloud infrastructure. Without centralized visibility, customers resort to manually track encryption across different network paths. Having to piece together multiple solutions, managing complex public key infrastructure, implementing application layer encryption overhead, and manually demonstrating compliance with regulatory frameworks like HIPAA, FedRAM, and others. BPC encryption controls address these challenges with simple controls. Available now, and we've already seen. A huge number of VPCs already start to be encrypted, uh, just over the last week. So in just a few clicks, you can audit the encryption status of your traffic, you can identify VPC resources that allow plain text traffic and modify them to enforce encryption across your entire network infrastructure. This extends AWS's proprietary native hardware layer, nitro encryption to major AWS services. Including fire gate tasks, transit gateway, application load balancer, and more. It eliminates the operational overhead and complexity associated with certificate and key management. Next, I'm also excited to announce another breakthrough innovation meant to make it easier for you to secure your applications at AWS Network firewall proxy. Firewall proxy is expanding the highly resilient and highly scalable Nat gateway functionality to include comprehensive proxy capabilities. You can authenticate source clients, decrypt and filter internet egress traffic, and provide protection against sophisticated attacks. It uses Nat Gateway's IP address for address translation and ensures egress only connect connectivity for private workloads. With this, Easily enforce tighter security controls against data exfiltration threats, prevent data leaks, detect compromised workloads, and filter traffic based on domain names, IP rules, and HTB header fields. Proxy offers multiple layers of protection, including domain filtering, DNS lookup, IP filtering, TLS interception, and response traffic filtering. You can now simply enable this enhanced proxy functionality on your existing NAC gateway and make it available to applications across different VPCs. No more managing proxy infrastructure. AWS handles the scaling, updates, and availability while you get enterprise grade filtering and data exfiltration protection. The firewall deployments at scale can create a paradox. The more connections you secure, the larger your footprint, the more complex your architecture could become. Customers were building dedicated inspection VPCs, managing red tables, and having to incorporate operational overhead that grew exponentially. With each new connection. But our focus is to make things easier for you. Focus on your applications rather than the undifferentiated heavy lifting of configuring complex network setups. So I'm excited to share that we've solved this complexity with transit gateway and network firewall native attachment, eliminating the inspection of VPC entirely. Now your firewall integration happens directly at transit gateway level, giving you centralized security, control across all VPCs and on-prem networks with no operational overhead, while also solving cost allocation challenges, even in large scale connected environments, as we now launch TGW flexible cost allocation. With AWS you can connect, scale and secure across all resources, automatically protecting against new risks while enabling innovation. This is security that not only protects your infrastructure, but actively improves it. Services we built are battle tested abstractions that scale without limits, opening the door to building global connectivity that extends beyond regions, beyond continents and towards planet scale operations. Throughout this talk, we've seen how abstractions simplify complexity. Like atoms forming molecules that scale into larger purposeful structures. Each layer builds on the one beneath while hiding complexity. At AWS our networking services follow the same philosophy. We've covered physical networking, from GPU to GPU communication to multi-data center networks. BPC abstracts this hardware complexity into logical components like subnets and security groups. Lattice further ts applications, connectivity, making services communicate seamlessly. Security follows this model too, with protections at every layer culminating in autonomous global threat detection. These abstractions don't exist in isolation. They're powered by something fundamental, the physical foundation of the Amaz AWS global network. I spent over 70 years building this network, so it is uh amazing to see just how far we've come. It's the lifeblood of our global infrastructure. AWS Global Network spans continents through terrestrial and subsea fiber connecting AWS regions, local zones, and points of presence into one cohesive global fiber. It ensures your applications could deliver content and services to users anywhere in the world. The highest level of security, reliability, and performance. AWS operates one of the most extensive cloud networks, spanning over 9 million kilometers of fiber. And that's a 50% expansion in just one year. I'm also excited to announce, uh, we recently announced Fastnet. This is a dedicated high capacity transatlantic cable connecting the US back to my home country, Ireland. The subsea cable will create alternative data pathways between Maryland and County Cork delivering fast and reliable cloud and AI services across the Atlantic. Operational in 2028, Fastnet will add vital diversity for customers by building a new data pathway with unique landing points, keeping services running even if other undersea cables encounter issues. This enhanced network resilience will improve global connectivity and meet the rising demand for cloud computing and artificial intelligence. If you look at our past record. Some of you probably have seen some of those uh cable cuts around the world. It is not by accident that AWS has survived every single one of them. We focus very heavily on ensuring that there is real diversity across all cable paths, and this is why we introduced the Fastnet because again, across the Atlantic, we noticed that there were certain points of, of failure with the various landing stations in the US. So, these uh projects like FAFSA, they represent investments that enable us to bring these cloud services closer to customers everywhere. Speaking of proximity, it's not just about network connectivity, it continues to grow. We're also rapidly expanding our infrastructure footprint to AWS regions and availability zones. Today, we operate 38 regions, 120 availability zones worldwide, all interconnected through that network. This year alone, we've launched new regions in Thailand and Taiwan, expanding our presence across South Asia, a new region in Mexico, our 2nd in Latin America, and most recently we've launched in New Zealand, expanding coverage to customers across the Pacific. Looking ahead, we've announced plans for 6 additional availability zones across 2 new regions, Saudi Arabia and Chile, further strengthening our footprint in the Middle East and South America, giving our customers more choice, redundancy and proximity to their end users. To extend its reach further, we offer local zones, infrastructure deployments that place AWS compute, storage, database, GPUs and other services closer to large population centers and industry hubs. Local zones enable customers to deliver applications that require single digit millisecond latency of data residency capabilities to end users, bringing the cloud even closer to where it's needed the most. And our footprint already includes 43 local zones worldwide, 35 metropolitan areas, 18 of those already located outside the US. And here's how how some of our customers are benefiting from that. So Sophos, a security company, needed to solve a critical problem. Their cloud-based threat intelligence was too slow for customers from uh far from AWS regions. So they deployed front-end services in AWS local zones worldwide while keeping core infrastructure in their parent regions, combined with Amazon RV53's intelligent routing. The results. Pretty remarkable. 69% latency reduction in Germany. 35% improvement globally, and resilience that scaled from 146,000 to 2 million requests per second during a traffic surge without missing a beat. AWS local zones didn't just fix their latency problem, it transformed their entire global delivery model. DraftKings, sports betting company, they needed to expand across 20 US states while meeting strict data residency requirements. Federal Wire Act and state regulations require customer data to remain within state borders, and traditional solutions require costly physical data centers in each state with extended deployment timelines. They deployed in the local zones for state by state expansion to achieve compliance without physical infrastructure. When. It went from deployment in from weeks to days, zero upfront infrastructure costs, 25% better latency while processing 500 million transactions in the first month of NFL. When leveraging our global infrastructure, whether through regions, local zones, or extensive network backbone, organizations face increasing need to connect and manage a global scale, their on-prem data centers, branch offices. And that's where Cloud Man transforms this equation. Unifying your VPCs and on-prem locations into a single cohesive global network. Instead of managing dozens of individual connections, you define your network intent through policy and CloudOAN handles the complexity. Cloud WAN enables you to create network segments, isolate sensitive workloads, implement granular traffic control and manage everything through a single centralized policy framework. And one of the most important uh updates the cloud around this year, routing policy. Previously, when implementing advanced filtering and summarization of better control and routes between cloud one and external networks, you had to invest in complicated, expensive third party routers to implement advanced routing techniques. Now, you get AWS native advanced routing capabilities that eliminate the need to invest in these third parties and provides you fine-grained routing controls to optimize rent management. You can set the advanced uh BGP attributes to customize your network traffic behavior, and you get advanced visibility into the routing databases, enabling rapid troubleshooting of network issues in complex multi-path environments. Cloud one gives you incredible control and automation for global connectivity, but you also need physical connectivity, your on-prem to AWS. That's where Direct Connect steps in, serving as high performance, dedicated connectivity that ensures your cloudWM policy can be executed, the reliability, bandwidth, and consistency. That mission critical workloads demand. Over the last 12 months, we've expanded to 150 locations and we continue to roll out 400 gig connections to meet the demand of AI. And this focus on connectivity reflects a broader principle fundamental to AWS interoperability. We believe you should have the freedom to choose technology that best suits your needs. Whether that's connecting your on-prem infrastructure or integrating with other cloud providers. But here's the reality, when customers try multi-cloud connections today, let's face it, the experience leaves much to be desired. We constantly hear from customers that the path is far more complex than anticipated. Often customers are left with DIY approach, leaving them with handling complexities of managing global multi-layered networks at scale. Innovation shouldn't be constrained by networking complexity, so we're addressing these challenges head on. Our vision is clear. We want to radically simplify operations so you can establish private cloud to cloud connections in minutes. Security must be built from the ground up, built in from the ground up, ensuring data protection across cloud boundaries. And you need single interface control that provides visibility and management across all cloud providers and on-prem networks. That's exactly why I'm excited to announce AWS Interconnect Multi-cloud. Already in preview with Google, and I'm also excited to announce that Microsoft will be joining us in the 1st half of next year. The AWS Interconnect multi-cloud enables customers to quickly configure private high-speed connections with dedicated bandwidth between their uh VPCs and other cloud service providers. We've built extensive pre-cabled capacity pools between AWS and other cloud providers. So when you need connectivity, it's there, ready to be activated with ease. Both AWS and your other cloud provider manage the scaling and operations, while support is directly owned by the cloud providers, all backed by an SLA. This is why, this is what multi-cloud networking should be simple, reliable, and focused on delivering value rather than managing complexity. So stay tuned because there are many other providers that are in the works. And again, with multi-cloud, we have actually defined the standard. We have published the APIs and the standard of how to interconnect, going far beyond what anybody else has ever done. Now, I'd like to introduce Salesforce to come out and tell their story. So please welcome to the stage Jim from Salesforce. Thanks, Jim. OK. Hey, good morning, everyone. I'm Jim Ostrogni from uh Salesforce um D360, formerly known as the Data cloud. Um, I'm excited to share how Salesforce leverages AWS networking innovations to deliver seamless experiences for our customers worldwide. As Rob just introduced AWS Interconnect multi-cloud, something we're really excited about. I want to share with you how we're putting this and other AWS networking services to work at enterprise scale. Salesforce operates one of the largest enterprise platforms, and AWS is the backbone of that scale. Across 18 countries and more than 700,000 production and sandbox orgs, we do rely heavily on AWS's global regions and multi-region capabilities. To deliver consistent performance, we lean deeply on AWS networking services, transit gateway, private link, and network load balancers for the massive global throughput that we see. AWS networking isn't just infrastructure, it's fundamental on how Salesforce runs at global scale. Here's the reality, um, customers are multi-cloud. Their analytics may run in Google Cloud, their legacy workloads might, uh, live in Azure maybe, and their customer engagement layer is powered by Salesforce. They choose these architectures for good reasons though. They might be regulatory constraints, data residency requirements, and just to have the best, best of breed investment. At Salesforce, AWS is where the core of our business resides. It's where we run our primary platform, where we innovate fastest, and where we operate with deep expertise. But our customers need us to meet them where their data lives. Even when the data is locked inside highly secure private environments outside of AWS. And that creates a fundamental networking challenge, right? How do we preserve the AWS grade performance and security and operational excellence while extending Salesforce into private multi-cloud data sets that never touch the public internet. This is precisely the challenge AWS Interconnect multi-cloud solves for us. Our customers are multi-cloud by design, especially the large enterprises. Their most sensitive or regulatory data sets live inside private Google Cloud environments, yet they expect Salesforce running natively on AWS to connect to that data with the same security, performance, and operational excellence we deliver inside of AWS. With interconnect multi-cloud, we can extend our AWS native private connective model directly into Google Cloud. We keep the same transit gateway policies, the same private link-based security principles, and the same operational tooling, but now it works seamlessly across clouds. For customers, that means choice without a compromise. A financial services firm can keep core trading systems and regulatory data sets in Google Cloud, while still privately connecting them to Salesforce Data 360, all without touching the public internet. As for Salesforce, this means that we can finally erase the boundaries between clouds. We can secure the customer's crown jewels, if you will, of data. Between wherever it lives, satisfying the strictest compliance mandates without asking them to move a single row. Consider this use case here, we have a large US health company who's already successfully using data 360 with a data lake hosted on AWS. Previously they were trapped in a compliance paradox. Strict security policies were forcing them to use brittle middleware and custom encryption just to move clinical data. It killed the real-time agility that they really wanted. Salesforce Private Connect broke this deadlock. We established a direct zero copy link between Data Cloud and their data lake. Crucially, we built this onto the secure AWS infrastructure their security teams had already signed off on. With Salesforce managing the complexity, they didn't have to build the pipes. They just unlocked the data. Now, we're taking this to the next step. Using the new AWS Interconnect multi-cloud, we are expanding that to secure fabric to other data lake hosted on Google Cloud. The result is profound. The customer doesn't see multiple clouds anymore. They see one unified experience. With Private Connect, they are securely integrating their data, lakes across the AWS and Google Cloud. And Salesforce, and their enterprise systems are now a single governed real-time data platform, powering AI agents and member outcomes at the speed of their business. Looking ahead, the partnership continues to open up some incredible possibilities. We're continuing to invest in additional sources and mechanisms, expanding our zero copy commitment to secure an open integration with Data 360. We're refining our security model to provide consistent zero trust networking regardless of where workloads are gonna run. Most importantly, we're simplifying the customer experience. Next enterprises will be able to onboard with Data 360 and immediately connect their existing multi-cloud, hybrid and on-prem infrastructure through a single AWS powered networking fabric. This is the future of enterprise software, not forcing customers to choose between clouds, but giving them the power to use the best of all the clouds. Thank you, Robert. Appreciate it. Thank you. Thank you. All right. We didn't stop at multi-cloud, by the way. We took that specification that we had built and we thought, hey, how can we actually go further even into the last mile. And so we know that hybrid architectures span on-prem and cloud environments, they all require the same thing, secure, reliable productivity, uh, connectivity. So we're deploying last mile connectivity between direct connect locations and your on-prem, again, it could take significant investment, time and effort. You gotta find a co-location facility that we're in. You gotta identify a suitable partner, negotiate contracts, and technical requirements, plan capacity, and procure those circuits. List goes on. However, with AWS Interconnect now last mile. And now in gated preview with Lumen, you can get a fully managed connectivity service that allows you to connect your remote locations to AWS in just a few clicks. So think about it. You already have a location that's got a Lumen circuit in it. You can go to our console, you can basically set up direct connectivity over that same physical circuit. Nothing else is required. And Lumen are only the first. There are already many other providers that we started to work with. We're going to bring this across the world and simplify this everywhere. And Even though we have an expansive global network, there can be limitations to terrestrial networks. And that's where Amazon Leo comes in. Obviously, former uh Project Kuiper, through our constellation of low Earth orbit satellites, we're extending the same global connectivity promise to every corner of the world, no matter how remote. Where traditional terrestrial networks can't go, LEO satellite networks seamlessly takes over, creating an unbroken bridge between ground-based infrastructure and space-based innovation. So the same interconnects we just highlighted with Multi-Cloud and Last Mile, we've developed them for Leo as well. Now wrap things up. Let's take a look at Clairfront. We have over 750 points of presence around the globe. We continue bringing our network even closer to your users. We have 1100 embedded Pops, a type of clever infrastructure owned and operated by Amazon, but deployed directly into the internet service providers and mobile network operators' networks. Embedded Pops are custom-built to deliver large scale live videos, video streaming, video on demand, and game downloads. And we didn't stop, uh, in terms of how we've been innovating to bring cloud front to folks. Um, and so we're equally committed, even with our large enterprises all the way down to startups and small businesses. We wanna enable everybody to get access to the same world-class infrastructure. We've heard from developers that they want the power and performance of Cloudfront. They needed the pricing that's as simple and predictable. As our new onboarding experience. They want cost certainty, that lets them focus on building great applications without worrying about variable expenses from traffic surges or security events. So, we introduced flat rate pricing packages that bundle cloud front, web application firewall, DNS and storage with transparent usage allowances and no overage charges. The result, predictable monthly costs and zero financial uncertainty. Cause when developers stop worrying about builds, they start building the future. And developers are responding. In just 2 weeks, we've seen over 15,000 subscriptions to our flat rate bundle. Whether you're building, whether you're a startup building the next breakthrough app or an established organization like the NBA, Cloudfront scales to meet your needs. The NBA leveraged our global infrastructure to deliver NBA to prime to millions of viewers globally without interruption. And Epic Games, just look at what they achieved. For their last Fortnite season launch, Cloudfront delivered 268 terabits per second, an all-time high that demonstrates the incredible scale our network can handle and your biggest moments matters. And everything was handled through our network automation, traffic engineering, no humans involved. Over the last few weeks, we've had over 50 launches across IP management enhancements, DPC futures, VPNs, RV 53, and security. From our physical hardware in regions, AZs and LZ's to our services like Cloud One, Direct Connect, and CloudFront, we're going beyond the boundaries of connectivity, reaching not just across continents, but also literally to the stars above. Thank you very much. Really appreciate it. You're all free to go get lunch now. Thank you.