```
---
video_id: z9ewFwkmc_g
video_url: https://www.youtube.com/watch?v=z9ewFwkmc_g
summary: "In the session \"Accelerate gen AI and ML workloads with AWS storage\" (STG201), Monica Vevajare (Amazon S3) and Jordan Dolman (Amazon FSx) present a comprehensive strategy for aligning storage infrastructure with the evolving needs of Generative AI. Monica opens with the \"Inference Spectrum,\" moving from simple Prompt Engineering to Retrieval Augmented Generation (RAG). She identifies a key bottleneck: traditional vector databases are often expensive and complex to scale. To solve this, she announces the general availability of Amazon S3 Vectors, a fully managed, serverless vector store native to S3. By decoupling ingestion, storage, and query pricing, S3 Vectors offers up to 90% lower costs than alternatives while scaling to 20 trillion vectors per bucket. She also introduces Amazon S3 Tables and the importance of metadata filtering (using Glue catalogs or custom tags) to refine RAG searches. A critical part of her presentation focuses on the Model Context Protocol (MCP), which AWS is championing as the standard for connecting AI agents to data tools. She reveals that AWS has released open-source MCP servers for Bedrock Knowledge Bases and S3 Tables, enabling agents to autonomously query structured usage data or unstructured documents using natural language, effectively bridging the gap between static storage and dynamic agentic reasoning. Jordan shifts the focus to the \"Training Spectrum,\" addressing scenarios where model weights must be updated via Supervised Fine-Tuning (SFT), Distillation, or Alignment/Direct Preference Optimization (DPO). For these \"Model Builder\" workflows, high-throughput, low-latency shared storage is non-negotiable to keep expensive GPUs saturated. He advocates for Amazon FSx for Lustre and FSx for OpenZFS as the premier solutions for storing training data and checkpoints. He highlights the new FSx Intelligent Tiering, which automatically moves cold data to infrequent access tiers, allowing data scientists to keep massive datasets online cost-effectively without manual management. He also details bi-directional sync capabilities that allow FSx to act as a high-speed cache in front of an S3 data lake, giving researchers the best of both worlds: the low cost of S3 and the POSIX compliance of a file system. For the most extreme use cases—Continued Pre-Training and Training from Scratch—Jordan discusses the need for \"supercomputing-class\" storage performance. He showcases Amazon S3 Express One Zone, a storage class purpose-built for AI that co-locates data within the same Availability Zone as the GPU clusters. Using a session-based authorization model (avoiding per-request overhead), it delivers single-digit millisecond latency. He cites a real-world example from Meta, who used S3 Express One Zone to achieve a staggering 140 Terabits per second (17 TB/s) of throughput for training their Llama models. The session concludes with a look at optimization tools like Mountpoint for S3 and the S3 Connector for PyTorch, which streamline the loading of data from object storage directly into training pipelines, ensuring that scale never comes at the expense of speed."
keywords: Amazon S3 Vectors, Model Context Protocol (MCP), Retrieval Augmented Generation (RAG), Amazon FSx for Lustre, S3 Express One Zone, S3 Tables, Supervised Fine-Tuning (SFT), Model Distillation, Intelligent Tiering
is_generated: False
is_translatable: True
---

Hey everyone, thanks for joining. Good crowd. My name is Monica Vevajare, and I'm a senior technical product manager with Amazon S3, and I'm joined by Jordan Dolman, uh, principal product manager with Amazon FSX. Today we're going to cover how customers are using AWS storage as they build and scale new AI use cases. It's no secret that everyone wants to build with generative AI, but the real challenge is how to make it work for your data, for your business at a cost that makes sense. Today we're gonna show you how customers are achieving this all built with AWS storage. The key to making AI work for you is your data. Genericc AI gives you generic answers, but your data, like your customer feedback, your usage patterns, that's what makes AI valuable for your business. What we're seeing sets customers apart today is the ability to access relevant data to fuel multi-step AI agentic workflows. When your AI agents can get access to the right data at the right time, they can better understand your business and your customers, and that can dramatically help you improve your productivity. So let's start with the core challenge. I have a given task that I want to improve productivity for, and I want to use an existing large language model or an LLM. I wanna use a foundational model out out of the box, but I needed to respond based on my data. But here's the problem LLMs are drained are are trained on static data. They don't know your latest information or your business context or your use cases, so they give generic answers and sometimes hallucinate. So we're gonna discuss several approaches on how you can build AI workflows for your business using existing foundational models. This is where most people start today because it's pretty easy to go from an idea and the data you have to higher productivity. As we cover different approaches today, they're going to increase in cost, time and complexity, but they're also gonna dramatically improve an output quality so you can kind of pick where on this scale which approach fits right for your business. Let's start with the simplest approach, prompt engineering. If you have a single task in mind and data, odds are you're starting with prompt engineering. You give the LLM examples, context and constraints to guide the response. I can give you a personal example. At Amazon, before we build or launch anything, we start with the PR FAQ document. This consists of a press release as well as a series of internal and external FAQs. To anticipate what customers are going to ask, what are the hard questions they're going to challenge us with after we launch. This helps us work backwards from the problem to know exactly how to define the right product shape and build the right thing. To help me make this PRFAQ document, I have a saved prompt and a markdown file that I use with QCLI. In it I've included examples so these can be good PR FAQs that have gotten approved in the past, some big launches like you probably heard yesterday. Some context like notes from my meetings with customers so I can specifically define and hone down on what the real problem statement is as well as constraints like legal guidelines from our legal department on messaging do's and don'ts to include. And the result this has really helped me with my personal productivity, a process that used to take weeks to get to a good first draft, to get to a review point. Now I get uh something in minutes and I get a reviewable draft in within a few days that I can review with stakeholders and my team. So this has really helped my productivity, but how do I extend this capability to my team so that everyone can benefit from this? Does it scale if I need to include hundreds of documents or multiple different data sources? I can't manually add in all of that context because as as you must have experienced, the context window overflows and once that happens, quality tends to degrade. What we really need is to give the model access to massive amounts of data, but give it the ability to find and use the only the most relevant bits of information, so the context window doesn't get overwhelmed. Rag, a retrieval augmented generation, is a scalable solution to this problem. You retrieve the relevant information, augment your original prompt with that relevant context, and use it to generate a better response. Remember, we're trying to give these foundational models access to your evolving data without manually having to load everything into the prompt. So to bolster your prompt with rag. Sorry, to bolster your prompt, we're going to use RAG, which uses semantic search to find and return relevant data from any size data lake. Here's how it works. We start by converting your data into a vector. A vector is basically a numeric representation of your data that captures its meaning. Once all of your existing data is converted into vectors. Next, when you make a prompt to your AI model, that query is also converted into a vector. We use spatial similarity to search against that query query vector against all of your other vectors to find similar other content. We then use those close matches to augment your original prompt and then together that augmented prompt is then fed into the model to get a better response. This focused relevant context instead of generic knowledge that the model was trained upon helps you get more accurate answers then and and it's also less likely for the model to hallucinate. So before you can start using your data for semantic search through RAG, you do need your data in some type of format to access to to vectorize it all in the first place, and most of our customers use S3 to adjust their data. That's because S3 is low cost and scalable to any workload size. There are 2 main approaches to get your data into S3. 1 is batch ingestion, and the second is real-time ingestion. Batch is a useful technique when your data doesn't change frequently, so think of documentation, historical records, product catalogs, they're not evolving all the time. Another good reason to use batch is if you need to pre-process your data. You can imagine if you need to pre-process like chunking or to generate embeddings. This isn't something you can do scalably real time. The second technique is real-time ingestion. Conversely, this is a good tool when your data is changing frequently. So for example, your live social media feeds or live transcripts from customer customer support calls, you don't wanna have a dated pulse on what your customers are saying. So you can't use batch operation from a week ago or every month. That's gonna have sale data. You can use Amazon Kinesis or SQS as a simple way to ingest real-time streaming data. It collects, processes, and loads the data into S3, and there are also several ways to do batch ingestion. For rag workflows, usually batch is what customers choose because it's simpler and more cost effective. OK, so now we have our data into S3 and we want to vectorize it so we can use it for semantic search with RAG. What are vectors? Vectors are basically that mechanism that makes rag more powerful through semantic search. You can vectorize any type of data. For this example, let's do text documents. So you have your documents and they are then converted into chunks, which is basically a finite set of characters of text in this case. Those chunks then go through an embedding model. And some uh to generate vectors and some embedding models also attach metadata to help refine your search to the vector. These then embeddings and metadata are stored in a vector database. Remember, these vectors now capture your evolving data so that we can use it later to augment your query. There are several choices you need to make in this pipeline. One, you need to choose your data source, so this could be an S3 bucket or a prefix if you want more granularity. You also need to choose your chunking strategy. So imagine you're a film studio, um, and instead of text documents like we're doing here, your data is movies. You may choose to chunk by act or by scene or some more granular time frame. Then you need to choose your embeddings model and then finally your vector store. You can manage this pipeline on your own. But we also have bedrock knowledge bases which can manage it for you. It has a simple way to configure this entire pipeline and the nice thing is when you're uploading new documents, those documents are already ingested as vectors. And they land in your vector database. So now vectors are essential for rag, but managing large volumes of vectors can be challenging and expensive. Over the last years we've heard three main problems from customers. First is cost. Many traditional vector databases bundle storage, memory, and queries as a single, um, as a single, uh, together, making it cost prohibitive to deploy large vector sets. We've also heard that scalability is a challenge, so it's it's difficult to scale from small proof of concepts again to those large production data sets. And then finally granularity. We've heard that many customers need millions of separate indexes for multi-tenant applications, and once you start to get to that scale, costs tend to spiral out of control. The real theme that we've heard across all of these was cost. Customers needed a more effective way to store and manage vectors. That's why this week we launched general availability for Amazon S3 vectors. We're really proud of this launch. It's the first cloud object store with native support for Vector's store and query. And it has completely transformed the economics of AI. S3 vectors offers up to 90% lower costs for uploading, storing, and querying vectors. We offer 100 millisecond warm query latency because we're able to cache queries that are frequently made and subsecond cold query latency. You can store up to 2 billion vectors per index and up to 10,000 indexes per bucket. That's over 20 trillion vectors in a vector bucket, and of those you can have 10,000. We also offer fast ingestion so you can get to work quickly. You only pay for what you use, and the best thing is this is built on top of S3, so customers get access to the attributes that they know and love about S3, like availability, durability, security and compliance. Vector pricing is fundamentally different from traditional vector databases. Like I said, most of them bundle compute, memory and storage, and you pay for it all together. You provision capacity upfront and then pay around the clock. The way S3 vectors has changed that completely is we've introduced 3 pay per use pricing components that actually align with your usage. So first you pay for ingestion, you pay for the vectors that you're putting into the vector database. The second is storage. You can leverage S3's industry leading economics to store vectors at a fraction of the cost. And finally, queries, you pay for the queries that you're actually making. There's no capacity planning necessary and no infrastructure overhead, and this is really useful when your application has varying workloads. You can imagine during the workday maybe your team is making lots of queries, but at night there's hardly any out of business hours. This way you're only paying for what you're using and when you're using it. Here's a customer example. We launched public preview of vectors this summer, and this customer has been with us since then. We've gotten to really work with them and see how they've scaled. This is a biotech firm, and they've been using S3 vectors for semantic search on scientific literature. Their team consists of PhD scientists and entrepreneurs, and really their goal is to discover the next breakthrough in drug development. To do so, they need to know and absorb the large corpus of scientific literature and knowledge that already exists, and this isn't just reading a few papers every morning with breakfast. The scope of this problem is 30 million scientific papers. So before they integrated with S3 vectors, this research phase would take weeks, and still you can imagine it's not possible to absorb all of this knowledge. Since integrating with S3 vectors, this is what their pipeline looks like. They've ingested the entire corpus of scientific literature that they had access to. Those were then generated into millions of vector embeddings which now have landed in S3 vectors. Now when they're exploring a hypothesis, it's as simple as performing semantic search on this on these vectors to understand what is nearby and what is relevant. This has dramatically reduced their research timeline. OK, so let's recap what we've done so far. We have all of our vectorized data in S3, and we're, we're, we're performing rag workflows on it to get more relevant context. But here's the thing when you perform a rag workflow on this data, it's searching against all of your data. We can make rags even smarter with metadata filtering. To this metadata filtering helps to narrow the search space so your queries are faster and more accurate and more targeted to what you're actually trying to achieve. So for example, instead of those 30 million documents, this firm could choose to select only once about gene synthesis. You can use glue to catalog structured data, and you can use, you can add custom metadata for unstructured data. You can use metadata filtering then directly in line in your rag workflow. It's like adding wee clauses to your search to make it faster and more effective. And here's where it gets really powerful. As you continue to process your data, more metadata gets generated. And this rich metadata becomes the nervous system of your entire AI operation. Metadata gives you context like what is the data I'm looking at. This is Q4 sales data. It also gives you lineage like where the data actually came from and how it's been transformed, and this has been really useful for root causing issues. And finally it can give you classification like it can auto tag sensitive data such as personally identifiable information so that it's managed correctly. Metadata helps AI understand not only what your data is, but what it actually means for your business. So so far we've built a complete rag system. We've ingested data into S3. We've vectorized it for semantic search. We've empowered it with metadata filtering to make it more effective, and we've introduced a cost effective vector storage. Many customers choose to stop here because this is already a very powerful AI engine and it delivers results at scale. But what if you want more? What if you instead of a single question like what are similar scientific documents to this problem I'm exploring, what if now you want to break down a complex task, give it power with tools and access to other data. And then have it reason through a complex problem. Earlier I introduced prompt engineering with an example on how I've used it to improve my productivity with PRFAQ writing. We are now entering road map season for next year and it's been great to have many conversations with customers this week at Reinvent as well as over the last year to help us define what to build for next year. Can I make a road map agent using all of this information and using several different tools? What if I want customer feedback for my notes from this week as well as product reviews we have online. I want to check our S3 tables for customer usage data to find top customers, and I wanna combine those to find the top pain points that customers are discussing, and I wanna write a PRFAQ for each of those as a pitch for a new feature that we should launch for each of those pain points. That requires the LLM to now reason through a complex task, use multiple data sources and tools, and perform those actions. Rag isn't gonna cut it anymore. Now I need agents. With agents, you give your LLM a set of tools up front. Things like bedrock knowledge bases for access to documents, S3 tables for access to usage statistics, APIs to pull customer feedback from social media. And here's the big shift. Before this with prompt engineering, we had to guide the model step by step. Now agents figure out the steps, figure out what data is needed and what are the right tools to get it. So if we look at this diagram, we were already prompting the model with goals and instructions and context tools are the missing piece here. This is all fed to the agent who's using this foundational model. To perform actions. Managing all of these tool integrations can start to get very complex. This is where MCP comes in. MCP or model context protocol is becoming the standard for how tools connect with agents. It's really similar to how HTTP standardized how applications can talk to back ends. There are two parts to this MCP clients and MCP servers. MCP clients define the how. How do you query a database? How do you search through a knowledge base, and MCP servers actually execute on that. They check those constraints and those rules and then actually execute that query and tailor the results based on um how you've configured it. AWS is building several MCP servers for our services as well as external services to help your agents find the right data through the right tools. For example, we have an MCP server for Bedrock knowledge bases. Now you can query knowledge bases with natural language, no API calls necessary. With this, you can filter to target specific sections of the knowledge base. It can configure the result size and re-rank outputs to improve relevance to what you're searching for. You can also do this conversationally. So now my road mapping agent can ask, OK, what are the key limitations that customers are reporting in our product reviews, and it's all configurable. So in addition to rag for semantic search, we also need traditional ways to filter and find data. That's where metadata comes in. You can generate and search metadata with S3 metadata, and this works for both structured and unstructured data. So here's how it works. Let's say you have lots of types of data like PDFs, CSVs, audio files, video files, you name it, and they're all landing in your S3 bucket. First, you want to configure your source bucket. You want to configure S3 metadata on your source bucket. Second, you want to create an S3 table bucket. This is where a queriable metadata table will live. You can do both of these through a single API call or just a few clicks in the S3 console. Finally, S3 will generate a metadata table in that table bucket, and it automatically updates every few minutes as your data is evolving. It auto updates for new objects that enter it auto tracks system metadata as well as you can configure your own custom metadata. Rag workflows can leverage this data for metadata filtering for more optimized and targeted searches, and you can also query it on your own for your own use cases. You can use Athena, QuickSuite, Spark, or any SQL-based process to gain valuable insights from your metadata. So this is very useful for rag and for analytics, but it's also useful for agents. Now you have this rich cataloged metadata and your agents can also access this. This year we also launched MCP Server for Amazon S3 tables, so now you can use natural language to interact with S3 tables and S3 metadata. No sequel required anymore, so coming back to my roadmap agent, it can ask, it can check the table of customer usage and see who are the top 10 customers for a given problem I'm solving and what is their monthly usage look like. This is all available in the AWS MCP open source repository and it's really easy to set up. OK, let's bring this all together. We started with prompt engineering, giving your LLM examples and constraints and context to get better responses. Then we added rag, giving your LLM access to more relevant data to fill that context and augment your original prompt. We vectorized our data to support this. We discussed metadata filtering to target the rag search, and we also talked about S3 vectors as a cost effective way to manage your vector storage. Then we leveled up to agents. We gave our LLM the access to connect with multiple tools to perform complex tasks. And finally, we talked about MCP, the standard that makes it easy for your agents to connect to tools. This here is the modern AI stack, and it's where many customers stop because it's really powerful and it's scalable. Some customers want to go further. To to discuss more advanced workloads, I'm gonna invite Jordan to the stage. Thank you. Thank you. All right, so Monica talked a lot about how you can use prompt engineering to provide better structure and RG to provide more data to improve the, the outputs of your model now. This works in many cases, however, a lot of that data and structure we just talked about lives in the context window of your application that's kind of like the short term memory of the model and because that's fixed, sometimes you have to go further and actually embed some of that content in the model itself rather than using something off the shelf. So let's think about how models work today and how they're built. Every model that we have access to that's already pre-existing has been trained on a large data set, and the knowledge that was used for training is now deeply embedded within the weights of the model itself. That data is accessible when you, you know, introduce a prompt to the model and get a response, but if the knowledge you're looking for isn't available, if the structure doesn't match your application, then we actually need to try to tweak those weights, update the model itself to get the right kind of output. Now of course that takes data and depending on how much data you have, there's different techniques that you'll be able to use in order to actually train and improve the actual outputs of your of your model. If you have a small amount of data, rather than trying to update the weights across the whole model, we really wanna focus on just updating portions of the model itself. And if you're really going to get a big bang for your buck with your data, you want to provide even more guidance by labeling the data so it's very clear you have an input and a desired output that you're looking for. You're trying to guide the model to produce certain outputs based on that small label data set, and you're going to focus it on a small portion, a subset of the model. If you have more data, then you can actually still with that labeling to provide that guidance expand and actually update more of the model weights that's kind of like rather than teaching the model of a pointed piece of information, it's like helping it think in your domain that you're working with uh that's maybe not already embedded in the model itself and then if you have a huge volume of data, you can actually skip that kind of labeling step. And go straight to something that's more akin to continuous pre-training. It's kind of like picking up where the original model developers left off and then embedding that new content in those model weights. Now the services that we have available to help with this effort are Amazon Bedrock and Amazon Sage Maker AI. If you find yourself mostly on the inference side, the application builder side of the equation, Bedrock is really where you'll probably want to operate. And if you think of yourself more as a model model builder, model developer, then SageMaker AI is, is the place to go. In both of these cases you have capabilities to do what we're gonna talk about now, which is updating the model itself, but each of these services has really been refined or or tuned for specific types of user experiences. So let's go into some of the different techniques we have to actually update the model itself in this case with that labeled data. We'll start with 3 techniques supervised fine tuning, distillation, and alignment, and I know that there's a lot of these words that get thrown around in this ML space. So one of the goals I have today is to try to walk through and make sure that everyone has a clear understanding of how each of these differs and what it means for data, what it means for storage. OK, so supervised fine tuning. Again, I mentioned before labeled data is really helpful when you're trying to tell the model here's an input, here's the output I'd like to see, so this is kind of in that same vein what is the input going to be? What's the user saying? What other context do we have available as part of this exchange and then what's the single best response or output? A simple model may be used for something like translation or summarization, uh, that's text to text, text input, text output, uh, might have label data that looks like this, just a very simple, here's the prompt and here's the desired output I'd like to see. This is a non-conversational model. If you have some more conversational models, you know, this is more of like the chatbot type of experience, you'll notice that the the labeled data is actually going to need to have tags for the user, the assistant, kind of like who's actually introducing the prompt, who's responding, and some of that label data can also have multiple turns. It can go back and forth between the agent and the user as part of that label data set. Now I should say the reason why I'm putting all this up here on the slide is because when you're thinking about the data that you're generating and collecting across your organizations, thinking ahead to how this data might be used can be really helpful, especially if you aren't expecting to be generating, you know, hundreds of thousands of data points to use for future training. And you're going to be relying on labeled data sets, thinking in advance about how you might wanna label data that's coming out of maybe a call center log, uh or any other kind of interaction can be really helpful. Another example here, different type of model this is image to text, but again we have an image reference and then an explanation, a caption that basically says when you see this kind of image I'd like you to respond with a cartoon of an orange cat with white spots. This supervised fine tuning example that I've shared today is available in Amazon Bedrock. So again, even though it's intended in general for app builders, you still have this kind of capability available to you when you're working with Bedrock uh to customize those models. The next capability where you might be tweaking the underlying model itself is distillation. So this is something that might be useful if you're working with a large model and you really like the outputs that you're getting. But the large model isn't quite fast enough for your your user behavior, for your desired application. Maybe it costs more, but again, you like the like the output, and when you try to generate the same kind of outputs from a smaller, more nimble, lower cost model, you're not getting the results you like. Distillation is kind of like the cousin of supervised fine tuning. So again we have the prompts coming in, but in this case you don't actually provide the output responses. The outputs get generated by the larger teacher model and then get fused back with the input and then used as basically labeled data to train the small student model. So again this gets a bit messier here. I'm trying not to overwhelm with a lot of text, but the key here is you see in this third row, there's this role of a user. And then there's some text, some context and a question, but there's no assistant response and that's because again the assistant response is gonna come from the larger teacher model and so again this is pretty useful if you don't actually have all the answers but you know that a large model would produce content that you like you can use something like distillation. Alright, and then the 3rd technique with labeled data that can be really helpful is alignment. This is less about training for knowledge and more about training for tone. Or for uh maybe compliance adding guardrails into the response, uh, sometimes this can be something that's really helpful for like the branding of of an organization and similar to fine tuning we wanna provide what's the prompt, what's the context, but in this case we wanna provide preferred and non-preferred responses and so this is where I think the example is actually quite helpful, uh. This labeled data that you're going to be providing also includes kind of a score or guidance of different responses and which one it prefers. Here you're really teaching the model not just what output you want to see, but how it compares to another output so that it can really start to understand and shape towards the preferred outputs over time. This is something that if you were interested in doing this kind of direct preference optimization, this kind of alignment technique, you would need to be shifting over to Sage Maker AI. This one isn't available in Bedrock today. So we have 3 different techniques using labeled data. It's also worth noting that the examples I shared today are real examples you could use that kind of structure and syntax for supervised fine tuning, distillation, and alignment, but they don't work with every model and so you you'll want to look at the specific inputs, specific label data requirements for the models that you're planning on working with uh in advance so that you're actually structuring your data properly or you can obviously modify it after the fact. OK, so we've gone from the left here through the label data, now we're all the way on the right side, continued pre-training and training a new model from scratch. We'll come back with one of those slides at the end as well. Um So, here again, we have a lot of data, we're not happy yet with the output of the model, and we wanna take it forward. The the best place to go here would be continued pre-training. This is where you don't have to have that labeled data set, but you can deeply embed new knowledge, new tone, new relationships between your data into the model itself. But the fact is there are also cases where the the model that you're working with was actually trained on data that is just structurally so different from the model you're trying to create, maybe it's a different language. Or maybe it's not a large language model at all. It's something like a weather forecasting model or a foundation model for drug discovery. There you're really going to be starting from scratch with building a model and that's fine as long as you have all that data and that's what we're gonna be talking about here. One of the big differences if you're in this kind of uh continued pre-training or training a model from scratch, the big difference from some of the previous phases is this is a much more resource intensive effort and the the integration between your data, the compute and storage becomes really, really important. And the reason that that's the case is because getting all of that data that we just talked about, that vast amount of data you need to train a model from scratch or do continued pre-training, needs to be loaded from storage efficiently into your GPU or accelerator instances, and then periodically for a number of different reasons, you'll also want to write these kind of intermediate checkpoints back to storage. Sometimes you'll want to write checkpoints because you want to make sure there's a safe point to restore back to in case there's some kind of infrastructure issue. And in other cases it might just be a point for you to evaluate the model and maybe go back to if you want to try training with different data sets over time. In all of those cases, there's a lot of data that you need to get into the GPUs and there's that checkpoint data that you want to get off the GPUs very quickly. To give you a little bit of a sense of the rates that these GPU and accelerator instances can process data, uh, if it's text-based models you're working with, typically we see around 128 megabytes per second for every GPU. So if you imagine distributed training on a number of different GPUs, that number can get pretty high gigabytes, tens of gigabytes per second. If you're working with richer media, multimodal models or video, the throughput required to actually drive um data into these accelerator and GPU instances can be quite material. The other thing that can impact performance is actually the size of the IO, the size of the data that you're trying to retrieve at a given point in time for a given request. If you think about reading data from storage, the time it takes to get that data back is a combination of overhead for the request and then moving that data itself, the payload. Moving or getting the request that overhead is authenticating that you have access to the data. It's the network latency to actually go and retrieve the data between the GPU or accelerator instance and the storage, and then it's the metadata look up to figure out within the storage system where the data lives. Now if you're working with small files or small IO small objects, then the amount of time you spend on that overhead actually dominates the read. And this is where having storage that's low latency, storage that is able to respond and do that authentication, shorten that network path, and do that metadata look up really quickly can have quite an impact on the job completion time. And again, training these models, doing this continued pre-training or training a model from scratch can be quite expensive and so making sure these these instances are kept busy, that the training is efficient. It's helpful on the infrastructure side. It's also helpful just to get responses to the people who are developing these models in the first place and iterate quickly because this is really collaborative and and iterative um uh workload. So within the storage portfolio that we have at AWS on the file side, we have Amazon EFS, our elastic file system. We have Amazon FSX, which is commercial and open source file systems that we manage on behalf of our customers in the cloud. Uh, this is kind of akin to RDS. We have a variety of different file system offerings within the FSX family. We have object storage with Amazon S3 and then we have block storage EBS. For model training in particular, the shared storage offerings that our customers use fall within the FSX and Amazon S3 families in particular. And then just to kind of go maybe one step further, how customers think about which storage services to use typically if you're working on the research and development side, file systems are the preferred approach for shared storage, and if you're kind of looking for something more optimized for production use cases, data pipelines that are more fixed, that's where something like Amazon S3 and object storage becomes more optimized. So let's dive into the file side. Again, my goal here is to make sure that you understand all these different terms that are typically thrown around with AI and machine learning, and that you know which services to use for each given use case. In the case of research and development, We have different file systems that have been basically built and uh and and architected for specific use cases, scale up file systems, file systems that rely on a single server that can be larger or smaller. Are one part of the portfolio, one type of architecture, and then we have these scale out file systems that rely on multiple servers stitched together to deliver higher levels of performance. Amazon FSX for Open ZFS is an open source file system that we fully manage for our customers. It's a scale up file system that uses NFS for communication. This is the most standard way for file systems to communicate with EC2 instances or any other client instances. And it's ideal for ultra low latency workloads like home directories, storing your condo environments or Git repositories. This is where your developers are gonna want to keep their data so that if you have a, let's say a Kubernetes based uh developer environment stack where researchers are just having their instances spun up and down, but you want to give them shared storage to work with in between. Something like an open ZFS shared file system is incredibly valuable. On the flip side, stitching a bunch of servers together is how you get to something like Amazon FSX for Luster. Again, this is a file system we manage on behalf of our customers so they don't have to think about what is luster, how does it work. For for our customers this just becomes a mount point that you mount on your instances and you do file APIs. You open, read, write, close files, very simple in terms of the interface, but the power you get is the ability to drive very high levels of throughput and IOs or transactions, uh, to your storage. And so this is the, the solution that we recommend for customers to store their training data. To receive and restore their checkpoint data because it offers this really scalable performance. Maybe one example of the type of thing we've done with these offerings is allowed our customers to actually use some enhanced networking capabilities like elastic fabric adapter or GPU direct storage with our FSX file systems. This offering or this networking stack allows customers to read data directly from our FSX4 luster file systems into the CPU memory or the GPU memory. On those GPU and accelerator instances and again this is optimized for performance this is why we recommend these file systems for these particular workloads and you don't have to take these steps to use these networking stacks, but they're available to you and once you set them up you don't have to do anything special to achieve the high levels of throughput that are actually enabled by these networking stacks. So there is one challenge that has historically been associated with file systems. Many file systems are based on SSD based disks, solid state disks, because a lot of file-based applications expect very low latency responses and high transaction rates, and one of the challenges with SSD-based file systems is they can be expensive. If you have large amounts of data and you're some of it's hot, some of it's cold, storing all of that on an SSD-based file system isn't ideal, and if your data volume is increasing and decreasing quickly, having the right amount of storage on SSD disks to support that data can also be challenging. And for those of you who are taking on that challenge and trying to move data between hotter and colder storage offerings, that's just more operational overhead. And so one of the things we've done over the last year with our FSX offering is we've added FSX intelligent tiering. This is very similar to the concepts that we see on S3 intelligent hearing. We have virtually unlimited elastic storage on our file systems. We have that half a penny kind of price point that you, you might be familiar with with something like Glacier instant retrieval. And then again we're automatically moving data between hotter tiers including an SSD based tier all the way down to that colder archival instant access tier. And so for quite a while customers who were interested in working with file systems but had a lot of data, they would do something where they'd split their data between Amazon S3 and their file system really just for cost reasons, even though everything was going to be accessed through a file interface and so. This really allows our customers to just work with one single file system and be able to take advantage of all the different tiers and get the performance of SSDs for their hottest data. Get the cost of a frequent access tier for their hotter data that's kind of staying, you know, really being used over the last few days and then as data gets colder and colder we archive it down and you get that automatic cost savings again down to that 0.5 penny price point so super easy and super simple to work with if you've got that mix of hot and cold data and again accessible on that FSX for luster or FSX for Open ZFS file system. Now, if you are a customer who has data stored in S3 in an S3 data lake, we also have other capabilities to integrate FSX with your S3 data lakes. One option you can do is connect your file system to your S3 bucket. You can load the metadata, so those pointers to your data onto the file system, and they just show up as files. So you open a directory and you can kind of see your S3 data on your instance just as a regular directory. When you go and access that data, it gets pulled lazy loaded behind the scenes onto your file system and obviously communicated or shared back to your EC2 instance. If you add new data to S3, it can automatically show up on your file system, and if you write new data to your file system, it can be pushed automatically back to S3. So super powerful architecture for those of you who watched Matt Garman's keynote, you may have seen this architecture a couple of times, and customers like this a lot because the storage and IT administrators can think about working with their data in an S3 data lake, and the researchers can have the intuitive collaborative interface that file systems provide. One of the examples that was in the keynote was Adobe. They work with this specific architecture actually to store a lot of their data on S3 and access it through a file system. And they use this for a lot of their research to figure out what models are actually going to be moved into production, what models are helpful and add value, and eventually they use S3 to do some of their production training. Now If you do choose to store your data in a file system, it's probably also worth noting that we've added some new capabilities this year, in fact this week, to make that data accessible to a whole wide range of Amazon Analytics services, so. In the last example I mentioned, you might have a data lake with S3 data, and you can access it through your FSX for luster file system. In this case, if you have your data in an FSX file system, but you want to access it with Amazon S3, now we have this concept of an access point that you can actually attach to your FSX file systems and then you can actually work. To access data that might be stored in your Open ZFS file system or now in a NetApp ONTAP file system. This is super helpful if you are a customer who's maybe migrated data from on-premises with a NetApp file system into the cloud. And you want to make that data accessible to Amazon Bedrock for some of that fine tuning I mentioned before or for that distillation use case before. So all of that data is now accessible to all of these AWS services as well using these S3 access points. Now we've talked about using Amazon FSX to accelerate and deliver the performance we need for these uh research and development applications. Let's talk a little bit about what we've done on S3 to optimize for these ML workloads as well. So Amazon S3 has a lot of storage classes. The ones on the left are the ones that are going to be used for that hotter data. Both Amazon S3 Express 1 zone, which is all the way on the left, and Amazon S3 Standard provide very high levels of throughput for a bunch of different applications. Amazon S3 Express One Zone differs from Amazon S3 in that it provides much lower latencies, and we'll talk about that in a little bit. But really if you're working on continued pre-training or training a model from scratch, you should expect that all of your data is going to be in one of those two classes just because it's going to be accessed frequently and you're not going to want to have the request cost profile of anything in those kind of further to the right tiers. One of the ways that Amazon S3 Express one zone has actually optimized itself for these use cases is it offers a lower latency profile by not making uh a uh an authorization request on every single get request or put request. It actually uses a session-based authorization, so you do one authorization and then you can read and write or get input from your your object storage to get that that lower latency. The other thing is in the name you can see it's Amazon S3 Express 1 zone. So we've actually deployed this S3 storage class within the availability zone, so it's co-located with your GPU or accelerator instances, shortening those network paths as well. And so those two capabilities that session-based authorization. And the fact that we've co-located the clusters with your GPU or accelerator instances allow us to deliver that single digit millisecond latencies that can be really helpful when you're working with small objects. The other thing that kind of goes hand in hand with small objects is having very high levels of TPS out of the box, so hundreds of thousands of transactions per second available with the storage class. One interesting example of a customer that's actually done a lot of this kind of training working with the S3 Express 1 zone offering is Meta. They came to us asking for a very high throughput object storage class that they could use with very high levels of transaction per second and very low latencies, this kind of single digit millisecond latencies. And so we worked with them to scale some of our S3 express clusters in order to meet this 140 terabit per second throughput level. That's 17 terabytes per second. We are in clear supercomputing storage range here available in the cloud to train these models. Now I don't expect everyone to be doing anything near this kind of scale. But it does speak to the capabilities that we have, both for small scale and the ability to scale to meet the needs of even the most demanding workloads. In addition to moving some of the infrastructure closer to our compute physically in our data centers, the other thing we've tried to do over the last few years is bring the APIs a little bit closer to the workloads that you might be running with S3 for machine learning. Two examples of that are Mountpoint for Amazon S3 and the Amazon S3 connector for Pytorch. Because a lot of machine learning workloads and libraries rely on and expect file interfaces to be used to access data. We added Amazon Mountpoint for S3, which is a connector that's fully optimized for Amazon S3. There's a lot of connectors out there in the world that work with object and provide a file interface. We wanted one that was fully optimized to take advantage of how S3 works under the hood. And so this is super helpful for read only machine learning workloads. If you're gonna train a model and you're gonna read through a lot of data on S3. It's a very simple plug-in to just say let me use Mountpoint for Amazon S3 to access that data, read only, and to just translate those, those file open and read read APIs into an S3 get effectively. On the other side, there's also some workloads where we know that just swapping the interface. Isn't sufficient When that happens, we want to go further up in the stack, and that's where something like the connector for Pytorch comes in, where we actually swapped out the whole file interface and developed something that was really optimized for Amazon S3 and allows us to do more streaming and prefetching to get data from S3 into those EC2 nodes for that training effort. OK, so now we've, I think, finished the whole spectrum here of different ways for you to work with your data to improve your models both for inference if you're not really going to change the model itself and you're just trying to get a better output at inference time, that's your prompts, your knowledge bases, and your metadata. Or if you need to change the underlying model itself, that's where getting that labeled data or unlabeled data and doing some of the techniques here fine tuning, distillation alignment or continued pre-training or training your own model from scratch that's where that comes in more of the the model builder um uh kind of persona. If you're interested in learning more, here's a couple of sessions over the next 24 hours that might be interesting. And then my speaker Monica, my co-speaker Monica and I will be off on the side if you do have any questions. Thank you.
```
