---
video_id: tFiDkSG2ess
video_url: https://www.youtube.com/watch?v=tFiDkSG2ess
is_generated: False
is_translatable: True
---

OK, um, good afternoon, everyone. Um, so you must be, thank you for attending our session. Um, you'll be hearing a, I'm sure you'll be hearing a lot about agentic AI generative AI in this week of Reinvent, uh, but, um, and you'll hear a lot more in this session as well. All of this artificial intelligence, its promises to solve some of our biggest challenges, but its very development has actually led to one of our more critical environmental crises of the time. So what we aim to deliver in this session is sustainable and cost efficient generative AI with a agentic workflows and some practices that you can probably take away. Again, welcome everyone. I'm Isha Dua. I'm a senior solutions architect here at AWS and uh with me I have Parth. I'll let him do one quick intro. Hi everyone, um, I'm Parth Patel, senior solution architect at AWS focusing on machine learning and sustainability. Awesome. Thank you. So let's go right into it. Um, so what we're gonna be covering in the session today is we're gonna look at the rise of generative AI. Uh, we're gonna also talk about sustainability at AWS. Uh, we're going to look into the generative AI life cycle and how we can optimize the life cycle at each of those phases. Um, and then towards the latter half of the presentation we're gonna go into agentic AI systems and we're gonna talk about bedrock agent, core and multi-agent systems. So with that. So let's look at, you know, everybody knows AI models are getting bigger. Resource consumption is increasing. For over a decade or so, data centers had kept their energy consumption pretty stable. It was around 100 terawatt hour, and also we were maintaining a lot of, there were a lot of offsets that were put in place to manage that growing demand, but as of 2021, we saw that that number just sort of increased drastically. The model training size increased 350,000 times over the energy consumption. The demand for electricity increased quite a lot, and there are so many studies that are indicating this now. As of August 2025, there was this research that Goldman Sachs had done. And it forecasts that about 60% of this increasing electricity demand for all of these generative AI systems and what we have to build for our agentic AI systems, it's actually going to be met by burning fossil fuels, and that's likely to produce around 215 to 220 million tons of carbon dioxide. And to just give you some perspective, if you have like a gas-powered vehicle and you drive that around for 5000 miles, uh, that's about, you know, that's about a ton of carbon dioxide that you're releasing. And not just this Goldman Sachs, there's so many other researchers that you can go and read. One of them is like from the World Economic Forum, where it states that you know, the demand for this energy, the energy consumption is actually doubling every 100 days. So yeah, all of these numbers are staggering. They are very, they look very big. So as developers, as scientists, as hyper scalers, as AWS and other hyperscalers, it's sort of our responsibility to make sure that we're building innovations. We have the right interventions in place to mitigate this ballooning carbon footprint. Um, so we not only like efficiency of the hardware that we're using, efficiency of the algorithms that we put in place, but also how we design our data centers, how we implement cooling techniques in our data centers, and there are lots of other efficiencies that now need to be thought about much more carefully. So, um, let's talk about like sustainability a little bit and uh what it means at AWS um. In 2019, AWS took the climate pledge, and the climate pledge was that one of our tenets, one of our core tenets of the climate pledge was to power our operations with 100% renewable energy, and we met that goal as of 2023. Um, now one of the things that you can do when you're building your generative AI systems or your agentic systems or when you're consuming these services is if you are building them on AWS, you're automatically sort of a little more sustainable. AWS is about 4.1 times more, it's more energy efficient than on premises, and this is because of all of the efficiencies that we have built into. into our ecosystems, so we have, like I said, we are powering our operations with 100% renewable energy. We have hardware efficiencies that we have built into our services. We have optimized silicon that we're going to talk about in the presentation that is very optimized for model training model inference. We have the right. We have some cooling efficiencies that we are building into our data centers. The data centers have low carbon concrete. That's being used. So if you look at all of these efficiencies that we've sort of incorporated just by moving to the AWS cloud or just by building it on AWS, you have the ability to sort of lower your carbon footprint by up to 90%. So it's a huge carbon reduction opportunity. So that's something that I'd like you guys to think about when you think about generative AI like building building these systems at scale. Now, um, I want to pivot over to like uh what we call the generative AI life cycle. Um, when I talk about the life cycle, I'm talking about 4 primary stages, and these are the prime, like the high level primary stages that we're gonna talk about. So the first stage is problem framing. When I say problem framing, I mean like any idea, whatever idea you have, whatever use case you have that you want to build a generative AI system for, uh, whatever business outcome you want to achieve, um. It starts off as an idea, you know, a developer, an architect or a senior executive has thought about it. They want to build, they want to build this model or they want to consume a model, and they want to do that for achieving a certain business outcome. They have some data that they've collected for training of the model if they want to build one. So there's the problem framing stage where you sort of identify that this is a generative AI problem. Um, and then once you, uh, have identified the outcome and, uh, what you need, you're at that stage where you decide, do you want to train a model or do you want to use an existing model. So it's the model training or the adaptation phase. Um, once you have the model training or the adaptation figured out, that's when you go into model uh model. Inference and model deployment. So you have a model ready. You're deploying it for inference. You want to start generating prediction responses. That's when you are at the deployment inference stage. And the final stage, actually I won't call it the final stage, monitoring is something that we'd like you to think about across the life cycle. You want to build observability at each and every phase. What you want to do, but you also want to make sure that once the model is deployed, you're continuously monitoring it, you're continuously working on the improvement of that, you're looking for like data drift, you're looking for model drift, you're checking whether the output is actually relevant in the real world or not. So this is what when we talk about the life cycle, that's what we that's what we're going to cover the as we go ahead and we're going to talk about certain optimizations at each of these phases. So let's first go into the problem framing stage now. I think this is the stage where you ask yourself the question, the, so for this outcome, for this business outcome, and for this use case, do I even need generative AI? Do I need to build an agent, or do I just want to build an agent because it's the cool thing to do right now? Uh, so the first question is, like, um, is this a simple, like, you know, think about a business rules engine, like is this a simple if else rules engine, or is this something that you need to generate open-ended content for. Is this just like, is this, can I just get away with using maybe a less resource intensive traditional machine learning approach? What if it's a simple classification problem and we're overcomplicating it using a foundational model, right? So the first step here in the problem framing stage is ask yourself these questions. Now there is a scenario where you ask yourself these questions and you're like, yes, this is a generative AI problem. I have to use a foundational model. So now the first recommendation here would be that if you have to use a foundational model um and if you have to build something from scratch, use a managed service. why we say use a managed service is because when when we build managed services, you sort of we we help you operate more efficiently. Because we've shifted the responsibility of having to optimize the underlying hardware of maintaining the maintaining the underlying capacity or the high utilization and the sustainability optimization of that hardware to AWS, so we've taken away the undifferentiated heavy lifting from you. And you can just focus on actually using the service. So one of the services that we would recommend if you were to consume a model here would be Amazon Bedrock. It's a serverless, fully managed service. It provides access to a diverse range of models and also through a single API. There are about 200 plus models that are available on Bedrock. They are from different kinds of providers. We have Anthropic. We have we have Amazon. We have our own models. We have Cohere. We have Lama. There are image models from Stability. There's Meta and there's Mistral, and the list keeps going on and on and And it's not just the availability of models. There are lots of other capabilities and features that come with Bedrock being a managed service that you can utilize. For example, you can fine tune your models on Bedrock. You can use retrieval augmented generation on bedrock. There are some features that we'll talk about in the upcoming slides that are also very relevant to making easier choices, more sustainable and more cost efficient choices. Because of these capabilities that Bedrock offers, and not just that, it also has guardrails built into. You can also apply guardrails to your systems if you want to do, if you want to build in responsible AI tenants into your application, you want to make sure that your outputs are correct, they're fair, they're explainable. There's no potential harm or any kind of bias in the outputs, so that's something that you can do with Bedrock as well. Now, you know, you would, if you could ask me the question like if there are 200+ models, how do I select my model? Like what's the, what's the criteria for model selection? So that's a very important step as well. Now here as well, I'd like to point out that. The biggest, brightest model out there is not the best model for your use case. You don't, not just because it's a 200 billion parameter model, let's go and choose that for a use case. We may not need a 200 billion parameter model. They're more expensive. Their inference is going to cost you more. Their inference is going to generate more carbon footprint. So just be aware that. You need to select your model more appropriately, and when you want to select your model, you wanna ask yourself again some questions regarding your use case and the outcome that you're trying to achieve, um, like it could be any kind of question like. Do I need, I need the model to be open source? Do I need it to be proprietary? Do I need to fine tune the model? Do I need it to be, do I only need to serve English speaking customers, or do I also have some customers that need Japanese output? Do I need a multilingual model? Do I, how many parameters should it have? Is it a general model or is it a domain specific model? Am I building something that's more generic, or does the model need to provide more very focused healthcare related outputs? So ask yourself these questions before you think about the model selection process and remember that at inference time. A larger model is going to consume more resources, more memory. So again, the largest is not the best solution out there, and that's what I'm going to try and indicate here. Chat GPD 3.5, uh, it was 175 billion parameters, and there was a team at Stanford that actually trained like that transferred the knowledge over to like a smaller model, and they built Stanford Alpaca which was 77 billion parameters, so much, much lesser number of parameters than Chad GPT 3.5, and both of the models behaved qualitative. Similar. So again, the point what I'm trying to get across is that sometimes a smaller model can actually help you with your use case. It's going to cost you less. It's going to be more sustainable. So that's the choice that you should make again, even if I have to give you another example from the Nova models, we have 3 different types of those Nova models. They cost different. I know that the numbers look very small, but these costs can accumulate over millions of data points. So if you do not need multimodal capabilities, if you do not need very high levels of accuracy, then you can just maybe get away with a Nova Micro or a Nova Lite. You don't necessarily need a Nova Pro, and this sort of applies to other providers as well. Um, So we talked about bedrock features that can help. One of those features is bedrock evaluations. It's a very, it's, it's a very cool feature, sort of helps you assess and compare different models, um, and it can, you can choose how you want to do it. You can use LLM as a judge. When I say that, it can actually evaluate your output based on correctness, completeness, potential harm. You can also use traditional metrics like the B score or an F1 score, and there's a third way wherein you can use a human workforce. So you can use your own private workforce or you can use AWS provided workforce for evaluation of your outputs. So it's a Very fairly easy process. You define the task type. You give it your custom prompts. You set up your evaluation metrics and you assess which model which one is producing the best results. That's going to be the most sustainable choice based on how we've designed the service. So this is a critical feature. If you haven't explored it, I would recommend that look at look at model evaluations. It comes in handy. Now Now we're at that stage, we've crossed stage two. We, we had the problem framing done. We selected the right model. There could be two things that could happen here. One, we didn't find a right model, so we have to train one from scratch. Or we found the model, but we still need to customize it a little further to meet our exact use case, to meet the quality of the output that we're looking for. So it could be model training or it could be adaptation and customization, so. Uh, this is a very, I think this is a very important infographic, uh, that, um, I sort of like have, I've spoken about it in multiple places, but I would love that if you remember this, like when we think about model training and adaptation, this is the order, this is how I wanna think, this is how I wanna make sure that I think about my strategies like in a progressive manner. So if I choose, if I had a model, uh, if I selected a base model from Bedrock and I have to customize it. The simplest approach here would be try prompt engineering. That's the lowest here, which is PE. I couldn't fit prompt engineering in there, so it says PE, but try prompt engineering. Pass it custom, you give it your prompts, see what kind of output it's generating. It's the lowest effort, least cost, least carbon emissions option. If that solves your use case, if that's meeting your requirements, that's the best choice. But there are. where prompt engineering may not not not meet your use case, and you may have to do a little more enhancement. In that case, retrieval augmented generation comes into the picture wherein you can provide some proprietary information to the model and add a little more context so it's generating more tailored responses. So let's say you have like an automotive company and you're building some sort of Chatbot that's helping the customers so you can provide your car manual or your service manual as these proprietary documentation that the model can use for additional context. So retrieval augmented generation is the second approach which you can try. But then again, there are scenarios where prompt engineering didn't work, retrieval augmented generation didn't produce the kind of result you wanted. That's when we have to think about fine tuning. There could be parameter efficient fine tuning, or there could be full fine tuning. Again, this is the increasing order of emissions. This is the increasing order of cost. This is the increasing order of resource consumption. So parameter efficient fine tuning like Lora or prefix tuning, these are when you do not train all of the billions of parameters that are in the model. You only train a subset of them. So this is an efficient technique where you can fine tune and you can maybe see if it's actually meeting your requirements. If that doesn't work, you go to full fine tuning. But if none of these work, that's the scenario where we would recommend that you think about training from scratch. Training a model from scratch is a very resource intensive, very expensive, very time consuming process. Um, you need to look for, you need to have all of the hardware available, uh, it can take a lot of time. So this is something that I think you, you should only think about when none of these other techniques have worked out for you. And if we were to see this infographic, but we were to see it from a bedrock perspective, so very simple. Now you have some tasks at hand, and there could be two scenarios. That particular task does not need the latest data, or it does need some external data. So if it does not need the latest data and it's a very simple task that you're trying to achieve, then prompt engineering on Bedrock can actually solve that for you. But if it's a complex task, you know, it's like a little domain specific, you have to do some fine tuning here, so bedrock fine tuning can actually come in handy in that stage. Now if we move to the left side here, that we actually do need external data sources, and we do need some up to-date information. In this, we can have two scenarios one, It's relatively static information that you need. You need documents, so you've provided it to the Bedrock knowledge base and you're going to use retrieval augmented generation. So that's the scenario where you augment with rag. But there is a scenario where real-time information may be required. You may have to access. You may have to hit some databases, call some APIs. In that case, uh, we are to augment it with agents and tools, and that's where bedrock agents comes into handy. And this is what we'll talk about in the latter half of the presentation, which is the bedrock agents. So this is just again the same idea just from a bedrock perspective. Now, I just wanted to put this out there, but in case you have to train from scratch. There's no other option. You have to train from scratch. None of the customization techniques worked out for you. That's when we have two recommendations again. Use a manage service. Uh, there are multiple manage services here in, uh, that are available. We have the Sagemaker ecosystem for those of you who are more comfortable with Sagemaker, so you can train your models in Sagemaker. And then there's like EKS as well. So we also have Sagemaker Hyperpod. uh, so for those of you who are more familiar with, like who are coming from an HPC background and are more familiar with, let's say, slum-based orchestra. Sagemaker Hyperpod is something where you can train your models in and you can use SLM-based orchestrators, whereas EKS would be Kubernetes-based orchestrators. But most of the, but all of these are managed services. Again, the use of managed services is going to let you leverage all of the hardware efficiencies that our service teams have built into this managed service, so you'd be able to focus on the training and focus on the task at hand. And when you use these managed services, um, use the right silicon. So there are lots of EC2 instances. Uh, if you need GPU instances, there are instances available from, you know, the like the the P, there's the P family, there's a G family, there are other families of EC2 instances, uh, but there are also instances that we offer, uh, which is the ranium family. And they have been built in such a way that they are more energy efficient than comparable EC2 instances, so they will produce less emissions. Tanium 1, as you can see, was about 25% more energy efficient. Tanium 2 is 3 times more energy efficient than ranium 1, and we're coming out with tranium 3, which is going to be more energy efficient. So when you're thinking about training from scratch. Using a managed service and using the right silicon will help you a lot in terms of both sustainability and sometimes also in terms of price performance. So now, um, I think now we're at that stage where, you know, we're good we either trained the model from scratch or we selected a model, we customized it on based based on that those strategies that we talked about, and now we're ready to deploy that model for inference. Again, when you want to deploy it, you have to think about the silicon. You have to think about where you're deploying it. So again, we have the option of deploying it to EC2 instances. There are many. There's a family of EC2 instances that you can deploy it to, but we have inferentia from the Annapura family, which is again 50% better performance per watt, so you have the ability. To use the inferential family of instances to deploy your models, there may be scenarios where you may not need GPUs, for inference. There may be smaller models. There may be classifier models. In those cases, you can actually use graviton instances as well. They are also 60% more energy efficient. They were built with that in mind and they've improved performance about 4 times since their launch in 2018. So thinking about silicon, even at the inference stage is very important. And not just silicon. There are lots of techniques that you can implement even at the deployment and the inference stage. There are techniques that can help you reduce the model size. It can optimize the memory usage. You have the ability to compress the models. You have the ability to sort of make sure that you're distributing the models. You're actually building the models for distributed processing. You're using it for like the right kind of hardware efficiencies. Removing any kind of unnecessary weights, you're pruning the unnecessary weights, you're transferring knowledge to smaller models through distillation, or you're trying out different precision types for efficiency through quantization. So there are techniques even at the inference stage that you can probably use to sort of compress the model size further, make the memory usage more efficient again. It's going to help you with cost. It's going to help you with emissions. It's going to help you with resource consumption. There are libraries like Deep Speed and Hugging Face Accelerate, Faster Transformer that can actually help you do all these very easily, and these are also available, like it says they are in our language model, large model inference containers as well. So these techniques are something that you should definitely think about to sort of optimize the inference stage as well. And for these techniques, again, there are some capabilities that are specific to bedrock that can help. One of them is model distillation. It's an efficiency focused tool. What it's letting you do is it's um training. It's transferring knowledge over to a smaller student model from a larger teacher model. What you'll do is you'll select a larger teacher model. You'll provide it, you know that this teacher model works. This large model works for your use case. You provided some prompts. These are your custom prompts that you're providing the model. And this sort of fine tunes the smaller model based on the results that the teacher model is generating, and then the smaller distilled model, it can actually behaves very similar with almost 98% accuracy to the larger model, to the teacher model, and it saves you about 75% on the cost. So 75% cost reduction, 98% accuracy. So very good for scenarios where You know, um, you want like there's if you want to trade off between like model size and performance, and you want to make sure you save some money and you get, get the performance benefits. Excuse me. Another one of bedrock features I'd like to point out here, so there's prompt caching. So when we talk about prompts, we talk about two things. We talk about prompt optimization and we talk about prompt caching. Prompt optimization, obviously, as the name suggests, do not write large spaghetti prompts, very, very convoluted. The model has to process thousands of tokens because you've written a very convoluted, structured prompt. So make sure that it's optimized. Make sure you have prompt templates in your organization that you've formed for certain repeatable tasks. So that's the optimization bit, but there's also the caching bit which Bedrock lets you do. And what it's letting you do is sort of avoid recomputation of like repeatable patterns or matching prefixes. Again, it's like helping you save cost. It's helping you with latency, so you can reduce costs by up to 90%, latency by up to 85%. What you're doing is you're avoiding the computation of these repeated parts. Like, let's say you have a coding assistant model. And there's this piece of code, a snippet of code, that needs to run again and again and again and again. So that is the scenario where it would just pre-comput the results of that snippet and it would cache cache the results and it would use that in the subsequent subsequent prompts that you pass it. So prompt like coding assistance, sometimes there are system instructions that are repeated very often. So this particular duo of prompt caching and optimization. It sort of reduces your cost. It accelerates your time, like response time, because it helps with latency. So using bedrock prompt caching is also something that we would recommend. Um, and finally, I'd also like to talk about this another bedrock feature. It's called intelligent prompt routing. Um, it's a very smart cost optimization feature, sort of, uh, what it lets you do it is that it lets you select multiple models like you could choose. A Nova and a Cloud and a llama, and you could say that these are my routers. Now what I'm going to do is I'm going to provide a prompt and I'm going to let Bedrock intelligently decide which model it needs to route that particular request to. And Bedrock is going to do that by looking at the complexity of the prompt. Let's say this is a very fairly simple prompt. It does not need a lot of convoluted processing or complicated logic. It's going to maybe send that particular prompt or system instruction to maybe a smaller model, to a more cost efficient model. And if it's a bigger, if it's like a more complicated prompt, it's going to process that with a different model, maybe something with a bigger model that that it thinks is more capable of handling this better. So you can use intelligent prompt routing. You can select multiple routers in here. It can actually reduce your expenses roughly by about 30% as well, so it's a good cost optimization feature. Uh, it sort of saves you from that particular, you know, model selection thing as well. So if you're very confused which model works for my use case, maybe try this out for a bit, and each request is going to be traceable. So you're going to be able to see that which particular prompt went to which model, and you're gonna be able to see some metrics about that as well. So you're gonna be able to analyze the results later. Uh, so this, uh, yeah, like I said, sometimes it like eliminates the need for model selection. And like complex routing logic, you don't have to build it. Bedrock is intelligently going to just do that for you. All right, now we are at the final stage. We have, we framed our problem, we, uh, sort of trained or adapted or customized our model, uh, we deployed and we looked at some deployment and inference techniques of the model as, uh, for the model as well, and now we're at that final stage where we need to continuously monitor and optimize what we have running. What we've deployed, but we also need to remember I mentioned that monitoring and observability is something that we don't just want to look at the last. We don't want it to be the last thought or the afterthought. We want it to be something that you are thinking about across the entire life cycle, even when you're training the model. You want to be able to have these, you want to be able to use these tools, and there are multiple tools that are available. Cloudwatch, the Sagemaker Profiler that really helps with training jobs. Cloudwatch, obviously you can look at CPU and memory and other kinds of metrics. Sage Maker Profiler, you can look at specific training metrics as well. Sometimes you can look at data distributions, how the training is going, the loss, the accuracy. You can look at training specific metrics again. Neuron Monitor was a service that is focused on. Monitoring of neural network training, so like deep learning. So if you're using any, if you're doing that, then using neuron monitor is another good option. And of course if you're using the Nvidia family of instances for training. Or for inference, uh, you have the ability to use like the Nvidia systems management interface where you'd be able to see like GPU metrics and you'd be able to optimize the resource usage based on what numbers you're seeing, um, and maybe identify potential bottlenecks as well. So with that, um, we complete the life cycle, um, and I'm gonna pass it over to Parth and he can talk to you about the agentic AI systems now. I think. Thanks, Isam. So we learned about the life cycle of generative AI. Now we are hearing the whole year about like this is an era of agents and generative AI is not a generative AI is going to be used only with the agent. So does it mean that generative AI is over? The short answer is no. So both has a different application usage and both has a different perspective. So generative AI that you are using for quite a while now, like generating an image, generating an article, summarizing uh the content or translation, all those sort of things and uh generative AI fantastic for that use case. However, for agentic AI is taken one step further. It uses the same underneath large language model, but it is, it is used for a specific task. So it is a goal oriented where specifically used when the decision is subjective. It is not following a standard path. We believe that with agentic AI we are able to leverage the full potential of. Now, what are AI agents? Agents are essentially a type of AI that can act more like an agent, means it takes its own decision to achieve a goal. It is not following a specific predefined path or workflow. It takes a decision. It has the flexibility to take autonomous decision based on the data or context it encounters. So let's understand how an agent or agenttic system works. So we have all seen that for an agent, you need to have a specific goal. You can give the agent a certain goal based on the instructions. You should also provide some of the tools so that agents can utilize the tool to achieve certain goals. You provide all these details like what is the state that agent is being invoked, what is happening, what it needs to achieve, all those details as part of the context. Once you have those base information, you can kick in the agent. The agent will utilize a large language model underneath and generate a plan how to achieve the goal. While executing the goal, it will take certain actions. Mostly utilizing the tools. It analyze the output of that particular action or the tools and then reiterate the process until it achieves the goal. Once the goal is achieved, it will be either the output like text or generated image or maybe action done by the tools. So that's a base agent or any agentic system where you can have multiple agents chained together. Now to create an agent, you need a lot of things. You need to make sure that you provide a context. Agents are stateless, so you need to make sure that you provide certain tools. You go back and forth, and agents understand what action is taken before, what action needs to be taken after the first step. You also need to manage the orchestration. So for an example, if an agent decides a certain plan and based on the action it realizes that that plan is not working, so it needs to reiterate the plan. So you need to manage the orchestration for that. Most of the cases, a single agent could work, but we realize that organizations have much complicated use cases. So you will have multiple agents chained together or multiple agents talking to each other. So you need agent to agent communication as well. Lucky for us, we have a large open source community, they have so many of framework available. Amazon has um a strands SDK, which is specifically designed for production grade agent. Lang chain, Crew AI, those are, uh, one of some of the popular in the developer community that's uh that started alongside with generative AI search. We also have a, a framework from model providers like OpenAI or Google. So there are so many frameworks available, and this framework takes a lot of boilerplate code to achieve to make an agent, to create an agent. With utilizing this framework, you can create an agent in your development environment or as a POC. But scaling agent is really hard. Gartner mentioned that by 2027, 40% of agentic AI projects will be scrapped because citing certain restrictions like governance, security, and scalability. Underlying LLM doesn't have a state, so you need to make sure that you need to provide memory or you need to provide context for agent. Uh, you need to make sure that, uh, agent is also able to uh adhere to your organization's security and governance rules. You need to make sure that all those actions taken by the agent is auditable, so that you need to make sure that whether it is tracing is available, you need to make sure that all the responses are being logged and all the decision has been made for the reasoning. So all those things need to be tracked, and those things required a specific type of infrastructure. It doesn't require the same infrastructure that we are used to use for application. That's why we have a bedrock agent core. Bedrock agent Core comprise fully managed services that can use together, or it can use separately. So Bedrock Agent Core comes with around 5 services internally. Uh, it's a different feature and you don't have to use all services together. You can choose any services with your existing framework or existing agentic uh workflows. On top of it, you can use any framework that we talked about. You can use openAAI framework. You can use lang chain. You don't have to use a specific framework. Also, you can use any model. So you can use model available from Bedrock, but if you are already using model available with OpenAI or Gemini, you can still use those models even while using uh Bedrock Agent Core. So Bedrock Agent Core is specifically any framework, any model, uh, any protocol infrastructure that helps you build production grade agent, meaning you don't have to make a choice between the open source flexibility and AWS's reliability and security. Now let's understand how Bedrock Agent Core works. So as we discussed, for any agent to work, you need a base components like instruction, tools available with the framework, or providing the context. As part of Agent Core, we have an agent core runtime where you can use any framework or any model. Now, you can have a production grade agent just with agent core runtime. Uh, it's, it's a basic model, a basic agent that you can work. But as most of the organizations, you have a complexity where you need to call tools which is outside of the purview of this framework or use your existing APIs or existing applications. So for that we have tools that you can leverage alongside with agents. Agent Core Gateway, as the name suggests, is a gateway for your MCP tools, model context protocol tools. You can call any of your existing API or tools outside of your network or outside of your environment using Agent Core Gateway, and I'll talk about Agent Core Gateway in a bit. Agent could also have a browser and code interpreter. We see that some of the use cases where you're required to open a browser of the URL because the older application doesn't have an API, so you can have agent open a browser, use the information from the browser, or you can have an action taken like click a button and see the output, and agent can use those information. You can also have certain use cases where you need very precise output. You can have a code interpreter where it will actually open an environment, execute a code, and then return you the output. All these things required security, so for that we have agent core identity. Agent core identity will help you secure it, secure, um, secure your agent, so which persona is calling the agent or agent is working under which persona, as well as agent is calling the tool, so it will also uh leverage your existing IAM security or credentials to call the tools which is available on AWS or outside. So identity works both the sides, inbound and outbound authorizers. Along with agent, we realized that agent agent may work a long time. And we realized that your agent will run out of the context window because eventually it's a large language model and a lot of information and a lot of iteration fill up your context. So we have an agent core memory to help you reduce the context from your LLM context and put it into the memory so that as and when needed, you can pick the information from the previous step and continue the process. We also have a long term memory inside the agent core memory where you can save your preferences so that you can make your agent more inclined towards certain behavior. And all those things doesn't happen without traceability. So JI observability is very important more than ever, where we need to understand that why the agent takes certain decisions, what information was available, and why it takes different decisions at a different point in time. Everything will be logged so that you can trace it and make it auditable. Uh, we will talk about Agent Core runtime and Agent Core gateway, uh, a little bit deep to understand how it helps you save the cost and reduce your carbon footprint. So let's take a look with Agent Core runtime. As we see, agent core runtime, you can run a production grade model just with the agent core runtime. Uh, it supports uh open framework, any model, or uh any protocol. Alongside with it, there are a few differentiators. It provides two session isolation, which means that every agent will have its own environment to execute. It also supports 100 megabytes of a payload for multi-modal support. And alongside with it, it has very less 200 millisecond. Startup time, as well as it can run up to 8 hours. Now, let's understand the life cycle, how agent core runtime works. So whenever a client starts a request, uh by default, it's a 15 minutes of timeout, and if it is a streaming application, it will be 60 minutes. But whenever a session is initiated, you will have its own isolated environment. Suppose that your request is not responding or you are waiting for the response, uh, it will be suspended. The CPU cycle will be suspended. This is a key differentiator. Whenever you start an agent, you have certain variables in your code or you're in framework, and you have a state that agent will maintain. Most of the time, agent is waiting for response from LLMs, so your agent is calling a tool or calling LLMs to. Process certain data that uh it has been following. It is waiting for that. With a firecracker technology, we are able to reduce the CPU cycle while the agent is waiting. So you can still have your variables, your application state in the memory, but your CPU cycle is not being used, so you will not be able to charge and you are able to compute while agent is waiting. Let's say your agent session times out after 15 minutes, and when it times out, it will release the memory and compute whatever has been occupied. For use cases that you need a longer time, you can use agent up to 8 hours. And we would love to know that what are the use cases where you require a longer period, a longer time, and there are different ways where you can achieve it. When you see the cost, you will see that it's a consumption-based model as it is a managed service, but you don't have to choose how much CPU or memory is required. Agent code itself will identify based on the framework you use, based on your code, it will allocate and identify how much CPU or memory it is going to be used. As I mentioned, when agent is waiting for LLM response, which is most of the time, you will not be charged. It will be charged only when the active IO is happening. Memory will be charged throughout because it needs to manage your variables throughout the process. Now let's take the example of a scenario. So we have two scenarios. One is compute light and compute heavy. Compute light is very standard request. So there are two components. One is foundational model and agent code runtime. Any foundational model over here is cloud Sonnet 4. The charges will be millions per million tokens, whatever standard charge for influencing. Agent Core will have a VCPU and the GB memory per hour charges. Now, if the agent works for 60 seconds and your input token is 200 and output is 600, very standard request, you will, uh, usually you will lose one CPU and 2 GB of memory. You can see that out of 60 seconds, only 20 seconds CPU is being used, so you will be charged only 20 seconds. You will not charge 40 seconds extra that CPU is waiting. That way, uh, on the right-hand side, you can see that 9 95% of cost is just a model and only 5% cost for agent core. So this way we are able to reduce the CPU cycle a lot. If you see the compute heavy scenario where the agent is running for one hour. We have 30 calls, 60,000 input, and 18,000 output tokens. We are using 4 VCPU and 8 GB of memory. Overall, for 1 hour, 60 minutes, we are able to save almost 50% of time for your CPU cycles. Now in the future, when you see this scenario where you have hundreds and thousands of agents running multiple processes together for your all the employees or for multiple processes, you will see a huge amount of cost saving as well as your compute saving. That will help you to reduce your carbon footprint as well as help you to scale your from the cost perspective. Now, let's take a look how Agent Core Gateway helps. So agent core gateway is a unified way for agent to access MCP tools. Uh, for an example, if you create an MCP server, first of all, you need to have a server always available. You need to manage all your, um, permissions, uh, networking compute for your server. And alongside with it, you need to make sure that tools available for your MCP server is discoverable. Now, um, Agent Core will take all the heavy lifting alongside with identity and observability. So Agent Core Gateway is a unified way to, it's a fully managed service for unified way to agent to access the MCP tools. It also helps you convert your existing APIs, whether it's an Open API or Smithy tools, uh, to make it as a MCP tools. You can also have your existing lambdas without any code, make it as available as MCP tools. Now Gateway also have a semantic search feature, so it's not only um utilizing these tools available, but you can use a semantic search. And one more thing, um, Agent Core gateway doesn't require agent core runtime. You can use Agent Core gateway with your VS code, with your Kiro, with your existing, let's say, MCP inspector or any other application as well. Now, uh, most of the time, uh, most critical expected developer misses out that in production, you have hundreds of tools. There are two approaches. You either you have multiple MCP servers with a lesser amount of tools, or you have a single MCP server with multiple tools. In this example, let's say an organization will have 300 tools. If you understand that each LLM call for any tools call or call, you need to provide a definition for your tools so that LLM can understand which tool required to perform certain actions. In this case, you are providing 300 definitions every request, which is taking a lot of your context windows. What Agent Core Gateway does is provide a semantic search. So what it does is it tries to understand and filter out which tool required to perform this action. So instead of passing 300 definitions, every request. Gateway can be able to pass only 4 requests in this case that will be able to reduce 90% of the context window taken by your MCPs. This will help multiple ways. It will be able to make your normal agent or alarm call much faster because it reduces the context. It will be able to improve the accuracy because you don't have additional information that you do not need to process. It will definitely increase the speed and reduce the cost. And as we understand that if you reduce the context window, you are able to save a lot of carbon carbon footprint. So with a semantic search, you are able to also Leverage that now you have an additional context window, you can use more information to make improvise your agent and get more accuracy as well. Um, if you see the cost side of it, um, it has two types of costs. One is the API call and the search call, like tool call and the search call. Uh, it also has a tools indexing, which is, uh, uh, charged per month, uh, only for indexing the tools so that semantic search can work. Now, let's take a practical example where we have HR assistants. This is the time of the year where new enrollment happens in the US. So you have a new benefits application available. Um, you have, you want to check your payroll, you want to check your benefits or, or, you know, new balance, that kind of application where you have an HR agent. Now, for a mid-sized organization, um, let's consider that there is roughly around 50 million requests per month. For all those requests, we consider that one search API and 4 tools because of the semantic search, we are able to reduce the tool. So each request will have 1 search API and 4 tool calls. So overall for 50 million requests, we have 50 million search and 200 million invoke tool calls. All of this is under $2500. So this uh way, we are able to help you with the managed services without maintaining any infrastructure. You are able to leverage um your MCP as well as your agent core runtime. Now, if we see what we, what we uh learned so far, um, how the agent or agenttic system works. For production grade agent, we have Agent Corp, Bedrock Agent Corp, where you can choose any framework, any model, any protocol. Agent core runtime separates compute site compute from memory to help you reduce compute for uh compute and cost for your uh agent agenttic application. And agent core gateway helps you unify your uh MCP tools uh and also using semantic search to reduce your um context window. And on the first section, Isha explained about how utilizing the managed services helps, how we can, we should use the base model based on your use cases, use the right silicon or right inference optimization technique, and how we can continuously improvise our generative AI life cycle. With that, we have a couple of resources and we are available for any questions that you have, and thank you so much for your time today.