---
video_id: V2wFP-qzahY
video_url: https://www.youtube.com/watch?v=V2wFP-qzahY
is_generated: False
is_translatable: True
---

Hi everyone. Welcome to A&T 216 where we are gonna cover all things new with Amazon Sage Maker. Before I get started, I'm gonna take two quick questions. Uh, I wanna raise your hands. How many people here are managing or processing large amounts of data for analytics? Can I see a show of hands? OK, about half of you, something on that order. OK. Second question, how many people here are trying to prepare and leverage data for AI initiatives? OK, more of you, more of you. Good. And that's a pretty good overlap. Well, the good news is if you do one or both of these things, this is the right session to be in. Today we're gonna talk about how the next generation of Amazon Sage Maker brings together widely adopted machine learning and analytical capabilities into a single integrated experience with unified access to all your data. My name is Sean Ma. I'm a principal product manager with the Amazon Sage Maker Unified Studio. Hi, and I'm Iris Xu, a senior technical product manager with Amazon Sage Maker Catalog. OK, so give you a quick overview of what's happening. If you were here last reinvent, you may remember that we launched the next generation of Sage Maker at Reinvent 2024. So the goal for today's session is to bring you up to speed on everything that since our launch all the way up until this moment in time. So we're gonna start off with an overview of the next generation Sagemaker, just a level set there, followed by launches during the year, and then we'll move into the most recent launches that just came out over the last few days and weeks. OK, so, let's set the stage. Why are we here? What, what, what is the purpose of the next generation sage maker, right? Why did we solve it? Why did we introduce this next gen? Well, it's because the data landscape is changing and more and more people are investing in AI as part of their core enterprise strategy. The problem is, Based, uh, according to Boston Consulting Group, over 74% of companies surveyed aren't actually set up to be successful with this initiative. That's a huge problem, a very, you know, concerning, uh, concerning statistic. So why is this the case? Well, after many discussions with our customers. We learned that the challenges primarily fall into 3 big buckets. First, they told us that the rapidly evolving nature of the data and AI landscape means that there's a lot of new and complex tool sets that customers have to manually stitch together. This impedes the progress and increases costs. Instead of becoming more efficient and agile, it's actually slower and more expensive. Second, they said, They have a challenge with data sprawl. Well, this is actually not a new problem. We've always had a challenge with data sprawl, right? When you're dealing with complex environments, whether data on premise, in the cloud, SAS system, databases, data warehouses, data lakes, that's always been a challenge. But now we're also seeing increased demand for unstructured data, so it's just multiplied greater. This data sprawl naturally results in more data. Silos, which then leads to a fragmented governance experience. This is challenging because now you got to make sure you have access to the data, but also the right data and how do you ensure that's the case? All bottom line is all this means is that customers are having difficulty utilizing their main competitive differentiator, their data to be effective, to drive the business forwards with AI initiatives. So, how can Amazon help? Well, as you all know, AWS has an unmatched experience for reliability, security, and performance, with many services enabling more than 2 million customers innovating with data and AI. And it's in our DNA to listen to you, our customers, on how we can make our better, uh, product better. So customers such as yourselves have been pushing us to go beyond just more, uh, uh, our breadth of capabilities, more services and more capabilities inside of them, but really invest in how these services work better together. So to do this, we focused on how the nature of work is actually changing for our users and what we could do better to improve that. What we saw was there was an urgent need to integrate data, analytics and AI at scale, which allows various teams to work together across an end to end workflow, whether it be SQL, queries, ETL jobs, building models, generating applications, all those need to come together in a for multiple services in an integrated development, plus combine that with centralized data governance and data management. Right. Therefore, our challenge was to select the right set of services that will work, work well together in a unified experience. And that's why we took Amazon Sagemaker, last year we took Amazon Sage Maker, which was already used by hundreds of thousands of customers to build, train, and deploy machine learning models. And we introduced the next generation of Amazon Sage Sage Maker by expanding what Sage Maker had to add data and analytics capabilities into that. This next generation includes virtually all the components you need for fast SQL analytics, big data processing, search, data preparation, AI model development, and generative AI all on top of a single view of your data in a manage and govern manner. This new offering consists of 3 layers. On the top you'll see a single development experience with multiple tool sets called the SageMaker Unified Studio. In the middle is the SageMaker catalog, which is built into the unified studio, so you get end to end governance for your data and AI workflows. And finally, the foundation is built on using a lake house architecture that enables access to all your data through an open Apache Iceberg uh standard interface. I'm going to break these things down a little bit further so you can understand what each part means. Starting at the top, the unified studio, which removes the pain associated with separate development experiences. So as you can see here, it's integrated for individual use case specific tools, right? Bringing that all together in one end to end complete uh experience. Not only are these different interfaces available, but it also integrates with the underlying AWS data processing and compute engines such as the original SageMaker, now called SageMaker AI. Other services such as EMR Glue Athena, Bedrock, and, um, and, uh, Redshift, all this is done to bring it together into one seamless experience, but we're not there done, we're not done there. You also see that we can, we will continue to expand what we support inside the unified studio, including streaming, business intelligence and search analytics. This is something that we continue to invest in and continue to build out on. With this unified studio itself, teams can collaborate without the burden of switching context. You log in once, it's one it's one set up, and then you can go between these different interfaces. Your data is all accessible in one place and can switch between them without having to lose your train of thought. In addition to this, to cater to a wide range of users, we had said that we want to support a wide range. We have multiple different interfaces for customers to use. So whether you prefer to write code, Python code, we have a code editor. If you prefer que uh if you prefer to query with SQL, we have a uh a query editor, uh, using SQL visual editors for people who prefer the drag and drop, uh, boxes and line interface, that's all there. And on top of this, as expected, we also support natural language through our generative AI assistant, so you can just ask questions and interact with the services. This is a unified studio. I'm going to move on to the next component, how to access data in a secure manner. I'm going to put it a different way. It's nice that we brought all these tools together, but really it's not enough if you can't get access to your data while ensuring trust and security, right? What's the point? So the challenge here at this layer is that customers are trying to find the right balance. How do you democratize data access for your end users, give them the tool, give them the data sets that they need to complete their job while still maintaining compliance with your uh regulatory obligations like GDPR and HIPAA, right? Getting the right data at the right time to the users. So to help customers move faster with the data, We built the Sagemaker catalog, which is built into the unified studio, so you have a single place for a data and AI governance with all the data access controls in the studio itself. Using this catalog, you'll be able to securely discover and access approved data, models, compute and generative AI assets through a published subscribe workflow, or you can assign individual assets without having to go through IT. It's all done through, again, that's one interface. You get semantic search across all the assets in the catalog, which means easy collaboration and comprehensive view of what the data has been accessed, the data quality, sensitivity, and end to end lineage. Finally, on the data processing, on the, sorry, on the data preparation process, we have built-in data quality, sensitive data detection and lineage that makes sure that it's constantly tracking from end to end, really streamlining that end process. This is that core data governance capability that now the StageMaker catalog data catalog has StageMaker catalog has, but on top of this, we also want to address some of the unique challenges that arise because of generative AI in this world that we're dealing with. So we've also added the ability to support Amazon bedrock guardrails. OK. With this, you can implement responsible AI filters in natural language, which includes blocking sensitive information, reducing hallucinations, and these guard rails work with any models, not just the one from Amazon Bedrock, but anyone that may be available through a third party. OK, Now, the last part I wanna talk about is the open data lake house, really the foundation of where the data is being stored. Often customers use workload specific storage in their data warehouse or in their data lakes. However, this means that the data is split, naturally, it's split into sometimes in the data warehouse or sometimes in the data lake. This is only a problem because each tool and each engine tends to access the data differently based on that format, right? The problem with this is that it can lead to inconsistent data access, or worse, it can lead into performance concerns later down the road that are hard to debug. So to address this, the SageMaker platform is built upon an open data lake house architecture, which is fully compatible with Apache Iceberg. What this means is rather than you having to worry about setting up access to each of the different kinds of storage, this unified data access using Apache Iceberg goes across S3 data lakes. Um, iceberg tables through in S3 tables as well as redshift manage storage, OK. Plus, it allows Iceberg compatible engines such as EMR, Redshift, Trino Spark to query and access the data in a consistent manner. So that's how we ensure that there's a consistency in terms of what you're accessing. Now, what about the data itself? Well, you also have access to bring in operational and enterprise applications using over 150 ETL integrations which have without having you to build or manage ETL pipelines. And for data that's sitting outside, you can use the federated query connectors as well as AWS Glue connectors to tap into on-premise systems or third-party applications, so that you can query all your data in one place, all available through the lakehouse itself. OK. So this is that summary of everything we announced at Reinvent last year and we eventually made it available generally available in March. As with any new product launch, we didn't stop investing on this platform once we launched it. In fact, since then we've made dozens of enhancements and expanded the availability, so this is now available in over 15 different regions. OK. We've also had the privilege of having a variety of customers across different industries, whether it's healthcare, tech, automobile, financial services, use SageMaker to address their enterprise data and AI needs. It's been really humbling and exciting to hear how these customers have been using the next generation of SageMaker and really benefiting from this unified approach. So we're going to spend the rest of this talk talking about how we've continued to improve it and some of the key launches through the year and with that, I'll hand it off to Iris. Awesome thanks a lot, Shawn. So now that Shawn has established what StageMaker Unified Studio is, let's talk about a bit more about our launches. So what our team has been busy building. Before I really dive into the launches though, I just want to kind of share the framework of how we think about how we build in our product roadmap, and that's really with uh our data team. So you're our customers and data teams in mind. So whether you're a data engineer maintaining ETail process. And pipelines, a data scientist building ML models, data analysts, building dashboards and analyzing data, or a data steward responsible for data governance and access control and security. Our vision is that Sagemaker is the single tool that you need. So with that context, let me walk through 4 key launches that represent how we're addressing each of the users that we just talked about in the previous slide. Fortunately we don't have the time to talk about, uh, over the, uh, talk about each of the over 40 launches we've had this year. But we'll, I'm focused on these today because I think they do a good job to represent all the different capabilities, each, each layer of the stack of SageMaker. So we'll start by discussing visual workflows and job monitoring for data engineers. Uh, and then move to how we onboard unstructured data stored in Amazon S3 and the SageMaker catalog. Then we'll talk about our Quicksight integration in SageMaker and finally talk about S3 table governance with tag-based access control. So let's start by talking about our data engineers who are responsible for maintaining complex pipelines that involve multiple steps and ETL processes, often using tools like Apache Airflow. Today, thousands of our customers are using glue jobs as one way to automate these ETL processes, and we've brought the visual ETL interface into SageMaker that our customers love. We've also expanded upon this with visual workflows that allow you to automate Glue job, Glue jobs alongside other tasks. And so our vision, our goal with this launch was really to expand the accessibility of who can create these automated workflows and pipelines without needing to know the nuances of Python DA code or Apache airflow, and we think that. This is something more advanced data engineers will also appreciate because you can easily visualize all the steps by uploading uh already created files you have and since this is such a visual feature, let me go through a quick demo of how this works. So I'm starting here in the StageMaker portal and I'm gonna go ahead and create a new workflow as you see here on the left hand side, I have a bunch of tasks that I can choose from. So when we launched in March this year we just supported 3 tasks, but we've now since expanded to over 100. Uh, these tasks range in categories from analytics to various machine learning tasks, so they're bedrock operators, and you can also use the quick filters on the top for specific services. So it's pretty easy to create a workflow. What we're gonna do, uh, for the purpose of this demo today is start with a simple example of a three-step workflow where we're going to, uh, take a glue job I've already created. Um, it processes sales data per store, and then I'll run a notebook that takes the after the glue job has completed, it'll run a sales forecasting model and then sends uh S3 result at the end. So as you can see here, I'm simply drag and drop, dropping the, the glue task here and I can search for the, the glue job that I've already. Created in my list, but if I wanted to, you also have the option to actually create a Glue job from scratch within this interface as well. So now that I've selected that, I'm gonna go ahead and, uh, select the notebook. So it's pretty simple here. I will go and search for that and select so it adds the next node here that will start only after the previous task has completed. In the interest of time we're gonna fast forward. I've completed each, uh, I've added all the additional nodes as well, updated the parameters, and as I've been updating what's been happening is that the code is getting is automatically updated as well, so I could download this and use it for infrastructure as code in pipelines if I wanted to. And uh if I have previous code that I've already saved, I could also upload it here so you can visualize it, make changes to it, and, and save those results as well. But once I'm happy with this workflow, I'll go ahead, save it, name it, add any descriptions I want to, and then click run. So we're gonna make sure that it's running so you can see that the workflow has started, so I'm gonna click view runs here on the right hand side. So when I click on view runs I get to all the historical runs that have happened so I had a previous run failed, but I'm gonna click on the one that's currently running and then I can see all the different tasks. So my glue job has completed and it's now currently uh running the notebook and. Some weird technical challenges, but the. OK, but the notebook is, um, so as the notebook was running, I get the logs for it and then I could troubleshoot any errors if I wanted to, uh, from there as well. All right, so next I'd like to discuss how we made it easier to discover and govern unstructured data. So in this age of generative AI I'm sure we all know how important it is to have a data foundation that's built on all types of data and not just structured tables, but also your uh unstructured photos, images, videos, etc. as well. However, with our customers have shared that working with all these different data types can be challenging because data is separated across different environments and they often need to use different tools to work with different types of data. And so that's really what we're we're trying to solve, uh, with this launch. So we've brought us 3 buckets into the SageMaker catalog to in order to support unstructured data. Uh, and this allows unstructured data to be a managed asset in the Sage maker catalog. Data producers can then enrich that, uh, the unstructured data with business, uh, metadata and semantic meaning so that data consumers can, such as your data scientists and developers can search for the right data they need for their machine learning or generative AI model development. And uh after they can also write results back to S3 buckets if they have the right permissions, which are controlled by S3 access grants which allows access through a user's corporate identity. And so this allows unstructured data to be discovered, shared in the SageMaker catalog just as you would with unstructured uh with structured data and work with it within the same SageMaker uh interface for, uh, for querying and processing data. And so with launches like this, what we're really trying to do is create that single unified view for your data assets in the Sagemaker catalog. Now, let's talk about how we help data analy analysts build uh dashboards to answer their business questions and derive business insights. One challenge we've heard in creating the right dashboards is that first it can be difficult to to find the data they need to answer their questions. For example, a marketing team might want an analysis built off of sales data, but need, uh, but needs to request access that could take days. And once they get the access, it can be difficult to understand what the columns are and the the schema and everything. On the other side of this problem is too many ungoverned dashboards. So, um, this problem I personally faced as well before is understanding what dashboard is, uh, you know, there could already be a dashboard that exists that answers the questions that I'm looking to, um, looking for, but I, I don't know where to find it or which one is, is the right one. And so that's what we aim to address by bringing Quicksight into, uh, Sagemaker. As Sean mentioned earlier, we are looking to bring in uh BI capabilities in SageMaker, and that's coming soon. But uh in the meantime, we took this first step to launch Quicksight from a data set in SageMaker. And how this works is that you can click the option to visualize a data set that you have access to in a StageMaker project, which opens up quick site and behind the scenes, what we do is create a quick folder with the same name as your project and also uh provisions the same role and uh access credentials so you have the same access to that data as well. And so that kind of takes away the steps of setting up the right permissions, um. And once you create any dashboards, those can be saved back to the pro uh project itself where you can again add business metadata, add some context that makes the data more easily searchable by others so you can publish it to the Sagemaker catalog and allow others to. Discover and request access to the uh dashboards as well. So in this way we're trying to make it easier to find the right data you need, create dashboards off of it, and share, share those dashboards with others so that you can more quickly get to those business insights. And finally, I'm also gonna talk about how we've been expanding support in the Sagemaker Lake House. So, uh, so we previously supported tag-based access control for resources in the Glue data to catalog, and we've since expanded that across S3 tables, Redshift data warehouses, and federated catalogs as well, such as those such as SQL Server and Postgress SQL. And what this allows data administrators and data stewards to do is define permissions based uh via tag-based grants that are automatically inherited by resources so that you no longer have to specify permissions at the resource level. Uh, and you can also create multiple lake formation tag expressions to grant combinations of tags to multiple users. So I'm gonna walk through a little bit more detail of how this works. So to start, you'll define like formation tags, uh, and you can assign these, uh, and you can define these based on your team. So, uh, as an example here we have tag of a key of team and values of developer SRE and QA and you can also control who can actually create these tags and assign them. Next, what you'll do is attach these tags to to the resources so you can assign them to databases, tables, columns, as well as S3 tables and federated catalogs. As you can see here, our sales database isn't tagged, so none of these teams will have access to it. However, this on-call database is tagged to allow developers to have access to all the tables. One of these tables allows read-only access to our QA engineers. It's a pre-production data uh table, and SREs have access to one of the columns. So you can get pretty granular with with the um access controls. And so once those tags are assigned to the resources, you can create uh policies for IM principles such as your users and roles and scope those permissions based on tags so that as you uh scale your resources. Let's say add or remove delete uh resources, you don't have to worry about updating policies each time as long as you're tagging the resources appropriately so that really allows you to scale across hundreds of thousands of resources. And this really works in parallel with attribute-based access control. And so, um, in tandem with attribute access control, attribute-based access control, you can define a tag ontology that matches your users attributes, let's say your departments and um and what level of data sensitivity that they can get access to. And you can tag the the IM principles again and when the tag values match resource, uh, the tag values match the same tags as resources access can be granted or denied and, um, actions on certain resources can be also be controlled with these tags. So these are the options. These are just show how we're providing more flexibility. In the Sagemaker Lake House to really scale access permissions across multiple users and uh this is also helpful because as users are, you know, change departments or uh leave the company or join the company, uh you can now assign tags uh to users to control their access rather than having to make policy updates each and every time. So with that I'll now pass it back to Shawn to talk about some of our more recent uh SageMaker launches. Thank you Iris. I hope all of you folks are paying attention. There's probably gonna be a quiz at the end, so we'll just remember that, um, so Iris has brought you up to speed up until literally, uh, this event, uh, talking about all the launches so far. What I'm gonna cover are things that have come out in the last few days with new, uh, the SageMaker, uh, platform. So, while there are many things to talk about here. I'm gonna be focusing on 4 key improvements. The first one is gonna be getting faster, getting started faster with our new one-click onboarding, OK? Following that on the far on the far right is our new serverless notebook experience, which is an all in one notebook for developers to interact with their data. Below that is a purpose-built built a purpose-built data agent specifically for Sagemaker notebooks. And last but not least, we're always investing in in uh for performance at scale through our new Amazon Athena for Apache Spark engine. With that, let's begin. And of course, I'm gonna start off with getting started, right? So, getting started with StageMaker Unified Studio has never been easier than with our new one-click onboarding. Our goal is to get customers into the product and working with their data in minutes. So you may ask, how did you do that? What what's the trick here? Well, we focus on the one thing that all AWBS customers have in common. Can anyone guess? It's AWSIM policies, roles, and permissions. If you're on AWS, you're definitely using it and probably built your data around accessing data through uh AWS IAM. So what we did was we added the ability to reuse your uh existing IAM role and we take this role and we automate the rest of the setup and configuration for the SageMaker Unified studio. Which means by using that role, the unified studio now has the same access to data and resources that you had in that IAM role that you gave it, OK, so it's seamless in that way. If it worked, if it worked outside, then it'll definitely work because it's using the same IM role. This includes S3 buckets, S3 tables, glue data catalog, and lake formation policies. All of these are accessible through the unified studio using the IAM role provided to it. Now, with that. Let me show you what I mean by this. At the top, you'll see the Sage Maker, Amazon Sage Maker console landing page where you'll see a new getting started. This is our new default getting started option, that orange button there. And if they click it, you're actually gonna see one setup page. And while it's one full page, you really have one primary decision. Choose your IM role. This is how you determine how what permissions and privileges the unified studio has access to. You have two options. One is to auto. Create a new uh role if you have the right permissions to create a role, then we'll create one for you and we'll give you default permissions to that account. Alternatively, you can use an existing role, something that you've already got working, something that's already been approved by your IT, and you can use that, um, to set up your SageMaker environment. This will set up a new Sage maker. This will set up a new SageMaker environment and it's appropriately called an IM based domain because you're using your IAM role to set it up and log in with it. After a few minutes You'll be taken into the product where you'll see our new updated visual design. So when we said we were improving on boarding, we didn't stop with just the setup process. We also want to streamline how you access the various tools and interfaces available in the product itself. On the left hand side you'll see a navigation available with all the various tools you have access to, whether it's a general capabilities, notebook workflows, data analytics, um, AI dedicated AIML tools, as well as code developers which I mentioned earlier. All these are available now easily accessible through this left hand nav. In the middle we've uh we've had calls for common actions that customers take, including discovering ML models and exploring your data. And for those users who are new to SageMaker and wanna kind of get started to understand what it can do, we've actually got sample data with pre-built notebooks already ready to go, so you can just click and try it out from there. OK. The important thing to take away from all this is that using that one click experience and I using an IM role, you didn't have to do an additional setup to get access to your data, which means you can start working with it faster. What do I mean by that? This is exploring the data inside of this without any additional setup. You can go in, see what S3 buckets are available. This is, for example, what's available based on the IM roll that I chose. You can see what's inside your Glue data catalog because that'll be all available. You have S3 tables and things of that sort. There's so many more that you can explore as a result of having access to this data. OK, so now that, um, now, now that we've had this core improvement to StageMaker, uh, you know, uh SageMaker Unified Studio, we said, hey, maybe others can benefit from this too. So as a bonus, we recognize that there were several other customers already using standalone AWS services that could benefit from the same type of simple onboarding, right? Especially those using individual consoles like Redshift, Athena, or S3 tables. If you're using one of those ones there, then it's an opportunity where you can launch the same um same one click onboarding from any of these consoles. Any of these consoles, as you can see here, this is the Athena console, and when you launch it, it will also take you through the same one click on boarding where you choose your IM role. The benefit here is that if you use the same IM role that the console is using to access the data, so in this case, Athena is using an IM role to query data, you can use that same IM role to set up SageMaker Unified Studio, and guess what? StageMaker Studio can continue with your process. It can also make the same queries because it has access to the exact same data, OK. All this is available again on Amazon Athena, Redshift, and Amazon S3 tables. OK, so we made it hopefully easier and faster for you to get it on board, whether it's, uh, whether it's as an admin or as an end user. Then let's talk about how we've also improved the developer experience within the product itself. So we looked at making how we, we took the learnings from our existing interfaces. You saw we had a whole range of interfaces available and we identified three key areas that will really benefit customers, in particular, benefit those users who are new to this space, are new trying to learn data and analytics, uh, data, AI and analytics, and how that combines together. One area of emphasis that we heard consistently was uh uh interactivity and visualization. Today customers typically have to load additional libraries or in some cases for sim uh additional libraries for simple visualizations or in other cases actually have to go to another tool to to better understand and visualize their data. This takes time and effort and often is error prone, right? The next part of this is um supporting multiple languages. Now supporting things like Python and SQL, that's not particularly new, so that's something that people expect. The challenge is actually as you're starting to as you're interacting with the different languages using the language that's most uh uh most effective for your tasks, oftentimes you have a problem where you're trying to bring the data from one from one from SQL into Python and vice versa and transitioning that data is actually more difficult than you think. So the challenge is not supporting multiple languages but actually transitioning the data between the languages so it can be processed and used by them. The last thing I want to call out is um uh how cust how oftentimes users get started with their um development experience. Many customers start off with a local environment where they've set up themselves, set up the data access, and start processing inside of that. Then when they want to deal with larger and larger data sets, they have to connect it to a new engine and set up the infrastructure, provision it, everything upfront. So once again, we looked at this as an area of area we could improve and make easier for our customers. Which is why We're happy to introduce a new notebook for Amazon Sage Maker. This is a fully serverless web-based workspace designed for users to act on their data using an immersive interactive experience. For the end user, it supports Python and SQL natively. That's out of the box, plus it has a built-in AI agent so you can use natural language to develop your code. That's all there. But the main thing to call us because it's serverless means you don't have to pre-provision it. You don't have to worry about managing any of it or tuning it, OK? It automatically starts as soon as you launch it. It automatically starts and scales to the environment uh for based on the scales the environment based on the task that you need. So nothing that you have to worry about from there. And since um since it's part of the uh I uh I am based, uh, same IM based domains, it has access to the same data assets that are available as well as it has built in visualization libraries so you don't have to in this case you don't have to worry about importing additional um uh libraries. OK, So What do I mean by that? Here's a classic example of the notebook of the new notebook experience. As you can see, it uses a popular layout on the left hand side here, you'll see a data explorer where you can access, you can view all the different data set, uh, data sets that you have. On the center there you'll see the popular cell-based interface where you can choose to write SQL, that's the one on top, or Python on the bottom, all within the same notebook. OK? In this example we're using SQL to query and get some results inside of that. And then at the bottom you're using the uh uh then you're gonna use that data that you queried in SQL to process uh with Python. Now I mentioned that one of the challenges between these two is not just having SQL and Python support but actually making sure the data gets passed properly for it. So what you see here on the arrow is that the notebook will automatically store the data set that you queried, make it available in a variable so that you can then reference it without having to do any additional steps through the rest of the cells. So this is something that's done automatically. In this way, it makes it so very easy to switch between Python and SQL, whichever language is best suited for the task, and under the covers of all this thing is the new Amazon Athena for Apache Spark engine, which is scaling out to support all those types of data as well as process process the code here. OK, well, I did mention there's gonna be an AI agent for this notebook. So let me, let me talk about that next. But before I do, I just wanna say at this point, I think everyone's expecting a chat interface for an agent. That's pretty much uh that's pretty much a standard set. We didn't want to do just that. We said we don't, we want to go beyond just a standard chat interface and really have a purpose-built agent, data agent that is context aware. So we're introducing Amazon Sage Maker data agent available in that new notebook. This means this agent uses model context protocols MCP to explore the metadata stored inside your data catalog and understand what your assets, uh, understand what type of assets you have and use that to help generate the code. So when it is generating SQL and Python code, it can be very specific because it's actually using the metadata inside your data catalog and, and it can find out what uh columns, what attributes you're focusing on, what are the information associated with it. In addition to this, we expanded its ability instead of just generating individual cells worth of worth of uh code, it actually can break down sophisticated end to end workflows into individual manageable steps so it can break down one whole workflow into multiple steps that you can inspect and review individually. Ultimately, our goal with all this is to really make the agent much more aware of the data and of the task that's completing, so they can give you targeted guidance on how to proceed in your end to end solution. OK, so the best way for me to show you this is not gonna be through more slides, but through a demonstration, which I'm gonna switch to right now. OK. So what we're gonna do is we're gonna start off from the same, this is the Amazon Sage Maker console page. We're gonna start off from this point here and we're gonna start from scratch and build a new environment. Now obviously this demo's focused on new things, but if you have an existing SageMaker domain, you can also access it from here as well. But let's click on the get started. Once you click on get started, again, this is that single page for configuration. And the main thing again to focus on is what IM role do you want to use to set this thing all up, OK? By default it'll create a new one if you have the permissions, and we'll use that. We'll I'll show you the default set up in this example here, but once you've done that, that's really the main decision you have to make. The rest of the choice are optional, uh, things that you can proceed with. You click set up, that's it. Under the covers, StageMaker Unified Studio is automating the rest of the setup here. It's provisioning whatever computes necessary, setting up the environment, setting up the permissions necessary, and pretty soon it'll take you into the new experience. And this is what you'll have access to. OK, uh, I mentioned on the left hand side you have, uh, uh, you have the various tools, uh, available to you and for today's demonstration I'm gonna focus on a notebook, the new notebook capability that we just launched. Yeah. Now in this example what I'm gonna do is I'm gonna show you an exploratory case where I'm starting from scratch. I don't have anything preplanned. I don't have any data sets I had already in mind. I'm actually going to I'm gonna show you the full life cycle of exploring data, analyzing and understanding what's there. I'm not going to use any of the sample data notebooks. I'm just gonna launch something new. So here, here's where we launched it. This is that, uh, this is that experience of brand new where you have something, uh, new to try out. You can see the cells are available Python SQL, uh, marked down from there. As I mentioned, since this is new, using the new IM based domains, probably the first thing that I'm gonna wanna do is actually take a look at what data I have access to. So I'm gonna go navigate over to the um to the Data Explorer and understand what's inside of that. Right, so first look is have any connections. I don't have any. This is a new environment, but I do have my AWS data catalog. This is my Duke Glue data catalog, and I can see some of these things that are available on my account that I could take a look at. That's great. Now, I mentioned I don't actually know what's inside here, right? So one way I could actually explore this is I could actually use the cells and type SQL and try to query and understand what that is. But since we have a new data, uh, data agent that is context aware and can understand what's inside my data catalog, why do I wanna ask it to help me figure out what's going on? With that, I've launched a new uh data agent here. It comes, uh, comes with a pre-can set of prompts up front. As you can see, you can ask what does this agent do? You can analyze other things uh available to it. But one of the pre-can questions is, hey, list to me the tables that are available inside my sample database. So, hey, I'm just gonna use that and see what kind of results it gets me. I just clicked it It's um using that prompt to generate some results and pretty soon. It's actually going to be able to understand what's in my data catalog, go through and explore the individual parts of it, and now it's generating a summary which covered 3 areas. It had 3 core data sets, had 3 core data sets, including a weather data set and this digital wallet LTV. Well, let me take a look at it. Let me look at the summary that it provided for that. It says it's a digital wallet customer lifetime value. OK, so LTV is customer lifetime value data sets with about 20 columns. OK. Let's say this is the one I'm interested in. I've not seen it before. I don't know all that much about it. Why don't I, I try to see if I can explore and understand, right? The fastest way for me to do that is probably going to be SQL, but I don't actually know the syntax of the exact table I'm looking for, so I'm just gonna ask the data agent, hey, can you get me a sample of this and let me take a look at it. OK. While it's doing that, I'm actually gonna look some more at the summary and I realized, oh wow, it's got uh several, it's got various metrics inside of that that LTV has transaction metrics, engaging metrics, satisfaction scores. All this is related to the long term value of a customer, uh, based on their interactions with this uh interactions with your company. OK, great. Here it is, it's it's generating a SQL query based on what I requested. Right? And now what I'm gonna do is I'm gonna let it generated and I'm gonna run the SQL query that it uh uh created based on the data set. It's running that again. Under the covers this is all, uh, this is all done serverless so all that set up, all the provisioning under the covers was done automatically and it generated a quick result for me so I could see the top, uh, top 10 rows. I'm gonna, I'm gonna scan through it. It's made it easy to understand the distribution of it, go across the various, uh, columns and, uh, columns and rows available here. Just taking a look and seeing what's interest of me. So you're going to see here some interesting support tickets, issue resolution. Oh wait, there's a customer satisfaction score. So I actually am interested in this customer satisfaction score and I'm thinking I'm, I have a hypothesis that customer satisfaction and long term value are probably correlated, right? So I'd like to understand what that is and it's really hard for me to do it from kind of this uh tabular mode so. I'm thinking maybe I can ask the agent to help me and help help me visualize this in different ways, right? It's a hypothesis. These two things are related. Let's have it visualize the impact between a, uh, between the customer satisfaction score and long term value. OK. So from there I'm learning I'm just typing it in. And in this case I'm just gonna let it generate options for me in terms of what that is before um before I do though I wanted to run on the full data set so instead of just a limit of 10 I go back and edit the original data set here and now I'm generating against the entire data set then I kicked off that analysis uh for uh comparing the two. I OK, so while that's working and trying to build that visualizations, uh, score again, I'm, uh, if you recall I mentioned that that output, that full output is now available as a variable so that way that SQL output that I just generated can be used in the rest of the cells that are being generated. Here's, uh, here's that uh sample code that's being generated based on that. That's good. It's also generating some key insights on the right hand side here. Um, what it's doing actually is it's suggesting different ways for me to visualize the data. I didn't specify how I want to understand the data and it didn't know necessarily, so it's providing me different options and I can just, uh, in this case what I'll do is I'll just rely on options and see if that's something that is useful to me, if it gives me any indication what the data is. So I'm gonna hit run. Accepting this, let's take a look at the results. OK, so it's generated for me multiple uh plot points based on that, based on the customer satisfaction score and the overall long term value. Um, just looking at it, I mean, it's OK. This is like, it's got some data, some correlation. The red line in that first, in the first uh left hand side boxes as a red line which is the overall um. Uh, the overall LTV compared to the customer satisfaction score, it shows some correlation, but it's not particularly strong. And then when I look at the rest of the graphs, I'm not, I'm not seeing anything that really stands out. It's like, OK, there's some correlation between the two, but probably not, nothing that would make me say, yeah, that is the one correlating feature towards LTV. So that makes me think maybe it's not just one factor that's causing this, maybe it's gonna be multiple factors to consider. So I'm gonna look at some the summary of what's being generated here and see if there's anything else that stands out. I don't see anything being stand out based on what's being generated so far. OK, so my original hypothesis is probably not so great and, and I, I, I, I wanna expand how I wanna analyze this data. And in this case, we had mentioned there were 20 other columns inside of that. I had just picked one at random and tried to visualize it. It didn't work out. Maybe I'm gonna leverage the power of AI to actually look at the entire data set and maybe find other factors and other features that are gonna impact my long term value. So why don't I, I'm gonna leverage again the data agent to do this, but I'm gonna give it something more complex. I'm gonna ask it to actually put together a model that will analyze all the data, find all the various features that are as part of that, and uh train that model to predict long-term value based on the multiple features that are available in the data set, OK? To do this, I'm actually gonna use the same exact same data set I had in the beginning. We use about 80% of it to train a new linear regression model and 20% of it to test it. And I'm gonna see what kind of results I'm gonna see 3 things. Number 1, what are the features that are relevant for my long term value? And secondly, um, what, how well does this model, does the neo regression model match up to the actual data itself. OK. So I'm starting to work on this, is starting to build on this linear regression again taking into full context of what's going on. And as it's working through this, it starts to generate the code into the multiple uh with multiple cells, and now comes a multi-step process we're generating multiple steps, data preparation, model deployment, model training all the way through this entire thing. So I'm going to quickly scan through this one here. It's clearly multiple steps that are broken out. So why don't we walk through each one and just try to quickly understand what it's doing. So this is, uh, get, uh, fetching that model. It's generated the code associated with that. From there it's actually going to split the data sets that we need into the training data sets as well as the testing sets. It's finding what are some. Of the features that are available as part of this generally this is looking, uh, pretty good for that. It's gonna apply the linear regression and then execute a training job uh for that and finally it's gonna visualize the end result. So I just quickly scanned it. It looks OK. I'm gonna accept and run this uh uh uh suggestion. OK, while it's work, while it's running, you can see actually the results. It's actually gone through, identified a set of features. Uh, it's figured out which, uh, that 80/20 split in terms of data sets. One note interesting to note is that because of this, it's actually identified 5 more features that I had not, that are not considered earlier. So now there are 23 features to consider. It's gonna train that model and, uh, uh, uh, uh, train that model based on that initial training data set, and then within a, uh, and then it's gonna generate the end result. And we're going to see how well this model does, how well this new linear regression model does against the test data set. OK. So at this point here again these are visualizations that it suggested, but notice I didn't have to specify any of this. It did this multiple steps of getting the model, deploying it, uh, separating out the data sets, training it, and then eventually, uh, looking at the results here. It did it all on its own with me just accepting the results. Of course I could have gone back and tweaked any of these things up here, but for the sake of this demo we'll take the default suggestions. What it's showing me is this linear regression model is actually not bad and. It has a relative, uh, it has a relative correlation between LTV and predicted LTV. It has some correlation at the lower end, but the higher end is not matching up so much. OK, so not bad, great initial try. But I, I'm still curious, like I had that initial hypothesis that it was customer satisfaction scores. So what exactly were the key factors to be associated with this? OK. So in addition to this part here, I actually want to understand what was being generated and what are the main factors that affect long term value. So if we keep going down, what we'll see is that we can actually explore some of the factors that were available to, to this model. And this is probably going to be the most immediate insight for me just based on this demonstration. So income level is the most impactful towards LTV. This makes sense. I didn't see it obviously, but it does make sense in that regard, how much, uh, how much value has come in. On top of that, my customer satisfaction score, while is impactful, is nowhere close to as impactful as the overall income level, but something that did pop out is support tickets raised. As support tickets are raised, guess what? LTV has been decreased overall. So, now this is kind of the end of this demonstration. So I'm gonna kind of, I'm gonna recap, I'm gonna recap what we saw in this. We saw that starting from scratch, you built a brand new sage maker. Unified studio experience you built a brand new StageMaker unified studio experience using the IM based domains. Then you were able to use the new notebook interface without provisioning any infrastructure. It was all built out. It was able to plan together, discover data, and plan an end to end flow which deployed models, trained them, and executed a result. With that, I'm going to switch back to the presentation. And uh uh just summarize this part. Just here. OK. So though these are new capabilities that were just GA recently, we had the privilege of having several beta customers try out our product, and thankfully they were able to give us some feedback. In this case here, Sachin Mittal from Deloitte, they were able to apply these new capabilities to their ML life cycle, and really they saw faster delivery and experimentation because it was a developer focused environment that more and more of their users felt comfortable with. OK, so that's what hopefully you will also benefit. We encourage you to try it out now that's much easier to get started with StageMaker Unified Studio. And with that, I'm going to Uh, passed back to Iris, who's going to summarize and close out the session. Awesome so I know we've covered a lot today but hopefully uh you've come away with some ideas of exciting things that you might wanna try out. If there's anything to take away from this session, we hope it's that, you know, this product really is about you, our customer, uh, since going GA in March, we've been continuously listening to and hearing feedback and as a result of that feedback we've been doubling down on ease of use and bringing consolidated capabilities into the StageMaker Unified studio. We've we're also really committed to continuing to innovate and bring new experiences to Sagemaker, as you can see with the data notebook and data agent, so not just bringing existing capabilities. So, uh, with that, We're at the end of our time today. Thank you so much for coming to the session all the way in Mandalay Bay. I know it's really far, uh, but really appreciate you taking the time.