---
video_id: qen3nBhEalM
video_url: https://www.youtube.com/watch?v=qen3nBhEalM
is_generated: False
is_translatable: True
---

What's up guys? Hey. How's it going? All right, guys excited to talk about fantasy and AI. Come on, make some noise for us, please. Thank you. All right, all right, well, good afternoon everybody. My name is Michael Butler. I'm a principal deep learning architect in the AWS Generative AI Innovation Center. I get to work with amazing technology every day, learn it, build it fast, and share what I learned with customers and builders alike. One of those customers is here on stage with me today. I want to introduce the NFL's Mike Band. Thank you, Michael. My name is Mike Band. I'm the senior manager of research and analytics at the NFL's NextGen Stats Group. My team's responsible for the tracking data, collecting it, processing it, distributing it, and also turning that into products that we'll talk about today, Henry. Yes, my name is Henry Wong and I'm a senior applied scientist with AWS Generative AI Innovation Center, where every day I think about innovations and how we can build delightful experience for our customers. Now, uh, AWS and the NFL actually has been working together for multiple years. We're one of the technology. Provider for NFL NextGen Stats and we're the official uh cloud AI and machine learning provider for NextGen Stats and uh really we've been constantly innovating together to think about how to transform the experience for players and fans to experience the game of football. Now, without further ado, we'd love to let Michael kick us off to talk about the journey, how we built this fantasy AI system. Alright, thanks, Henry. So folks, what we're gonna do, we're gonna spend a little bit of time showing you what we built today. We're gonna walk you through the architectural decisions that made it possible and then share with you some of the nitty gritty. This isn't a presentation about perfection, it's about what it takes to get real agentic applications into production in an actual customer environment with all the mess and the fun that goes with doing that. But before we jump in, just let me take a moment and set the stage. So It's Sunday, it's 12:45, 15 minutes to kick off. You've been staring at the same analysis all week long. Do I start Caleb Williams? Do I start Justin Herbert? Are my waiver wires gonna pull out? Is my streaming defense right? Did I make the right call? You, you don't know, but you do know that you're gonna hear it tomorrow if you didn't make the right call. Fantasy managers live this life every day. All sorts of data coming at us fast. We have roster decisions to make, trades to evaluate, injury reports, it's updating by the minute. 5 different experts say 5 different things. Overwhelmed by data and looking for a trusted confident, fast advice, this is the problem that the NFL wanted to solve, not just for the fun of it, and believe me this was a fun problem to solve, but because they had something that no one else has. So I'd like to welcome Mike Van back to the stage. Mike, tell him what you were sitting on. Thank you, Michael. So when Michael and his team approached us in June of this year, we had an idea, we had a dream, and Michael said, we can help you build that. We had no formal design, no requirements, just an idea. And we were 2 months away from the start of the regular season, so we said, hey, can we build something that we could bring to market to fans in 8 weeks' time. That's why it says 0 to Pra in 8 weeks, because we literally went from an idea to a product for fans to use in just 8 weeks. Today we're gonna talk about how we leveraged a new series of APIs. We leveraged the NGS database, and we combined that to bring a fantasy AI assistant to NFL Plus users through our DTC platform at the NFL Media Group. Via NFL Pro, a website where you can go and find next-gen stats, insights. Analysis, film, and beyond and now with this fantasy AI experience we've now brought a whole series of new platforms and new tools to the fantasy, uh, player at home. So without further ado, I'm gonna let Scott Hansen tell a little bit of a story about the NFL Pro platform. NFL Pro. NFL Pro is the ultimate fantasy football companion, including the first of its kind fantasy AI assistant. It's gonna help you draft smarter, optimize your lineups, and gain the winning edge. Burrow airs it out. Chase, hello, touchdown, Cincinnati. So what does it mean that we delivered? Well, in about 8 weeks' time we made it, we managed to hit our 3 key pillars. The first, it had to be analyst expert approved. So it's a team of analysts on the NextGen staff's team. We went through thousands of possible questions during the QA process and what did it mean for us to be accurate and right? Well, first, the information had to be accurate. We had to present the data. In an accurate form, there's a true, there's a true number of how many times a player completed a pass or there's a true number of how many times they scored a touchdown. If we were to hallucinate those answers, the, the fan at home can't trust the output, so we had to make sure it was right and accurate. That's why we leveraged our APIs that we built that had structured data that we knew that there was a low risk to, to add a little bit of that error. And secondly, when we say 90% directionally approved, what we mean is, is that when we evaluated all of the answers, we wanted the expert, the fantasy expert that we have at NextGen Stats to be able to say, yeah, that passes the smell test. I agree with that. And so in a, in a way you're trying to not make foolish decisions. You're not trying to avoid the landmines, the fatal flaw decisions, and with, and with the, uh, APIs that we've built through NGS and NFL Pro, you now have all of that info and analysis available for you at the touch of a, of a question. Second, we needed it to be fast. If you've experienced or worked with an AI assistant, you know that speed can affect your experience. And so we needed initial question answers to be in 5 in less than 5 seconds and fantasy rich, deep analysis in less than 30 seconds. That is our, uh, 95th percentile under 30 seconds of every one of our. Answer so far, uh, meeting that threshold and finally probably the most important thing from the NFL, we are the shield we cannot put out a product that is not secure and reliable and this meant that we had a rigorous process to go through legal approvals our AI council approval and all of that meant that we had to have a really buttoned up system. Well, what that means is we don't want our AI assistant to be able to answer questions that are outside the bounds of fantasy football. You could imagine that someone could think, oh, I could use this to make a bet on sports. Well, what that would do is we would essentially create liability for us. We allow recommendations to be through betting, uh, for, for people to ask. And and then they can come back and say oh you told me the wrong thing so we wanted to limit the ability and to say that this assistant could only do a fantasy football analysis and because of that we have had 0 incidents since launch in our first month of 10,000 questions, not one incident was reported from a user or through our logs. Now, I know it's always good to talk about the product, but why don't I show you exactly how it works. I'm gonna do this a couple of times throughout today. So first, I'm gonna ask you a question. What's up with the NFL's fantasy AI powered by AWS? Let's just ask QB for their answer. So we'll wait a second for it to load. Alright, the NFL just dropped a game changing AI assistant that's about to revolutionize how you dominate your fantasy leagues. Key features include exclusive NFL data, real-time analysis, personalized advice, and rolling out through NFL Pro for serious fantasy managers with who want data data-driven championship advantages. Bottom line, while your league mates. Still comparing basic stats uh across different apps and websites, you'll have AI powered analysis delivering insights that actually move the needle. This isn't just another fantasy tool. It's the difference between hoping for wins and engineering them. That was an answer directly from our AI assistant and later in this, uh, in this talk, I'll show you it. I'll show you a a discrete example of how we really use it on game days. So I'm gonna invite to the stage Henry to talk about the architecture and how we built it. Thank you very much, Mike. All right. Before we talk about the architecture, I'd like to take a quick poll of the audience here today. Please raise your hand if you have heard of the word agent. I'm expecting all hands, OK, since you are a ring man. Don't lower them yet, don't lower them yet. Keep them up. A little bit of exercise after lunch is good for your body, OK? All right, now, keep your hands raised if you have heard of the term workflows. Wow, OK, everyone. All right. Now keep them raised if you know the difference between an Asian and a workflow. Great. OK, awesome. Just the mix we're expecting from the audience today. Now everybody you can lower your hand, rest a little bit. So, Just a quick refresher on what agents are, right? So agents are nothing but an autonomous system that can reason, plan, and take actions on behalf of humans or another system. Now we think about a simple chatbot that you just get turn-based responses with that uses a large language module, internal knowledge ability. The Agentic uh chatbot can do a lot more. So here is a quick more detailed look into what agentic loop will look like. When this agentic system takes a user input, it will perceive it, try to understand it with a large language model behind it, and try to think about what kind of tools should it use to answer the question. It will execute the tool, see the results getting responded back, and decide if another loop is needed in order to achieve the final goal it's instructed to do. For fantasy AI where you need to compare which quarterback you want to start for your line off, you need to actually think about the matchup, the upcoming, uh, stats, the weather, so many different things that is very complicated and a complex system to do, but exactly what agent excel at. And this is the reason why we decided to go with building agentic architecture for fantasy AI. Another quick primer, um, MCP. So MCP is the short for Model context protocol, which is a standard plug that allows your large language model to be talking to different tools in a standard way. So the advancement of MCP really allow us to have this, every time you need to connect to a new API or to talk to the database, you don't have to build new things, right? Consider it as a universal portal where you can just plug in so that this way as long as both sides support it, you don't have to be creating new things and really makes the LLM integration with different systems much more seamless. All right, now with those primary thing, let's talk a little bit about the architecture. From the left, we have the um the user input a query. And then it feeds into the fantasy AI which is hosted on the EKS that auto scales. It uses the, the large anger model from Amazon Badrock to reason and pulls succession memory from F3 bucket, so it has the context. More importantly, It is connected through MCP to different data sources. It connects to the NFL NextGen stats to get all the player information about their stats. In addition to that, it also connects to the rotor wire for additional user stats and player stats. So this way, everything just gets connected and the system is able to, uh, large language model, our fantasy AI agent is able to process a call. And try to provide the best response to the users. Everything happened from end to end, from left to right in under 5 seconds, even during the Sunday of peak traffic. So, this is really amazing, right? And the right question you should be thinking about is, how do we actually pull this off in just a matter of weeks. If you're thinking about it, great, that's the right question to be asking because we will be talking about some of the key decisions we made. To enable this to happen. If you think about building such a complicated agentic system from scratch, there's so many things for you to think about, right? Like you need to have large language model processing, you need to have a different prompt management system, you need to consider how to manage your sessions and how to control fallbacks, etc. when the models are throttled. So, what are the some of the decisions that we decided to make? It comes down to 3 key pieces. The first is that we decided to go with Strength agent, which is an open source framework that makes it much easier for you to build a production level agentic frame uh agentic system. Second, we decided to use MCP as our semantic data layer. This way, it gives us separate responsibility and we can scale them independently between agent and our data layer. The third is that we link in very heavily to use AI assisted coding, which will talk about some of the best practice and learnings we found. Now the first thing is that strength agent is an open source framework that really with a few lines of code allow you to build a uh agent framework in production. With agent equal to agent, it handles so many things for us. It will help us with managing the sessions, will help with uh managing the prompts, it will uh support multi-language models so that we can plug and play with different models when we see fit. So really the strength agent framework provide us with scaffolding, so that allows us to focus on building the agent logic for fantasy AI rather than uh the, the plumbing part of it. The other decision we really need to make is think about how we want to let the agent talk to different data sources. Now there are different options you can do here. The simple option is of course you can use Python decorators to just give the tool definitions to the agents inside the agent codebase. This way everything is in one place, but the issue comes with that is our data layer is really complicated. We need to actually identify different parameters to be filtering for the data sources. We need to have business logic validations. After the data, we need to do additional uh parallel processing in order to get data very efficiently. If we embed all of these code into agent code, it actually makes the code very hard to maintain and very hard to scale. Now the other option is to separate the agent logic with the data logic, and that's actually the choice we decided to make that offers a few benefits for us. The first thing of course, it's a, a separate layer of concern. So this way we're able to uh be building agent so that the agent code focus on the logic and the orchestration and then the MCP will allow us to basically manage all the data layers, what data sources to talk to, right? The second thing is, once we build this data layer MCP we can introduce additional agents in the future, say different personas, different agents for different purposes. They can all talk to the MCP data layers. So you build once, you can use it for multiple times in the future. And the third benefit is that it allows us to actually scale each part independently, because it's really hard to predict the traffic. So sometimes we may need to scale up the agent, uh, computation, sometimes we may need to scale up the MCP data layer. This way it allows us to disentangle those two, so that we can scale each as we see fit. And the third thing is that due to the time crunch, we were really thinking hard about how can we speed up our development process, and this is where the AI assistant coding comes in. Now I'm not recommending you to say give a prompt to your favorite AI coding assistant and go lie on the beach and drink your pina colada for like a few hours and come back. No, that's definitely not what we're recommending, OK? But there's a few things that we found really helpful for us when we were building Fantasy AI. The first thing is that it allows you to learn a framework, a new framework, much faster because you can do customized Q&A and you can actually just answer, you can just ask questions that's specific to you right now. It allows you to speed up learning instead of reading all the raw documentations line by line. The second thing is, we all know we're not knowledgeable on everything, so there are always cases we need to learn about a new concept or there's a concept we're not familiar with. With AI's help, we can actually dive deep into some of the things and know, OK, how is this framework working? How does this EKS, for example, we should be orchestrating them, right? That allows us to know why and how for this new technology we'll be using. And the third is we find this AI coding assistant is really helpful for writing uh undifferentiated code. For example, for writing the test suite that really shrinks down from like hours or days of work writing test cases into just a matter of minutes. Of course you still need to validate the, the, the, the right of those, like whether those are true, but that really takes off a lot of the work off our shoulder. All sounds rosy, right? Now, I'd like to welcome Michael back to stage to actually talk about the challenge and the lessons learned we, uh, we had throughout this journey. Take it away, Michael. So I get to talk about the fun part. How do you actually get this into fraud? How do you make it happen? So we're gonna spend a few minutes talking about some real production challenges we faced, and the theme I want you to think about is pragmatic production beats perfect every time. We're trying to get production grade software cranking. We're not necessarily looking for perfection, so I'm gonna take you on the journey that we went on. Challenge number one, getting our gentic playbook together. How would we take this type of data and provide it to an agent quickly and under pressure? If you're building an agentic application, you're probably dealing with some domain specific context, some domain specific semantic meanings, and you're trying to figure out how to get that data to your agent quickly. Challenge one was how we solve that for next gen stats. The very first challenge was the data itself. NFL's next gen stats are incredibly vast. There are hundreds of unique data fields, and they mean different things in different contexts. Snaps aren't just snaps. Snaps could mean snap share. They could refer to the number of times the quarterback takes the ball, drops back, what the ratio is, the percentage, under what defensive conditions. Snaps aren't just snaps, domain expertise matters. So we had to figure out. How do you focus the agent on the right data? We didn't try to. We let the agent tell us. We let the NFL tell us. The first place we started was with NFL pro analysts. We asked them how do you break these questions down? When you write insights on NFL pro, how do you think about answering these types of questions and when do you use the data you use and why? We wanted to understand the mind of the analyst so we can encode it into our agent. We took that information and we compared it against NFL's Nextgen stats. Each API, each source within NextGenats has a contextual usage. And so we asked pro analysts, categorize this for us, give us basic instructions where we would use certain types of data to answer certain types of questions. And from that we built a basic data dictionary, not a data layer, not a third party service, not another product, because we're going from zero to production in 8 weeks. We needed a tactical, practical solution, and this was a semantic stats dictionary. We stripped out all of the context, all of the rich descriptions that NFL's NextGen Stats API provides, and passed just the field names to our model. How would you use it? What do these stats mean? And then we used a larger model to evaluate the responses, and bit by bit. The LLM informed itself about what the data was, how to use it, and when to apply different types of stats to answer different types of questions. This was key because it allowed us to focus on just the right data to provide to the agent to solve a different type of question. Otherwise we'd be dumping hundreds of thousands of tokens if we tried to provide a vast level of rich domain expertise to every single question. We needed just the right information filtered at just the right time to the agent. Having built that semantic data dictionary, we could pass in just the semantically relevant portions of the dictionary at runtime and allow the agent to apply the information. Because we were using strands agents, which has model driven orchestration, we had a loop cycle to go through where the model, or excuse me, the agent could choose what data it needed and retrieve it. What did this do for us? Dropped our initial token consumption by 70%. Instead of pulling in vast, highly descriptive human readable API specs, we brought in information in a language that the LLM could understand. We taught the agent what and how to ask for and when to use it. We didn't try to provide it with everything. That gave us the data we needed, but we still had to figure out how is the agent going to get a hold of it. We've made a design decision for MCP and that led us to our next part of the challenge. Tools This was the first place we messed up. So, I told you it was a production story, right? It always goes perfectly. So we sat down, we're like, all right, we got MCP we're gonna use tools to, to access the data. Let's write a tool for each of the major use cases we think we're going to encounter. That should be great, right? Nah. When you're dealing with an agentic loop and you have 29 different tools or whatever that number happens to be, and you give an agent instructions to be complete and thorough, it's going to be complete and thorough, and you're going to have dozens of loops and tool calls, each one with narrow bits of information that lack the rich context to answer a broad question. How do you give the agent the right tools without essentially hardcoding the decisions into tiny slices that are effectively a new set of APIs? We're dealing with autonomous agents, we want them to think, reason, plan, orchestrate, not just call a singular little bit of data. The answer was to consolidate not on our use case but on the boundaries of the data itself. For example, projections. You wanna get a projected fantasy score or a projected number of touchdowns or projected yardage for a player, or across a game. What do you need? Well, we started out with weekly projections, weekly projections, season projections, rest of season projections for a player, a team, or a defense. 6 tools that gave highly fragmented answers. And what we realized was that we needed a much broader tool. We needed tools that looked at data boundaries and gave the agent the autonomy to express rich and nuanced intent. The get projections tool allows the agent to select one or more dimensions of projections at the same time, so it can think ahead of the data that it might need, ask for it at once, instead of multiple round trips and multiple loops and the latency that it goes with, you get one call, one call from multiple players or multiple teams or defenses. The trade-off was that we had to build in some fairly complex logic for how to compress the data. This was a production system, and we're returning quite a bit of information when. The model is able to request a fairly rich response. Now, How do we get our agent to understand this, when to use those parameters so that it didn't just result in a very large call each time. We use the same approach that we used with our data dictionary, LLM assisted refinement. We stripped out everything from the dock string, everything from the decorator except for the parameters, and asked our agent, how would you use these. Then we took that response, used a larger model to evaluate it, and continually refined until we had a dock string that the LLM understood. Didn't make a lot of sense to the developer first reading it, so you add human readable comments beneath it. No big deal. But now we had a dock string and a, a data dictionary that LLMs could almost natively understand, and it gave them domain expertise, the ability to understand the data and how to get it. This also effectively reduced our tool spec by 50%. Why 50%? Well, we increased the size of the dock string for a smaller number of tools. If you're caching your tool spec, this probably isn't a massive change in your token consumption. But what it does mean is that you have far fewer tools, fewer round trips to the LLM, fewer invocations, lower latency. More throughput. Combined data and tools gave us intelligence. It was the intelligence to produce fantasy grade advice fast. But intelligence doesn't ship. You need to deliver it. And so we had our second challenge, making sure this thing could survive game day. Now we weren't going in blind. We had a scouting report. We knew from NFL Pro what their web traffic looked like, the expected user base. We had lots of questions, hundreds of questions provided to us by NFL Pro analysts that we could use to evaluate quality, to performance test. We could still do our due diligence. But this is emerging technology, non-deterministic emergent behaviors. When you're dealing with emerging technology, history isn't the true test of success, it's users, actual user behavior in production environments, and we didn't have time for that. We were going 0 to production in 8 weeks, so. We had to build defensively. 3 plays kept us in the game here. The very first play. What do you do when you hit capacity limits? So, again, show, show of hands cause we're coming off a lunch here. Who has attempted to build or use a frontier model in the last few months? Build with or use a frontier model of any kind. How many of you have seen a lovely message, something to the effect of. Throttling exception, service quota exceeded, service not available. Yeah. Yeah, me too. That's not really cool to show to your users, is it? Can't show a throttling exception to your users. Now, we had planned. For a certain number of volume based on NFL pro traffic. We'd done our service quota due diligence, we'd set our thresholds appropriately, but. What happens on Sunday morning? What if the users that came were More than we thought. What if they ask more complex questions than we thought? What if our emergent behavior was different than we thought? If a user sees a spinning. Notice or doesn't get a response back or heaven forbid gets a throttling exception, they're not coming back and this is NFL Pro's brand on the line. We couldn't abide a situation where we ran into service capacity or throttling exception, so. We build around it, we architected around it with a fallback provider. The strands agents framework allows you to extend its model class with what we call our bedrock fallback provider capability. At the time of the agent processing a request, this sits between the agent and the bedrock service. If the bedrock service returns anything that looks remotely like a throttling exception, account quota exceeded. Service not available, we intercept that message and send our message instead to a secondary model. We chose to use the anthropic family of models for this with a frontier model and a relatively well tested model as our fallback. In the situation where we encounter throttling, that message is intercepted, sent to the fallback model, back to the user with additional latency in the order of a handful of milliseconds. The user still gets a response, and we get battle tested information on the true extent of token consumption throttling the throughput in our production application without the user ever having to realize that they were part of the test. We also introduced a circuit breaker because even though we're adding milliseconds worth of latency with this fallback capability, we don't want to continue hitting a service that's overwhelmed. The circuit breaker stays open for a period of time, reevaluates the primary model. If the throttling exception is gone, we fall back over. Now, Some of you right now are thinking, wait a minute, wait, wait, wait, wait, wait, wait, wait. That's an anti-pattern You just introduced bimodal system behavior. You don't know which model is going to service your request. You're right. We did We chose to take 90% reduction in throttling exceptions on launch day, to not have a user see a throttling exception, a capacity limit, and to provide the best of frontier models available on Amazon Bedrock in exchange for building something that we will have to replace in a future version. When we have battle tested real-world information about consumption, token burn, throughput, we can adjust our service quotas accordingly, and we won't need this functionality anymore. But day one is too important. We couldn't allow our users to have a subpar experience, and so we made the decision to introduce something we might not otherwise introduce. Play number 2 was predicting our problems. We're dealing with emerging technology with emergent behavior. A model can produce different results for the exact same data with the exact same tool calls. How will the model respond when we actually see production behavior? How do you test the unknown? How do you know what your model is going to do when it hits the real world without it ever actually hitting the real world? We extended the strands agent framework to give us per turn reasoning insight. Strands agents has a very vast observability framework in it. And when we started working, that observability framework was still in progress. Uh, we started on roughly version 0.3, it's in version 1.18 right now. We added per turn reasoning instrumentation to allow us to see what the agent was doing, the tools it was calling, with what parameters, returning what data to give us an understanding of exactly how it processed the hundreds of questions NFL pro analysts asked during our UAT. This exposed certain decision patterns that allowed us to avoid some pretty unfortunate things on game day. Here's a look at what happened when we asked the question, who's the best wide receiver to pick up for week 10? Fairly straightforward question. That's 1.3 million tokens that that question consumed. Why did it consume 1.3 million tokens? It wasn't the answer. The answer was actually really good. It passed all of our, our QA, all of our UAT checkpoints. The tool calls made sense. Why was it 1.3 million tokens? Because part of the the instruction to the agent was to produce complex and thorough reasoning, defensible and backed by data. So the agent requested stats, backstory, and projections for every wide receiver in the league. You don't need that. Fantasy managers are going 5, 10 players down if they're evaluating a waiver wire decision, they're not going to player number 80. We observed just in this little example that even though we had the right data, we had the right tools, we still needed to govern and observe the behavior of our agents and put appropriate guard rails around the maximum data it could pull under what circumstances it could do it. That let us avoid. A simple question, burning 1.3 million tokens. Can you imagine the impact that would have had on throughput if we had released that? So the takeaway here is. Don't trust your, your new resources. This is emerging technology and even though you may have a unit test, even though you may have UAT, even though you may have performance tested it, interrogate the behavior of your models until you understand the emergent behavior inside and out. Third and final tactical challenge we had to solve. Passion. And you're like, Cashie, wait a minute. Why are we talking about caching? Well, we're talking about caching because NFL's next gen stats is incredibly vast. And even with the data dictionary that we used to bring in only semantically relevant data, even with the tools to choose just the right data for the time, even with the instrumentation to know when we were pulling more than we needed, it's still an incredibly, incredibly token rich environment. And if your data constitutes a token rich environment, this is something that can help you. How do you allocate cash when you don't understand what users are going to do? When you can't predict the conversation pattern well in advance. Don't. Don't try to predict it down to the nth degree. Don't build an incredibly complicated predictive algorithm to try and understand something that haven't, hasn't launched yet. We studied the tape just as a coach or team would study the tape before they go onto the field on game day, we studied past conversational patterns of the NFL pro analysts we work with. We asked real users that we could find how they ask questions about fantasy. And from that we understood a handful of things. We used our understanding of conversational patterns to allocate our cash points in the anthropic family of models we worked with. Now, most of you are already using two of these. You're cash in your system prompt. And you're cashing your tools back, and you're getting a lot of good results from that. You're not burning tokens at every invocation on prompt or tools. But you've got two more. How should you use them? Fantasy users tend to ask open-ended questions and refer back. They ask about player A, how is Justin Jefferson looking this weekend? Well, when we ask that question. The agent's gonna do something like get stats, that might be a 50 token response. It's gonna get some relevant news, look at injury reports, look at context. That's gonna be another 280 tokens or so. What's the follow-up question gonna be? It's not gonna be about Devonte Adams, it's gonna be about Justin Jefferson. You wanna retrieve all that data again. Fantasy managers tend to ask follow up questions and so caching the heavy hitters, not every single call to the LLM, not every single tool, but the ones that were really meaty and provided heavy token consumption allowed us. To then slide. Those heavy token hitters. Out once we were out of that part of the conversation, and we used a very simple mechanism. Remember, practical beats perfection in production. We used a simple sliding window mechanism that looked at the heaviest MCP tool calls and when a new tool tool call came in, we slid the old one out. Sounds fairly simple, right? Simple patterns work. This simple pattern increased throughput on our agent by 2 times. That's it, just caching the most two recent heavy MCP tool call results, increased throughput by 2 times, and folks, this is after all of the other optimizations we've shown, reduced our cost by 45%. The answer isn't build a sliding window to manage cash points. The answer is when you're thinking about speed to production, find the simplest pattern that enables your benefits and go chase it. You can analyze later. You can analyze with real world data once you've been in production and gather those patterns, and then you may find out that this isn't actually optimal. But get the 80% win first, take it and move to production. Simple, dramatic results beat perfection in production every time. These are 5 of the challenges that we face across those two categories, building intelligence and getting ready to deliver. But you might not be building an assistant. You might be building some other type of genetic application and you're like, yeah, this is great. I don't deal with heavy MCP tool calls. I don't, I don't have a lot of uh heavily semantic information I have to deal with. OK, fine. I'm gonna share what we learned and how we made the decisions of what we shipped versus what we didn't ship. This is a mental model for thinking about the evolution of agentic applications and how you should prioritize features when you're trying to get to production quickly. Agentic applications only have 2 layers. The first is intelligence. Intelligence is comprised of reasoning, tools and data. Reasoning is the ability to think, to plan, to orchestrate, to execute steps in order to understand based on information what it should do to handle a query. The agent applies reasoning to tools and data, and between the two of those, it gets intelligence. This is what matters. Don't get caught up in the perfect infrastructure. Don't get caught up in the perfect user experience, because if you get this wrong, your users won't come back. Intelligence may not ship, but it is the product. We are no longer in a world where features are the product. Intelligence is the product in agentic AI. Get intelligence right first, and then figure out how to ship it. Ship it just good enough with delivery. I'm not telling you to neglect the well architected framework. Think about resilience, think about security and privacy, think about compliance. Your infrastructure is still important. And how you make it accessible to your users, how you make that intelligence accessible to your users, is still important through the user experience. But if you don't have intelligence, if your agent doesn't have intelligence, you don't have a product. You may have a wonderfully resilient architecture with a UX that your users aren't going to come back to. So when you're prioritizing your own agentic applications, think of this mental model. Get intelligence right, ship good enough infrastructure and user experience, and when you get real world data, begin to iterate. This is the mental model that we used that allowed us to ship in 8 weeks. We applied that mental model to job number one. Mike told you about it when he was up on the stage. That was to deliver NFL pro analyst grade fantasy advice fast. And because we prioritized intelligence of our agent, we did. We're able to achieve 90% analyst approved responses. We're able to stream those responses in under 5 seconds, initially, full response with complex analysis by the end of 30 seconds at high throughput because of some of the challenges and trade-offs that we encountered. We had multimodal resilience and the ability to handle Sunday, Monday night, Thursday night football traffic spikes. There's a lot we didn't ship. We didn't ship conversation history. We didn't ship integration with leaks. We didn't ship user feedback loops or custom preferences. Because these are valuable features that don't prove job number one, and when you're trying to get a product to production. Job number one is the only job that matters. Get it to production, make it work with the simplest patterns in architecture that produce the results that you can't compromise on. Analysts great advice, timing. And then build upon it. I'm not saying do an inferior architecture. Conversation history and fantasy AI is a reconfiguration action, not a refactor. We use the Strands S3 session manager to persist conversation data. When we're ready to expose that to users, it's a front-end exercise. Those are the types of design decisions you get with a model-driven orchestration framework and building with MCP. So Proved job number one, had a lot of challenges along the way. But we deployed to production in 8 weeks. And to tell you what we've seen, what it's meant for the NFL, and what's next, I wanna welcome back Mike Band. Thank you, Michael. So when Michael asks us, where do you all see. You all taking the fantasy AI assistant, the first thought was, yeah, we could add more data, we could add more context, we can add articles on NFL.com from our fantasy analysts, and we could make the answers better and more robust. But what if we took it a step further? What if we thought outside the box? So one of the ideas that we have is we have a team of analysts right now that are at the office preparing for week 14 right now. They're going through all of our stats dashboards are going through all the tools that we have to work with by hand, more or less, using their own football, uh, expertise to come up with these storylines and to write the most compelling narration that we give to broadcasters like Greg Olson and Cynthia Freeland alike. And how can we then bootstrap those efforts? How can we create more insights, more richer context, and, uh, really drive the output of our team to not just, you know, uh, to, to 2x, 4X, 10x our team and the amount of output that we have. So we had an idea. What if we could get our fantasy AI assistant to help us write weekly insights on a week to week basis to increase our output on a week to week basis. So let me show you exactly what that would look like live. So I pull up NFL pro. You can do it on your phones right now. Go to pro.com. Go to pro. NFL.com. I'm gonna type in analyze. The fantasy performance. By patriots. Rookie running back Traveon Henderson. In week 13 against the Jets, uh, sorry, I'm not that fast of a typer on Thursday Night Football. After conducting your analysis. Sorry, I'm really, really fast here. Summarize your findings in the format of a four-sentence Nexgen stats insight that can be featured. On NFL Pro. Use next-gen stats and advanced stats to support your analysis. So what what we're doing here is we're prompting our AI assistant to write an insight in the exact format that our team would do on a week to week basis. And in this case, we get this very rich answer. So it'll give us some of the the reasoning and it'll start. Based on my analysis, Patriots rookie running back delivered a breakout three touchdown performance in Thursday night's 27-14 victory over the Jets, recording 19 carries for 62 yards and 2 rushing scores while adding 5 receptions for 31 yards and a receiving touchdown. His 27.3 fantasy points ranked him as the RB1 for the week, showcasing his red zone efficiency with 7 red zone carries that resulted in touchdowns. Now, I won't read the whole thing because I want to go a little bit deeper in this. We had a human analyst that wrote a very similar insight, completely separate in a parallel path, and I wanna show you the similarities between these answers. So, they both have, and they both mentioned that they, that he scored 27.3 fantasy points. They both mentioned that Ramondre Stevenson, the previous starting running back, was out with a toe injury that led to Henderson's, uh, emergence and breakout performance. They mentioned that he had a 90% team snap share, so it really emphasizes that the team is relying on him and participation, meaning that he's got the opportunity to score points. They mentioned that he had 19 carries for 62 yards, adding 5 receptions for 31 yards through the air. And that he forced 9 missed tackles. Now, what it, what the AI assistant did not output was that he had 70 rushing yards after contact or that he's playing the Bengals next week. But what they did note was that he had a 37.5% missed tackle rate. On just 3.3 yards per carry. And more context about the game itself and that the Patriots won the game 27 to 14, so you compare those two outputs and you can see, man, did we just add another fantasy expert to our team? I think we did. And now our fantasy experts who write these insights on a week to week basis are using the AI assistant to power their work on a day to day basis. It is not about replacing in human analyst because that research and that football context and that football acumen is so important. And if you don't know that a defensive coordinator was fired and that has an effect on the way that they're playing, then that's gonna be a missing piece if you just go fully automated but what we expect to use is if we can have an AI assistant help us either find research nuggets. Put it into a concise format of a style of a paragraph and help us write on a week to week basis. Our output of a team goes from 1X to now 10X, and that's what we're gonna do and continue to evolve with the fantasy AI system, not just an experience for external public users to help make decisions, but internally how do we make our team more effective on a day to day basis. So with that you can uh scan the following QR code and have access to the NFL Pro Fantasy AI assistant right now. Uh, there are only a few weeks left of the fantasy season if you have a team. I hope you're in the playoffs and if you are in the playoffs, Fantasy AI is ready to help make that, uh, a reality, Michael. All right, we talked about our building experience. Now it's your turn. Go build. Put your hands on the keyboard. Production is not about perfection, it's about practical application of frameworks and resources. Here are two of the best that we used during our time. The first. AWS prescriptive guidance for agentic patterns. You may not be building a human to agent assistant. You may be building a different type of agent. Agentic patterns through our AWS prescriptive guidance provide real world guidance on how to build those agents and build them to production grade. Second is a link to the strands agents framework. We found the model-driven orchestration power of strands agents to greatly accelerate not just our build, but our evaluation of our agents' capabilities. If you're looking to build fast, build agents fast to production grade, Strands agents can be a great place to start.