---
video_id: bNiFEpzv3_A
video_url: https://www.youtube.com/watch?v=bNiFEpzv3_A
title: AWS re:Invent 2025 - Enterprise-scale ETL optimization for Apache Spark (ANT336)
author: AWS Events
published_date: 2025-12-02
length_minutes: 50.38
views: 3861
description: "Apache Spark on AWS Glue, Amazon EMR, and Amazon SageMaker  enhances the optimization of large-scale data processing workloads. These include faster read and write throughput, accelerated processing of common file formats, and expanded Amazon S3 support through the S3A protocol for greater flexibility in write operations. In this session, we'll explore recent enhancements in Spark for distributed computation and in-memory storage to enable efficient data aggregation and job optimization. We'll a..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

A few months ago, I was working with her team for their slow spark job. It was processing a large amount of data. And ran for hours. So they added a number of optimization parameters in it. Everything in the code looked right, but the job was still not performing well. After some back and forth. We realize the issue. The job was configured with S3A parameters. But it was using EMRFS. It's, it's a very common situation. With so many connectors out there. It's easy to mix things up. That's one of the problems. That we will aim to simplify in today's session. Welcome everyone. My name is Vivek, and I'm here with Giovanni today to talk about enterprise scale ETL optimization for Apache Spark. I'm a principal delivery architect. And Giovanni leads the Spark team within AWS that is used by AWS Glue. Amazon EMR and Amazon Athena. So here is what we will cover today. We will start with the pain points. What makes the ETL. I and complex today. Then we will look at how AWS and Spark solve these problems. Through security Unification And performance. We will go through a few real world use cases. And wrap up with some action into the show sets. That you can take back to your teams. This is a 300 level session. And having prior experience will help you with the technical details. Businesses move fast. This happens in seconds. Not in this. But the data system. That can still feel stuck in the past. A team recently told me. That their ETL jobs are so slow. That by the time their job finishes, The question they are trying to answer. Has already changed. It's a race against time. That many feel today. Enterprise customers spent a significant amount of time. Tweaking their spark configuration. The data platform team of a large fintech company. spent nearly 25% of their engineing time. Tweaking spark configuration. Across their 500 production jobs. Still, Many either are overprovisioned. Or fail altogether. A big travel company shared something similar. About 40% of their job failures. Due to a spark misconfiguration. Meaning engineers have to jump in. Fix things. And rerun the shops. And that's the reality for so many organizations right now. Before we dive deep, Into our session Let's take a quick look at the ETL landscape within AWS. It usually starts with your raw data in S3. That's your data leak. Then comes the glue data catalog. Which manages metadata like schema, table, partitions. To secure and govern the data. We have lake formation That steps in with fine-grained access control. And centralized permissions. Once that foundation is ready. Sorry. You plug in your analytics engine. Use Athena for interactive or ad hoc queries. And AWS Glue and Amazon EMR for large data transformation and machine learning pipelines. This spark integration has been there for many years. And it's a pretty reliable. Part of the ecosystem, AWS ecosystem. In short, S3 stores your data. Glue cat locks it. Lake formation protects it. And Spark help you analyze it. ETL today is powerful. But it's also challenging. Let's start with security. Teams still manually. Enforce row and column level rules. Policies are scattered across rules. And data leaks happen because governance isn't consistent. Then there is performance. Spark jobs slow down. From metadata bottlenecks. Rendered scan. Or schema or encryption complexities. And finally Consistency Spark behaves differently. Across EMR Glue and Athena. Each environment has its own tuning. Its own libraries. And its own maintenance overhead. The result A fragmented ecosystem. That is hard to govern. Hard to trust. And hard to tune. So let's take a step back for a second. What if we completely reimagine this experience? From a different perspective. What if your spark job didn't just run, but fluke? Reading smarter, writing faster, and making the most out of S3. What if the governance wasn't voted on at the end? But baked right into from the start. And what if you could use one spark engine? The same one Across Blue EMR And Athena Without a single tweak. That's exactly what this new chapter of AWS. Um, Spark on AWS is all about. A platform that is secure. Unified And fast by design. All right This brings to the core theme for today. Apache Spark on AWS. Built for security. Performance and unification. Think of it this way 3 services One spark Foundation. Security Making sure your every query is protected and every data set is under your control. Unification. Giving you the same spark experience everywhere. Whether it's EMR. Glue or Athena. And performance Delivering faster results that scale effortlessly. As your data grows. Each of these pillars. reinforces the others. And together, They completely redefine. What modern data processing looks like on AWS. Let's break this down a bit. What's secure? We now have fine grade access control built right in. Native integration with lake formation. And even multi-dialect views that can use invoker aware permissions. In simple terms, Your data stays protected. No matter who is squaring it. Next, Unified There's now one consistent spark run time. Across glue. EMR and Athena. EMR in fact now uses S3A connectors by default. And it supports every storage class, even glacier. So wherever you are running Spark. It behaves the same way And finally, perform it. You will see faster iceberg reads and rights. Optimized JSON processing. And materialized views. That dramatically reduce query time. The best part All of this works right out of the box. No code changes. No extra configuration. It just works. Now we have covered the foundation. Let's move into security. And what that means across Spark engines. If you look here You will notice these challenges feel pretty familiar across data teams. Power users are asking, why can't they access everything. Everything they need Analysts are stuck filing access requests again and again. And data admins are juggling policies across EMR, Glue, and Athena. It sounds chaotic. Because it is. And it all comes down to one big issue. Governance is fragmented. Policies are scattered. Enforcement is manual. And visibility It's pretty bloody. That's when the real problem starts. Security gaps Compliance issues. And a whole lot of frustration. Before we dive deep, Let's take a look at the spark and lake formation integration history and how it has evolved over a period of time. You can see a clear shift towards simplification and native control. So back in 2022, When we first introduced lake formation with spark. Everything flowed through an internal component known as record server. It acted like a middleman. Enforcing table-label permissions through a centralized full table access control, also known as FTAC policies. By 2023, We introduced it with fine grade access control, FGAC. Row, column, even cell-level filtering. But it still relied on that. Intermediary layer. Then came 2024. When the enforcement finally moved. Inside the spark itself. No more proxies. The spark runtime could directly. It could directly apply lake formation permissions. And this year, We have gone fully native. Allowing spark. Directly sparked to Read, write. And many stables. Under lake formation governance. seamless Secure And far more efficient. But what does that impact on the ETL processing? When Spark access data through lake formation, It goes through a lot of hoops. A lot of hoops before data can be, uh, data from S3 can be accessed. Every read and write request is checked against permissions. Under fine grade access control. These checks happen at the row or column level. Which means Spark doesn't always have full visibility. Into partitions. End column statistics. And that can affect query performance and optimization significantly. With full table access control on the other hand, Spark operates with complete visibility and direct eye. And it can perform DML operations like create. Update and match. Making it for trusted ETL pipelines, ML preparation, and batchmark loads. Where speed and flexibility are critical, and your ETL job will see a performance improvement. However, If your workload involves Interactive. Multi-user analytics or environment where data privacy and compliance matters. Fine grade access control is what you could choose. That will ensure Row and column level segregation. So far We have focused on how lake formation secures data and spark workloads. Through FTAC and FTAC policies. Now, now I want to shift. Gears to a challenge that shows up even before authorization. Meaning SQL logic across multiple analytics engines. Today The team often rewrite the same business logic. Separately for Athena. Spark, whether it's Glue or EMR. That means a single logical view. Turns into two physical views. One in the sequel. And one is in spark sequel. And even though the intent is identical. The syntax and function differs. This lead to duplicated logic. Logic drift And governance issue because it's unclear what is the source of truth. Let's zoom out and look at the broader challenge organizations faces. When managing SQL view across multiple engines. Couple of things uh comes out very clearly. First, Fragmented SQL dialects. Every spark engine. Speaks a slightly different version of SQL. So teams end up rewriting the same logic and dealing with schema drift. Second Inconsistent access control. That means separate view definition. duplicate policies. And gaps in audit visibility. And third, High operational overhead. You need to rewrite views, fix them, constant onboarding of new users, constant change, maintenance, etc. All of this reinforces why a unified cross engine view model is so important. So now we have outlined the challenges. Let's see how AWS data catalog views all these things. So data catalog view. Give us one centralized definition. That supports multiple sequel dialects behind the scenes. First It provides unified governance. So instead of managing separate views in every engine, We keep one logical definition. And lake formation enforces consistently. Second, Crossing inconsistency. Whether teams are using Athena, Glue, Spark. They will get the same result because the logic behind the scene is the same, it's in one place. And third Faster collaboration. Teams can reuse. The same logic Without rewrites And without coordination. Let us see how to create a view. When you want to share data. You first pick the tables you will expose through a view. And make sure they are governed by lake formation. Then The designated role creates the view using SQL syntax. And that you would see over here. Let's dissect the syntax. When you label it protected. It's lockdown for the Gordons. Multi-dialect means single view works across Athena. Glue EMR and even red shift. Security definer means the permissions of the definer rule apply, not the end users. So after it is created, you get, you grant select on that view, not the underlying table. You get simplicity in access management. Strong governance. And you keep the raw underlying data. And the table locked down. So let us see how glue data catalog views fits into existing governance. The good news is Nothing changes Once a view is created. Lake formation treats is exactly like any other lake formation object or table. So you can grant similar type of formations like select, describe, alter, drop, the same way you do it today. Your existing models still apply. And then you can use it to share this view across accounts to other consumers also using lake formation, governed process, using resource link, etc. So the lake formation enforces access consistently. Across engines. So even if the view uses a different sequel dialect, Behind the scene Your governance stays the same. Centralize Predictable And easy to manage. Now let's take a look at how Um, we have seen how glue data log views were. Let me walk you through some of the key business benefits. First, We can use it to target, use it to targeted data sharing. That means that you can expose only the slice of data that a team needs. Without duplicating anything. And with paired with lake formation permission, it makes um secure sharing much simpler. Second, Multi-service analytics. Create a view once. And use it across Athena. EMR Glue Even red shift No extra pipelines. No data copies. The 3rd Centralized compliance. Because all you live in the catalog. You get unified cloud trail auditing for easier reporting and regulatory visibility. All right. With this, let me hand this over to Giovanni, who will take us to the next session, next part of the session. Thanks to Ira for the first part of the talk. I'm going to make the presentation a little bit more dynamics and provide some visibility on several aspects that we just released. However, I'm continuing picking up the torch from Vivac and talking about the second pillar that is unification. Now there are several challenges to unification. Most of them fall back to a simple root cause. Several versions spread across EMR, Glue, and Athena. Several different versions of library like Iceberg, Delta Lake, Hoodie spread across these services which bring. Several challenges. Why do I need to maintain different configurations across the board? Why my functionality that is working in EMR is not working in? Why do I need to rewrite the job and so on, which brings a concept of silo. Between services. As I mentioned, In AWS we have 3 services running on top of Spark Glue AWS Glue, EMR with all the flavor, EKS, EC2, and Serveless, and Amazon Attina. All of them run on top of Spark. And I'm glad to announce that we just made a unified data experience. Last week. We released EMR 7.12 with Spark 356. Iceberg 1.10 with V3 support, Hoodie 1.02, and a bunch of other libraries. The same version, the same flavor of Spark is now running in glue 5.1 that got released last week. And similar to Athena, released last week, we have refreshed the code and it's running the same flavor. No more silo, no more query right. Consistent performance, consistent experience, consistent features across the board. Which leaves to you guys only one decision where do you want to run your pipeline? Do you want to run in GluL for interactive service? Do you want to run on EMR because you want to manage your own cluster with the KICC2, or do you want a fast speed up? With Amazon Attina, that's the only decision that you guys have to make. To run your ETL pipeline. No. No more breaking changes. No more creative rights. Now Unification is not only unified version across the board. is also stepping away from customizations. And here I do want to provide some historical context. Imar released. Early on, a storage connector called. It was customized because there was nothing open source that matches with it. After EMR released EMRFS, open source, the Adua ecosystem created a connector called S3A. However, history was never at par with the. However, at that point we saw in the recent years, we saw some challenges. Customers coming from on-prem where they have S3A. They get challenges. When they went on board to AWS. It mainly goes back to the case that Vivek mentioned at the beginning of the talk about the configuration mismatch. At the same time, we saw that the entire community, plus we as AWS, we started contribute to 3 in open source. I'm glad to announce. That with the MR 7.10 and of course glue 5.1 and Athena, we made S3A the default storage layer. At this point we have open source alignment, no more. Customization. We standardize the configurations.s3. the entire name space. We have unified authentication and optimized committers, and we will see at the end of the pillar, what are the benefits of those. But this is great. This is great. We step out from customization, but if that's all? S3 comes with new capabilities. S3 is based on AWSK2. Which gives access to all S3 storage classes. Of course, can read S3 standard. It adds the capability to read S3 Express one's own for low latency object retrieval. It gives access to S3 on outpost if you have your own storage on-prem. And my favorite feature. Of all, glacial access. With all the 3 storage layers. Storage classes, S3 glacier instant retrieval, flexible flexible retrieval, and deep archive. And usually customers use glacier for cost saving to store. Archive data. Now why is glacier is important for E? We have several use cases compliance, regulation, and so on. For example, you want to run an audit for some data that is more than 10 years old and is a glacier. What used to happen was that you had to restore an object. Convert that object to S3, standard, and move it to the right directory, because with the MRFS and previous versions of S3A, you could not cross-pollinate folders with glacier and standard objects. But now thanks to this capability you can cross pollinate it, you just need to restore the object and configure your job. With a simple configuration read restore object. Riddle, skip Glacier. Read only the restore. No more data movement. Just restore the object that you need. Again, this is my favorite feature in S3A. We wrote a blog. Feel free to check it out. The link will be at the end. But now, We mentioned about open source. We mentioned about new capabilities like glacier. But that's all The best is yet to come. We made AWS S3A faster than EMRFS. And in comparison, of course, faster than open source S3. This is a simple test where we run a standard benchmark, TCPDS across the 3 storage layer. As you guys can see. S3A, that is the purple one, or the 3rd column, is the fastest one and also the most cost effective one. We also made S3A. Not only faster but also cost. Efficient by reducing the number of get and less calls to 3. But these numbers are great, but not the best yet. The best are Thanks to our committers. In ETL pipelines, the last stage, load, it has a right operation, because at the end of the day, the data needs to be written on an end table. AWSS 3A. Is up to 15.8x faster for dynamic rights. Which is great for your health. And it is 3 times faster than the. Also in case of a static insert. We made it around 10% faster than the MRF. And with this, I want to talk about the 3rd pillar. That is performance. Because unification is important. Security is extremely wonderful. However, you want your job, your ETL pipeline to be faster, because that's the first thing that you notice. You want that dashboard to be refreshed every hour. You want that report at 8:45 a.m. For your 9 a.m. meeting you want. That job to finish within ILA of 4:45 p.m. for your end of the day email out. There are several challenges here, but at the end, The real challenge is I want my job faster. I want my pipeline within a specific. I do want to talk about a database construct called materialized view. Matern's view can be seen as a cache. You create this materialized view and there is an external component that refreshes this cache and query engine. Can use it to optimize their query execution. Why materialized view is important for? It's important because reduced overhead across all the stages. During extraction, If you can create a Excuse me, you can create a pre-filtered data thanks to the view to minimize data movement. During transformation, you can create a view to cache complex intermediate results to avoid recomputing them. During load you can maintain reporting views with schedule. Schedule refreshes. No OK, we got the point of materialized view, but what this has to do with AWS PARC? I'm glad to announce that literally last night, AWS Spark released materialized View. I'm going from bottom up. Thanks to the consolidation and unification, these materialized views are integrated with all the AWS services described before EMR, GL, and Athena Spark. After you create a materialized view, this materialized view is fully managed by AWS. You put a refresh rate and the NWS infrastructure will refresh that cash. Everything is managed We gave AWS the capability to understand. The query or the job that you submit can benefit from existing materialized view. If the answer is yes and the cache, the materialized view is updated, is refreshed, Spark Spark will rewrite your query, your job to speed up performance up to 8x faster. The materialized view itself is built on Apache iceberg format. Which means It's an iceberg table. And other engines that do not understand materialized view will see this as an iceberg table, and you will be able to use it as well. And finally, it's SQL base. And creating a materialized view is as simple as create a materialized view as select. In this case We want a customer's name. With the amount of orders they did and how much they spent. This is a materialized view, however, it can be seen as respectable. Now I mentioned that the materialized view comes. With automatic refresh done by AWS infrastructure. What if you want to run a refresh? Because your job requires it. We have added a new syntax called refreshed materialized view. That will trigger a spark job to refresh your materialized view. Without waiting the automatic refresh. Now, Materra's view again are a great database construct and during rebind there will be several mentioning about this again we just released it last night. And I do want to shift the gear. And provide some announcement soon. However, These announcements are related to optimized optimized pipelines. And something we have observed and also by talking to customers. are 4 patterns. Iceberg Jason, encryption, and 3 manipulations. All of these are common or are starting to get common in ETL pipelines. Let's start with Iceberg We started seeing a lot of customers using iceberg as Starting point for your ETL pipelines or endpoint during extraction or during load. With the MR 7.12. We made Iceberg with Spark run 4.5x faster than open source. In this slide I put EMR 7.5 because I was on stage literally last year explaining the same slide with just EMR 7.5 and as you guys can see, we also improved compared to last year. And this is for extraction. What about Lo? With DMR 7.12 and of course as I mentioned before, unification, Glue 5.1 Amazon Attina. We made simple, right operation like merge, update, delete, insert more than 2x faster. Than the respective open source. This benefit will also apply to the view as they are based on top of Spark on top of iceberg. The second pattern we see in ETLs are reading raw data coming from. Format like text file and so on. Because these are formats that come from sensors, machinery, and so on. We took a look at Jason. And with the MR 712, glue 5.1, and so on. We made The performance of Jason Reader 20% faster than 7-Eleven. No application Changes required. You just need to upgrade to the latest version. And this improvement will be seen across the board. Because we have improved how Spark Reader reads Jason itself, so. Every job that we read from Jason will see these benefits. The 3rd pillar. is encryption Something we start seeing. And we see specifically in jobs. Is that customer enabled encryption because they handle data. That that needs to be secure, so they enable it, they enable it at rest and in transit. However, encryption comes at cost. It has an overhead. To encrypt and encrypt data. And we have validated that debt overhead is around 32% of the entire benchmarking. With 7.9. We reduce that impact. We reduced that impact to only 6%, reducing the overhead by 85%, and the entire, the entire benchmarking ends 20% faster. We just released a blog a couple of days ago that talks about this. We have done this by improving the encryption itself and the shuffle performance. Therefore, any job that uses encryption will benefit by this. Last but not least. string manipulation. String manipulations are extremely common on pipelines. However, standard benchmarking like TCPDS, TCPH barely use them, and these are like substring, concat, trim, uppercase, lowercase, and so on. And why they're really common. In the in the industry they are common because usually during. Pipeline, you want to do a general tax clean up like trimming waste spaces. Correct cases, remove noise, and so on. In telecommunication, you want to parse your phone number, a phone number by putting the country code +1 + 32, 39, and so on. The area code by parenthesis. And the hyphen between the 6 and the 7 characters. You want to standardize. The ID and this is due to government and public sector or you want to mask the data like credit cards or birthday or whatever else. As well as you want to standardize the name like the first letter in capital and all the rest lowercase. We took a look at them. And we made them faster. In this graph we show The result Of the of the performance improvements from one character to 100 characters. And these are linear, so by growing the size. The improvement will be even higher. However, what we see in pipelines are usually between 1 to 100 character characters as input. Here we see upper and lower case that are the most common in pipelines get a burst of 10 to 12 x. Dream. Also gets a nice boost around 4.5x to 3x and then we have A chunk of other functions like concat, substring, and so on that they are not in the graph and they get an improvement. Between 10 To 70 80%. Which is still great, but it's not as great as upper lowercase and trim. And we have 2 more functions. That gets so much improvement by applying the same methodology. And we had to put in a different graph. And they are length and reverse. length At 100 characters. Gets a burst of 96 X, at most 1 X per character, and as you guys can see, it's linear, so it will continue to grow. A length is really common to validate if, for example, a phone number has 10 digits or. A specific name cannot be passed 25 characters and so on. It's extremely common in pipelines. The other function is reverse. With a burst of 96, sorry, 56x improvements. With this, I want to go to the summary. And we have seen how in the past year or so from fragile pipeline. We make them futureproof. Security is not more add-on, it's built in. With native integration of lake formation. And its performance. Spark is unified across all AWS services that run on Spark. One foundation to run them all. And we have scalable performance. For your ambition. With this I leave to some resources. Some of these are the blog that we spoke about during the talk. There will be other, um, other blogs, so, uh, feel free to ask at the end of the talk. Thank you so much. For your time I hope you How do you optimize your ETL pipeline. Thank you.