---
video_id: cdFr5L8dhYE
video_url: https://www.youtube.com/watch?v=cdFr5L8dhYE
is_generated: False
is_translatable: True
---

Hello. Welcome everyone. To start, I'd like to start with a quick question. By a show of hands. How many of you used your mobile phone to make some sort of payment within the last week? As I guessed Practically everyone So payments are an integral part of our daily lives. Whether it is paying a few dollars for a cup of coffee. Or like in my case, sending a whole bunch of dollars for my daughter's college education to her university. Now, behind the scenes, that is a complex and comprehensive technology infrastructure that makes this money movement and these payments possible. And in this session today, You will learn how FIS collaborated with AWS to build a high performance and massively scalable payment solution on AWS. My name is Samir Sharma. I'm a lead principal technologist and architect at AWS. And today I'm joined by Ad Sterling, vice president of Money Movement at FIS. And Nlango Sundarajan, who's a senior delivery consultant at AWS, and together we will share some key learnings and insights from our journey so that you can take these away and apply as you build your own mission critical applications on AWS. So let's get started Here's a quick overview of our session today. We'll first start by telling you about FIS and the critical role FISS technology plays in global financial services. And aid will share with you why FIS decided to rethink payment solutions from the ground up. Then we'll show you a quick demo of instant payment processing. The goal here is to show to you the complexity of the processing that happens to make every single payment possible. Then we'll touch upon briefly the requirements, technical requirements and challenges in building such a solution. But most of the time in today's session we'll spend diving into the solution. We'll share with you the architectural design decisions that we made. The trade-offs that we balanced and the technologies and services that we use to build this solution. We will also take a deep dive so that you can understand how we approach building massive scalability, high performance, high resiliency, and end to end observability. All of these are key pillars of building business critical applications on AWS and hopefully you'll learn some unique insights as to how we did it. Then we will pivot and talk about a few things that are enabled by the architecture that we have designed. In the road map and finally we'll conclude by sharing some of the key learnings and lessons that you can take away. With that, I'd like to invite Aid on the stage. Thank you, Samir, and good afternoon. I'm A Stirley. I'm the global product manager for enterprise payments in FIS, and I'm co-inventor of the solution that we're going to show you today. I know everybody is really keen to see the technical deep dive solution beats, but if you just bear with us for a little while, I wanna try and give you the why we did this, um, based on our history of, of providing payments in the uh payments ecosystem around the world. Every day, payments move silently, securely across the globe. In fact, trillions of dollars move silently and securely across the globe. FAS is behind a lot of that movement. We're a silent, invisible player in the background of processing those payments. When Samir transferred money to his daughter today, when you guys bought coffee. When you checked your retirement balance, it was very likely FIS was running behind the scenes in that area. And we think of money in 3 stages. Money at rest, money in motion, money at work. Money at rest, being the secure. And an appropriate core banking storage of the accounts that you use every day. FIS processes 58%. Of large and retail organizations in the United States with our core banking and payment and Digital solutions Money in motion. Very much around moving payments, treasury, risk management. Today we process 16 billion transactions annually. Through our card and non-card networks. And then money at work. This is where we're using trading and lending and wealth and retirement. And we're managing 18 trillion assets for our customers today. So we're not a startup, we're a big company, we process a scale. Over 95% of the world's largest organizations use us globally. We move over $16 trillion of payments annually. And we work with over 50% of the world's most innovative companies. And the scale piece of this is really important for everybody. If you imagine this, the years that we've been working, around 60 years overall. You can start to see how we've had to evolve that of the payments and on those processes over time. And that makes us and seeing us go through many revolutions of the payment ecosystem. And this is important because We're doing this again. And partly this is because payment. And experience and expectations are changing. Everybody in this room is is making payments over their mobile phone. Everybody in this room and outside wants things to be faster. And I've got a great story to tell you that just happened to me this weekend to give you an example of why our expectations have changed. For some reason, I accidentally overpaid a credit card. And I want to try and close the account that was associated in the banking because I'm moving banks. I phoned up. I was told to call back between 9 and 5. In a weekday. I called back and the base the option was, well, what we'll do is we'll refund you the the the amount of money on your credit card and we'll send you a check in the mail. Now that means I've got multiple processes I've got then got to do. I've got to receive that check, I've got to go down to a branch, I've got to put that into a different bank. So it's not automated. The second option was transfer the money to an account and then up to you to move your money out of that account. And again, I'm into that non-automation. So we're seeing a disappointment in the way that we that we want to actually move money. I wanna be able to take money out of when I want to, I want to be able to transfer that as quickly as possible. And I want to do that in an embedded, constructive way when I'm not using multiple apps in multiple different ways and taking my time to make that process. So our expectations are causing a revolution in payments that we want everything to be done really quickly. And this causes problems. It causes problems for the financial institutions themselves. If we think about Rails, payment rails. The traditional batch types of pavement rails that we're all used to are now changing. They're moving more instant into instant rails, but they're being added to. We're moving on from ACH and wire and other forms of payments, and now we're seeing all the other types of payments come to fruition, including instant payments. We're seeing things like Fed now and TCHRTP gain traction. We're also seeing the additional payments coming from things like Venmo and Zelle and other types of payment, payment initiated types. We also expect things to be always on, so as things operate in a bank which has traditionally been very much a batch oriented service, or a 9 to 5 service, now we're always on. I wanna make payments any time of the day, doesn't matter when they are. And that changes fundamental batch operations or fundamental operations within within an institution. And if we think about how these were all wired previously, lots of payments were wired into digital environments, they were wired into core environments, and it's like spaghetti. And we're only adding to the problem when we bring new rails forward. Like digital currency. And the way the banks used to win, they used to do it by rates. They attract you by rates. But the trouble with that is now experience is taking over rates. The experience that we have is more important. The faster that we can make things happen is more important for us. So we're seeing experience win over rates. We're seeing the large big banks win that game because of the experience they're providing. And of course we've got to comply with all the regulations and the compliance and the risk and everything else that goes forward with that with that payment ecosystem. So we're seeing a change, a change in our behavior, and now we're seeing the problems it's causing. In the institutions. In FIS, we decided to rethink the way the payments are made from the ground up. This was despite us having lots of different payment systems. We had lots of legacy payment systems. We had ACH silo systems, we had wire systems, we had real-time systems. We deal with bill pay, we deal with Zelle. We're a major Zelle provider. And we, we do that in very much isolated cases because our clients needed those rails at the time that they needed them. But rails are table stakes now. The most important piece of the big brain of the power that we now need is in orchestration and execution. We need more flexible orchestration and execution to be able to determine how payments should be made. What is the fastest, safest, cheapest way, and the and the strategic way that they that we want our financial institutions to be able to make payments. Those are the things that matter. The value drivers in our liquidity. Risk management. They're not about moving money. And that's because we're able to move money in so many different ways in the choice that we make. But we also need to think about how We need to move money in a more intelligent way. Why, for example, do I need to make a decision whether I want to put something over ACH wire, instant digital currency? Why can't that be automated? And why can't I have that offer to be automated or allow me to make that that call? If I wanna make, and I wanna classify a payment as being, I wanna make it now, that's real time. If I wanna make it tomorrow, that's ACH. If I wanna make a high value payment, that's different. I wanna move money over a particular state coin, that's digital currency movement. It shouldn't matter what they, what they are. And the intelligent piece of this is the more appropriate. Area. So, rails aren't plumbing, rails, rails are just always going to expand. Everything's about orchestration. Everything's about execution. And we have to think about, therefore, how do you bring all the rails together. How do you orchestrate those from single exit points, entry points? And how do you make sure that your decisioning is done in real time? You have smart routing. You have intelligence behind the decisions that you're making. You're providing the capability of those things to be programmable for the institutions that we work for, and you're providing the mechanism to have inline risk management of fraud. So we thought About the payments from the ground up, despite, as I said, us having lots of payment systems around the globe. And we got to an aha moment. Probably the beginning of 2024. When working together with our AWS friends, we came up with a solution. They called the Money movement hub. And really, this was about putting all of those practices in place. The challenges that we were facing because of the expectations that were changed from all of us. And we the the challenges that we saw our finance institutions having that we had to go and provide fixes for and a different way of making payment processing. So we built a solution which was in the cloud, built from scratch, designed for multi-tenancy, designed to provide multi-rail access to our, to our customers. Designed to interact to multi-digital environments and designed to integrate to multi-core environments, both in FIS and outside of FIS. And that was a specific challenge for us. Given the fact that we've traditionally built these single isolated rails, which were extremely successful. But also building this for 1000 institutions. And moving millions and millions of payments every single day across those rails. The um the biggest part of the, the process in this is very much how do you bring together so that you protect and you protect the future of our institutions. So they don't have to introduce new rails, and we'll talk a little bit about that later on. I'm gonna show a little demo. I wanna show this from a perspective of execution, not perspective of what you see from a payment initiation. There are lots of people who make payment initiation. So bear with me a second while we bring this, bring this up. And you should see this. Well, you should see this. I'll get my alango can go fix it for me. While we're talking about the demo, It's really important to understand what we've put together, so we created a universal API for all payment types. That universal API is exposable to our institutions or their customers. That was critical for us to basically take any single payment type and drive it through those those areas. So we use a standard paying one message, if that makes sense for anybody working in instant payments, but we made that work with ACH and the other payments types. We also, however, creating SDKs and designing those from a payment initiation so to launch those SDKs into our digital providers who are much more turning now into what we call portal technologies. So important for us was putting something together that showed. Behind the scenes, how things would operate from an execution perspective. And what I'm going to show you now is more about that execution and that orchestration than actually the initiation of the payment. So, let's take a standard UI looking forward transaction transfer. You see your balances, you wanna make a transfer, you decide to basically go and click on the transfer, I wanna try and move money. You'll see a standard type of transfer money movement screen. We're trying to capture the information from the customer, we're trying to capture the information from the, the perspective of how much we're paying, we're trying to capture the information of who we're paying, etc. In this case, we're paying Jane. And we're just transferring money for the purposes of paying for tickets to reinvent. I'm gonna show you what's happening behind the scenes in this, in this area. So this is the API and you'll see how um it's basically constructed, standard pain 001 message. But you'll also see that if I go in here and I wanna make some changes. And I say I want to pay $100 and $150 you'll see the dynamic API being updated. And I'm going to put Disney World tickets in this one, but I'm actually gonna pay. I'm going to pay for reinvent. And you'll see how that's dynamically changed in that API. And again, the purpose was to expose that out to everybody so that this becomes a standard way of interacting with us. Whether you're doing a A ACH file in terms of an ACH set of transactions and mass transactions, or whether you're doing a single instant transaction or a wire, etc. This even works for the digital currency, slightly different screen, you'll see a lot of FX rates in there that we've got, but it kind of gives you the idea of what's going on. And Then I'm gonna scroll down, you'll see there's a semi-transfer. So as I submit transfer, we're doing very, very quick validation check. We're making sure that that transfer can be done. You'll see, as I scroll over the areas, you'll see the the areas in here being updated. You'll see the name of Jane and the changes that I've made in that payment transfer. So I'm gonna make a transfer. Things happen incredibly quickly in payments. We have to transact in less than 5 seconds for instant payments. That means a conversational with the com with the rail itself. I'm basically asking the question on a scan. I want to send you some money, do you accept it? But before I even get there, there's a load of things that are happening in the background. I'm gonna go back in a second and show some of these things. And then we're on the other side, we got the receive. So now it's about what am I doing as the receive institution. So let's go back and have a quick look at those things. So, all the validations that we did, right, as part of that that service. We checked the core banking systems, so we actually went out from APIs, calling to the core bank systems, is this account valid, does it have the money to make sure that transaction can happen? Is the status of that account in a good status so that it allows that to happen? We also looked at, OK, so what about fraud? Well, we wanna make sure that we're we're checking the fraud mechanisms so that we're checking for transaction screening, we're checking for AML and we're checking it for transaction monitoring. And we're doing that in line in the transaction. And remember that time frame is 5 seconds. Back and forth to the scheme. We're also posting to the core, so we're basically saying, OK, if I want to basically move the money that I did for $100 I now need to be posting that into the core as a debit for the send transaction. And again, Everything happens to happen with the back and forth to the scheme within that 5 seconds and that scale. On the receive side, you'll see that we did a very similar thing. So on the receive. Especially if it's not on us, on us is different, but if we're doing a received transaction it's coming from outside the scheme, then we're also going back into the account. We're checking that that that account can receive that credit. We're checking if there's any particular issues with that account receiving it, and we're checking the fraud of the incoming item on both the sender and the beneficiary of those of those transactions. So there's a lot going on on a very, very simple use case of an instant payment. And all of that has to comply with all the regulations that happen across the multi schemes and obviously within time frames that are set for the experience that needs to happen. So, I'm gonna hand this now back to Samir, who's gonna go finally, you say, into the technical details of the solution, and we'll speak a little bit later about the outcomes and where we, what we received and what we achieved. Thank you, Ed. So I have had the privilege to work with FIS and rest of the FIS team with aid and rest of the FIS team since the day they decided to rethink payments from the ground up. So what we did, the very first thing we did obviously was we started. We started collecting all the requirements and as we always do we collected all the functional and nonfunctional requirements. Now there's a humongous list of requirements for a solution like this, but let me just highlight three key technical requirements that we had to meet which define the key characteristics of the solution that we built. So first and foremost on the performance. Now each payment rail has its own performance requirements, but for us for money movement hub, we set the requirement to. Not only execute but settle, settle the payments in under 5 seconds. Now for payments, this is instant payments and real-time payments. For scalability Money movement hub is a multi-tenant solution. What that means is many, many financial institutions are simultaneously sending payments through money movement hub, which means at any given point in time the payment volume can be exceptionally high and it can also go down exceptionally low. So the, the solution has to scale dynamically and rapidly up and down. So we designed the system to process over 1000 payments per second, right? And finally on availability, obviously we said this is not a 9 to 5 solution, this is a 24/7 365 days of solution, so the requirement on availability was 99.995% availability in real terms what this really means is we can afford less than 30 minutes of downtime in an entire year of operation. Apart from some of these key requirements, we also had to overcome some key challenges. Payment solutions don't live in isolation, they need to interact with existing banking solutions. And believe it or not, a lot of banking solutions still use a lot of legacy technologies and still reside in on-prem data centers, so our solution had to seamlessly integrate with on-prem banking solutions while meeting all the requirements I highlighted. We also wanted to make sure that as new payment rails come on we can easily extend the system without a complete overwrite, which was the old way of doing things. One example of this, we are seeing digital currency, stablecoins, as becoming one form of new payment trail. And we will talk about how that is enabled by this architecture as well. And finally, because there are so many subsystems involved in a solution like this, it is critical for us to overcome the challenge of end to end observability. You know, we only care if the payment we initiated from us was received by the end receiver. It might touch many, many subsystems in between. So end to end monitoring's job is to collect and make sure there is complete observability from the start to the end and up and down the stack. Now I'll show you a very high level functional architecture which highlights some of the main subsystems involved in such a payment processing solution. At the heart of this solution is this thing what I call as the payment processing pipeline. Essentially from financial institutions the payments that come in go through a uh pipeline process and here I'm showing only few key subsystems like payment validation you saw that in the demo payment orchestration finally payment execution on the and and then the payments are sent over one of many payment rails and we discussed earlier on the payment rails could be fed now real time payments, wire payments, swift payments, and now maybe even digital currency based payments. And while the payment is being processed through the payment processing pipeline, we not only have to interact with banking systems that are uh located in on-prem data centers, we also have to do real-time fraud and risk management. Now this is different than payments that used to settle in, let's say 2 days, 3 days, or even after 24 hours where you have time. Here we literally in real time have to evaluate if the transaction has a potential fraud risk. Now, having seen this functional architecture, I will slowly build for you the AWS solution. How does this solution look like on AWS? So let's take a look at that. Here we have the AWS cloud. Money movement hub solution has been built and is running in production on AWS. But outside of AWS there are a couple of entities. First and foremost are the financial institutions who are the users of this solution now as we said, the solution exposes APIs and the financial institutions interact via APIs. This interaction can happen over multiple channels. I'm showing here public Internet with secure HTTPS based API interactions, but some financial institutions are already on AWS. In that case, they will access money movement hub over AWS's own secure private network. They don't need to go over the Internet. Secondly, we have banking systems in on-prem, uh, data centers. So given the low latency, high performance requirement of our solution, we decided to use AWS Direct Connect. And for those of who are who are who are familiar, AWS Direct Connect offers a. Very high throughput, very low latency connectivity between AWS and your data centers, and that's what we are leveraging with for on-prem connectivity. Then what we did at a very high level, we chose a multi-account strategy. What does multi-account strategy give you? Multi-account strategy essentially enables you to isolate different big sub subsystems of your overall solution into isolated accounts. AWS accounts offer a beautiful isolation boundary. It can be a compliance boundary. It can be a fault boundary. It can be a security boundary. But for us there was one more important thing we had different teams working on fraud and risk solution, so we had a separate account for them. We had a separate team working on integration with different payment rails. We had a separate account. Then we had a core team working on the main payment processing pipelines. Each of these teams had a different operating model. For example, the Payment rails team was a small team that did infrastructure and application all in one team. They could work that way. The payment processing team had a dedicated infrastructure team and a dedicated applications team, so a different operating model. Now having multiple accounts allowed us to have these different models work seamlessly, and we interconnected all the services again using AWS Private Link which goes over AWS Private network. Now let's take a look at the heart of the solution, which is the payment processing. First, let me touch upon the compute layer as we highlighted, the compute layer is going to be critical in payment processing because it has to have high performance, high scalability. At the very highest level, the design decision we made was to use what we call event driven design, event driven design with microservices which are decoupled and completely asynchronous processing. What this architecture allows us to do is to independently scale different microservices which allows us immense horizontal scalability and also creates resiliency because one service can fail at the same time all other services can continue working. Now we chose to use. Containerized compute to implement these services and as we all know EKS or Elastic Kuberative service on AWS is probably one of the most popular orchestration solution for containers so that's what we are using uh to orchestrate the containers now here I'm showing you three of the key um microservices, but there are dozens and dozens of microservices in the overall system. Here I'm showing you payment validation, orchestration, and execution services just as an example, but all are built using Kubernetes, uh, built using container and orchestrated using Kubernetes. Now, not only did we use these native AWS services, we also used a few open source technologies like Kada for the node scaling. We used Carpenter. Cara for pot scaling, carpenter for node scaling, and we also use another open source tool called Conductor for Workflow management. And soon we will dive a little bit deeper to explain to you what these technologies are and what they offer. Now any high performance payment solution not only needs high performance compute, it also needs a high performance transactional database. So for database, we chose Amazon Aurora with a Posgress engine. Now, Amazon Aurora is a cloud native database that is built from the ground up. To support high res uh high scalability and high performance, it offers up to 15 reed replicas that you can have within the same region. OK, it's IO throughput is extremely high, and you can get IO latency as low as a single digit milliseconds. So for all those reasons, Aurora Postgress was the perfect database, transactional database for our solution. But we weren't just satisfied with having a high performance database, we also felt the need for a caching layer. Cashing layer that provides not millisecond but microsecond latency and for this we chose Amazon Elasticash Reddi to build our in-memory cash and in-memory cash is used by the entire payment orchestration workflow, especially to keep real-time state, especially as we scale in real time to thousands of payments per second. And finally, not only are we processing payments in real time, we are also generating a humongous amounts of data. We need to store and process this data for compliance reasons, but we are in the age and era of AI. AI is fueled by data, and so for all the good reasons we decided to build an S3-based data lake to store all the data generated by the system. I mentioned to you we are using an event-driven design, which means there are a lot of events being generated by the system and consumed by the system. To process these events, to move them from producers to consumer, we chose to use Amazon MSK or managed streaming for Kafka. And for all the non-real time processing, The data that is sitting in S3 buckets, we use AWS glue. Again, these are very standard services, but for a high performance system, these were the right choices for the solution. Now observability I mentioned to you was an important element we had to address. Not only are we using all the capabilities of Amazon Cloud Watch. For observability, but we're going to highlight to you one unique capability that some of you may not be familiar with. It is a relatively newer capability called application signals. This allows you to monitor the application level, not just the infrastructure level, meaning service level objective monitoring. We'll talk a little bit more about that as well. And finally, to round out, we all know the system like this needs to be extremely secure, so we use Amazon. Uh, Security services, AWSS security services like KMS, Secrets Manager, and HSMs. Now that you have a picture of the overarching solution, I would like to invite my colleague Elango to really dive deep into how we address scalability, performance, resiliency, and observability. Elango. Thank you, Samir. The solution that we have to build has to provide scalability, performance. Resiliency and end to end observability. Let's dive into all these aspects in the subsequent slides. Mannu woman Herb is written in Java 21. And the front end is written in angular. The entire solution is deployed in EKS. To provide the massive scalability that we require, we use scalar, the pod out of scalar. Cader looks into two metrics, HATP request count metrics and the elastic gas que dub metrics. HTTP request count metric is a direct correlation to the number of payment APAs that we get in. Each payment API as we saw, is a complex orchestration. For that, we use open source conductor. Conductor is an orchestration engine. It converts all those orchestrations into workflow. Each workflow is divided into multiple smaller units of work called task. These tasks are put into rescue. And the QW is the number of tasks that we have to do to complete a payment. So Kera looks into these two metrics, and based on that, it scales the pots. We want the parts to be immediately schedulable and start running. For that, we use warm pools. And in the warm pools we run low priority parts. And all our money movement hub parts have high priority. So Coconut is using the priority class, preempts the low priority part from the warm nodes, and then schedules our high priority MMH parts in the warm nodes. So our parts are immediately schedulable and start running. This solution works for most of the cases. In case where we need additional worker nodes, we use Carpenter, which quickly brings in additional worker nodes and attaches to the ease cluster. The next problem that we have to solve is providing enough IPs for our parts. For that, we use custom VPC CNA plug-in. We use the secondary non-routable IP ranges for our ports, and the worker nodes run in a routable IP space. We use last 20 cyberblocks for our non-routable parts that gives us enough IPs for our ports to scale. With respect to application, we built our application using open closed principle, open for extension, closed for change. Our business demands a lot of changes with the application. As you also workflow payment is a complex orchestration of changes that we have to do. One such workflows here. You see how we are using parallel tasks and how we have multiple deciders, which changes that workflow based on multiple parameters. And conductor helps us a lot here. Conductor also manages multiple versions, so we can use any version of workflow. And if there is a problem, we can roll back to an older version too. Our operation teams love conductor. They can go to conduct every way, monitor what's happening with respect to all the workflows. They can rerun any workflow, so they kind of love it from the operations perspective. When it comes to performance, we look into performance in two aspects. One is part startup time and data access. For part startup, we use ball rocket, a purpose-built container wires. We also try to keep our application docker image small. We use multi-stage docker built to add only the real, add only the libraries that were needed for the runtime. We also carefully evaluate all the Java libraries and add only that's really necessary. This helps us to keep our docker image small. We also use spring's lazy beam loading. To load only the necessary beams that are needed for the startup. These techniques help us to start our pod in under 4 seconds. For data access, we created custom spring annotations to send all the Red sequels to the Aurorare replica. We use 2 to 3 reed replicas, which provides enough performance for our reed sequels. Anytime when we need a fast write, we use a write-through gas, wherein we write everything to the elastic gas, and then we asynically update our aurora endpoint. We're also evaluating a couple of things. We're looking at spring ahead of time to compel the image and the build time. Also, we're looking at gravel VM to create a native images. One of the key features of bottle rocket is a seekable OCI which does parallel. And Docker image pool on demand. So we're also evaluating that, which, which all gives us the fast part startup. For multi-resiliency, as Sameen mentioned, any service that we take, we look for whether it provides a native cross-region replication. Amazon Aurora provides a global database, Elastic Ca provides global data store, and S3 provides a native S3 replicator. For Kafka, we use MSKA replicator. MSK replicator not only replicates the images, but also replicates the consumer offset. So if the application is running on another region, it can consume from where it stopped. We use Route 53's failure routing to route the traffic based on the region failures. We constantly evaluate this setup using AW's fault injection service. Fault injection service provides multiple documents where we can simulate the failures. For in places where we need custom script, we use laptop. As we have multiple microservices which are running in multiple accounts, we want to provide a unified observ. For that, we use open telemetry. A AWS Distro for Open telemetry is the library that we chose. A Auto Instruments for Java and Python. It captures all the observatory signals, be it locks, metrics, and tracers, and sends it to cloudwatch application signals. Cloudwatch Application Signals is a new application monitoring service in Cloudwatch. It provides app dashboard, telemetry. Tracing, root cut analysis, and end to end visibility. From the, from our account, we send it to the centralized uh uh observ account. We also use uh Firehouse and AWS Lamber to send logs and metrics. For the traces, we use trace propagation, where we use the same span ID for the multiple APIs, so we get a tracer bullet view of how the API is flowing through. I want to highlight one important feature of application signals, that's called service level objective. Service level objective tracks the objective based on multiple service level indicators. You can see in this example, we are tracking objective, which is to receive API latency, which is to provide a latency under 1 2nd. And we had to, we had to do it for 99.9% of time. So that's our goal. And the indicator that here we are using is HTTP latency. So this dashboard. Quickly gives us a glimpse of what is our goal, what's happening with respect to that objective, and when we last met the objective. Our leadership team loves this because at any point of time when they ask, hey, what's happening with your app, we just tell them this metric, and they kind of quickly understand the performance that we're giving to the customers. The banks also like it because it helps them from the complaint's perspective. The SLO can be obtained not only from EKS workload, but also from ECS and EC2 workload. These are some of the techniques that we use to meet the business needs. I'll pass on to aid. Thank you, Olango. So we're going to talk about outcomes a little bit and Really just quite incredible, the time that We, we saved ourselves in having the technology behind The ability of building this product in a rapid time. From concept to production was 9 months, um, but actual build, we started in August 2024 and we were in production in January 25 in pilot with our first two clients. This was pretty remarkable for FIS. Traditionally, in our payment systems, you would go and install, you would take time to actually deploy, you were actually on board, um, and that could turn into many, many months. So it was a key requirement when we first started development that we would build something that would be easy to implement and easy to deploy. And we turned the deployment of what we would call regular implementation into an onboarding activity. So that means that you build it into the product. By doing that, however, you cause other issues within your organization. FIS has a pretty strict change management implementation profile. It also has a lot of manual testing and a very Even though we run sprints and we run um very, very quick development internally, traditionally, we've not done things in the way that we had automation around these systems. Observability brings a new problem that we needed to look at how our teams were orchestrated, actually built for support purposes. They needed to understand the new technologies from an orchestration perspective. We hired full stack people into our support organizations so they understood the infrastructure as code, the automation that we were using, and the deployment methods that we had. We moved to a CICD model using harness of the CD to allow us to actually deploy quickly. And we're now in a state that we can deploy 3 or 4 releases into production every single week. This was a complete change for FIS. We had to go change up, change management solutions. We had to bring different people in from the perspective of understanding how to actually operate in that environment. And we had to introduce new automation of testing. So with harness and zephyr and cucumber, we created test automation. For the ability to actually run those tests on a regular basis, so that as we implement new parts of into into production, we continue with that, that, that, that automated testing to get us to that end point. That was all new for us. And we carry on So the outcome was effectively into production in January. We embedded the system. We went through hardening of the system. We went through our complete testing for performance perspective. We looked at our timings in terms of our performance throughout the chain meeting our SLAs. We also adapted some of our core technologies to help us get to those cores and more rapid way. So once we start built this solution, it's, it, it got little fingers, tip tips everywhere into the FIS ecosystem, and that allowed us to improve that ecosystem and then share that with other parts of FIS. So this became a really strategic play for us. And when you look at, you know, where we are now, with 30 clients in production, we've signed 100 clients. We're going to start mass migrations next year. We've now been adding different capabilities onto it. So we started with the instant payments. We started with Fed now and TCH. We wanted to prove that we could build something at scale that would handle instant payments. And then we're adding additional rails. So, what's next in the solution? So new pavement rails is a given. I said at the beginning of this that pavement rails are table stakes now. Adding ACH and wire are the things that everybody does. The more important part is how we embed them now in the orchestration process. How do we get intelligence in our environment? How do we make smarter decisions on decisioning for those, routing those payments? And so, as we speak, we're adding digital currencies, we're adding stablecoin. We had a partnership with Circle. That that that partnership is growing, and we'll be in live in production very soon in Q1 in the hub, and it's a good example of adding another scheme very quickly into the ecosystem. Once we got our orchestration and execution correct, now rail add additions are pretty straightforward. We're also building, as I mentioned, SDKs for initiation, we're building ways of improving the experience so that our customers and our clients and their corporates can make quicker decisions. And have that automated. The AI components of this become really crucial. There's two ways we're using AI. One is the improvement of payment exceptions. How do you repair payments that actually have got, have gone into a state where maybe a core was unavailable? Or the account wasn't unable to post. We don't want to leave any payment behind on whatever rail that happens. So this was critical in the way that we actually built the testing automation to get us to the point of being able to quickly add on the rails. And then AI used for smart routing and intelligent routing. Now I start getting really interesting about actually satisfying what I said at the beginning about customer expectations. If I wanna move money today, I don't have to tell you what rail it needs to be. Let the system go figure it out based on my history, based on how it's processed, and based on the configuration of my consumer base. All that comes from data. So the data lake that that's been built is a massive amount of data that's helping us and feeding us into the loop of payment execution. And as we go through that payment execution, we go through the fraud data, we go through the fraud execution, we determine what that fraud looks like, that becomes a wider across our rails, that becomes more important data, which feeds into the loop of the data exchange, which allows the AI to start making more decisions based on where those payment routes should be. The next piece of this is also international, so AI is going to help us crawl across border. So we're doing, we're adding push too hard, push to account on both of the two major networks. And then we're adding universal rails for the purposes of moving money in a more efficient way, whether that's through digital currencies for off ramping and on ramping, or whether it's, you know, a particular own rail on on on on us payment method. So these are all critical items that are feeding into the next piece of this generation, and the technology that's been built is allowing us now to rapidly add. We have, we have 11 initiatives for next year. The initiatives cover refinement of the AI models for the data for the data lakes so that they start making decisions. The build out of the payment exception processing and the payment repair processing so that they're using the AI to make our operations more efficient. Self-service to our financial institutions. We're actually enabling our financial institutions to come in and make the decisions they need to make rather than we making the decisions, rather than them having to go through manual operations. Remember at the beginning, I talked about that expectation around automation. So we're now starting to put the automation in place, which is now possible because of the way that we built the platform in the first place. So these were all great outcomes. That effectively allowed us to get to a point of being able to mass deploy. Deploy into production every single week. And now start our migrations from our legacy platforms and start to decommission those platforms over time. I'm gonna hand over to Smear to do the lessons learned. I think from my perspective, this has been a great collaboration that we had, and the technology has allowed us to move as quickly as we can. So, Samia So we in this session try to quickly walk you through our journey from the time FRS conceived of this solution to us gathering the requirements, architecting the solution, building the solution, and now we have the solution in production on AWS in a very short order plus it is scaling up with a lot of customers. So whenever we go through a journey like this, we learn a lot of lessons. Not everything works right the first time. So what I wanna do is share 4 key lessons from this journey. Hopefully these will be valuable to you. First and foremost, whenever you are thinking of designing and building systems that require massive scale. Start by thinking about event driven architectures, event driven designs. What this allows you to do is decouple various subsystems. Implement them as microservices and do completely asynchronous processing. These technologies give you massive horizontal scalability, which means you can add more compute and your system automatically scales. We did this as you saw in our solution by using uh Kubernetes to orchestrate our containers. Second thing I wanted to highlight was on. High performance To build high performance systems, especially those that have real time requirements, it's very important for you to identify what is your real time processing pipeline. Not everything in the system needs to be real time, but the pieces that need to be, we need to optimize every single step of that pipeline. Now in our solution as you saw we focused not only on optimizing the compute layer, not only on choosing the high performance database layer, we added high performance caching layer we also optimized boot up times we optimized the size of Docker images as well. OK, we started keeping warm pools of nodes so that when the system experienced load, we could instantaneously scale rather than booting some cold nodes up from scratch. These are all techniques that allow you to build high performance systems. Now, resiliency is exceptionally important for a solution like this. In AWS, most services actually inherently offer you multi multi AZ high availability, which means most services operate across multiple AZs within AWS. That's a core architectural design principle we have for a solution like this where we could not tolerate more than 30 minutes of downtime in an entire year, we also chose to build multi-region resiliency, which means. This solution simultaneously runs in at least 2 different AWS regions, for example, US East one and US West one. Now, sometimes people think that building multi-region resiliency is going to be either expensive or difficult. In this case, what we showed to you was by prudently choosing some AWS services. You can actually get a long way there because some of the services natively have capacity to do multi-region support for example, Aurora global database has ability to replicate across regions, so does. Uh, REDIS Elastic cash REDDS, so, so does managed streaming for Kafka and of course S3 has cross-regional replication as well. And finally, To build unified observability end to end, it's important to think left to right, end to end, as well as top to bottom, left to right meaning you cannot leave your on-prem systems. Like they're separate silos for observability, we need to aggregate all the signals logs, matrix, traces, bring them all together. That really is a powerful unlock for a solution like this. You get deep visibility into if things go wrong, where did they go wrong and how to then fix them. Secondly, many times we focus on infrastructure or technical matrix, but in this solution we highlighted to you by using something called application signals we're not only monitoring the technical aspects of the solution, the infrastructure and the application, they're also monitoring, uh, service level objectives which are more akin to business KPIs. So hopefully, all these 4 lessons will be useful to you as you build your own solutions on AWS. And with that, I truly want to thank you for sharing your time with us. And if you'd like to continue your learning journey, I have some QR codes here that you can scan, but please do take a moment to share the feedback in the mobile app. It, it does, it helps us a lot to improve the sessions in the future. Thank you very much.