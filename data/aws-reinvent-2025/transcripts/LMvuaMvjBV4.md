---
video_id: LMvuaMvjBV4
video_url: https://www.youtube.com/watch?v=LMvuaMvjBV4
is_generated: False
is_translatable: True
---

Hello. Good morning, everyone. I hope everybody's doing good. Thank you for joining our code talk today. Today, let's see how member owned financials can use agentic AI to improve their member experience. I'm Uli Ramanathan, senior solutions architect AWS. I'm, uh, Rud Balasubrahmanyan. I'm a principal solutions architect. Um, we work with, um, AWS, not for profit financials. So how many of you here are from member owned financials? Awesome. Is it credit unions, insurance? So how many credit unions? Just one, how about insurance? Awesome. You guys outnumber any other member owned, um. Which one is that? Oh nice. OK, cool. Welcome all. Um, you're definitely hardcore if you're showing up for the very first session, uh, of reinvent, um, hope, hopefully we can get you off to a great start. Um, uh, and, and get you a lot of value out of this session for your time, you, yeah, in today's session, let's see the art of possible where, you know, we would be showing how we can use agentic AI for loan origination system and which will, uh, have the member own financials process loan applications in a much quicker manner, thereby increasing member satisfaction and experience as well. And this solution we, we are going to show today is built on, uh, AWS Agent Core for which Mulla is going to talk about the features of it. Awesome. Um, so how many of you here have heard of Amazon Bedrock Agent Core? Awesome and have you actually tried working with it? OK, not yet. OK, so you're in a good place, um, so the idea for the court talk today is to demonstrate a working completely working end to end solution, uh, that was built using, uh, AWS strands agents. Anybody heard of Strands? Awesome. Uh, anybody using other frameworks, Asiantic frameworks? You, you can just call it out. Autogen, OK. Anybody Landgraf crew AI? OK, yep, there you go. So, um, no matter which agent tic framework you work with, and, and we are very aware there are many of them, and they're probably more coming, um, agent core, which is, um, kind of the. Uh, it was, uh, launched last reinvent and became generally available, uh, mid-October. Uh, so this is essentially a comprehensive gente platform on which, uh, so the solution that we're gonna be showing you today, this is the gente, uh, loan origination system, uh, that many of you in trade unions and banks can relate to. Um, this is just an example of, but it's a, it's an enterprise grade production, uh, almost connection ready, uh, solution that was built using, uh, strands agents and then deployed to Agent Core. So before we get into the architecture, we want to kind of give you a, uh, you know, let's get the basics out of the way, right? What is Agent Core so that when we go into the core and show you what, how it works, uh, you know, exactly what it does, so. Bedrock Agent Core, like I said, it's a, it's a comprehensive agenttic platform where, um, you can essentially, uh, deploy and manage your agents. Um, it's agnostic to the models that you, uh, use for building your agents. It's agnostic to the, uh, frameworks that you use for building those agents as well, um, has a few key components which I'll go through quickly. Um, there's agent core runtime, which is essentially the serverless component. It's, it's the compute where you deploy your agents as container applications, um, and then you have, um, agent memory, uh, so especially with. You know, with agents, you know, you might start off with a single agent you might grow into multiple agents, you could have multi-agent collaborations. So the idea is that, you know, uh, you have a managed memory component that you can associate with your agent, um, and this can be either short term memory. That enables those multi-tone conversations but also if you're working with multiple agents you can have longer term memory as well because this then spans, uh, multiple sessions across multiple agents. So agent memory code is essentially that capability of bringing that short term memory as well as short term, uh, long term memory, uh, to your agent applications. Um, and then another piece that we're gonna be talking about, uh, and that we used in our solution is Agent Core Gateway. Um, how many of you have heard of MCP? Does that ring a bell? Awesome. You want to call out what it stands for? Awesome awesome, yeah, so, um, Agent Core supports these, um, open source protocols, definitely, uh, model context protocol, protocol MCP, and that's really where Agent Core gateway shines. That's essentially your managed, uh, MCP server. Um, that you can use as kind of that unified interface that your agents can communicate with tools, um, that are set up as targets behind those uh that agent core gateway. And um you can have um you know you might have existing APIs in your organization you could have um you know other applications which you might uh make an API call to using uh a serverless compute like lambda. So essentially you can have a lambda function running as an MCP tool behind the gateway you can have your existing APIs so it supports open APIs. or Smithy models on the back end. So all of those existing resources can essentially be MCP filed if I might use that, uh, behind that gateway. So, uh, that's really what that is. It's that MCP interface, um, so we're gonna be talking about that as well. So, um, we covered agent run time, which is the compute the gateway which is that man MCP server, uh, and then the memory, and then there's the identity piece so. Any enterprise grade application definitely requires end to end, uh, authentication authorization, right? And that, uh, agent core identity is that agent managed identity service. Um, it centrally manages the inbound and outbound o. what that means is, you know, as you have your users applications that are accessing an agent. Uh, and the agent is making calls to all those tools or in our case we actually have, um, A2A, so agent to agent communication is what we're gonna be using in our multi-agent collaboration application that we're gonna show you. Um, that inbound and outbound op is centrally managed by agent core identity which is seamlessly integrated with the compute which is the run time. So all of those different pieces coming together to help you build a scalable, secure enterprise grade agentic application. So over to you. Thank you. Thank you, Madulla. And that was a great explanation. I wanted to add a couple of more things. You are also seeing something or less agent code browser and agent code interpreter. These are the tools Agent Core offers. Agent core browser gives you the ability for the agents to do browse websites on your behalf, then get the results and process them. Agent code interpreter gives you the ability for the agents to run complex, create and run complex code at the run time itself. What that means is you need not write code which an agent needs to call and maintain that code. You can just define and say what code you want the agent to create and run. The agent will do that. We will see in the example now. Yeah, so one other thing that I want to add quickly is um. Uh, you know, typically with, uh, agents and agenttic applications, right, you tend to think of them as black boxes, so, um, visibility, uh, you know, into what those agents are doing, that observability aspect, right? So the, the three pillars of observability, the logs, the metrics, the traces, all of those are super important so that you can trust the system that you built, and that's where, uh, agent core observability comes in. Uh, it supports hotel, so that's your open telemetry. Um, and it's integrated with, uh, AWS CloudWatch if you're familiar with that, uh, so you can, you can essentially see at the session level, at the, at the trace level, as well as the spats, so the, the granular, the, you know, very fine, um, the granular aspects of your entire trace end to end. Um, request, uh, flow, uh, and the response flow of your agenttic systems, uh, all of those can be, um, seen, uh, and dashboarded using the hotel logs and metrics that come out of agent core observability. So I want to point that out because that's a big piece of putting these agenttic systems into production. So awesome. Yes. It's a first party too. It's a first budget. Now, let's, um, I'll walk you through the architecture of the solution we built, then we'll show you a demo and then we'll walk you through the code of the agents aspect. Now here you're seeing the architecture where of the loan origination system, you know, we built using agent EKI. Let me walk you through that from left to right. On the left you would see the document upload, you know, where an applicant uploads there, uh, all of the documents needed for processing the loan. As soon as those documents are uploaded, they come and land on an NS3 bucket. And once they land on an NS3 bucket, a lambda even. Um, fires off and then calls the, what we call as the supervisor agent. Then the the supervisory agent here is tasked with the role of saying you are a supervisory agent who will be processing loan applications and one of the tasks the supervisor agent would have is it has to find out what other agents are there which can work along with the supervisor agent to perform this loan application process workflow and then work with them to get the whole task done. The supervisor agent then, uh, using the A2A protocol, reads the agent cards of all of the available agents and then finds out that there are 3 agents it can work with, the document agent, the credit risk agent, and the compliance agent. Once it decides what agents it can work with, then it creates a workflow and then as an action item it calls the document agent with the information of where the documents are located. Now, the document agent, you know, whose role is to validate the documents and also extract the information from the documents knows that it can validate the information, but it does not have the data extraction capability. Then again, using AOA protocol, it reads the agent cards of all the other agents and finds out which agent has the document extraction capability, and that's the Bedrock Data Automation agent here, and then gives it to that passes that information to that agent. Then that agent has been tasked with the role of saying you are only to extract information from documents using whatever tools are available to you. Now this document agent extraction agent then uses the agent core gateway and does a semantic search of seeing what tools it can use to extract doc uh and data from the documents. Then it sees that, you know, you, uh, it has access to Bedrock data automation and Bedrock Data Automation is an AWS service which extracts data from documents images in an intelligent way using AI. Now, here we are showing the art of the possible where the Bedrock Data Automation agent is accessing the BDA or the Bedrock data automation using agent core gateway, which is simulating an MCP what you would have. So in a real-world scenario, you may not even have a Bedrock data automation agent. You may have your own data extraction software running and the agent, uh, which is connected to the agent core gateway as an MCP server, and your Bedrock data automation agent will automatically connect to it for data extraction. Once the data is extracted, then it gives it back to the document agent, and the document agent then performs all the validation to ensure that all of the data needed for processing the loan is there. Once it performs a validation, it gives a status check back to the supervisor agent along with the data saying the data validation is passed or failed. The supervisor agent, as part of the workflow it built, it then decides if the data validation is passed, then it'll send it further down to the credit risk agent and the compliance agent for further processing. But if the data is not there, using the code interpreter tool as a tool, it'll again send an email back to the applicant saying there is information missing from your loan application. Please provide all of this information for processing the loan application further. Once the, all of the information is validated, then the supervisor agent, as part of its workflow, then will, using A2A protocol, then gives the information to the credit risk agent and the compliance agent for credit risk, um, processing and compliance and checking as well. Now, here, the credit risk agent, using the code interpreter as a tool, it calls the credit risk machine learning model, which does, which does the actual credit risk assessment. In this demonstration, this ML model is stored in an NS3 bucket that the agent builds code on the fly using code interpreter. To download it and then interact with it to perform the, uh, credit risk assessment. In a real-world scenario, you will be using the code interpreter to actually call a Sagemaker endpoint or wherever your ML model is and interact with it to perform the credit risk assessment. And the same way, the compliance agent using the code interpreter tool builds code on the fly to perform the compliance agent checks and then passes all the information back to the supervisor agent. The supervisor agent then gets all of this information and then uses its decision logic, which we have provided in the prompt as a standard operating procedure for the, um, supervisor agent and then decides to either approve the loan or put it under manual review or disapprove the loan. Once it performs a decision, then using the code interpreter again. And using Amazon's simple email service, it sends an email to the applicant informing the status and also it creates a PDF document of what the loan application was, all of the details, and uploads it to an NS3 bucket so that the underwriter can review that. Uh, document and then perform the final decisioning as well. So this way with this solution we are making, uh, the underwriter's job very easy. We're making sure when the underwriter, when he has the informa he has all the right information and he can quickly assess that information and whether approve or disapprove the loan based on the criteria. Now let's go ahead and see, you know, a demo, demonstration of this solution, and then we'll walk you through the code, what, how, and how we built that. I'll switch the demo. Yeah, just yes. So when you have the credit risk ML model. This is something, you know, can you also implement with the MCP because what is the future engineering that is to be done? How is it? And I here all of the agent decide that you know what features it has to extract for this. Um, so, so in this case, we're using the MCP, the, the model for just simply inference purposes, right? So we're not actually training or, you know, uh, um, feature engineering data to build the model. So in this case, it's already a built model that's been deployed. Yes, yes. So. Yeah, um, that's a good question. So basically, you can have your own model as an MCP server and in the document, uh, in the compliance in the credit risk assessments agents prompt, you would define that as a standard operating procedure, like what features you would be need to passing on, what features you would be expecting it. Using that, it will build the code and then interact with the model. Yeah, so, uh, as you set that up I can clarify further, um, so, uh, prior to getting into the credit risk, um, evaluation, right, or credit worthiness check, um, before that there is another, um, agent in the workflow which is the, um, data extraction. So in the, in the demo you'll see if, if you're ready you can just go ahead with that and, and I'll explain to you more. So, here, you know, I'm playing now and I'm playing the role of an applicant where, you know, I'll be uploading the documents. And because this is a demonstration, we are not going the process of, you know, using a chat window and uploading all the information as that will take time. So I'm I'm uploading the loan application. I'm uploading the pay stub, bank statement, employee verification, tax returns, and all of these are demo data, by the way. And I have now uploaded the um my loan application as an applicant. Now let's see what happens in the back end. Now, we are using, as Rudella explained, using agent core observability from the logs, we would be seeing how the agents are processing the information. Now we are seeing that the supervisor agent is Discovered all of those specialized agents, and then it is tending the workflow, starting the workflow by sending the document to the document agent for verification. Then the document agent then knows that, you know, it has to verify and extract the data. Now it is searching for what are the agents which can do the document extraction. Then it it found the right agent, which is the document extraction agent. Then it gave that information to the document extraction agent, and the document extraction agent now is using a tool to communicate with the Bedrock data automation to extract data. It'll take a minute or so for the data extraction to come back. And I wanna add that um all of this is being done using A28 protocol um so that's the the design pattern that we chose for this particular solution because it gives us the most flexibility in terms of um demonstrating um at an enterprise grade where you have the sorry to interrupt you, the document agent came back with a response and now it the document validation agent would now take that process back and now it will be performing the validation. Sorry, go ahead. Yeah, no, I was saying that 8 ways is, is really good when you have, you know, um, you might have a, a handful of agents now, uh, using a few tools, but then, you know, as you start adopting, building more, you know, you could be buying, you could be building agents, um, so you soon might have a lot of different agents using a lot of different tools, right? So like end tools using M number of. Uh, uh, and, and, and Asians using a number of tools, so it really becomes, um, a, a huge matrix. So AA helps you because it's doing, um. The work for you in terms of discovering the agents that are already uh in your enterprise using an open uh protocol agent to agent right? so that's the one we chose yep now the document um has validated and it is passing all of this information back to the supervisor agent and the supervisor agent now has seen all of that information and is now passing it to the credit risk or to the compliance agent in parallel. Again, as you would see from the court here, we are not defining here for any agents like the supervisor agent or the document agent. You should use that particular agent. It is able to decide on its own using the workflow, which agent it has to use for the processing. Now, you'll see here the compliance agent has been, uh, done and the credit risk agent, it's now processing it using the code interpreter. You'll see that it is downloading the model from the S3, then it's interacting with the model to perform the credit risk assessment. Again, all of this, it's building the code on the fly and then executing it. Yeah, and, and while we're on the great risk, um, to answer your question, um, so once the data has been extracted from the forms, you now have the information that you need to send, uh, in the request to the model, right? So those are the features, uh, and those are all based on prompt. Yeah. Here, the credit risk has responded that the ML model predicts a 98% 3% probability of a successful loan repayment. Now, the supervisor agent has got all of this information. Now it's processing its decision logic to say whether, you know, should I approve the loan or not disapprove the loan. Any questions? I know it may be a lot to take in, but. If you have any questions, please let us know. Yes. Do you have thresholds set on BDA where you're, you're saying confidence score level needs to be asserted. Yeah, you certainly, um, can do that. I think we're using some default thresholds on the back end, but, um, BDA, if you, you know, we keep calling it BDA, but it's actually, um, Amazon Bedrock data automation. It's a feature of Bedrock. Um, and, and this is a, um, capability where you can, um, essentially extract data using your custom blueprints. So in this case, like Marie was showing, we have, you know, we use 5 different documents that are pertinent to a loan application, right? Uh, but you could have, uh, using bit of data automation you can essentially work with multi-modal data so that can be text, images, video, you know, what have you, and then you can specify blueprints, which is essentially the schema for kind of the overlay on your data so that you extract only the specific fields that you care about and then because it's AI powered, it's gonna come back with, um, uh, you know, uh, explainability in terms of, uh, the. Uh, probability, uh, for, uh, essentially the accuracy of extracting that information, uh, so you can then use your business rules to decide what the threshold should be, uh, and that's also a good, you know, the threshold is how the criteria that you would set to maybe incorporate a human in the loop. Uh, if you feel that it's, you know, below the threshold and, you know, uh, you know, and, and typically you, you know, something for like credit decisioning, you do want to have a human in the loop, right, because, um, especially if it's a, uh, a deny, not an approved, um, you wanna make sure your loan officer, uh, takes a good look at it, looks at the, you know, probabilities of the information that was extracted. Um, and, and use that kind of as the basis to see why it was approved or denied. That makes sense. Yeah, and here you see that the super agent has completed the workflow and it has given a final decision of approve and it's showing all of the workflow summary now all of these you are seeing is on the. Observability, which is the on the cloud watch locks. So anytime you know, you know, you want to check trace back what happened for a particular application, what were the steps the agents took, how did they arrive at the decision, all of them are there for audit purposes and you can see, ensure that the right decisions are being made all the time. Now let's look at the PDF it has generated. So this is the PDF it generated. Again, we will walk through the code now. It, um, we just prompt up the, in the prompt of the supervisor agent, we said, create a PDF with all of these details and it automatically created all of these. It gives all of the information for the underwriter, like what is the credit risk assessment, what's the compliant validation, what are the next steps, all of those things in the it'll be there in the PDF and the underwriter can use this to make their decisions very quickly. Now, um, any questions on the solution? Go ahead, please. How do you guys prevent prompt injection for the uploaded documents? How do you ensure that those documents don't have any sort of injective prompts in them to, uh, attack your model? That's a good question. So, uh, as Rudel explained earlier, all of these agents use A2A protocol, and all of them use OA authentication. So within the O authentication, when, when these agents wants to communicate with each other, only if they have the right token, they would be able to communicate and, uh, access those functionalities. Any other external, if some other agent wants to, uh, interact with these agents, as you said, to do a prompt injection, that would not be possible. So I'll add to that. That's because that's a very important question, um, uh, so, uh, we haven't used that in this specific solution per se, but this is what you can do to make sure that you don't have issues with, uh, prompt injection or any other, you know, any of the OAS, you know, gentic AI risks and how to mitigate them, right? So on that main. Um, so Strans Agents is the one that we're using here that supports guardrails. So, uh, if you're familiar with, uh, Amazon Bedrock guardrails, um, it, it's essentially a capability where you can, uh, you know, specify like automated reasoning checks, grounded checks, um, you know, denied topics, which is more around, you know, the question that's being asked. Um, uh, moderate content, things like that, right? All of those filters can be, um, uh, something that you can associate with the guard rail, and that guard rail itself can be associated with the model that is used in the definition of your agent. So as we go into the code, I can show you the exact lines of code where you can associate a guard railed model, uh, in the agent itself. So, uh, that is supported. Yes, there's a lot of numbers here that are needing to be very accurate. How are you passing that from the, the extraction to all and several agents in the way with several different calls? How are you ensuring that the data that's extracted is what the models are seeing from each of those individual calls? Is that some stored memory somewhere or like how are you actually? making sure that those numbers are accurate by the time it gets the supervisory. Yeah, great question, and I think that also leads into kind of the auditability of the solution, right? Yeah, so, um, when we talked about the Bedrock data automation, the BDA, uh, tool, which is the one, the, the workhorse that's actually looking at the documents, extracting the information, giving you the probabilities. For all of that, all of that data, extracted data gets, uh, persisted inside Object Store S3. So long after the process is done, you know, that data is still there for you to look at anytime by your regulators, your compliance, uh, officers, anybody. Um, and that then becomes the source, so the agents are simply just doing the kind of the back and forth of, uh, orchestration. They encapsulate your business logic, but the data itself is persisting in a, in a, in that objectstra. Now let's look at how we build this solution. And one of the first things, you know, I want to mention is, you know, we built all of the solution using keto or agent ID. So using Quito, you know, we worked with Quito. You can call Quiro as one of our members, you know, it helped us, you know, build out this whole solution in a very quick manner. So shall I do this, yeah, um, how many of you have played with Quiro before? Have you even heard of it? OK, uh, just give me a second let me. Yeah, so this is, um, our, uh, latest, uh, agenttic coding tool. It's, um, uh, agent agenttic AI IDE, essentially standalone. Uh, it's, it's, uh, built from the ground up. It's not a plug-in into any, uh, IDE, uh, like Q developer you might have heard of this is kind of a standalone ID, um, and it's really good at spec driven development. Um, so we use, uh, the ID for developing this, um, so. Uh, I want to show you the, uh, the spec-driven, uh, development, um, pretty briefly. Let's see. OK. So unlike wipe coding, um, any white coders here? Everybody, nope, nope, no white coders. OK, I thought, you know, if you didn't raise a hand that means everybody's a white coder. um, no worries. So Quiro supports white coding, but you know we're, we're not just looking at POCs, right? We're looking at how do you build an enter. Price grade application. So we're following a very rigorous process of um using spec driven development, uh, and, uh, what I'm showing you here are the um the assets of that, you know, essentially your requirements, your design, and your tasks so you can implement the solution. So part of the requirements it's um yeah essentially the entire SDLC right starting with uh what are your user stories and what are the requirements associated with each of those um so like we mentioned we want to use the A2A protocol um and that was our starting point that was one of our main requirements so as a systems architect I want all my agents to communicate using this protocol. So, um, you know, a lot of requirements there, um, agent discovery again this is based on, um, uh, strand supports this agent core supports this, uh, using agent cards. And then, um, there's, there's quite a bit, it's pretty comprehensive, but just for each of the agents themselves they have very specific requirements like the BDA agent has its own set of acceptance criteria and so on, um. And then going into the design. The idea is, you know, measure twice, cut once, right? Uh, make sure, um, uh, you come up with, you know, uh, the, the way we did this was we use Quo gave a kind of a brief description of the problem that we were solving, and Quito came up with, you know, all of these assets, right, the requirements, design and task. But that's just the first draft. It's a pretty iterative process, so, um, you can review what it comes up with, then you can modify using your natural language prompts, uh, so continue to define the design until you get to a state where this feels like it's pretty close to your use case, um, and that's exactly what we did, uh, so it comes up with kind of the as built, uh, uh, architecture. Um, the loan request coming to the supervisor agent, which is then communicating using A2A with all the specialized agents. Um, so lots of information, uh, when you work through this, uh, in terms of what the design should be, uh, and then into the tasks. So these are tasks that um you would um it it's it it's doing the tasks in a face manner uh starting with the infrastructure set up so this is infrastructure beyond just agent core uh these are, you know, basic things like setting up your S3 buckets, right, which is where your input data, so all your loan application forms, all of those get. Stored as well as the data that's extracted by data automation that gets persisted as well. So getting all those buckets set up, getting your Oops set up. So in our case we used Amazon Cognito, uh, for that. Um, you might have your own IDP that you want to integrate with, right, like Octa or Entro Entra ID and so on, um, so getting that OO set up is another, uh, piece of getting ready to build a solution. Uh, the other things are too is, um, uh, uh, how do you store your secrets, right, like secrets manager, that's a very important one, and then, uh, your IM roles, right, your permissions that each of your agents would have to interact with other, uh, AWBS services, um, then things like, uh, Amazon, um, Bedrock, uh. Uh, gateway agent core gateway that I mentioned, so that needs to get set up so that you can then, uh, the agents can then access the tools that are targets behind that agent core gateway. So some of that infrastructure needs to be done up front so that's phase one. Um, and then phase two is essentially building, uh, all of your agents and then deploying them to, uh, agent core runtime which is your compute. So, um, you know, a, a complete set of tasks which then get checked off as key role starts implementing your solution. So, just, uh, you know, a structured approach, uh, to having an end to end solution. Awesome. So now let me walk you through, you know, how we build the supervisor agent. So, one of, uh, all of these agents, you know, we'll be explaining them, you know, basically, we're breaking down into four sections. The first section is, you know, what are libraries you need to import. Here, you know, we are saying, you know, you need to import the, um, strands, multi-agent A2A from A2A servers. That enables the A2A communication, then you also need to import the A2A client and from the client tool provider so that it is able to discover the agents. Then we are also importing the code interpreters so that you can provide the code interpreter as a tool to the agents. And the other two libraries are making sure, you know, those are required for running the A2A agent as servers. Then we have the configuration section where we configure, you know, what's are AWS region, what are the buckets we are using, any other parameters, uh, we are using, and We are also have a code, and this is the only bit of the code you would see in all of this, where, you know, how we are, uh, handling the OR tokens between the agents. When this agent needs to communicate to another agent, they need to have the refreshed OR token, and this part of the code would handle that. Again, here, as Rudulla mentioned for the client ID client secret token URL used for OR tokens, we retrieve all of them from the secrets manager to make sure they are not exposed as well. And this would be the, uh, the main part of the supervisor agent, which is the prompt. In the prompt, we define what is the role of the, um, agent, what basically what task it's going to be doing. Then we are going to also define how we are going to be discovering the other available agents, what kind of tools you'd be accessing to, uh, discover the other agents. Then we'll also be defining like as a standard operating procedure when you're selecting other agents, what are the qualities you need to look for in those agents when you want to pass on the tasks to them as an example, if you want to use an agent for document validation, then you need to. sure that, you know, when you read the agent card, it has words like document, validation, compliance, completeness, etc. in their agent card. So only use those agents for document validation. So here we are making sure we're guiding the agents to make sure they perform the way we intend to. Then we are also making sure how agent communication, as you know, many of you ask questions, right? You know, how does agent know, you know, what data comes back and forth, how they would be interacting with other, uh, as an example, an ML model, this is where we define, you know, what is the type of output you would be expecting, what is the type of, uh, output you should be sending to other agents we would be defining all of these things here. And then specifically for credit risk assessment, what you should make sure, like one of the things we are ensuring that, you know, you are passing the entire validated data structure to the credit risk assessment so that whatever agent is performing the credit risk, risk assessment, it has got all of the data. Do not make, uh, do not just truncate some of the data and pass it across. So these are the instructions we provide to the agent. And Finally, we also provide the decision logic, how it has to say if a loan application is approved or manual review or rejected, what is the decision logic it needs to follow. Then after completing it, we are saying, you know, once you create your, uh, you have your decision, then use the tool to make a PDF, uh, report, and we are just specifying what information must be there in the PDF report. And what are the PDF report requirements we're specifying. Then we are saying, upload it to an NS3 bucket. We are saying, you know, which bucket you should need to upload it to. Then we are saying, you know, um, where you should send an email to because this is a demonstration system we, uh, built out we made sure that, you know, all of the emails go to just one email address, but in a real world scenario, if you can specify that pick up the email address from the, uh, loan application or from the conversation you had with the applicant, then send that response back to that email. It would go ahead and do that. And you would also want to say, you know, how you want your email to look like, what is the formatting, all of this. Now again, one of the key advantages of this is. If tomorrow if requirement changes, all you would have to do is come back and change the prompt and and which is very easy to do because this is more like natural language where you define in your own uh natural language saying what you want the agent to do rather than having it as written as a code and maintaining the code just come and change the prompter you will, you can, it can quickly adapt to your changing business requirements. Mhm. How you order can I see about the how of this models you run the model. This that you provided. If the outcome of the same for the next time. Maybe the first one is like this, but is there any guarantee deter state that the output for the second one is the same. Yeah, so, uh, yeah, you're basically, you know, bringing out the nondeterministic nature of these agents, right? Um, however, in this case, um, if you know, provided your in this case your inputs are still the same, right? It's the same set of documents, uh, and the, um. The logic inside each of these agents, like, for example, the calculation of trade worthiness, right? Whatever business logic you have inside the trade risk agent, for example. Those are still pretty deterministic because those are just mathematical calculations, um, so, uh, the agents themselves are, it's, it's not like a, a text-based prediction yes, you're using a lot of prompts to drive the work, uh, but you are under the hood, uh, uh, in those prompts being very specific, um, and this is where, you know, like we were mentioning the. Uh, SOPs, the operating procedures come into play, right? As long as you have that clearly defined in the prompts, uh, the calculations will follow, and then the results of those are still deterministic. Uh, and so the input stays the same, the data extracted stays the same, and as long as the, um, you know, even with like the data BDA, uh, tool that you're using for extracting data from each of those for each of those documents, uh, if you're still keeping that same threshold, um, uh, the, your, the data that's input into those downstream mathematical calculations still is the same, right? Um, so you should not see, for example, something flip from an approval to a denial kind of a thing. And as a best practice, yeah, go ahead please. So you're showing right what I'm seeing here is an identical. So how do you expect this? Like you put the business part. television that says, oh, today I'm making 3% of it. Uh, thank you. James what. That's my mistake. Um, that's the business requirement. Power.com. That So, um, if I understand your question right, you today you may have a DTI requirement. Tomorrow you have another DTI requirement. How you want to adapted as you know, all you can go at as, you know, can change in your system prompt. How would you want the decision logic to be done on based on the DTA. Yes, yes, business requirement, it's great for part of it as an engineer, it's great that I. I'm not saying a lot of business. Yeah, so the DTA prompts, that is not how it calculates the DTA that is in the compliance, uh, validation agent. That's where it is calculating the DTI using the code interpreter. There you can define, you know, how you want the DTI to be calculated. The supervisor agent in this, the supervisor agent is just playing the role of an orchestrator where it is passing information to all of these other agents who have their tasks clearly defined. As you said, for the DTI agent, it will be the compliance agent where, you know, you will have the, uh, prompt defining how you are calculating the DTI using the code interpreter, and there you can define your thresholds there. Yeah, I, I think we haven't shown that piece yet, but there's a specialized agent and the prompt which captures that. Yeah. Yes. So in designing a system like this, how do you recommend evaluating, uh, model accuracy between the different models if you were to switch out for let's say. That's a good question. Um, in, um, Bedrock, uh, we have a very, um, mm, we have something or less model evaluation where, you know, before you start building these solutions you can start evaluating a model. There are two types of model evaluations we offer. One is we have. Curated data sets where you use those curated data sets to have the model run against them and you can see the accuracy, or you can bring in your own data and use it against those models to see, you know, which one works best and then use it in your system as well. So, so basically it requires, um, you to, you know, have some kind of ground troop data, right? Something that you have historically for similar, similar profiles of customers what the loan decisioning has been, right? And then, um, there's definitely a lot of work involved you need to set up that evaluation harness on your end so that, um, when you deploy these kinds of systems you have something, uh, you know, some kind of a. Um, uh, gold standard to compare to, uh, the results that are coming out of it because, and that, that we can't, uh, you know, emphasize enough the, uh, the importance of testing, especially with these nondeterministic, um, um, applications. Is that framework. Um, so we have, um, uh, I haven't gone through, you know, the entire task list, but in, in the design itself, um, you know, some of the, um, aspects of the design include testing that's end to end testing, so that includes unit testing, integration testing, all of that. Uh, and you, if you know, if you have uh, you know, specific, um, evaluation, uh, you know, like metrics and things like that, uh, and, and, and a source of data that you can point to, all of that can be incorporated as well. Any genetic patterns. Around the areas like. The like design patterns so um we, we are in this case using the strands A to A design pattern um it's, it, it's pretty standard it, you know, strands also supports others like workflow which are more deterministic, uh, the graph based design, um, like, you know, a graph as in like, you know, each of these specialized agents can be nodes, uh, and they, you know, you can have like essentially like in a, in a graph, right, like you can have edges connect between. Uh, if you think of each of the agents as being a node, uh, the communication between them is an edge, right? So you can, you can very, um, you can, uh, define what that, um, multi-agent structure needs to look like in a graph. So that is one of the other. Design patterns that's supported natively by STRS agents, uh, A2A is, it's, it's a more flexible based on agent discovery that we're using, but you know this is one of the design patterns that supported Insight strands framework as well. Reflection. Yeah, that is. we have to separately define it, uh, like the react pattern, right? react. Yeah, these use the react part where they do the reasoning and an actioning of it, where the reason that, you know, given I have this, I have this task of as a supervisor agent, the, uh, this task is I have to process these loan applications and I have a set of tools to me, and they they'd use a reasoning to create a workflow, then use the available tools, the agents and the code interpreter to perform the tasks. Yes, so, so I think the, the, the, the, I think the best way to get to your, your question is, um, so reflection, right, so this is based on the model capability as well, uh, you know, the, so the late, the, in this case the model that we're using is anthropics clot sonnet 4.5, which is very capable of reflection. So it's built into, so that's the one that's the brains behind each of these agents, uh, you know, as newer models come along, right, that can support even more advanced reflection or additional patterns. You, you can have, like I was saying, um, these agent core strands, they're all model agnostic, right? Each agent can essentially, you can use a bedrock model or you, or you can use an open source model. Uh, and each agent can use a different model. It doesn't have to be the same model. So it's inbuilt into the agent itself based on the model selection. That's a good segue, you know, the last 4th section, you know, which we wanted to show in the code is, you know, after the pros, you know, how we are defining the agent. If you're looking at here, you know, we define the agent. What is the name of the agent, the description of the agent, the system prompt, which is the system prompt we saw in detail, and as Mudilla mentioned, the model, you know, which is, um, agnostic again, you know, you can choose what model works best for you and you can have that model here and the tools are the all the tools like agent discovery and the code interpreter tools. And finally we have the agent running as an A2A server so that way, you know, each of these agents, they use A2A communication to, uh, between them whenever they want to communicate. The advantage of using an A2A communication is tomorrow you may have another agents which is not running on agent core but still using A2A communications. These agents running on agent core can communicate with them as well. Yes. The, the data that, uh, that's passed between the agents. Yeah, so in this case, since we're using A2A, it's a JSON RPC message format, um, and so this is, um, the, the output of one agent gets passed in that format, the JSON RPC format to the, the agent, the like the supervisor agent, for example. So that message body contains all the information that's in the basically the input that's needed by the next agent. There's no passion. Um, so, um, all of these agents use this HTTPS protocol. So all of those, whenever they are, um, communicating between them, even when they are, um, communicating this JSON data between them, they're all encrypted. That that function right you know the great. You pass it down I Yeah, and, and that is, um, on that, uh, oop happening, um, every step of the way, right? So the oop, um, set up that we used, you know, at the beginning that I was talking about using Cognito it that, you know, you're using, so yeah, this is a very important piece that we probably haven't mentioned yet is, um, when. You deploy your agent, you configure the, uh, the, um, what, the oth configuration for each of those agents. So you can have something like, um, IM SIGV4 if you're familiar with that, um, that's the default, but it also supports OOT 2.0, uh, so in this case every agent that we deploy, there is a configuration that needs to happen. Uh, and we set that up as like a JWT, uh, custom token. So the, um, every single one of those agent, um, interactions is, uh, authenticated and authorized. Yeah, I think, um, you had one question. We'll take, yes. Mhm. Correct, yes. So the data across the sessions. No. Correct. The uh Uh, so our back control you said? What the agent is So these are tools behind the gateway. Are you referring to the tools behind the gateway? Uh, yes. So, uh, the agent itself, right, um, so in this case, for example, the BDA agent, the data extraction agent that's actually calling the making that, um, uh, it's, it's, it's, uh, you know, we set up an what is called an MCP client on the agent. So the agent is acting as your MCP client. And then it's uh communicating with the agent core gateway which is that uh MCP server right and behind that you have um essentially a lambda that's making an API call to Bedrock PDA right? um, so the communication between the MCP client and gateway is it's an 02, um, uh, uh, authentication. Um, and authorization as well, and then behind that, um, it's all IM permissions, so that's where our back comes in, yeah. Yes, uh, here you have mentioned like all the tools and the single agents. So I have experience like if we provide more than. Like 40 tools or something agent get started confusing like uh between choosing each of the tools. Yeah, you want to talk about the semantic search? Yes, so, um, one of the ones, you know, we have in agent gateway is what we call as a semantic search. And as you said, right, you know, giving agent more than 40 tools, you know, uh, and it does not decide, you know, what tools to use. There are two things. One is making sure our prompt is very clear of what our requirements are so that it specifically knows what the task it has got, and then it can search the right tool. And in agent core gateway supports something called as a semantic search. In this case, when, uh, when we say the BDA when the Bedrock data automation agent, when it has to do a data extraction, it first does a semantic search like data extraction. What tools are there available for data extraction and then the gateway returns available tools which are there for data extraction. Then it chooses that and performs the data extractions. In your case, if you have. Got more than 40 tools, you can have it behind a gateway agent core gateway, and then the agent can do a semantic search based on the task, what it wants. Then the agent core gateway will return maybe out of 40 tools, maybe 10 tools, and then out of those 10 tools, the agent Corps is then able to agent is able to select the right tool to perform the task. Yeah, and I'll add that, you know, the, the, the really the best practice is, you know, you want to think of the agent as being a microservice, right? Essentially that same mindset. Um, so depending on, you know, it's, it's definitely purpose built for that business function, right? So you want to give it the tools only that it needs and only the permissions that it needs, uh, for those tools, right? Um, so you, you know, you don't, you don't want to have 40 tools, for, for example, the specialized agents especially should only have, you know, like a couple of tools, you know, 23 tools that it really needs to get the job done, right? Um, in the supervisor agent, you know, um, this is like accumulating. Because it's trying to help the, the, uh, specialized agents, um, figure out which tools to use. It's listing them, but the, the supervisor agent itself is not really calling those tools. So there's a little, it's different, right? So, uh, but yes, that's a very good point, you know, you need to have purpose-built agents and just the tools that they need. We just have 1.5 minutes left. So I'll just walk through one more agent and any other questions, we are available outside the room to take them as well we can just wrap and I just wanted to quickly, you know, showcase the compliance agent, the prompt and show, you know, how it is doing the debt to income ratio. You would see in the prompt here, it's using the code interpreter to perform. These and tomorrow, you know, if your business requirement changes, you can come here and change it in the prompt itself. It's not the supervisor agent which is doing that. Here we have purpose-built agents for each of these functions. And when you want to change any of these functions, you can come to those partic uh purpose-built agents and change the prompt there and your newer business logic would come into play. And um do you wanna show them the code that they can go to we wanna give you something actionable so that you know as you walk out of the room today you have something to go build um so we have uh a couple of diff uh official reports one is um. Uh, the strands agents itself, which contains, um, it's a GitHub repo, uh, that contains all the different design patterns that you can build with. Uh, 8A is one of them that we use for today's solution. Uh, and then another official GitHub repo for Bedrock Agent Core. Um, it's, it's a really rich set of code samples. Uh, we based a lot of our solution, uh, based on, uh, those code samples that are in Bedrock Agent Core repo. So please use those, um. They, they just accelerate your entire development process, right? These are vetted, um, solutions meant for building secure scalable agent DKI application. So go have fun with those, um, and definitely use try, give it, give Quiro a try. It's, it's been very helpful for us. And thank you, thank you so much for listening to us. Hope you like this, and we are available outside the room for any questions.