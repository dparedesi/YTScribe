---
video_id: DrE6Hamd9fQ
video_url: https://www.youtube.com/watch?v=DrE6Hamd9fQ
is_generated: False
is_translatable: True
---

Um, my name is Abhijit Bose. Ah, I head the Enterprise AIML Platforms and Engineering for Capital One. Um, we have a few Capital One folks here as well. Um, we are responsible for strategy development and execution of our enterprise AIML infrastructure, which powers most of the AIML applications, uh, at the company. Um Here is a quick look at the agenda. I will start with um a few slides on AI at Capital One. Ah, it's actually a 10-year journey for us. Um, it's been, you know, a series of innovations we have done over the years, I would say transformations that led to this moment, you know, where we are at the frontier of AI. And then go to our strategy. There are 2 things. The one, the first one is the custom AI. Um, a lot of folks, a lot of companies are using AI. But core to our strategy is how do we build custom AI that run in tune with our infrastructure, that run using our own customized models. Um, and then the second part of our strategy is the AI platform. And in this presentation, I will focus more on the AI platform. Uh, there is a, there is another big conference going on this week, which is NRPs. If you follow NRPs, you can follow some of the Capital One, talks by my colleagues there. There you will find out more about our customized model and that custom AI strategy. But in this talk, it'll be more about the platform and infrastructure. Um, I will also go over. Um, a particular case study, ah, that's, I think, on a lot of people's mind. Uh, how do we deploy gentic coding tools, um, but the way we are deploying it is actually using our own platform to deploy that with the right set of guard rails, with the right set of, you know, gateway, uh, that actually makes it much more well-managed and scalable throughout the company. And of course, you know, none of this would be possible without having great talent. I'm really fortunate to have such wonderful, extremely exceptional, you know, talent at the company. So I'm gonna focus on Uh, a couple of things we are doing, uh, to really be at the forefront of, um, AI so that we build a brand name, so that we can attract the best talent in the, in the world to work with us. But first, I want to share some quick context about Capital One. We are a 30-year-old, founder-led Fortune 100 company. Uh, the founder lady is really, um, you know, um, kind of important for, for, for, for me, having come from Facebook. Um, I, our founder, Rich Fairbank, um, works very actively, hands-on with us on our AI strategy, and that actually has made my life much easier and, you know, a lot of other AI leaders at the company. We are just fortunate to have such a, you know, um, Uh, forward-leaning and very innovative, uh, CEO at the, uh, helm of the company. We are also the nation's largest direct bank. Um, and along the way, we have been recognized as one of the best places to work. But I will focus on, I'll just draw your attention to two, In the last 3 years, we have been number 1 in talent in the Evident AI index that actually takes in, I think, 50 or 60 global banks and then rates them on their AI maturity, you know, talent, um, how responsible they have been in deploying AI at the company, uh, etc. Um, and then the second one is that this just came out recently. We are now top 10 companies in the world, along with Google, Microsoft, um, you know, ah, Nvidia, Samsung, ah, in terms of the generative and agent TI patents. Um, The, you know, all of this is to say that, um, just to highlight the focus on innovation and the relentless focus on developing and attracting talent at the company. So You know, it's not a surprise that ah we are at this moment um of AI transforming so many, you know, businesses, ah, so much of society. But our history as a technology innovator actually positions us really well, um, We, er, before I go to the next slide, but the foundations of our company is built, are built on, ah, our heritage of data-driven decision making. Um, modern tech stack. Um, we have our in-house data science and engineering talent. Uh, we are all in the cloud. Um, we, um, I joined the company in late 2020, and that was the year we got out of our last data center. I, I still remember we celebrated that. Um, we have been in, you know, AWS primarily since then. Um We have done a lot of work in transforming our data ecosystem at the company. And as you know, your data advantage is really your AI advantage these days. Um, and then we have a history, very rich history. We have a DNA of, um, enterprise platforms. Um, we don't do bespoke application development at the company, um, from data to AI to cloud, wherever we see a common pattern, ah, we build platforms that everybody in the company uses, and AI is just, uh, AI is no exception, uh, to that. And then, of course, um, you know, we are uh um exceptional at risk management. But this does not happen overnight. Um, this takes, you know, many years. Uh, as our senior leaders say, you know, it's a, it's a lonely, um, journey, um, with a lot of hard work. Um, and for us, it's, uh, it started almost 10 years ago. Uh, and I'm not gonna go through the timelines, but you can see those like from open source to declaring we were all in the cloud, to getting out of our last data center in 2020. Um, launching Capital One Software, where, uh, we took some of the capabilities we were building in-house for ourselves, we saw an opportunity to actually take those to the market for other businesses, uh, other companies, and that actually brought a very different perspective to our, to our software engineers and product managers because now, We are not just the customer, but we are more service providers. So things like multi-tenancy became very important for, for all of us. Even in the AI platform, I remember the first kind of the impetus for multi-tenancy started, you know, when Capital One Software started working with us and we realized that not all of our platforms and capabilities were built for multi-tenancy, and now every capability that we build. It has to be multi-tenant because someday we might actually externalize that, that service to other customers. Um, now, We, um, in late 2024, we started kind of the uh groundwork for building our first gente application at the company. Um, and then in January of this year, we launched our first customer-facing multi-agenttic, um, uh, application called Chat Concierge. I'm gonna briefly talk about that. Um, And that, again, was a pivotal moment for us. You know, this is the first time a bank um really exposed a multi-agenttic system in front of live customers um live in the market. We learned a ton from that. And we are, and as I will show you, ah, we took, we learned, um, that we took all the lessons learned, and then we kind of like reformulated our platform plans and now we are all in agents and how we scale agents, uh, at the company. Um, we are also using AI, um, extensively, ah, at the company for all kinds of applications. Ah, here are some examples. Uh, I already talked about the multi-agenttic, uh, AI, uh, workflow uh called chat concierge. Um. We also use AI for real-time fraud detection and mitigation. Ah, many of you are familiar with Transformer architecture. We use transformers these days to personalize, ah, user experiences on our mobile and web pages. Um, if you use our Capital One shopping, uh, plug-in, It uses AI to find the right offer for you, ah, from the right, you know, merchant, ah, as you are checking in, uh, checking out your, you know, your, um, um, um, SKU. Um, and then if you use our Capital One Auto Navigator app, it uses image recognition, ah, to find you the right card that you are looking for. Um, again, those are, by AI I mean, not generative AI. A lot of these are, you know, neural nets, um, ah, but, um, you know, more and more we are also using, uh, LLM driven, um, and agentic AI systems. Now, let's talk about uh where we are going with AI at the enterprise. Now, we all realize that AI is not just a new technology. It is going to fundamentally change our business, ah, fundamentally change our processes that we use at the company. Now, when it comes to scaling AI, At any large, you know, regulated industry like ours. I think all of us have to deal with two things. One is the need for speed, you know, uh, every other day there is a new something happening, you know, a new model coming up, or some infferencing, you know, um, algorithm optimization comes up. Uh, a lot of work is happening in gen decoding tools. Um, we are seeing real benefits. So there is a lot of pressure on all of us to actually come, bring these tools into the company and deploy them. But we also have to be make, we also have to make sure that all these tools are well managed. Um, you know, we have zero tolerance for cyber or risk, ah, failures, and we have an exceptionally high bar on those. So how do you build at the frontier of AI? Um, At the speed of well-managed. Um, that is the core problem statement, I think, for all of us, those who work in, you know, large companies and regulated industries, um, we have to deal with. Now, before I go to the next, uh, uh, page, I want to define the speed of well-managed, uh, because that's also something we learned, you know, um. It's not the speed that we were used to, uh, maybe even 3 years ago. With AI we also have to lift our risk, cyber, um, you know, governance, all of the controls that we deploy, all of the audit functions in the company. Uh, so, the speed here is also increasing. Ah, it's not just, you know, AI is moving fast, but my processes are still slow. Ah, that's not gonna work. So, how do we lift all boats. Um, that's the, that's the core, you know, of the problem statement. Now, To To us To build that company uh that innovates at the speed of AI but also at the speed of well-managed, it comes down to these building blocks. Um, proprietary data. Cloud and serverless, GPUs or, you know, compute um um infrastructure. Modern tech stack. Enterprise platforms, I'll mention about that, in-house talent, risk governance, and then the last one is model customization. Um, it's not just using the, you know, third-party frontier models, um, that are available at an API. But the companies that are gonna be at the forefront of this, they will leverage their own data to build LLMs from scratch, um, and customize for various, you know, specialized tasks at the company, and they need to have the infrastructure and talent. To then deploy them at scale at the, you know, same level of sophistication as some of the frontier model companies have been doing. Now, as I said, you know, we set out to do all of those through two things, you know, customized models, and now I'm going to come to the enterprise AI platform part of it. Now, Um, we have been building, uh, this AI platform for the last 4 years, and we have learned a lot. Uh, we have redesigned a few things over the years. Um But a couple of basic principles have stayed the same. For example, for the Best AWS services, we take them, we build on the shoulders of them. So, for example, EKS, ECS, SageMaker, Bedrock, OpenSearch, um, um, you know, EFS S3, FSX for last year for high high performance file system for GPU cluster. Um, we have taken them, but we have also built a lot of other capabilities on our own. Uh, and then we have also incorporated a lot of open-source, uh, projects into our platform. So, this particular combination of AWS services, open source, and our custom proprietary capabilities, that actually has helped us to be at the forefront of AI and, and we are at a stage now where we can mix. Third-party and first-party capabilities, almost interchangeably into our stack, ah, to be, you know, very quick and fast in deploying a lot of capabilities at the company at scale. If there is one kind of like, you know, um, nugget of information I can give you is that Being able to design a control plane, uh, being able to design a stack where you can bring in these three things, uh, easily, uh, in a well-managed way, that is the, um, that is something, you know, um, people should take on, because it's not just about depending on a vendor for everything. It's not just, not about building everything on your own, then we'll miss out on a lot of the innovation. And then it's also not open source is not enough in many cases. So, in the next few slides, I'm going to talk about two things. Um, two parts of the platform. One is the, uh, model training or, um, our training infrastructure, how we build, using again, three different, um, you know, uh, pieces of, uh, capabilities. And then, uh, very importantly, the infferencing, um, uh, capabilities. So, if you think of like the modern GAI stack, you have to have a really well, uh, you know, designed training infrastructure. Uh, and you need to have very well designed, uh, high-performance inference infrastructure. Of course, traditional, you know, core ML had that too, model training and model inference, but in case of LLMs and generative AI, it is really complex, um, um, much more complex than the, what we have dealt in the, in the past for core machine learning. So, I'm gonna talk about our HPC clusters. It's been about, I would say maybe 2 years, and we learned a lot. Um, you know, at the basic level, it's basically compute, network, and storage. And you could say, how difficult is it? Like, you get your compute from AWS, you get your storage, you know, you get your network, like network like EFA storage like FSX, but We find that you need to build a lot of additional capabilities on top of that to be really at the tipping, at the, the kind of like at the bleeding edge of, you know, building very large models, um, that, you know, where nodes fail all the time, uh, you don't want to waste these, you know, GPU resources which are very expensive. Um And you also have to invest in your training pipelines, um, you know, ah, so that those, you know, are also very efficient. So, this is what we did. We took the infrastructure. A lot of it from AWS. Um, native, you know, capabilities. That's the, uh, left-hand side of the, the slide. Uh, things like high-speed networking, scheduling and queuing, high-performance, uh, file system, GPU nodes, recovery, etc. The software stack, the heart of the training pipeline, and these are training pipelines that need to run, you know, uh, jobs across many GPUs, hundreds of GPUs or sometimes thousands of GPUs in a distributed way. That was mostly open source and our own custom pipelines, and we tuned that to the hardware. Um, and then the third one, the thing on the, on the, on the right, user experience. A lot of that we had to build all by ourselves. So this is a perfect example of you can build, you know, world-class infrastructure. With combination of AWS, um, open source, and your own stack. And all of this has been built by our own in-house AI researchers and, and engineering team. But we didn't do this, you know, uh, in a day. Um, it was really a learning experience for us. We did it in 3 phases. In the first phase, we got a lot of the infrastructure procured and provisioned by AWS. In phase two, we focused on our training pipeline and really building that in-house knowledge of how Kuda kernels work. Um, you know, even the, even debugging, um, GPU, um, like training code that runs on GPU is very complex. So, we had to build a lot of that expertise in-house. Um, and then in phase three, we started to become more and more sophisticated. We, uh, focused on minimizing downtime. How quickly can you checkpoint restart, um, a job, you know, that has failed on one GPU. Ah, and so it's, you can think of this as a journey that you have to go through. Um, and it took us probably about a, you know, a year or so to get really, you know, um, to the point where we are now. And this is kind of the training infrastructure that looks today. Um, what, you know, I will tell you a few things that at the top, you see the diversity of the users we have, we, we support on, on this training infrastructure today. And this is not just one cluster that we, we maintain a few different clusters, you know, for custom build for certain kind of jobs like RL, you know, versus like post-training RL versus like pre-training or specialized transformer for certain, you know, applications, um. Uh, but there are a whole range of users with varying knowledge of infrastructure, but they don't have to deal with much of the complexity of a GPU cluster today. Um Thanks to a lot of the, you know, abstractions we have built in the, ah, in how we how you submit a job, how you debug, ah, etc. Uh, it's also built for multi-tenancy. Uh, we can support, we can stand up a cluster for certain applications, certain division within the company, where there may be strict data, uh, you know, restrictions, um. Ah, and again, you know, ah, as I mentioned earlier, multi-tenancy is kind of in our DNA now after, you know, Capital One software. Um, Yeah, and then, um, at the bottommost layer, um, you know, we have the data, um, data layer, ah, where we can bring in data from Snowflake. Ah, we have a pretty large snowflake, um, Uh, footprint, um, we have S3, we have other data lake, uh, and then, of course, a lot of the data need to come into FSX, uh, to feed DGPUs. Um, I'm gonna talk about, ah, next our model hosting and inference. And this is something, you know, uh, again, a lot of you have realized this, is that training cost is kind of like fixed or, you know, once you have trained the model, uh, you know, you, you, you have incurred a certain kind of cost, but your inference cost is just starting at that point, right? And If you are not careful with your inference platform, ah, that itself can be a huge expense. Um, you know, you're not gonna get the kind of the economics of this token or park query right. And, and as a result, you know, what works for 100 users as a prototype, ah, you will find that that doesn't work when you have to support 100 million customers, um, with the GA applications. And when you hear You know, like, oh, a lot of GAI applications are dying at the POC level. Uh, one of the reasons it dies, you know, is that, uh, people haven't thought through how to really, uh, develop the infrastructure and how to really create the cost structure to scale it to millions and millions of customers, you know, millions of tokens, um, uh, per minute, for example. So, we took this very seriously. Um, you know, in our first iteration, we had, we put together a system that worked. Ah, it supported a smaller number of, you know, uh, users internally for an internal application, but for us, it was existential that we have to get really world-class, in our inference, um, uh, platform capability. So Um There are 3 things here. Ah, first is the cost per token or cost per query at scale. unit economics. Um, then the user experience, latency and throughput. You know, when we first deployed the chat concierge, and I'll, I'll get to that, um, in a, in a couple of slides. Um, chat concierge we had, because it's multiple agents, multiple LLM calls, I remember we were getting something around 40 seconds, ah, from the point you ping the, you know, you put your query and then the system will come back and, you know, answer, um, um, give you the answer back in 40 seconds. Now it's about, about 1 2nd. So, um, that did not happen, um, you know, without a lot of the optimizations in our inference stack, on our model customization stack. Ah, this is why, you know, this, it's not just about AI but it's custom AI that you have to figure out what works for your own enterprise. Ah, that's where the leverage is and that's where I think things are heading. Uh, and of course, uh, reliability. Ah, if you're going to maintain a large fleet of GPUs, uh, for inference, um, Um, you know, uptime, dependencies and how much you can pack within each GPU, how many inference calls, those things start to matter. Now, we face a dilemma, and I think it's very Ah, common for a lot of companies who are experimenting or deploying Gen AI, and I'm sure some of you are familiar with this, that should I go to a managed, you know, provider, I get an API, I get, you know, a Lama or, you know, Cloud or, you know, a lot of different models behind an API and I get started right away. So, I have huge velocity. Um, you know, ah, and I pay, pay by tokens, right. So if my usage pattern is variable, it makes sense. Um, you can pay by usage. Um, and then, um, Of course, you'll be trading off, um, on, you know, data privacy, you have to audit them, you have to make sure that they are not using your training, your data for training or there is, you know, leakage going on somewhere, um, you know, exposing user-level information outside. Um, and then, even when you go to a managed provider, depending on your scale, as you are doing, going from like 30 users dog fooding to 1000 users to 1 million to 10 million to 100 million, at some point, you will find that, OK, this economics probably does not work for me anymore. Ah, because I'm paying so much, you know, for, for my tokens, ah, all the time. So, then the other side of that ah is the self-hosted option, where you have to have now your own DevOps team, your own AI engineers, you know, you have to get GPUs provisioned. There is a fixed cost to all of that. Um. And the answer for us was that, um, you know, like many other choices in life, it's actually both. So, we have been able to design a control plane where we actually host a third-party, you know, ah, platform for certain types of use cases. Um, and then we also host our own platform, inference platform, where we can really optimize end to end, like our own custom models, our own inference stack, our own hardware. Um, provisioned, um, to really optimize the, uh, latency, throughput, and cost. And these are some examples of ah optimizations that we have done in our in-house um self-hosted inference platform. Um, I talked about model-level optimizations, um, but we have also done a lot of infrastructure stack, uh, optimization, uh, which is basically how do you maximize the GPU usage by bin packing, ah, you know, by putting multiple tenants, uh, on the same GPU, but then you have to worry about, you know, data separation, uh, query separation, and all of that. Um. It's, it's, you know, it's, you have to build those systems, but ah we have been able to get orders of magnitude lower from where we started, you know, about a year ago on this optimization work to now where we are, um, to the point that we feel that we will reach a level that's comparable to any other provider outside. And in fact, that's our benchmark, like if we can get as competitive as any other. Uh, like OpenAI, anthropic, you know, GCP or, um, you know, Bedrock, um, we, we need to be in the same, um, uh, league there. Now, I'm gonna talk a little bit about chat concierge. Um, So, I talked about the LLM training, LLM inferencing, but then, you know, as we are building those things, there are also agents uh story happening um in the broader GAI ecosystem. So, in 2024, we started working on is ah our first kind of use case. Um, it's for consumers, um, when you go look for a car, A lot of dealerships offer you a chatbot and you can interact with the chatbot. You can say, I'm looking for a, you know, ah whatever, car, you know, this color, this model, you know, do you have it in your inventory. Ah, um, so, we have a very, you know, ah well established auto business, ah, very forward-thinking auto business. It's not the traditional auto lending. Ah, we also work with a lot of dealers, um, to build a lot of the dealer backend software platforms that go into lending, ah, maintaining the dealer websites, um, you know, a lot of the capabilities that dealer need to interact with customers. So, one of the ideas was that, can we build a chatbot on top of our AI platform. That will be hosted um on our platform, but that'll be, there'll be a little chat window on dealer websites. Ah, and then prospective customers will be interacting with it, but then our agentic framework on the Capital One side will actually do the actual conversation. So, that's the product. Um, it's called chat concierge. Now, to build the product, um, we, you know, did a few iterations initially, but then we landed on a multi-agenttic framework called Macau. Ah, to build this, and I'll show you a little slide on Macau next. Um, but then we also had to build a lot of the, um, you know, a lot of the vertically, kind of vertical stack to support this agent, uh, agentic application. For example, we used, uh, TensorRT, um, for, you know, for LLM hosting for that. Uh, we used lama and then, you know, fine-tuned LA, uh, models, uh, for the, for the task. Uh, we had to build some custom guard rails. Um, chat concierge today is on many dealer websites, and we have, uh, very aggressive ramp-up plans to many hundreds and thousands of dealers. Uh, if you get a chance, you can play with it, um, um, on the web. Um But This particular agentic framework turned out to be a very general purpose framework where we start with an understanding agent. That starts interacting with the customer to understand the intent. So, for example, if I say I'm looking for a blue BMW uh to, to figure out that I'm looking for a car called BMW with color blue, ah that's the understanding agent. Then it passes off that information to a planner agent, which does some planning. And then it, that planner agent hands it off to an evaluator agent that has access to a sandbox with a bunch of APIs with some synthetic data to do a simulation, essentially, to figure out, is my answer or reply going to be correct. And then once, you know, ah it's, it's, it's, it, it's it's satisfied, it hands it off to the explainer agent, which then kind of like, Um, frames the, the, you know, the code and the, the, the output of the evaluator agent in human language. Um, So that the customer, you know, um, it, it's for, for the customer, it's a natural language, conversation-based interaction. Um, and, and this is multi-turn, multi-pass. So, anytime, you know, for example, planner agent can go back to the understanding agent, ah, um, basically asking for more information for the customer, so that it can plan, you know, better. Uh, and of course, we supply it with, uh, memory, uh, knowledge, uh, tools and APIs, you know, for example, these tools would be calling dealer inventory. Um, in real time, um, or, ah, scheduling, you know, API so that the dealer can actually, ah, the customer can set up a time to come to the dealership to test drive a car. Um, Even to do the tool calling, you know, we had to do a bunch of work to make sure DLLM is calling the right tool, ah, in the evaluator, uh, agent, um, part of it. Now, it turned out that this particular goal-oriented, you know, ah, planning and um evaluation and explainer is actually very general purpose agentic framework. So, now we are in the process of generalizing it and deploying it um for many use cases at the company. Now, the first version of um chat concierge was more of a, you know, custom-built. Ah, we had a very limited time window and we didn't feel like we have to build a whole ecosystem. Ah, ah, we were not sure what the outcome would be. It was more of an, you know, ah, exploratory, um, ah, work. Once we validated the idea and once we started seeing, you know, good usage of it and Ramping up, you know, in, in dealerships across the, the country. Then we started to put together a plan for how do we now generalize it, how do we now build an agent ecosystem, an agenttic platform. Again, uh, sitting on top of our AI platform core capabilities, uh, so that we can really scale decisions uh everywhere in the company. That took us in a, in, in the path, you know, that you see here. Um, the, the last two ones, the Gen AI Core Services and Gen AI Foundation services were already kind of in place because of where AI work, like Gen AI work. But a lot of the agent orchestrator, agent runtime, you know, agent development kit, um, agent marketplace, um, application life cycle management, part of it. Those are new things that we are continuing to build and uh we have, you know, work to do here. Um, but once we are, once we have that infrastructure in place, Uh, anybody in the company would be able to create their own agent, um, you know, put it on our platform, and from that point on, using our SDK agent development kit, and then at that point, the platform will manage that agent on behalf of the user. And to us, again, it goes back to the enterprise platform idea that we don't want these things to, you know, to be bespoke capabilities across the company. We want everybody to use the platform and scale agentic AI for all kinds of applications. So, with that, um, I'm gonna talk about a case study on agent decoding tools. Um, I think everywhere, everyone in this room has heard about like cursor, Windsor, plot code, you know, ah, anti anti-gravity, and, and so on and so forth. Ah, the, the example that I'm gonna show you, it'll talk about plot code, but it's actually a general blueprint. Ah, and at Capital One, we have been deploying, you know, a lot of these tools and we are experimenting early days, um, you know, we are learning a lot how to really embed these tools into our software, um, development life cycle, um. But all the things that I talked about being, you know, uh, enterprise platforms, data, maturity, uh, our focus on, you know, risk management, automating a lot of our risk and cyber controls, uh, all of these are, we see the same parallel as we deploy agent decoding tools at the company, that they are actually supercharging our ability to, to deploy these tools, uh, to our, uh, you know, 14,000, uh, developers at the company. So The challenge is that um bringing LLMs into any regulated enterprise environment creates a lot of inherent risks. And, you know, if you, if you don't address the, the inherent risk, like you will spend a lot of time creating, you know, a lot of like meetings, documents, documenting risk, or you may have to accept certain risks, which are not always ideal. Um, at the same time, when you look at how fast these agentT coding tools are moving, you know, how, how quickly they are getting better at a lot of software engineering tasks, you don't have a lot of time to actually, um, you know, get all of these things in, in shape, um, to be able to bring these tools inside the company. Um, so, what we did is that we created a Uh, uh, kind of an architecture pattern. That will be a zero task AI environment um for one of these tools, clot code, um. But using some of the native security of AWS Bedrock. And again, you know, if you, if you see a common theme in, in this whole talk is the, our ability to uh commingle uh first party, you know, third party and AWS uh together to build a solution quickly. And I'm gonna skip this slide, but I'll come directly to this. And this is an example. Um, plot code is a CLI. Ah, it's a, it's a little tool that you download on your laptop, ah, and then it needs to have access to either anthropic, um, uh, cloud.AI, you know, their hosting platform, or access to clot through Bedrock or some other provider. For us, the Bedrock choice was ideal because we are using the account level permissions, um. And the Bedrock gateway as a, you know, enhanced security for us. We don't have to worry about, for example, whether, you know, the data that we are sending there would be used for training or not. There are a lot of reasons why you want to, you know, ah, use something like Bedrock. Um, for us, it's the, ah, the time to market, the security implications, um, and the scale of Bedrock. Ah, and accessing, you know, cloud, Sonnet and Opus, etc. through the, um, through the Bedrock ecosystem. Now, what we have to build ourselves is the AI gateway. That's where we, we focused on because that's where we put our controls. We put, for example, token rate limiting, you know, you don't want somebody to go in and, you know, kind of like spend $1 million worth of tokens in a single, you know, offline task, um, because, you know, it's, it's the plot code is not the same as like cursor or Windserve. Uh, where it's, it, it doesn't need an ID, right? You can write a bad job and you can say like, go, go through these 20 depots and then do some tasks, you know, do a code review or fix a bunch of things and come back, uh, and you accidentally give it like a huge amount of context, um, and then pretty, you know, you, you get a large, uh, token usage. So, we have implemented a bunch of things, uh, controls on, on our AI gateway. So, and we also built in a lot of like observability into the gateway so that we are doing a lot of detective control um on, on the traffic itself. Um, the other thing, um, we have, we, we are, we are still learning. Um, when you let an agent, um, sit on your desktop, like developer desktop, it has access to a lot of your files and a lot of other information, so you have to create the right sandbox around it. Ah, but we don't want users to disable those settings. So, we have a combination of like managed settings that we fix from the enterprise side, and then it also allows, you know, the, the developer to set certain settings on their own. Ah, again, a lot of that infrastructure we had to build ourselves. Um, but overall, you know, um, we were able to bring this to market, um, to, to live very quickly, uh, in, in a matter of weeks, uh, actually. Um Yeah. So where does it Lead us This is kind of like forward-looking. I think our focus on vertically integrated end to end stack will position us really well as new technologies come, new models come, new architectures uh come up um in the research community. Um, our conscious decision of intentionally choosing a combination of open source, AWS and third-party solutions. Um, that has, you know, worked out really well, and we, we think that it will also continue to work well for us. But we are here today because we spent a lot of time on designing our infrastructure in a way, our control plane in a way that allows us to be able to get these things from different places and to build kind of a hybrid platform. Um, we have full control over our infrastructure. Um, we don't depend on any, ah, particular, you know, vendor, etc. Ah, we can tune our stack, ah, very well at every level. And this idea of, you know, having custom models to custom hardware, to custom pipeline, to custom inferencing stack, or even if you take it to the agentic layer, Um, that is, you know, where I think a lot of the leverage will come from. Um, and then, um, the way we are approaching our risk and cyber controls by automating them, by creating, you know, specialized environments where decisions can, ah, play, um, so to speak, um, and then we are extending these to the, um, to all of the other capabilities, you know, that need AI, um, uh, some kind of agent. For example, you know, data platforms. Uh, when our data platforms build data agents, um, they're not, they're not gonna build a custom, you know, data pla data agentic platform. They will start to leverage our own agent, uh, platform, for example. So, we are extending the same controls, same infrastructure everywhere there is, you know, a need for AI or infusing AI into that particular platform. Now, key to this is our ability to attract talent, um, and develop them. I think if there is a single Um, you know, ah, differentiator among companies, um, going forward, it's really gonna be the talent. Um We have been really lucky, ah, fortunate enough to have the kind of the ecosystem and the environment and the culture, um, to be able to be in a position where we can, you know, attract a lot of great talent. And um one of the kind of like the um Validations of that. As I mentioned in Evident AI 3 years in a row, we have been number 1 in AI talent. Um But we are not stopping there. Uh, we, in the last, uh, couple of years, we have really started to look across and forming partnerships with a, with some of the most forward-thinking, leading universities, uh, in the US. We, uh, we have established, uh, research centers at USC, Columbia, University of Illinois, University of Maryland, and more recently, uh, maybe around summer. Uh, we were the only bank and one of the two companies in the US, uh, that became partner to NSF AI Institutes. NSF, uh, is awarding, I think, 5, academic, uh, groups, 5 national, you know, uh, Science Foundation AI institutes. We partnered with one of them, um, and we are the only bank there. And, and I personally see uh some of the benefits we have had, you know, we have brought in a lot of faculty, ah, a lot of summer interns. Ah, we just started an AI engineering internship program starting, you know, from next summer. These are our pipelines, um, and they come in, they get to work on world-class infrastructure, they get to work with world-class scientists and engineers. Um, and then, you know, they are able to actually deploy something that has tangible value, that actually goes in front of our customers. So, it's a great flywheel that we are creating in terms of talent. So, I will end here saying it's an incredibly exciting time for us in the AI community and particularly at Capital One. Um, you know, um, I have, as I said, I have worked at, um, you know, tech companies, um, but I just find Capital One to be such an amazing place, ah, to be, to allow us to think big and then to be able to build this kind of infrastructure at scale. Thank you.