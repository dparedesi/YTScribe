---
video_id: j8uPcCJQGYo
video_url: https://www.youtube.com/watch?v=j8uPcCJQGYo
is_generated: False
is_translatable: True
---

Hello everyone. My name's Etien. I'm an edge specialist essay based from Sydney, Australia. It's good to be with you here today. I've traveled a long way to be here. I'm here with my My, um, my colleague, uh, Nishid and with my customer, Ambrose. Thank you. Good morning everyone. My name's Ambrose Fay and I'm a cloud network engineer at Atlassian in our edge services team. Today I've flown in from Australia and I'm excited to share in our story, use case and outcomes of the work we've been doing over the past year with Amazon's Edge products. But for now I'll pass it over to Nishi. Hello everyone, I'm Nishat Sahni. I'm the director and general manager for network services at Amazon. These include Amazon Cloudfront, our perimeter protection services, and Application load balancing, ALBs. In condition. So you're in session net 211, the state of Edge. Uh, we're excited to share some insights today, and, uh, we're glad to, uh, bring you along with some of the, the journey that we've been involved with. We're excited to, uh, make sure that you uh take away some of the learnings that Ambrose and his team have worked through. And uh we hope you enjoy it today. This is a 200 level talk. Um, we expect you to know something about Cloudfront Wi and Shield. Um, however, if you don't, don't stress, we'll take you along for the ride. I'm gonna hand over to initiative. Thank you. All right. So let's start by defining what an Edge service is and what is at the AWS edge. Well, an edge service is a distributed computing service that brings network, storage, compute capabilities physically closer to your users, thereby reducing latency for your users. Now, at AWS there are several Edge services that run either partially or fully at the Edge. For the purposes of this session, we're going to focus on Amazon Cloudfront, Shield, and VAF. Just as a refresher, Cloudfront is a global content delivery network. SHIELD and VAAF are security services that protect your applications against layer 34, and layer 7 attacks. Our mission really is to protect your applications, make them fast, secure, and resilient. If you think about it, Our mission really is to make the internet fun and engaging for billions of users across the world. This is what drives me and my team at AWS. Now, we've evolved our services over the last 17 years. As per the evolution of the Internet. The internet has gone through massive shifts, each one redefining how users interact, what applications you build, and what infrastructure services like ours need to deliver. Let's start with how it all began. The web was simple static web. A browser made a request for a file, the server responded, simple get requests. Security was simple, merely IP blocks. caching rules are pretty simple. Along came the transactional and the e-commerce web. That's where APIs became prevalent. Put post requests. We and Cloudfund launched dynamic content acceleration for handling these kind of requests. Security became more complicated. DDoS attacks at layer 34, and layer 7 became prevalent. Then came the next big shift, which was social and mobile. If you think about it, that really democratized the internet, bringing millions and millions of users and scale on the network. Security then became even more complicated with bot activity. These bots acted as humans and tried to conduct or steal information from your websites. Latency also became even more complicated. Then the next step was really social media and uh short form video live streams. Here. Latency attachments became emotional. It wasn't just a number. A 100 millisecond latency difference could be really the differentiator between user engagement and abandonment on a platform like TikTok or Reels. Along the right came Internet of Things. Here, we saw a lot more devices and a lot more requests with different traffic profiles on the internet. But what came along with that was Opportunities for those devices to be compromised by malicious actors and that created these massive pool of uh devices and endpoints on the Internet that would launch DDoS attacks and other web attacks against the applications. We've evolved our services along these uh evolutions and one such thing that we did was security was no longer reactive it had to become proactive so we launched threat intelligence built based on a global set of sensors which we call honeypot Network. That Honeypot network allows us to collect intelligence from the entire web and feed that intelligence into our services at AWS and protect your applications. So what really happened here was the perceived latency tolerance for our users went down. The security and resiliency expectations has increased exponentially. But we are now entering the next era, which is the agentic web, or the AI web. I'm gonna ask a question. How many of you have observed requests on your web applications that are originating from AI bots or chatbots? Quite a few. This is the data that we see on websites that are hosted on Cloudfront and ABS WAF. On a daily basis, billions of requests are originating from AI bots like chatbot, uh, GPT, Cloud, Perplexity, Nova, Bedrock, etc. And this is not just for certain industries like retail and e-commerce. Every possible industry and customers to the, in those industries are experiencing this. And you might be wondering, well, don't we have a way to solve this with something called robots. TXT? Well, that really is the challenge. Robots. TXT was built for an entirely different era. It was when bots were well behaved and respecting. But the bots have evolved. The bots used to be traditional bots for crawl and refer, search bots. They've now increasingly become AI training bots used for training massive amounts of data and models. They crawl and consume. And increasingly, we are seeing agentic AI bots that are conducting transactions and actions on behalf of the users. I hear from a lot of customers, especially in publishing space, where they're asking for help to deal with this bot activity. First of all, to understand what bot activity is on their websites. To apply rules that are, that apply with their business logic, like allowing those bots, denying, or even in some cases, monetizing in a sophisticated manner. So really, the, the agent tech web era is defined by 3 defining trends. The first one is the traffic profile is changing. We see a lot of bust traffic on our network. We see a whole bunch of duplex connections, uh, to chat applications. We see not just large objects for live streams and video, but also tokens that are used for LLMs. Our network needs to evolve for that. The second big trend is security. Now, customers are asking for security of their AI waste applications, but also from AI powered attacks. And lastly, You all, developers are going through an unprecedented era where the development cycles are so fast. You can now build applications with AI coding assistance or agentic systems like Bedrock Agent Core, or even VIP coding platforms like Lovable and Cursor and Kiiro. And you all do not want to be hindered by infrastructure complexity. Our customers want the infrastructure services to evolve with the pace of these new development cycles. We don't want infrastructure to be in the way. These 3 trends are what are driving our investments in roadmap at Edge Services. In the next part of the session, I'm going to talk about These 3 trends and some of the enhancements we've made and how we've helped customers as well as where we are going. OK, so let's start with performance and resiliency. The underpinnings of great performance, resiliency and security is the massive AWS network. This comprises of ADB regions, as well as Distributed pops that belong to Amazon Cloudfront. This network has been growing over 50% year over year. We've added 750 plus pops. In 1140 embedded Pops that are deployed deeply inside high-speed networks. We are now in 200+ cities and 50+ countries, connecting to millions of users around the world. This network handles 1 trillions of requests on a given day, in 100s and 100s of terabits per second peak. But that's not just all. Even from a security standpoint, this network allows us tremendous amount of data for threat intelligence purposes. We analyze exabytes of data every minute. And just in the last year, we handled 6.2 million TDoS attacks, which is a growth of 150% year over year. The largest one being 25 terabytes per second. OK. On this network, uh, we just recently launched a global NEcast Bring Your Own IP. Up until now, you could, when you put your application on Cloudfront, you would get dynamically assigned IPs to your applications. Last year, we launched the NEA capability with static IPs and customers could use that to, to put their applications allow listed for enterprise firewalls or even zero rating agreements with, uh, relation with ISPs. With the recent launch, you can now bring your own IP and enhance that experience. This PYO IP capability comes integrated with VPC IPA as a unified way in which you bring IPs to To AWS. Think about what this does. You now have a global front door, any casted on the AWBS network. So let's go into under the hood of what happens when a request comes to this network. Your users are trying to access applications which might be in database regions or other clouds or on-premise networks. When request hits the airbase edge, the larron pop. There's TLS termination and the TLS optimization that happen through there. We optimize routing to the best pop from a latency point of view, and we get millions of measurements on real-time latency data from web bugs that are deployed across the world. That allows us to route your requests to the best pop. And we handle congestion on the network as part of this routing. Your dynamic content or APIs go over the privately managed backbone, the database backbone, straight to your, um, to your regions, to your applications. For cacheable content, it goes through a series of tiered caches on the AWBS network. These caches are on the ADBse pops, as well as in regions, um, in AWS. Optionally, customers configure Origin Shield, which is yet another layer of caching that allows better offload and better performance through persistent connections back to your origins. Now, As we've seen, a lot of our customers want to customize and personalize their delivery. We've added capabilities like cloudfront functions and lambda edge over the years, along with a global key value store that allows you to do AB testing or manipulate requests response capabilities on your requests. This again, allows you to put compute closer to where your users are and reduce the latency that they experience. Over the years, we've added a lot of performance enhancements to different parts of the request flow. One such enhancement was Quick STTP 3, launched in 2023. Quick STTP 3. Originally from Google, allows faster time to first bite. And it's, and leads to um PCP head of line blocking removal. What it does is it combines the TLS and the HTTP handshake into single connection. Over 25% of requests. On Cloudfront today are on Quick. And this is something that is available just by default. Uh, last week, we added another capability for, uh, enhancing the performance of QWI and HTTP 3. We added support for HTTPS DNS records. Now, this is a DNS record type that allows us to send hints about supported HTTP protocols as part of DNS query itself. Let's understand what that means. When your client or browser session requests an IP address for your application hosted on Cloudfront, it makes the DNS query, and that's where it gets the A or cod A address of the Cloudfront pop. Your browser then establishes a TCP TLS and HTTP connection with Cloudfront. That connection happens over HTP 1.1 or 1 or 2. Then Cloudf provides a clue to your browser saying, We support HTTP 3 as part of all service headers. That leads to an additional uh handshake and round trip time when connecting to the optimal protocol. What with the launch of HTTP DNS records, In the query itself, the DNS query itself, you, we will pass support for HTTP 3 as a signal. When you now make the connection to cloud front, it comes with Already the optimal protocol selection of SUTP 3. This has been a huge hit as it leads not just to performance benefit, but also DNS query, uh, cost reduction because Alias Records on Route 53 are free of charge. There's some other enhancements that we made on round trip reduction. Let me walk through two of those. The first one is TLS 1.32 origin. We just launched this capability that allows you to establish TLS 1.3 origins, uh, connections between Cloudfront and your supported origins. So if you have your origins that support TLS 1.3, this just works. We've seen almost 36% improvement in handshake time, as with TLS 1.3, it cuts the round trip connections for TLS exchange. The second one is TCP Fast open. TCP Fast Open allows data to be sent as part of TCP SIN exchange. We've added support for this and observed 50% improvement in TCP n times. This is available between cloudfront Pops and our internal caching layers. Again, works by default. The best part about these performance enhancements are that they are enabled by default. They just work and they're free. So, I would recommend that you look at these and see if these apply to you, especially STLS 1.3 and XTTPS DNS records, and take advantage of these with your, with your application delivery. OK. We're now going to switch to this next part, which is building a secure edge. I'm gonna invite Etienne to walk you through that. Thank you, Nishid for all the work that you and your team put together to, to improve this uh performance for our customers. So I'd like to talk a little bit about how we provide security edge, um, what you can do and what is provided to you by default when you use Cloudfront. So Nishit described a little bit about how the request path happens through our infrastructure. And I'm going to talk a little bit, in a little bit more detail about what we do at the Edge. So typically at an edge location. We provide connection from there back to. An AWS origin, which may be a load balancer, or it could be to your on-premise location. With that type of connection, we can provide to AWS origins using VPC origins where we can ingress traffic into a private subnet, thereby reducing the, the tax surface, reducing the need for public load balances. And recently just launched is a cross-count VPC origin support. So you can actually use almost a hub and spoke model for using Cloudfront to route into private subnets. Across multiple accounts. Again, we have Origin access control to help protect your access from Cloudfront to something like Lambda URLs or your buckets and some of our media services. So when traffic comes to your cloud front distribution. Claron will provide a tier certificate for someone to connect to. Right now, we've actually just launched MTLS support so a client can provide um a certificate and authenticate that way as well. On top of that, uh, the team have enabled, uh, post quantum, uh, cryptography support or cipher support. And so what this means is that, um, by default, when you use Cloudfront, You're future, uh, you're future-proofed for that type of attack risk. On top of that, Cloudfront provides the ability to protect against layer 3 and layer 4 attack vectors by using Shield. This helps you protect against issues of known offenders and provides a sin proxy. There's continuous inspection at the edge, and there's protocol validation and packet validation. There are automated routing policies for particular large attacks so that it helps mitigate the risk away from um your applications. Cloudfront also provides the ability to Uh, monitor and, um, check the resource application usage at the edge, so we can make sure that, um, attack surfaces that would be vulnerable to slow laris and HTP2 rapid reset are negated. There's further protections that you could have with using AWSYF. You can elect to turn this on on your platform distribution. Here you can use controls such as rate limiting. By aggregate key employ Amazon manage rules such as the anti-DDS rules and our Amazon partnered and manage rules as well. You can build your own custom rules and use bar control and forward control rule sets at the edge. So let's talk a little bit about Buck control and fraud control. Our anti-DoS rule set allows you to very quickly turn on protections, um, to protect your application against volumetric attacks. This is something that you can tune and we'll talk a little bit more detail about this later. With AWS Wiba control. You can choose two different types of modes of operation to protect against common bots that are easily detected through common signatures, and then looking at behavior and machine learning capability with targeted bots. These are for sophisticated bots. We'll talk a little bit more about some of the innovations that the team have introduced. On top of that, we have our account takeover fraud prevention rule sets, and our account creation fraud prevention rule sets. These are particularly helpful when you want to deal with credential stuffing attacks, or if you have a particular benefit when someone wants to sign up an account, but there's a financial benefit or a um a scraping benefit that someone would want to uh take advantage of. And these would help prevent, provide you the controls at the edge of the network to prevent that risk. So let's talk about anti-DDoS rules and how these rules reduce the attack surface. DDoS attacks, uh, when I've helped customers mitigate these DDoS attacks, rate controls alone are not sufficient. And so you would want to not spend time going through your logs and trying to figure out how to put these protections at the edge just with simple IP controls. What you can do is use the anti DDoS rule set, and which is quite quick to deploy and within a few minutes, you can have an effective control. What's really neat about this is that you can tune this rule set based on the type of workload you have and your appetite for absorbing a certain amount of traffic. Typically, you'd want to make sure that this rule set sits in the top part of your rules so that it's exposed to as much of the traffic signature as possible. This allows you to tune the rule sets, decide which paths you want to protect. You can scope these protections. You can add exclusion lists. There may be parts of your application that you don't want this control in place for. And then there's a rich set of metrics that are emitted into Cloudwatch that you can then use to drive alarms and to be able to build some dashboards to understand the type of DDoS risk that you're facing. This helps you make sure that you can more quickly adapt and mitigate for your volumetric attacks. And improve the accuracy of your DDoS mitigations. There's a challenge capability in the anti-DoS rules that allow you to have a conditional action to help prove out that someone is a valid user or not. And the granular granular controls allow you to change your rule sets based on the risk that you're facing at any one point in time. You may have an appetite that um you would have a lower risk profile during A large event and um. Um, you may have a, uh More, uh, less uh less of an appetite during a large event. Yeah. All right. Pivoting to what we've recently released, which is Mutual TLS or uh Mutual uh Transport Layer Security. Authentication, which is a security protocol that extends uh the standard TLS authentication into a bi-directional certificate-based authentication. If you've not yet used this, what this simply means is that a client provides a certificate to help prove that it's um who it is. And Cloudfront has two modes of operation where you can Allow for both standard traffic with a normal certificate and that of Mutual TLS and TLS Mutual TLS only. This allows you to migrate between the two models. We're quite excited to see what our customers do with us, uh, and please have a look, uh, at this capability. I'm personally quite excited about what we, we're doing in the space with um Agentec, uh, in the, in the gentec internet era. And this is an additional capability that was recently launched. What we have found is that businesses are facing um Um, an onslaught of, um, traffic via, um, agents. And what, what this means is that you potentially need to be able to have not only just a control at the edge, but you also need to have a rich set of metrics. Bar controls at the center of this. And this allows you to make sure that legitimate AI agents can access your distribution. Through a process of a crypto cryptographic control. And this allows you to then to uh create fine-tuned controls for your AI agents to access specific parts of your application. Whilst denying access to others. ASA have these managed rules that allowed that automatically allow verified bots whilst blocking unverified traffic. So The first step is to make sure that you can identify the risk and then decide what you want to mitigate. This is done through both categorization. Or through the simple um AI label that you can choose which specific bot you want to block. With we about authentication. You'll be able to allow your application to be more deeply integrated into your WA to make sure that those parts can be monetized if you choose to. So there's an additional level of capability that um is continuously being built on and we'll keep adding to this capability in time. Now, I'd like to hand back to Nisha to talk a little bit about some of the work the team are doing to help improve our developers' lives. Thank you. A part. This is my favorite part. So let's talk about Security. Now, lots of customers tell us that configuring security in this ever evolving landscape is hard. But it does not need to be hard. This is an area that we are massively investing in solving this challenge. Earlier this year, we reimagined uh a developer experience for configuring security rules with Cloudfront and ALBs. Here, you can add VAF capability as part of protection packs. These are tailored rules and rule sets that are designed for your traffic profile. It comes with improved visibility. As well as easier, more intuitive configuration controls. We're adding more capabilities here to bring smarter recommendations and automated traffic analysis on these rule packs in future. Already with these enhancements, we've seen about 80% reduction in the initial configuration steps and a dramatic improvement in the success rate for customers configuring their security rules on the front doors. Now, here's another favorite part of mine. Developer complexity is not just technical complexity, it is also pricing complexity. When Cloudfund was launched back in 2008, We disrupted the Syrian pricing industry with pay as you go pricing. Customers could simply use the CDN and pay as their traffic grew. And the pricing scaled accordingly. But over the years, customers have told us that while they like the pay as you go pricing model, They're worried about bill shocks. They might see their websites either getting really popular and viral, and it would lead to sudden increase in bills, or they might get hotlinked or even suffer from DDoS attacks that might lead to exponential request charges. We've solved that problem with what we call flat rate pricing plans. This was launched 2 weeks ago. Um, it basically starts with $0 pricing plans that include not just cloud front, but security services, Amazon S3, Route 53, CloudWatch injection, ACM, and a bunch of other services. These pricing plans come with no overages whatsoever. You pay a fixed monthly price or you start with $0 free plan. And then you upgrade to these according to your growth of your business, but you never would see overage or unpredictable pricing anymore. There's no long term commitments either. In just a week or so since launch, tens of thousands of customers are using, taking benefit of these plans and serving their applications. Something that if you haven't looked at, I would encourage you to take a look. Right. Another great simplification is Cloudfront Sas Manager. Lots of our customers host millions of domains or 100s of thousands of domains in multi-tenant applications. The white coding platforms like Cursor, Lovable, etc. also allow customers to build applications with lots of multi-tenant domains and, and tenants. Up until this time, you would have to create individual distributions and apply policies such as cash policies or security policies individually on those distributions. With Cloudfront SaS Manager, it provides a unified way and a dramatically simpler experience in which you can handle multi-tenant applications on the AWBS Edge. You can apply template policies that can apply on the entire list of tenants, or you can override certain specific policies because of, you may have some premium set of customers and, um, you know, other tiers of customers. This is again, one of the great simplifications that we are evolving. This is the first step, and we're adding more capabilities, such as tenant-specific analytics and reports. Great. To learn about the real-world applications of the services and enhancements we talked about, I would like to invite Ambrose from Atlassian. Thank you initiative. I'm excited to be sharing in the ways in which Atlassian has leveraged Cloudfront, WAF, and Shield to improve our global security footprint alongside some useful patterns we have found in our widespread adoption journey. To give some background, Atlassian as a whole aims to unlock the potential in every team. We do so by offering our applications across a variety of collections, but all in the goal to help teams organize, communicate and collaborate around shared work. Some commonly used applications you might have seen before include Confluence for information sharing, Jira for project and issue tracking, Loom for asynchronous video messaging, Bitbucket for code-hosting, and Rovo, our AI powered suite of applications and agents connected to the Atlassian platform. As of today, Alassian now serves millions of customers globally across a wide range of industries. Atlassian's come a really long way from when we first launched in 2002. Over the years we've released and acquired many products, and even went public back in 2015. Now in 2016, Atlassian began the journey to move all of its workloads to be fully hosted in the cloud, and as of 2022, we completed this journey. As of today, we handle tens of billions of requests, over 10s of thousands of EC2 instances across 13 different regions, handling multiple petabytes of data out to clients every single day. But that's not the end of our story. Over the years, the team has continued to improve and develop upon its cloud posture and architecture, leading to our resultant migration of our largest products, Duran Confluence, onto cloud front. Now Atlassian already has a really long running history with Cloudfront. We've invested several years and deployed thousands of distributions. This figure here shows a historical high level view of a request flow on what a page load might look like. Dynamic requests land on a network load balancer towards our ingress proxies, before getting routed to a number of different upstream services, and Cloudfront, accelerating our static, front-end and single page application delivery across the company. As of today, every single page load on every Atlassian product uses Cloudfront in some way. And to add to that, we were already using a mixture of off the shelf and internally developed technologies to mitigate inauthentic and malicious requests. So with that, what more were we looking to achieve? Building on our rich experience with Cloudfront, we wanted to leverage it beyond just acting as a basic CDN. We still had not yet adopted this infrastructure for the delivery of our first fragment or primary domains on our major products. And so our goals of moving Dura and Confluence on the cloud front were multifaceted. We wanted to strengthen our global security footprint against growing sophistication in modern attacks and compliance regimes. Deploy a layered defense at the edge, leveraging a consistent control plane to help with operational burden. Identify common and shared logic in our application which we could perform closer to our customers. And finally, improve our last malperformance for our end users. Now, potentially, you might be thinking if a large portion of our goals were pertaining to security, why not just place WAF on our existing load balances, and that would be a good question. Firstly, as network load balances are a layer 3, layer 4 construct, WAF logic just doesn't belong there. And whilst we could've attached them to our application load balances, this would have still left our ingress proxies needing to do a lot of the heavy lifting to mitigate inauthentic and malicious requests. We wanted to push this defense beyond the compute infrastructure we managed. With that in mind, the journey of getting Jira and Confluence onto Cloudfront would pose quite a unique challenge. As it would need to handle orders of magnitude, more traffic than any previously existing cloud front distribution. Accommodate logic to support customer data distributed across 13 different regions. Be able to host millions of custom domains and customer sites. And finally, support an architecture which aligned with our customers' security requirements. All in all, this would prove to be more involved than just changing where a domain record pointed and required us to use some of Cloudfront's newest features. One of the big questions before even moving to Cloudfront was to identify how we could differentiate ourselves amongst other Cloudfront customers. To build some context, Atlassian has a variety of customers who might operate their workstations or API integrations under strict egress controls. Others might be doing something similar to this figure, where they leverage split tunnel VPNs or secure web gateways to ensure that our products are only accessible from their corporate IPs. Such customers require that their cloud SAS provider present itself on the internet from a predefined list of IPs not cohabited with any other SAS software. Allowing them to perform enterprise application allow listing. Typically, when you use Cloudfront, your distribution will use a rotating pool of IP addresses to serve traffic from. As you can imagine, this would pose quite a problem for our customers. As there would be no clear way to differentiate ourselves amongst other cloud front services, uh, other services sitting behind cloud front. But this is exactly where Cloudfront's any car static IP feature came into play. By leveraging this new feature on our cloud fronts, we were able to dedicate a set of IPs across our cloud front distributions, allowing ourselves to be uniquely identifiable against other cloudfront customers. This was a massive unblocker in unlocking the transition of Jira and Confluence onto Cloudfront, as many customer network policies just would not have been compatible with CloudFront's traditional shared dynamic IP space. With the confidence that we had a unique way to present ourselves, our eyes turned to protection. Building on our previous experience, we launched the Shield Advanced and layer 7 application protection feature Enable. This enables WAF and Cloudfront to intelligently catch DDoS attacks. We then combined this with managed rules for anti-DDoS, common web attacks, IP reputation lists, alongside a variety of rate limits. Now, when crafting your managed rules, it can sometimes be overwhelming trying to decide which rules should go in what order. And this figure here is a fantastic example and mental model approach to use. But you might need to do something a bit more bespoke to your environment. What we have found to be effective for us is by layering our rules in the following order. First, label any traffic that you might want to apply modifications or exclusions against in different policies in your chain. So these might be for trusted IPs, countries, or even specific clients. Next come your strict blocking rules. So these might be your known blocking conditions or managed rules for common web attacks. Neatly behind those come your course grain rate limits and custom rules, and finally your IP reputation lists pushing out silent challenges and captures. Now, I want to take a second to call out the managed anti-DDoS rules set. This is something you might want to put earlier in your rule chain, and we've seen this being incredibly effective at stopping the initial and largest wave of attacks in a matter of seconds. It's also paired really well with SHIELD in blocking the remaining trail of malicious requests. All of this combined has allowed us to help create a cost effective protection funnel at the network edge. As part of all of this, you'll likely need to perform some tuning. This typically is quite an iterative process and does require both time and patience. As per this figure, it is recommended to first implement your rules set into count mode, allowing you the opportunity to observe and adjust any logic before promoting it to a block or challenge action. In our case, some interesting tuning scenarios included confluence pages with .comf, MD, or Java exceptions in them. Others included scenarios where customers would put bits of source code or program outputs into their pages, searches, or even jury ticket comments and descriptions. The result of all of this work though, is we're now blocking hundreds of thousands of requests now made by bots and scanners. As well as the occasional 10 and 100 million DDoS attack. Another capability that Atlassian provides to its customers is something commonly referred to as data residency. This gives organizations the ability to choose where their application data is hosted, such as whether their data is globally distributed or stored in a defined geographic location, such as the EU or the US. This is particularly important in regulated industries where customers need to meet a number of data management requirements. So using this figure as an example to help paint a picture. Let's say we have a company whose confluence tenant resides in the EU but they might have employees living in the US. Previously, requests would land on the nearest ingress proxy to them, so let's say in Virginia, before getting routed cross-region to their backend tenant in the EU. Bringing our attention back to Cloudfront, we wanted to find a way to optimize our ability to route our customer requests to their respective data tables which could be located in one of our 13 different regions. This is exactly where the origin control helper method for cloud front functions came into play. Now, leveraging cloud front functions and this new helper method, we were able to implement logic where we take a customer's site name, look them up in a key value store to identify their backend region, and subsequently steer traffic directly to that customer's back-end tenant. We were still able to route traffic to the closest possible region where that made sense, such as for loading common front-end components and assets. In total, across a number of experiences we saw improvements in our time to first bite, but more importantly, for requests that were region steered, we managed to completely remove cross-region charges and benefit from a zero rating of data out between AWS Regions and Cloudfront, resulting in some very natural cost opttimisations. Now, with the mention of cost savings, you're probably wondering, but how much? Well, here are the results. Each color in this graph is representing an individual region, and in total is representing the stacked total cost over time. As you can see, we've seen significant reductions in our data transfer out fees, thanks to the 0 rating between Amazon Regions and Cloudfront. We've also seen similar reductions in our cross-region fees thanks to Cloudfront doing the work of determining the data residency region. Additionally, we enjoy zero rated requests to Cloudfront that get blocked by WAF opposed to if they were blocked on an ALB. This region steering feature has been so well received across the company that we've had many requests from teams across Atlassian to help provide similar functionality to their services too. So all this combined now means that we're delivering a more secure experience presented behind unique static IPs and we're leveraging Cloudfront to pull data from the correct origin region, all the while saving money doing it. How amazing is that? Before I move on to the next section, I want to share some additional notes that we've learnt along the way. First, if you're using a network load balancer as your origin, make sure your proxies return the HTTP connection close header during draining events. This can help prevent some surprising connection terminations on in-flight requests as your proxies scale in. This isn't a problem with application load balancers as they'll add the header for you to tell Cloudfront to close the connection gracefully. Next, Cloudfront supports the use of stale if error and stale while revalidate case control directives. These respectively let you serve stale cache data during transient errors and allow Cloudfront to asynchronously update its cache, reducing latency spikes and origin calls. Finally, a low hanging fruit to improve performance. Look to tune your origin keeper life timeouts where appropriate to benefit most from persistent connections. For us this has helped us yield around 100 to 150 millisecond speed ups. In parallel to all of this work, the team also wanted to find a way to improve the observability we had at the network edge. Cloudfront itself comes out of the box with a range of different metrics and logging. And these might range from giving us insights into where our users are coming from, the types of requests that we're getting, and even the responses that we're sending back. But these are all from the perspective of Cloudfront. Our team wanted to enhance the visibility we had into the network and enrich our overall view. This is exactly where network error logging came into play. For a little background, network error logging is a client side mechanism that is supported by a variety of browsers, particularly Chrome and Chromium. Which can generate and send reports about network issues as seen from the client side. As members of the Network edge, this data about the availability of our Edge as seen by our clients can be incredibly valuable when diagnosing a variety of network issues, including ones uh about issues that might never reach our infrastructure. So, what did this implementation look like? Believe it or not, it wasn't overly complex. By getting our reverse proxies to add the report to and AEL headers on all client responses, we can tell supported browsers where to report errors to. Leveraging Cloudfront to act as a globally accessible endpoint, we run a lambda at edge function to intercept and process all requests to be sent towards the kinesis endpoint before further processing by our observability and monitoring services. This architecture allows us to leverage cloud front and lambda at Edge tourlessly process these network error logs into Kinesis. And this enhanced visibility has already proved useful when investigating low grade network failures. One example of this was when we were investigating an intermittent TCP reset issue, impacting a percentage of customers across a random country. These errors were occurring before requests even reached our CDN infrastructure. But thanks to our serverless network error logging architecture, we were able to gain insights on failed requests that were not yet visible from our cloud front metrics or logs. This information has helped us accurately identify start times, error rates, affected clients, geolocations, and the exact TCP error itself. By collaboratively working with Amazon, this allowed us to confidently and accurately identify the root cause of this connection issue pertaining to a network change outside our own infrastructure. Alright, we've talked about WAF, Shield, and a few different ways we're using cloud front distributions across a range of our products and services. There's one last product that the Cloudfront team has shipped that I would like to share how the team is looking to use, not just scale our services, but also scale the operational effectiveness of our team. Everything I've discussed so far has been to has been to do with standard distributions. These distributions are singular and contain all the settings pertaining to origin configs, cache behaviors, and security settings. What that has meant for us is that by deploying these distributions in front of a variety of our products has been an involved task between members of our edge team and our service owners, where we provision the distribution for them and then begin this dance of collaborative effort to transition traffic across, tune a variety of security rules, and just in general, smooth out the rough edges. With that in mind, Atlassian has thousands of ancillary services which range from supporting features and functionality on our main product to the purposes of internal tooling. This approach of standard distributions just did not scale with our DevOps model and philosophy of you build it, you run it. On top of that, operating and managing thousands of distributions at scale in a conjoined way just created too much friction. We were hitting service limits, account limits, and it was going to be too operationally painful to issue our parent level configuration for these shared distributions. We really just wanted a way to have the flexibility of enforcing a strict base configuration that that the Edge team could manage, control, and roll out. Whilst allowing our service owners the ability to own and run their own cloud fronts without needing to be cloud front experts. This is exactly where Cloudfronts SAS manager played a huge role in unblocking this operational nightmare. Clive Francas Manager has a construct called multi-tenant distributions, which act as a parent template for child distribution tenants to inherit. This effectively gave us a really clean demarcation point for what and how our team manages the global configuration that we expect all distribution templates to adhere to, whilst giving our service owners the flexibility and the ability to load parameters into these defined templates, to define custom origins, domains, and even apply custommisations to their specific distribution WAF. So, in practice, how did this look like? We deployed numbers of distribution tenants based on use case to to help improve reliability and reduce blast radius, including the addition of a canary style template to soak initial changes on. Next, we integrated a service broker for our service owners to interact with, which provisions a distribution tenant on behalf of them. This additional broker also helps us enforce final base configuration elements which we want to restrict our service owners from having the flexibility to adjust. Such as specific inclusions of WAF rules we want permanently enforced. As a result, this will pave the way for our ancillary services and their service owners to onboard onto Cloudfront and WAF, being able to do so in a self-service and self-managed model, leading to increased speed of cloud front adoption and allowing our developers to focus on building more features at the edge, opposed to the handholding of individual cloud front adoptions. As I look to wrap up, I want to reiterate on a couple of key takeaways. First, look to leverage Cloudfront's new any car static IP feature in scenarios where your customers have specific firewall requirements. In multi-origin and region scenarios, think of how you can leverage the new origin origin control helper method to steer traffic to different origins. This can potentially help reduce or even avoid cross-region region data transfer fees. When you need a scalable multi-domain solution whilst keeping configuration and infrastructure simple, consider the use of Cloudfront SAS Manager. And finally, Ben, a principal engineer in our team, has open sourced the Python lambda function which prepares cloud front logs sent to S3 for kinesis. This can help save a lot of money versus real-time logs at scale. Finally I'd like to just say thank you for taking the time to listen to our story and our journey, and I hope you've all found something useful to take away. But for now, I'll pass it back to Etienne. All right. Thank you everyone for, for having a listen. Um, we hope that you've enjoyed today's session. I just wanna highlight that we have a number of sessions that will dive a lot more deeply into a number of the topics that we've discussed today. I'd encourage you to look them up. Uh, this is worth taking a photograph of it. If it's too small, have a search for any session that starts with N E T. And that will get you, uh, in the realm of these sessions. Lastly, I'd encourage you to go build. Um, it, it's never been a, a, a better time to go build with Cloudfront, and especially when you start layering in Shield and WF. Um, thank you again for your time today and, um, please, uh, uh, uh, provide some feedback for us. Thank you. Thank you.