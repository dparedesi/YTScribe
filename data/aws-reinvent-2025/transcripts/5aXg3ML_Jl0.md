---
video_id: 5aXg3ML_Jl0
video_url: https://www.youtube.com/watch?v=5aXg3ML_Jl0
is_generated: False
is_translatable: True
summary: |
  AWS technologists Don Simpson and Devanhu Budhiraja share patterns for architecting multi-cloud data and generative AI solutions. They frame multi-cloud as running workloads across cloud providers (not SaaS) and highlight common drivers: mergers and acquisitions, differentiated services, and regulatory demands such as data sovereignty or concentration risk. Using the cloud maturity model, they emphasize top-down data strategy, people/process/technology readiness, and skills gaps. Customer quotes underscore pain during M&A integration, the pull of data gravity, and the need for intentional workload placement rather than hype-driven cloud hopping. Start from the business outcome and work backward.
  Devanhu catalogs multi-cloud data scenarios and challenges. Data gravity pulls apps toward where data already sits; governance and controls differ by provider; forced multi-cloud after acquisitions slows time-to-value; and inconsistent APIs complicate security and lineage. To mitigate, he outlines materialized views (precomputed, cross-cloud snapshots to trade storage for speed), federated queries (Booking.com-style access without data movement), decentralized architectures such as data mesh, and strong metadata catalogs to track lineage. AWS examples include Athena with Apache Iceberg and connectors to BigQuery; Google’s BigQuery Omni is a converse example. In a data mesh, domain teams publish data products to a shared marketplace so consumer teams can self-serve, but success depends on shared standards to avoid duplicate or noncompliant datasets. Federated query engines need metadata discovery, access/security layers (keys, certificates, IAM), and performance tuning to minimize egress; poor sync or stale catalogs lead to errors. Recently announced AWS-GCP interconnect can reduce latency and egress for these patterns.
  Don then covers multi-cloud retrieval augmented generation (RAG) patterns. Classic RAG embeds a query, performs semantic search over a knowledge base, and injects retrieved chunks into the prompt. In multi-cloud, keep knowledge bases near their data while running the AI app elsewhere, but use consistent embedding models and consider replicating vector stores (e.g., S3 Vector) for resilience. Structured RAG converts text to SQL and queries relational stores (e.g., Redshift via Bedrock Knowledge Bases) to blend structured data without moving it. Graph RAG uses graph queries (Cypher, Gremlin, SPARQL) against Neptune to traverse concepts and relationships across document boundaries; the graph can reference data that remains in other clouds. Network reliability, security, and consistent models remain key design constraints.
  For agents, they show how the Model Context Protocol (MCP) can expose tools and data sources when a federated connector doesn’t exist. Place MCP servers close to each data store in each cloud and let the agent call them via MCP clients, gaining “federated” tool execution without broad network access. To reach models unavailable in Bedrock or in constrained regions, an LLM gateway/router (OpenAI-compatible or Agent Core gateway) can route requests across providers, apply load balancing, and even cost-based routing while presenting a unified API. Combined, an agent can use the gateway for model access and MCP for data/tool access.
  Recommended next steps: invest in multi-cloud skills, define workload placement and data governance up front, pilot patterns like federated query or data mesh with clear standards, use managed interconnects to cut latency/egress, and bake security and performance into multi-cloud RAG and agent designs. AWS offers deeper 300-level sessions, kiosks, and account-team engagements for hands-on guidance.
keywords: multi-cloud data strategy, federated queries, data mesh, RAG patterns, MCP agents
---

All right, can everyone hear me? Great. Do you have any trouble, just kind of get their attention in the back, they'll come over to help you. Uh, first of all, thank you very much for coming to our session today. We're gonna talk about architecting multi-cloud solutions from data mesh to generative AI. My name is Don Simpson, and I'm a principal technologist on our cloud and AI innovation team, and our team focuses on things like multi-cloud, data mesh, generative AI, and many other things. I wanna give a chance to my colleague Devanhu to introduce himself. Hi everyone, my name is Devanha Budhiraja, and I work as a senior solutions architect with our financial services customers, and I've been dealing with many of the financial services customers, insurance customers who are in this multi-cloud situations and wanted to build an effective data strategy. So yeah, we're here today. All right, thank you. All right, so we're gonna get started, um, so a few things I wanna set some expectations, um. Today we are going to focus a lot on cloud agnostic patterns primarily we will secondarily talk about AWS services that relate to the patterns that we're talking about, um, and then at the very end since this is a silent session, we will have an opportunity afterwards if you have questions to talk in the hallway. Um, so the first thing that we wanna talk about is our agenda real quickly. We're gonna go through a data strategy overview. We're gonna level set. We're gonna go through, uh, voice of the customer, uh, things that we're hearing from customers. We're gonna talk about scenarios and challenges, and then we're gonna go into recommended design and architectural patterns. And lastly, um. I think we could not get away with not having a generative AI component, so the very last session or part of this will be about multi-cloud generative AI. So first of all we wanna level set here on what do we mean by multi-cloud. Um, when we talk about multi-cloud at AWS, there's a few things that we think about. One is where you have multiple workloads, IT solutions or applications that are running on more than one cloud service provider. Uh, so we don't necessarily put things like SAS into that, um, so I think it's good for us to kind of have that understanding. Um, the other is that we, uh, base our understanding and what we'll talk about today on the cloud maturity model, um, which is part of our open alliance, uh, for cloud adoption, um, and then it's also important, I think, for us to understand kind of primary and secondary drivers for multi-cloud. And one example for primary cloud or primary driver would be mergers and acquisitions or M&A. So a lot of times we have customers that end up with a multi-cloud operating environment because of because of an acquisition or multiple acquisitions, and oftentimes those acquisitions come with um. Oftentimes they're abrupt, right? They're sudden, they're not announced and they happen, um, and there's, you know, some transition challenges that we see, we see with customers going from diligence to integration. The other is that we see customers looking at multi-cloud for differentiating capabilities, so differentiated services, and then we also see it from a regulatory perspective, and that could be data sovereignty, that could be cloud concentration risk. These are things that we hear from customers that are coming via regulators. So as we're talking about this definition of multi-cloud, I just want to do a quick show of hands from the cloud from the audience here. How many of you have data that spans multiple clouds, and that can be siloed, that could be you're trying to integrate and correlate. OK. And then also if you answered yes to that question, how many of you have challenges actually achieving your business outcomes because of it being spread across multiple clouds. OK. Almost everybody who raised their hand the first time, um, so when we also let's go into that multi-cloud approach that I mentioned a minute ago, um, the cloud maturity model, um, this is focused on people, process, and technology, right? And we believe that a top down vision, um, and strategy of creating data as a strategic asset. Or um you know, value that is created, you know, by either efficiency of operations or additional business value, we tend to think of that that top down is extremely important. So when we look at people, one of the areas that we find that our customers are struggling is with the multi cloud skill gap, right? So we think from a people standpoint it's important for us to, uh, upskill our employees, augment augment them where we can with agentic AI and AI assistance, um. And then also when we look at the process, right, so the maturity model we tend to look at organizations based on possibly a CCOE, how they've constructed that CCOE in terms of specialization, whether they have specialization at individual CSP level across certain areas like architecture, uh, platform engineering, and then lastly technology. We tend to look at technology first because we're technologists, but multi-cloud architecture, multi-cloud services, and some recent announcements like our Interconnect for multi-cloud that we just announced, those are steps that we're taking as AWS to offer services that allow our customers to operate in a multi-cloud environment better than they could before. So let's real quickly go through a few definitions here. I like to put up these two definitions because I think they, they complement, but they're also different enough. So the first is from Gartner, um, and this data strategy is really, it focuses on a few things that are really important, right? Highly dynamic process, right? And I think we all have experienced this. You build something today, you find out tomorrow that you have additional requirements, something's changed. It's dynamic, um. And this definition tends to focus in on the acquisition organization analysis and delivery of data, right? So really about that process, whereas you take an AWS definition here, um, and again you're seeing some similar themes, right? People process technology and rules. And these two, as you think about this, one is a little bit higher level, right? The other, you know, kind of driving into the technology, and you'll start to see that in our discussion later, later today. We'll get into things like data mesh. We'll get into things that are federated query, things that you can use as patterns. So I'll use a few examples, some voice of the customer, um, in this particular case I've highlighted the things that I think are relevant to these quotes. Um, we talked about M&A a minute ago. I mentioned the integration phase, right? And so you're hearing from, we hear this often from customers, right, that this is really where they struggle, um, with the integration. They've acquired a company. They've, you know, they're kind of their day 0, day 1, depending on how you look at it, and they started their process, right? There's a lot of business process associated with M&A things like workload placement strategy, like where do my workloads go. I have certain workloads in an acquisition that are on one cloud service provider. Do I migrate those, right? Do I repurpose it? Do we re-platform it? What do I do with that? Um, and then lastly we just get into that technology integration, right? Like do I have specific integration patterns that allow me to bring in an acquisition. You know, for example, connect, you know, so that we can transfer data, you know, safely between the two organizations without getting full network access. And this, what's the most important here is to come away with a proactive strategy. So I mentioned things like workload placement, right? Having intent when we make acquisitions so that we can actually be proactive about how we integrate that. Uh, the next year we're going to focus in on multi-cloud architecture, right? Um, this is again kind of a differentiated capability, um, having the right data and technology in the right cloud, and there's some things here that aren't said but that are actually present, such as data gravity, which is a real challenge, right? The more data we have in a particular location, the more data and the more applications and services we attract to that. Um, so this becomes a challenge, right, for us, um, dealing with multiple clouds, and this is a, you know, kind of a leveraging differentiating capabilities, um, and a lot of times, you know, we look at this, um, when we're thinking about differentiating capabilities, we like to come in with a good understanding of why we're doing something, and I think that's extremely important when we're thinking about multi-cloud. And last quote, um, this one here is an interesting, you know, observation about a lot of a lot of talk about dynamically moving from cloud to cloud and throttling up and down and compute is overhyped. The reality is for some workloads in some industries that may be OK, but for, you know, a highly regulated customer this might not be the right strategy. So again. Um, we often times hear everyone else is doing this, so I need to consider and do that. We just like to say start with a good intention, right? What is the business outcome you're trying to achieve and work backwards from that, um, so again, ensure grounded guidance, right? So make sure that you have done your research and that you know is this the right strategy for you. So at this point I'm going to pass it over to Devonhu to talk about multi-cloud data scenarios and challenges. Thank you, Don. Uh, I hope you all still hear me OK? OK, perfect. Yeah, so, um, as Don mentioned, uh, and very briefly talked about, uh, the definition of multi-cloud, we're gonna talk about some of the scenarios related to data and the challenges, and then how do we solve these challenges. So, let's talk about some of the scenarios first. And uh we just talked about the data gravity. Yes, that's a, that's a real challenge for multi-cloud because. You know, um, the data gravity, uh, although you would want to use, let's say more than one cloud for different reasons, uh, maybe because of cost or the regional availability or best of breed, whatever the reason is, the reality is that, uh, data has gravity. So wherever you have your key applications, that cloud would pull all your data towards it. And that data gravity challenge kind of discourages you from using multi-cloud effectively. So we're going to talk about how do we solve such challenges. Uh, also with multiple clouds, uh, you have the data governance and control challenges, so challenges like how do we implement the governance and security policies, how do we implement the controls because each cloud might have different ways of doing things, different APIs, etc. Uh, and Don also talked about like mergers and acquisitions, uh, which is sometimes I kind of call it as a forced multi-cloud situation even if you did not want to do it. Uh, the intention was to kind of get the best out of another business which has been acquired. But at the same time, the data which we wanted to get out of that business, the time to realize or the, the time to monetize the data sometimes becomes a challenge because you don't have the right data architecture or controls for the two clouds. So, um, the scenarios which we talked about, uh, there's like no one on one relationship between the scenarios and the challenges. For example, mergerges and acquisition can, uh, be responsible for all these challenges which you see on your right side like gravity, governance, etc. so there's really not one on one, kind of a mapping between them. So any of these scenarios could, uh. Give you the problems which are listed here. So let's talk about some of the capabilities on how to deal with these challenges. So first of all, um, I wanted to ask you, has anyone heard about a materialized view or federated queries? OK, some of you probably. OK, many of you. That's great. OK. Uh, for those who don't know materialized view, uh, let me give you a real world analogy of, uh, understanding materialized view. Uh, I always like to, uh, give the example of, uh, like, you know, pre-cooked meals. Uh, Sunday night we cook meals for, like, you know, for the rest of the week so that we don't have to gather those ingredients, those food items again to cook on Monday, Tuesday, and, and the following days. So in this in the case of multi-cloud and in case of materialized view. We have the raw data in different clouds and instead of computing joining the data from different sources, we pre-calculate and pre-join that data and call it a snapshot of that data so that whenever we have to refer to that data we don't have to go to these clouds again and we don't have to recomput everything because any of the data access requires compute, right? So to save that processing we precomput that and we create. A materialized view out of it which gives us the benefits of saving time of course it does cost some some amount of space because you have to store that precomputed data but then end of the day you have to balance between the cost and the time and I guess in most of the cases, in many cases in fact, uh, time is, is, is of an essence. Uh, federated queries is another way, uh, where we can chart, uh, we can solve the data gravity problem. Uh, so another live example or kind of a live analogy of federated query, uh, you can think about, uh, booking app like Booking.com, uh, ClearTip, or, or whatever you're using. So if you realize, um. Booking.com is getting data from all different uh airline provider and the hotel providers without actually moving that data to any central location, so it's like a federation layer created which can crawl all these data sources through APIs or whatever mechanism it is and it gives you the data without moving any of the data. So even though you have a data gravity problem, but. In order to fix that, you don't have to like do an anti-data gravity kind of a thing or you don't have to move the data out of your primary cloud, whatever you designate as, uh, and then you still have the ability to access the data without doing a lot of back and forth movements. Uh In order to solve some of the data governance and data monetization, time to value and acceleration, uh, another effective way is to build decentralized architectures, and we'll dive deep into, uh, more about it. How can you build decentralized architectures? What are different architecture patterns to do that? But in a sense it's like you have data distributed across multiple clouds and how do you create like a governance layer. Uh, a data access layer which can talk to these different sources uh in a, in an effective way. Uh, in order to capture the data lineage, which is very important because you should know where the data is originating from, where it is coming from, how do you trust that data, you got to have an effective, uh, metadata strategy and management to solve the problem of data lineage across multiple clouds. Uh, so one of the concepts I was just talking about, federated query, in fact, uh, that kind of serves as a base for materialized view and, uh, in some cases, decentralized architectures as well. So it innovate it enables, uh, these architecture patterns. So let's talk about the practical examples of materialized fume. So today, uh, actually there are different cloud providers have different ways of defining materialized views. For example, uh, Google Cloud has a BigQuery Omni, uh, to define materialized view. Uh, Amazon has, uh, something called S3 and Athena, uh, to support the materialized view. So, uh, it has got the support for the Apache iceberg tables as well, wherein it can refresh your materialized views. And then, uh, again, federated query can also be created, uh, let's say using Athena Federated query wherein we have a built-in connector for Google BigQuery. So you can define, you can actually choose the data source as BigQuery within Amazon. And uh using the Athena connector you can access that data without moving that data out of Google Cloud right uh uh sitting in the AWS cloud and same thing you could do with Google Cloud. So it's like kind of an either or choice, uh, wherever you want to build the federated query architecture depending on what's your primary, what's your secondary cloud, and, uh, where is the data architecture, uh, like where is the majority of the data sitting. Uh, so as I was talking about decentralized architecture, um, you may have heard about this word, uh, data mesh or data fabric. Uh, now data mesh defines a, a way to share the data, uh, between different cloud providers or between different data sources by calling, uh, data domain owners or by creating data domain owners, creating data as a product wherein each domain owner creates their own data products and there are consumers who would consume that data product uh through the data mesh architecture and we'll just talk about in a bit more detail in the next few slides. Um, and then the metadata catalog, uh, is again, as we just talked about, uh, how do you effectively manage the data across different cloud providers. You should be tracking, uh, the metadata for that data, like the size, the origin, how do you access it, uh, using different cloud native tools or the third party tools which can work across different clouds. So, uh, let's dive a little bit deeper into the Deramesh architecture. So in a typical data mesh architecture, you create a data marketplace, like a central market, data, central data marketplace um wherein the producers of the data. Uh, would create the data sets and I'm gonna, I'm just gonna take an example of an insurance company again because I've been dealing with a lot of insurance customers. So think of if an insurance company wants to create a data mesh, uh, think of data producers as, uh, one of the, uh, teams which is uh responsible for the management, uh, of the customer data. For the policies for the claims they're creating a lot of data about the customers and the the insurance policies and then on the right hand side there are consumers of that data, the pricing team maybe the marketing team maybe who wants to consume that data and see how effectively they can sell uh the insurance policies to the new region to the new customer base. The team in the middle is enabling both the producers and consumers uh with that data marketplace so that they can all share the data uh boundaryless like even though the data is sitting in different clouds or different sources are on-prem, uh, the abstraction layer which we are creating with uh the data mesh enables the boundaryless data sharing. Uh, so, there are, there are some, uh, trade-offs, uh, some obviously pros and cons, uh, with this architecture pattern, if not done correctly, um. So, you know, the autonomy of the self-service BI or the self-service data access through through data mesh, uh, that advantage can sometimes come with the cost of data silos because different domain owners like in our previous example, uh, the customer relationship management team, uh, or the policy or the underwriting team. They sometimes create overlapping data sets which already exist, duplicate data sets, and sometimes they create data sets which are not, uh, which are which other teams are not able to use due to strict compliance issues. So we've got to be very careful when we design a data mesh that we. Have agreed upon principles we have agreed upon standards how do we want to create data sets? How do we want to share that it shouldn't become a technological or a systematic challenge for the other teams uh to consume the data which has been produced by the data producing team. If we don't do that, then we end up creating many, many data sets which could be duplicate and, and create silos. Uh, likewise, for, for standardization like data mesh creates a standard for creating, uh, sharing data, producing data, and, and the marketplace and everything that requires a lot of upfront, uh, engineering complexity to begin with and oftentimes, you know, there is a lack of, uh, support from the leadership. There is, there is, uh, technical support, uh, the limitations around the capabilities. All these things must be considered before we jump on to, uh, create a data mesh strategy. And you know it gives a lot of control and flexibility when you create data mesh versus the central data architecture when you're dependent on the IT teams, um, this control and flexibility because you as a domain owner as a data owner, uh, you can define your own data sets and you can create your own data sets and you can let the other teams access now this control and. Flexibility also can sometimes come with the cost of inconsistent implementation between different data teams between different uh data owners because one team might wanna let's say create some security control, some encryption, whereas the other team uh may not be able to follow those controls due to their own limitations. So, um, let's talk about the other, um, architecture pattern which is federated query. So in case of federated query, uh, we submit a query to a federated query engine, uh, and that engine has some of the core components like for example, metadata management and discovery is one of the key components. Now, this query engine actually, uh, maintains the metadata or the technical data about your data sets, things like where data exists, what is the size of the data. Uh, how can data be accessed? All these things are defined here. Because this information is already there in the federated query, your query actually uh knows how to fetch that data. And similarly, it also has a data access and security layer. In this layer we define, let's say, uh, I have my primary workload in Amazon and I have a secondary or maybe some data analytics related workload. I want to push it out to other cloud, let's say Google Cloud in this case, so I should have the right controls configured. In Google Cloud and in Amazon, like the right keys and certificates, APIs, API keys and whatnot within this layer so that I can access data seamlessly without, you know, doing any sort of manual intervention or without uh any dependency on the IT teams to configure permissions manually. Uh, and then the last layer or the last core component of the federated query engine is the, uh, the optimization, uh, and the performance, which is very important in this, in this case, especially in case of multi-cloud because. You know, there could be a lot of back and forth, uh, data traveling over the network, so there has to be the right set of configuration settings in order to optimize how do we let that data travel faster and in the most cost effective way, saving the egress cost and the network cost when we want to enable the uh federated query in case of a multiple, in case of a multi-cloud. And as a result, when the query travels through these core components, these core components function in tandem with each other, you get the result of the query, and I would highly encourage to try this out with with Athena connector for Google BigQuery. It's seamless, easy to configure, and you would appreciate the qualities of the federated query and the advantages of this pattern. Uh, so Like every, every architecture pattern would, would come with its own pros and cons, um, if not configured correctly, while there is less amount of data movement in in case of a directed query because, uh, you know, we are just pulling the data from a remote source rather than, uh, getting the data or copying that data from that from that source. This low data movement can come with high latency again if not configured properly at, uh, uh, from a network point of view. And we just announced the network connectivity between Amazon and Google Cloud, so I would highly encourage to take advantage of that. And we have the experts here who would want to talk about maybe after this session if you have any questions about this newly launched capability. Um, So when we create a catalog, uh, the metadata and everything, uh, which goes into that catalog, uh, there are sometimes, uh, metadata sync issues, issues like, you know, you dropped something, let's say in other cloud, and in your primary cloud, that definition of the table when you dropped it or you altered the definition of the table, that was not synchronized in the metadata catalog. So your federated query engine would not know the data has changed. So if you don't synchronize it, if you don't have a mechanism to synchronize those metadata, then you would end up in a challenge wherein you know your query might give you incorrect results or it may give you some some sort of runtime errors. Uh, and same with the, with the access control, uh, that's true as well. Uh, when the permission changes, uh, between the data sources, then you got to, uh, have a mechanism in place which can update those permission sets, and, you know, it's sometimes complex to manage those credentials, rotate the certificates, rotate the keys, update the keys, and all these things, so you should all be considering all these factors, uh, before you, uh, create the, uh, federated query architecture pattern. So, uh, I think at this point, we will talk about the, um, data augmentation from an AI standpoint, and I'm gonna hand it over to Don. All right, thank you, Devonhu. Alright, so, uh, first is, we're just gonna level set, um, many of you may be aware of what rag or retrieval augmented generation is, um, just to kinda walk through this for people who do not know, um, we're gonna take a user query in kind of step one here, um, we're actually going to do a vector embedding of that query, um, ultimately to achieve and execute a semantic search against a knowledge base, um. So that knowledge base, um, we're gonna do that similarity search based on cosine or Euclidean distance we're actually gonna come back with document chunks that are relevant that can be fed in as facts if you will into the context and ultimately we'll get an augmented prompt that comes out of the rag engine. So you can think of that rag engine in this discussion today as being an AI application rag engine, um. So those are, those are kind of the basic concepts we want to get across first. Next we wanna start talking about what that could look like in a multi-cloud context. Now as we talk about these patterns, we always wanna kind of start with a backdrop of, um, they're always, you know, security is, you know, top priority. So things that we need to think about are security. We've got performances, as, uh, Devonci was mentioning earlier, latency in the network, right? So things that we wanna take into consideration in terms of design. But as we're looking at this, one approach that we can take. If our CSP1 is where our AI application or RAG engine is. We can have our knowledge bases again data gravity we don't necessarily may not want to or cannot move the data so we can leave that in CSP too, but there are some key considerations, uh, that we need to be aware of. Um, one is consistency of our embedding model that we're using cross cloud, um, and then secondarily. Um, vector store and they don't have to necessarily be the same, but there's there's opportunity here as we think about, you know, highly available systems and resilience where that vector store could be replicated, right, so we could replicate that data store either on the same vector store on the same CSP or we could replicate it across things like S3 vectors as a cost efficient mechanism for storing vectors is one potential source or destination for that. Um, so as we're thinking about that, there's some benefits that we need to think about, right? Minimizing the data movement, um, as we were talking about, you know, so similar to the, uh, the same strategy we're taking with things like federated query, but there's always those considerations, as I mentioned, network latency, security, um, but again, that consistent embedding model you have to have across your two CSPs. Um, and then again that, that kind of that resiliency right vector store replication and potential sharding that we wanna take into consideration. We wanna talk about a couple of other additional types of rag. So first we're gonna talk about structured rag, um, oftentimes referred to as S rag. So in this case we're kind of following our similar flow from before, we have a user query that's coming into our structured rag. We're doing a text to sequel in this case. And currently, as just one example, um, we have databases and data warehouses, um, could be a federated query engine right that we're querying, um, and today Bedrock is an example with our knowledge bases we support uh structured rag with Redshift. So think of that as kind of a, you know, a particular service that you could think of implementing. So again in this scenario we're executing that query, we're coming back with retrieved records, um, and then we're actually creating that augmented prompt from that. So there are some benefits here. So again, we're actually instead of just being able to do a semantic search across unstructured data, now we're actually bringing in structured data that we have in our enterprise. We're also leveraging those database, the, the investments that we've made in those assets, right? And again we're not having to necessarily move them, right? So this is really when you're thinking about like scenarios like M&A, um, that becomes, you know, one mechanism that you can leverage to start experimenting, um, again consideration, security, network reliability, uh, and performance are always things that we wanna take into consideration. I wanna touch on another pattern uh for rag which is graph, right, or graph rag. And what's interesting about this, um, as we go through it, I just wanna talk about a couple of things that are going on here so graph rag, the biggest benefit is that we are getting to concepts, right? So we have concepts in a graph that can relate across different document chunks, right? So we're able to span beyond that kind of document scope. So we're getting the ability to do hops, iterate over relationships and traversals, right? And in this scenario, similar to structured rag, we have a user query coming in and we're going to attach to a graph query. So that graph query could be cipher, could be gremlin, it could be sparkle, and we're executing that against the graph database. So on AWS we have Amazon Neptune as a graph database that you can leverage. Currently this is supported with Bedrock knowledge bases. There is the ability to have Graph Rag and Neptune as your store, your vector store. And then thirdly we've got this retrieving the graph and then we're going to the augmented prompt. One other scenario to think about in a multi-cloud graph rack scenario is the graph and what's in there, those concepts and the relationships and what they point to. We don't necessarily have to move all of the data, right? So you could think of this, and sometimes customers will refer to this as a semantic layer, but you have the ability to create that connective tissue across the data and that data may not live in the same CSP. So we think this is a a really good um implementation and pattern for certain use cases where you wanna achieve that and you wanna leave the data where it where it sits um so primary benefits. As I mentioned, the ability to traverse those entities and relationships, um. You know, so you can think of that as, you know, spanning, you know, and trying to iterate over relationships that could be going back to kind of the financial services that could be an anti-money laundering scenario that could be a know your customer, right? So think of those traversals that you would make from a from an individual or person through additional associated identities, bank accounts, locations, etc. right, to, to kind of ground that. Um, but some key considerations again, the, the thing, the same considerations tend to pop up here, right? Um, and when we talk about network reliability and performance, and I know we said it at least twice, but we're gonna say it again, and that's the multi-cloud, our interconnect product that reliability on the network, um, becomes key, right? The performance of that, um, is something that we think is extremely important and beneficial to our customers. So now we're gonna talk a little bit about multi-cloud from an agent perspective. So As we roll into this and we built this up in a very structured way so that you know data is key to our agents right in terms of their ability to perform, generate accurate results, reduce hallucination, etc. so we're gonna talk about model context protocol as just being one standard, a very popular standard uh for allowing agents to leverage tools, right? So MCP. Um, as you've, as you, many of you probably have heard that, um, in this scenario, let's assume a few things, um, we talked about federated query before, um, and we talked about some of the trade-offs, but one of the trade-offs and things that you'll have to consider is what if your data source is not supported by your federated query. So let's say Amazon Athena, which allows you to do federated query, doesn't have a data connector for a particular database, right? In that scenario, you either have to build a custom one or as an alternative you could consider an MCP server that actually. Implements tools and is it allows you to expose data from a data source. So on the far left here, I'm gonna kind of fill in the picture a bit, um, we have an agent. We could use strands as an agent framework here. Let's just assume we are, um, we have an MCP server and in this case we're putting the MCP server close proximity, right? We wanna localize and optimize, um, the efficiency there, take advantage of, you know, the network boundary being close, uh, you know, same partition. Associated with CSP 1, we've got an MCP server there and let's say we've got another one over here, an MCP server 2, that's with our data store too. So MCP has a client and so we have a client for each one of those, and you can imagine that our agent is now capable of leveraging MCP to actually federate that access to the data, right? So that tool, we can go beyond just kind of access to data, we can do other things, but in this context we're really talking about gaining access to the data, right? So it could be read, could be right, depending on the MCP server. By leveraging the protocol. It could be Just about any agent out there is going to support MCP, right? So we've got access, we've got interoperability, and then also we're leveraging streamable HTTP transport. So everything here is standard, right? Um, but we're leveraging the best of, you know, MCP servers, the wide support, um, that we have across many different cloud service providers. So let's go through some of the challenges we talked about data gravity, that's kind of our constraint, that's what we're trying to work around here. Um, I talked a little bit about the data source compatibility, um, take it one step further, for example, there may be a uh a discrepancy in terms of data type support. 1 may not support a blob just as an example, these are scenarios where you may have to consider something else, um, and then in this case the agents can access the relevant data, right? So gives us control over what happens with that MCP server. Um, and then from the benefits as I mentioned, the standard and widely adopted protocol that local locality optimization, um, and quote unquote federated tool execution and processing. All right, so next we wanna talk about a scenario where um you have a model that may or may not be supported on AWS in Bedrock. Maybe it's not supported in a particular region, maybe there's a capacity, so you wanna gain access to that LLM in a different cloud service provider. So we're introducing the concept here of how an LLM gateway and router can help us in a multi-cloud context. Right, so this LLM gator gateway and router is actually gonna provide us a standard unified interface on the, on the input, the request side, common, you know, commonly we'll see open AI as the standard for that, um, light LLM. We also have Agent Core gateway. So we've got a couple of different scenarios, things that we could leverage here that are services, um. And in this scenario we've got our agent actually communicating and leveraging different LLM, different models that are running in different cloud service providers. So as I mentioned, um, a lot of times these are the things, so these are the driving, you know, factors, right? So model selection, availability and capacity, um, also, you know, being able to round robin, right? To the various LLMs, right, if one is not available. Throttling, etc. we can actually send that to another, another LLM. So again these are the primary benefits of that specific protocol so I have a unified, um, you know, writing to one request, you know, format specification, and then I'm having it translated for me. um, I'm prioritizing and routing so one of the other things that we could do with this gateway depending on how much functionality we wanna put in there. We may wanna introduce a cost-based routing, right? So based on cost, based on the request, we can route that separately, um, based on different features and capabilities, and then as I mentioned that kind of that dynamic load balancing. And then lastly just to kind of bring the whole picture together here, um, both of these, the LLM gateway router and the MCP client to server communications that we talked about in the previous slide, all of that can be utilized by an agent. So we'll just talk about a couple of next steps and and resources that we have. So there's many different sessions that we have, um. Kind of a one on one starting with, you know, a lot of the best practices, uh, from a multi-cloud perspective, um, we also have some deeper dives, right, so 200 level conversation here today, but we have a 300 level that'll actually drill down into more detail of the things that we were just talking about with respect to generative and agentic AI. Um, and then we've got security best practices, network best practices as well. So a few other things we have a kiosk, um, in the, in the expo hall that you can actually visit. We've got demos. We've got some use case deep dives. We can have conversations with you. And then as a, uh, and you know, as a program, the multi-cloud program, we spent a lot of time talking to customers, you know, just like you all about multi-cloud strategy, uh, so happy to engage with you all via your account teams, um, so feel free to reach out to us and then lastly, as I mentioned earlier, if you have any questions, we'll be available in the hallway, um, to actually answer any of those. Thank you.
