---
video_id: 8ybMtzb6ytQ
video_url: https://www.youtube.com/watch?v=8ybMtzb6ytQ
is_generated: False
is_translatable: True
---

So today's session is gonna be. What's new in Apache Iceberg V3 and beyond? My name's Ron Ortloff. I'm a product manager at AWS. I'm joined here today by Yuri Zarubin, who is a principal software engineer at also at AWS. So just a, a quick side note. You're in an open source track session, so this is what's new in Apache Iceberg. More focused on open source and what's new in the spec, um. There are a bunch of other iceberg focused sessions specifically on AWS services leveraging uh V3 features, things like deletion vectors. There's a great session on, uh, how you can use AWS services with deletion vectors and a bunch more. So just a quick side note on that one. So what are we gonna talk about today? It's really Iceberg V3, why should you care? There's gonna be the, the 3 top features that we see most of our customers talking about, the variant data type. Deletion vectors and roll lineage. Um, those are really kind of bubbling up to the top of the stack in terms of what people are most interested in, where they're seeing the most alignment with their, with their use cases. We'll wrap up the V3 talk with additional features. There's a couple of buckets of, of great additional features that are in the V3 spec. We'll talk a bit about those. And then Yuri's gonna come on and talk a bit more forward looking about Apache Iceberg V4, where the community is starting to go. We're starting to see some proposals formulate some momentum around those proposals as things are starting to, to shape up and look forward towards Apache Iceberg V4. We'll have call to action, some resources, and then you'll be on your, your merry way to your next session. OK, so before we get into V3, I wanna talk a little bit about Apache Iceberg as a project and the project history. So everything started back in 2017 at Netflix. Dan Weeks, Ryan Blue, Jason Reed, noticed a lot of common patterns around the big data solutions that they had at Netflix. Started a project that they called Iceberg. In 2018, they started a process of getting that moved into the Apache Software Foundation. So 2018, Iceberg became an incubator project, which is kind of the first sort of toe, you know, toe step into the Apache Software Foundation. In 2020, it became a top level project, so this is where you get the formal governance and you're following the Apache Software Foundation processes. 2021, that's when the version two of the spec came out. That's, uh, you know, at that time, the, the big kind of release and, and feature that came in, in V2 was the capability for merge on read. So doing role-level deletes, which is kind of the precursor to deletion vectors, which is what we'll talk about today for, for V3. 2025, so this year in May, the version 3 of the SEC was ratified. And then, as I mentioned, we're starting to see some formulation around where things are going with the in development proposals for V4. So with that foundation kind of established, when it comes to Apache Software Foundation projects, there's a couple terms when we're talking about like versions and what's new and, and those sorts of things that it's important to kind of understand how, how things operate within Apache Software Foundation. So you have specifications, those are basically documents. It's a contract that anybody that's working with Iceberg needs to adhere to to be SEC compliant. This is a little bit more important, I believe, in the Iceberg space where interoperability is really the golden ticket of why Iceberg has gained so much popularity. You have multiple vendors implementing, you wanna make sure that you're adhering to a spec so that you can have that single copy of data and use a bunch of different compute engines on top of that data. So specifications are just that, that contract. Um, they have to be voted on by the PMC, so the Project Management Committee, before they're officially ratified. But keep in mind anybody that wants to participate in an Apache project is more than welcome to vote. Have your voice heard, something you, you, you really wanna champion, go vote for it, something you wanna provide feedback on, go vote for it. Everybody's welcome to participate. Only the PMC member votes count though. On the release side, this is where the, the specs start to come to life. So you have spec features that are implemented in releases either through reference implementations or SDKs, but the releases go through a formal Apache Software Foundation software release process. Those again are voted on by the PMC just like specs are. So why is this important? So a lot of people heard a lot of kind of marketing buzz, a lot of talk about the Iceberg V3 spec being released back in May, and they're like, well, hey, where, where are my features? Why can't I go use feature X, Y, and Z that I heard someone talk about in the Iceberg V3 spec? So since the 17 release all the way through up to the 110 release, this is where we're, we're seeing the community start to implement. Those features that were ratified in the spec, so that's gonna continue to go forward, but just keep note that there is a lag between when things are ratified in a spec in that contract and when they actually, you know, make it to market in a reference implementation that you can get your hands on and use. So that's, that's kind of the whole point of the slide, giving some kind of foundation on, on kind of where things are at. The V3 spec is not 100% implemented in any release yet. So, that's, that's really the, the kind of dry residue on this whole slide. OK, so with that all out of the way, let's jump into, to V3 and let's start talking about some of the features. The first one I'll talk about is the variant data type. Um, by the way, real quick show of hands, like, how many people are using Iceberg today? I meant to do this before, so, OK. Anybody had a V3 table created yet? We have one brave soul. He, we gotta get this guy a prize. That's awesome. Well done. OK, so come on up and join us. You can, you can give the rest of the talk. Um, OK, so before we jump into variant data type, let's just do a quick background. There's really a few different. Kinds of data types and iceberg, there's primitive types. These are atomic data types, can't be broken down further. Think of like in string. Those are your basic atomic primitive types. There's also structured types. Where you, where you can actually stitch together multiple of the primitive types, think, you know, list map array, those types of things. Key here is you have a, a fixed schema though. And then this is where a variant comes in. Now with the ability to do semi-structured data support. So here you're getting flexibility to handle. A varying schema. Uh, there is no fixed component to the, to the semi-structured nature of variant. We see a lot of people leveraging variant data type to handle and process JSON data that they receive. So in terms of components in the, the variant spec. For V3, there are 3 different components. There's a metadata component. This is used to support things like file pruning. There's an encoding where you're taking values out of the, the variant data set and putting a data type on those. And then the last component is shredding. So shredding is basically taking elements out of the variant data type, the variant data set, and materializing those as hidden columns. We'll talk a bit more about shredding here in a, in a moment. So, before we get into use cases of the variant data type, let's talk a little bit about some of the pain that some of you folks that are using. Iceberg V2 may be living through today when you're trying to handle semi-structured data. So first thing we, we saw a lot of customers doing is, is basically trying to make their semi-structured data look like structured data. And they're doing that through transformations, picking uh the closest structure type that they can to align to their data. Um, and that's just how they're living, right? Um, with this, again, you have to operate. With a fixed schema, there is a little bit less profile on the performance that we'll talk a bit more about later as well. But we do see some people trying to, to operate in this manner and, and you're again also paying for additional uh costs around the transformation of that data. Second pattern we see is people just materializing out additional columns. So they may Take in a data set, find. A number of elements that are key to them, and then they'll run transformations and put them out into separate columns in the table. Um, so again, you're paying transformation costs, compute costs, but you are getting pretty good performance in this model. These are legit columns in the table that you've transformed and, and pulled out of a semi semi-structured set. They're gonna have stats on them. They're gonna perform quite well. And then the last one is. String. I call this kind of like the give up model. Like I've got a semi-structured data set. I can't fit it into a struct. I don't want to materialize columns. Uh, let's just throw it into a string field, right? And we'll do string parsing, parse JSON, uh, you know, we'll just grab data out of that thing whenever we can, right? Um. This one, you know, it gives you that flexible schema, but, you know, I do have performance listed on here twice because the penalty for this can be quite substantial. Um, it, it really, really can. So that's, that's what we see customers doing today. In terms of like use cases, what we really see customers going after with variant first one I'll I'll talk about is IOT workloads. So here, you know, you've got eventing type models you've got different event details that you may be getting from a number of different IOT devices. Schema is really best applied kind of on read, not trying to figure out a way to get that data into a fixed schema upfront. You also have a, a tendency to add in fields in, into your semi-structured data set more often with IOT workloads as, again, you're getting these varieties of, of different types. So this is a case where variant being semi-structured in nature can just absorb that semi-structured type data and give you an ability to, to report on it. The next one is data pipelines. So here, this is more about landing the data, landed into a variant data type, and instead of doing transformation logic to get it to fit into a schema, you're basically building logic into your data pipeline to apply a schema. So we have one customer. They're taking in data from a number of different suppliers. They couldn't scale up the operation to actually enforce schema on their data providers. So they basically had a data set. If it fit into the schema, great. It went into the, to the main pipelines that they had. If they didn't have a, a supplier that gave them spec compliant data or schema compliant data, they created something they called the imparsable fields column. That was variant. They would just dump the data into that variant field and use pipelines to extract pipeline logic to extract the elements out of that variant field. That allowed them then to keep things consistent as they moved the data through the different phases in reporting and analytics, um, for, for their solution. The last one is, is real-time analytics. So, again, you wanna be able to query the data as it lands. You wanna avoid preprocessing of data. This is, you know, in a model where I need access and I need analytics on top of the data immediately. We had a, a FinTech customer doing lots of quote analysis, um, you know, stocks, bonds. Crypto, whatever those quotes all come in in different shapes and sizes. They don't have time for this solution to be able to do any preprocessing or transforming up front. They wanna throw it into a variant field and just let the quants and the smarter people than me query on that data quickly. So that's kind of variant use cases, just to distill down some of the variant benefits. Performance and cost is, is really a big one with variant. You're getting data put into column or storage. Which gives you stats. Gives you the ability to do predicate push down. Talked a lot about flexible schema in the different use cases and and build up to this slide. You're operating now on a schema that's dynamic for you. You get in new evening scenarios and details you almost get an automatic scheme evolution, right? You can still take things in that have new elements in them and you can still operate on top of them without having to go through a formal schema evolution. Efficient storage, so you are getting compression when you do the shredding and, and build out the hidden columns. You're going to get the compression done for you out of the box. And then last benefit is around querying the data. So you get schema navigation through dot notation on some engines. Some other engines have a slightly different syntax, but it's very easy to go through and navigate down a hierarchy on a variant data type compared to wrapping multiple string parsing functions or anything like that. So, I've talked a couple times about shredding. Um, Spoiler alert, like this is my favorite slide of the entire deck, so be prepared to be wild, right, Erie? OK, so, shredding, what is, what is shredding? So, I have a table here that's been defined in storage. It's got event date, time stamp. It's got source ID as an integer, and then I've created my event details. As a variant field. So 3 columns, pretty simple. I get in this type of data file from my source system. And I want to load that now into the table. So what, what happens in this case now with, with V3? So the primitive types, those easily mapped, get loaded through my engine, put into the table. Now with this variant field, I'm going to pass that through logic that you would have in your Iceberg V3 engine to actually shred that out. So here's my handy dandy variant shredder. That data flows through the shredder. This is where it gets broken out now into the sub-columns. So that's the uh the engine implementation of a variant shredding process. And then those elements then are put into sub-columns, hidden virtual sub-columns in the table. So you see SKU with integer account, ID with integer, and then you have the pricing as decimal. So I've got data types, I've got these hidden columns. I still reference that column as event details. OK, but now underneath the covers and the implementation of variant, I have those hidden subcolumns that that I can then query into. And that queer would look something like this. OK. So, here, with this predicate column, I'm actually gonna be able to do pruning on that data and return just the values that match that predicate. Any other type of model outside of variant, there's a higher chance I'm just gonna be doing a full table scan. Especially if I'm doing like string parsing or something like that. It will certainly Certainly be doing a full table scan. That's kind of rough when you think about it, but then also if you think about doing joins, if I wanted to do a join on account ID to another table with no statistics. Like, I think you guys know how that story ends, right? It can be pretty ugly. Mismatched joint conditions, scanning multiple tables, performance can be very, very bad. Speaking of performance, so just to give you kind of a, a rough sense. Kind of relative sort of query performance difference that you may see. With variant kind of being the 1 X baseline, you could see up to 4 times additional degradation and performance with structured types strings could be 10x worse and honestly that 10x number like I could craft a benchmark for you that did table scans to very tight predicate lookups and it could be much, much higher than 10x. So just some, some ideas, you know. You start looking at variant for potential semi-structure type data solutions. This is the type of performance profile that you can start seeing over some of those alternatives that we discussed. OK, so you're all experts on variant now hopefully let's, let's move on to the, to the next one and talk a little bit more about deletion vectors. OK, so deletion vectors. are a right optimization feature. They're a storage optimization feature that's been enhanced over what's been in the spec since the, the V2 version of it. So how does the right modes inside of Iceberg work? You have the default option of copy on write that's been there since day one of Iceberg in that model. You have a data file, you run a delete query against that, and then you rewrite a new data file that's got whatever was deleted out of it removed. So I, I use this analogy, you know, if I've got a table with 10 records in it. And I want to delete one of those records out. I'm gonna rewrite a new data file with 9 records in it. That's copy on write, OK? Now that may not seem like that big of a deal, but if you do that across thousands of data files, your right amplification will be enormous, right? You'll you're rewriting an awful lot of data. Conversely, with merge on read, that same model now, data file, delete query, I'm just writing a delete file with one record in it, one positional delete, right? Or one delete condition, depending on the, the type of uh role level delete I've implemented. So. The right application problem is solved. You're gonna have Faster rights with merge on read, you're gonna be consuming less storage. But you're gonna pay a tax on the read. OK? Now I've got to join those delete files to my full data files to figure out what's the filtered reset result set to return, OK? Copy on right. I've got a new file with 9 records in it. I just need to scan that entire set and not worry about putting it together with anything else. So that's copy on right, merge on read. So when we talk about the delete types that are there in Iceberg, first we had in, in V2, we introduced the equality deletes capability. So this is writing a condition to a delete file. As, as you see here, like remove all user ID equals ABC, and then that delete condition is used on the reads to filter out records. This is pretty popular for streaming workloads like Flink. Um, we see a lot of equality delete solutions being, uh, using Flink. And then the other piece that was introduced in V2 was positional deletes. So this again, you're writing a delete position. To a, a delete file, um, and then you're using that delete file on read, brought into a bitmap to then filter out the, the results set. So equality deletes were there in V2, equality deletes are still there in V3 spec. OK? Positional deletes were there in V2. Those are not in V3. They've been deprecated and replaced with the deletion vector feature now in, in the V3 spec. So instead of writing positional delete files, deletion vectors is updating the values on a bitmap, and then persisting that bitmap to disk in the form of a Puffin file. So there's a slight nuance to that. It's still positional deletes. But it's in a much more optimized format with deletion vectors. So let's talk a little bit about use cases and and why you wanna implement something like deletion vectors. You know, we talked about the right amplification piece, but there are other higher level scenarios where deletion vectors can come into play. So GDPR compliance, we see a lot of people running into right amplification issues doing GDPR compliance. Oftentimes when you're removing information about individuals can be different than how the data is fully consumed for other business purposes and that's where you end up in these sort of random right scenarios where your right amplification is, is very, very high. So if you have to do GDPR deletes. Um, deletion vectors is, is, is a feature that you should certainly look into. Another one is, is data cleanup. So we see people using a, a medallion style architecture, bronze, silver, gold, where your bronze is more of like a raw kind of staging area. Um, you're gonna land a lot of data, there's gonna be maybe a lot of noise around that data. You're gonna wanna do a lot of data clean up on that bronze layer. Um, so that's where, again, deletion vectors can, can come into play and, and aid in the, the right speeds for those data cleanup operations that you do. And then the last one is, is incremental data pipelines. So if you're running merge statements. You know, in your, in your data pipelines you have, you know, a potential to be doing again more of those random right type of operations and here's where. You know, deletion vectors can play very nicely with, with merge operations. OK. So Just a, a, a slide here then to, to kind of understand a bit how the moving parts operate within deletion vectors. So in this example here, I've got a table with a couple snapshots that are created on disk. We've got snapshot 1, snapshot 2. If I wanna do another delete query, so now I've got a data file that I'm gonna produce, and then I'm gonna have a bitmap that I'm gonna go ahead and update. So as part of that S3 transaction, then we'll, we'll make sure that the data file is committed on disk. We'll make sure that the bitmap is also committed on disk. And then that transaction is, is committed into my table. OK. So when it comes time to then consume that data, We'll grab the data files. We'll grab the bitmap and then we'll use that bitmap to filter out the results set as we're returning the data to the calling application. OK, so to kind of wrap up. On deletion vectors. Comparing them to, to kind of V2 positional deletes and really help hopefully drive home why this is such a great improvement. So with positional delete files, there was a propensity to have a proliferation of delete files. There was no spec, there was no guidance given around. Producing delete files. So we ended up and we see with a lot of customers using position deletes on V2, they have just tons of these delete files, small transactions, small deletes, small updates, small merge statements, whatever. There's tons and tons of these delete files. Puts an additional compaction burden. You have to clean that data up, fix up your, your underlying data files, but that's one of the challenges that we're we're certainly seeing with V2 and the, the positional deletes. The other is the positional delete files are translated from a bitmap into a parquet file. And then on read, that is reversed, right? You're pulling data off of a parquet file, and then you're building a bitmap on the fly to filter out results in. With V3, then a lot of these challenges are solved. Per the spec, you're only allowed to produce the writer implementation. You're only allowed to produce one delete file per snapshot, so it knocks off that problem of all these small tiny delete files. The bitmap itself is also stored in a Puffin file directly, so there's no deconstruction, reconstruction process that happens around that bitmap and the, and the Puffin file. Um, and the writers themselves maintain that bitmap while the right operations are happening. So you have a, a, a fully efficient bitmap that's being built and maintained. OK, that's deletion vectors. Let's go ahead and talk a little bit about role lineage. So roll lineage. Is really a, a great change tracking feature that's been brought to the, to the V3 spec. So the components of, of role lineage within the spec, there's a writer specification for you to be a compliant Iceberg V3 writer. You have to be producing record level change information. You can use as a writer, you can use Iceberg V3 metadata to understand sequence numbers and row IDs that have been committed to snapshots, but there is a responsibility on the writer's side for you to produce these changed records that allow for row lineage to be consumed. On the reader's side, there are new hidden columns added to V3 tables that give you that row lineage information automatically. So you get a row ID and you get a sequence number, and that's on every single V3 record. And then from an operational standpoint, Role lineage is a required feature. So it's on by default. There's no knob to turn it off. Um, and that information is just there and it's, it's with you, um, to, to, to consume. The other thing I like about role lineage, um, for those that have ever dealt with like a split brain type scenario where you've had state information get out of sync with data. Um, it's really kind of neat, a neat little side benefit with raw lineage. The role lineage information is stored right on the record. So you have that role lineage sort of state information in the snapshot in the record in the table itself. If you wanna go do time travel, you're gonna get the time travel equivalent of what that role lineage looked like at the time you query. So In terms of role lineage use cases. The incremental processing again, this is, is really a nice sweet spot for row lineage if you wanna be reading out of V3 tables as an input into data pipelines, you can leverage the sequence numbers, the row IDs, the information there that's on the rows to understand what's changed as a source to feed into your, to your pipelines. Event life cycle tracking is also another good one. You think of like an orders table, right? An orders table goes through, or an order, I should say event goes through a series of changes, right? It's been submitted, it's been billed, it's been processed, fulfilled, shipped out for shipment, you know, delivered, right? You wanna understand kind of what the, the life cycle or the state changes were. Here row lineage, each one of those is gonna be on that same row ID for that order. You're gonna see the sequence number increment each time and you'll be able to quickly and easily stitch together what the, you know, that life cycle looked like, right? Slowly changing dimensions, type 2 dimensions, uh, you know, similar sort of model with those, you could feed in row lineage information to help build those types of things out as well. Debugging is another one, maybe not as flashy as some some of the other stuff on here, but if you keep track of that role and age information, you know, within the transformations that you do and the and the data that you enrich and and build in your system, you can then trace back to how those calculations came to be, right? You can use the row ID you can use the sequence number for which you did the, the calculation, and then you can be able to trace back over those events and understand. How you got 1 + 1 equals 7, right? Something like that. And then lastly, with, with raw lineage, it's a, it's a great compliance capability, um, enabler, I should say. So you're gonna have a sequence number stamped on each modification to that table. So you'll be able to understand if someone went in and updated my salary in an employee's table to a billion dollars, right? You'd be able to see that sort of change of events and trace back on, on those steps. OK, so, a couple clicks here on how role lineage works. So I have a table here. It's got two columns in it, first name, last name, and then I've added in this gray box on the side of that, that's the role lineage columns that come automatically. Um, if I do a select star from the table. I see F name, L name. I don't see the. Roll lineage columns automatically. But if I actually key those in and select 4 columns, you'll see all the information like you see on the screen here. So initial load, I put 2 rows. In the table, and then I have the row ID and and sequence number equal to one there. On the next transaction at time T2, I update Diego to Pablo, and then we see the sequence number increment up to 2. And then for the 3rd transaction here, we'll ratchet up the complexity a tiny bit. You'll get a merge statement that does a new row, and then it updates Carlos to Chuck. So here you see. The new record added, new row ID is generated, and then the sequence numbers, both of those come in as sequence number 3. Some people, when we talk to them about this, like, well, hey, why is the new insert, why shouldn't that be sequence number 1, right? It's gonna be a, an increasing monotonically increasing number, but it's gonna start at where the current sequence number is within that table. Which is important now when you wanna actually start consuming those changes. So here on the select query. If this is like when I was talking about doing those incremental pipelines, if this was a source query for you to feed into a data pipeline, you can keep track of that max last sequence number on each of the batches that you run through, right? So now when I come through, if I say, hey, where the sequence number is greater than the last time that I pulled, I'm gonna get those last two records that we saw come out of that merge statement with this example here. So to close up on roll lineage. In the V2 spec, there was in the spark implementation. A procedure and, and a view implementation for doing change log. So change log is basically a mechanism that does snapshot diffing. So you're taking, you know, snapshot at, at T1 and comparing that, joining that together with snapshot T2 to understand what the difference is. are between those two snapshots. So, if you compare and contrast that to the raw lineage, because raw lineage is basically doing the same type of change tracking for you. With change logs and V2. You're having to do, you know, pay the compute cost to to do that snapshot diffing. Right, and with raw lineage in V3, those details are just stamped right on the record for you. There's no additional compute. You just query those, the writers are tracking it automatically. Change logs you have view maintenance. If you do scheme evolution, you have to make sure the change logs and the views are, are maintained correctly.Row lineage, again, that's done automatically. Those metadata columns, the writers, everything is done for you and that that information is there with, with zero conflict. And then lastly, If you think about correlating changes over time, over time travel queries over different snapshots, if you wanna be able to to do that with change logs, it can get a bit tricky where you're having to do an iterative approach to comparing multiple snapshots over time, whereas with raw lineage now again because that information is stamped on the records, it's very easy for you to stitch out the full life cycle of changes. On straight up queries as opposed to doing a bunch of joints. OK, so. To wrap things up on V3, we'll, we'll cover. Some of the other stuff that's been delivered in the spec. So in terms of additional features, there's really, like I mentioned at the start, there's, there's two buckets. Um, there's a core infrastructure set of changes or features that came in on the V3 spec, and then there's also some additional data types. So we'll take a minute here to, to talk about the core infrastructure pieces. The first feature that came in the, in the V3 spec in the core infrastructure area that we talked about here is default values. So default values. You know, inserts data when there's no value specified. So you're gonna create a table, you're gonna specify for a column what the default value is, and then when you're running your pipelines or inserting your data, if no value is specified, you're using the default value that's configured on the schema. What's interesting about the iceberg implementation. The default value itself is stored in metadata, so you're not persisting a constant value in the file itself, you're actually using the metadata on read to replace the value that wasn't specified. So that's a kind of a nifty little nuance being able to leverage iceberg metadata to get that uh default value populated on the results set. Um, and then you're gonna get some conformity, some ease of use around the fact that the, the value has persisted in the table metadata. Uh, you know, I've, I've done worked on systems where you've had multiple developers getting creative with how they specify a default value. Some people use -1, some people use some other value, and it can become a mess, right? Uh, here that default value is stored right with the schema. And you don't have to worry about any sort of pipeline logic or any sort of rules that you want to implement, um, for replacing in default values. OK. Next, in the core infrastructure area is table encryption keys. So table encryption keys gives you the ability to specify. An encryption key at the table and at the metadata level. Uh, it's gonna give you a granular set of control across your, your tables that you need to encrypt. You can also integrate that encryption key with KMS. Um, it does support key rotation as well, so you can rotate keys. And then, you know, lastly on, on table encryption. Uh, you know, this is more of an advanced feature for you to really align with some elevated security compliance requirements on your data, so it's gonna give you more granular control, allow you to, to integrate with KMS as well, but also to, to get that higher level of, of encryption on your, your iceberg data. The last one I'll talk about in the core infrastructure area is the multi-argument transformations. So with multi-argument transformations, now you're able to specify more than one field in a transformation. So transformations can be used for partitioning, or they can be used for sorting. But this gives you a bit more control in scenarios where you have, you know, you have a harder time picking just one column. You need more than one column to actually help with query scenarios, help to avoid data skew, um, but the net for. Multi-argument transformations is this is a a performance feature, so you'll be able to align. Your end user query patterns more efficiently with specifying multiple columns in those transformations for partitioning as well as sorting. OK. And then the last section we'll talk about on, on V3 is additional data types. So the first one in this section is nanosecond time stamps. Nanosecond time stamps have been talked about in the iceberg space for quite a while, well, probably a year before the Iceberg V3 spec came to be. Uh, you know, we had customers that I've worked with that have nanosecond time stamps in their parquet data or from some other system, and they wanna be able to keep that fidelity, that precision as, as they move into Iceberg, and it's been a pretty big gap for quite a while. Uh, so what you're getting is the, the, the nanosecond time stamp support, you know, for your, your high frequency, your, your temporal type of, of workloads. You're getting, you know, the increase from the microsecond to the nanosecond. Fidelity in Iceberg E3, you're getting this both without, with and without timestamp support as well. Um, in terms of workloads, I think this one fits very nicely into the streaming workloads where you wanna be able to have more precise measurements on. Uh, you know, frequently arriving data. OK, so the next one is the geotypes. So this is actually two types, one slide, uh, geography and geometry. Uh, you're getting now support for location and mapping queries with the geography data type, and then you're getting the ability to do measurements and shapes with the geometry data type. So both of these data types, separate implementations, separate. Um, separate data types. They do follow the open OGC standard, so we have an open standard following an open standard, which is great, uh, and then I, I set a 1 + 1 equals 3 on this. Like I think having the geography of the geodata types inside an iceberg and V3 is really a killer killer feature. Um, you're getting geo enablement now on data plus all the goodness that comes with Iceberg around schema evolution, time travel, and interoperability, right? So now you have kind of the power of Iceberg and the power of, of geospatial on, on the data, which, which I think is really great. Last one I'll talk about unknown. Unknown is, is a little bit of a kind of a cryptic implementation. Um, it's a kind of a null placeholder value. So you're gonna get some protection now if you're doing schema evolution and. Types or or data doesn't exist in files you can use the unknown data type as a known kind of null placeholder across engines. Um, sometimes we see customers running into issues where the null handling isn't always handled gracefully across their iceberg engines. This is something that's helping to kind of combat that that scenario we've actually seen some extreme cases. Where customers have gone to kind of rewriting data to make sure that they're not breaking uh their iceberg implementations. OK, so with that. I will hand it off to Yuri. Thank you, yep. OK, so clearly there's a lot of features in V3. And after Ron's excellent deep dive, I think we have no excuse not to know about it, right? Um, and so what I'm going to do is I'm just going to cover, uh, some of the new things that the community is cooking up in V4. And then we're going to just close out the session, uh, and do conclusions. Uh, one thing to note is that all the stuff that I'm about to talk about, you know, it hasn't been ratified. So these are just proposals, uh, community is still thinking about what to do, um, and, um, and the way I would actually characterize the changes so far is they're very much performance based, so focused on raw performance, non jam packed full of features like V3. So I'm just gonna quickly go through some of these, um. So let's start, start off with, uh, improved column stats. Um, so if you're not familiar with column statistics, it's like this special information about columns that's stored inside of the iceberg metadata files, and it helps query engines to effectively scan data, right? And one of the problems with them right now is that, uh, the way these stats are implemented is not super efficient in certain use cases, especially if you have lots of columns, um, and that's because, uh, when queer engines. Go and read them they have to like deserialize these large maps which then creates memory pressure, um, and so one of the things that the community is thinking about is just creating a proper structure for these, um, and that's gonna help engines efficiently look at particular stats that they care about. If you're just using Iceberg like me, all you really need to know is that, um, you're gonna get better performance in certain use cases. Uh, next is the adaptive metadata tree. So this is motivated by small write and delete performance, something that Ron mentioned earlier, right? So if you have lots of small writes and or deletes, it can become problematic. Uh, because Iceberg kind of has this, uh, you know, these layers, right, starting from the catalog layer to the root mid adjacent layer, and then there's 3 more layers where you got the metadata, no sorry, manifest list, metadata, and then the actual data file. So every time you want to insert a file or you know, you do some sort of update or write into your, into your table, you kind of have to go through all of these layers, and that can be inefficient. And so the proposal is to uh combine the manifest list, uh, and the manifest together into like a single structure that's gonna have these like root nodes and leaf nodes. There's, it's a very long proposal, uh, which I'm not gonna go into, but basically at the end of the day, it's gonna skip one layer for the small right use cases. And again, like if you're just using Iceberg. Um, this isn't really something you're gonna think about. Just know that your small right performance is gonna improve, uh, with V4 if this proposal goes through. And so lastly is uh relative paths, um, so this is actually super useful, uh, because one common problem with manifests is that they contain absolute paths to the, uh, to the perk files to the data files, right? And the reason that's problematic is that if you wanna copy your table, like if you're, if you have an iceberg table on general purpose S3 and you just wanna copy it, it could be copying it to a different, uh, S3 bucket, it could be a different account, could be a different. Uh, storage, uh, cloud storage provider. You kind of run into this issue because once you copy the actual files, you can't actually read them or none of the queer agents work because the, the metadata files are referencing the previous data files. And so the fix to that is, well, like it's in a name, right? So it's to make paths relative. Um, and this is gonna be, I think, a very useful feature, especially if you're using something like S33 replication, right, where you're just replicating your, uh, iceberg table to a different bucket for, you know, for data protection, backup, whatever, um, with relative paths you're gonna, you're gonna be able to just query that data without having to muck around with manifest lists. And that's about it. So just to wrap things up. Uh, Ron talked about the key V3 features, which are deletion vectors, roll lineage, and, uh, variant data type. Then he, he talked about the core features. Uh, so the core functionalities such as default values, multi-argument transforms, and the new types. So the geo, nano, and the unknown. And I just went through the D4 proposals. So that's the improved stats, adaptive metadata, and relative paths. Um, we do invite you to go and just try things out, you know, there's only one gentleman at the beginning of this that raised their hand when asked if they tried out V3, so we hope to kind of move that number up, right? Uh, so hopefully by, you know, at the end of this week, once you go back, you go try some of this out. So whatever vendor you're using with, uh, let you know, ask them what V3 support do they have, and then try converting your, uh, your table from V2 to V3. And if you're interested more in the, you know, in Iceberg, uh, do join the community so you can attend the meetup, uh, you can join us on the mailing list, uh, on Slack, uh, or even make a contribution if you're a developer. And with that said, oops, I pressed the wrong button. Uh, thank you for coming and uh hope you enjoy the rest of your being there. Thank you, thank you.