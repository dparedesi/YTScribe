---
video_id: QlWGWZIfeJQ
video_url: https://www.youtube.com/watch?v=QlWGWZIfeJQ
is_generated: False
is_translatable: True
---

All right. Good morning, everyone. And uh thank you for being here. Uh, I know it's day 3 of the conference, so I appreciate you all being here early in the morning. Congratulations to all of you and thanks again for being here. Uh, we're really excited to share uh Lincoln Financial's AI journey over the last 2 years, which has been taking place in partnership with AWS. Uh, and Deloitte, it's gone through multiple twists and turns, evolved, uh, over time based on the learnings and, uh, successes and also, uh, failures that we've had, and we really want to share, uh, the overall journey with you. Now, we, we feel we've come a long way, but we also think this is just the beginning, and we are very excited about what lies ahead. Uh, so before we dive deep, uh, quick introduction. I'm Gavvora, managing director, uh, with Deloitte, uh, focusing the insurance practice on, uh, AI and data, uh, with over 20 years of experience, and I've had the pleasure of partnering with Talbot, uh, on Lincoln's. Data first and then AI journey now for over 9 years and Talbot is the divisional CIO and uh overall AI leader uh for Lincoln. Uh, so if we, before we go, I just wanna set the stage as to what we're gonna walk you through today. Uh, we wanna make sure we share what lessons we've learned, how Lincoln's strategy has evolved. Lincoln started fairly early. Uh, you know, when Gen AI came to prominence in 2023, we started with multiple leadership sessions, education sessions, and by mid 2024, we actually have a, uh, a number of use cases in production, and that was very deliberate, uh, to make sure we learn. So we'll talk you through over the next few minutes what were some of the lessons learned, what were the challenges that we encountered, and how those lessons have actually shaped our journey. Uh, we then also talk about some of the enablement, some of the use cases, uh, that we've implemented and how, uh, we are seeing results, uh, from them and what lies ahead for us as we think about, uh, our future capabilities, whether it's gente and, and other capabilities. So, so that's really what we wanna cover, uh, with that, Talbert, uh, I'll hand it over to you to introduce yourself. Yeah, well, first, again, thanks for coming. Uh. I know some of you probably had some late nights and you still made it, so I appreciate that and thanks for providing this platform for us to share our story. So as, as Gore said, you know, I'm with Lincoln Financial. I'm a divisional CIO in a technology organization of Lincoln Financial, and my remit is over AI data and the corporate technology that supports the corporate functions. I also have some of the enablement technology functions such as enterprise architecture and DevOps engineering. Uh, I've played many different roles at Lincoln, but primarily I've always been involved in, you know, business and technology intersection with transformation and modernization, um, and so that's been great and that's really been a privilege. When we think about, um, kind of our strategy or at the, at the top of the house, you know, one of the things that, you know, I wanna emphasize is just first get some context on who is Lincoln Financial. I often find that, uh, some people know who Lincoln Financial is, especially if they're in the northeast area and you watch the Eagles play. You think about the link when we got the naming rights for the field, but many people might not know who Lincoln Financial is, so I'll give a little bit of background. We're in the top 250. of the Fortune 500 $18 billion in revenue, about 10,000 employees, we really go to market against two primary segments. So if you think about retail solutions, we offer life and annuity products to retail customers, and then from a workplace standpoint, we provide retirement solutions so like 401ks and group benefits, so like disability and and dental products for people to offer to their employees. So that's our big, you know, go to market segments, um, we've just celebrated our 120 year anniversary, uh, which is, which is amazing, and if you think about that, there's no way you've been around for 120 years and haven't had to reinvent yourself multiple times, right? And so when we look at AI, we feel like this is one of those times where we need to think about reinvention and you know, a lot of that is driven by this intersection of compute massive amounts of data. Innovation in the algorithms driving this AI as a platform shift that we believe is gonna impact us, you know, holistically, and we need to think about what that means now for us we talk a lot about tech. Look, I'm on tech guy, so you're gonna hear a lot about tech today, but you're also gonna hear a lot about what this means for people. This is really a people change. This is really about how we can amplify people, how we can augment our products and services, extend the impact and reach, you know, that we have. You know, across the customers we wanna serve and so that's how we really focus on it. It's not just tech and even though I'm standing up here today, this is an organizational wide every leader has to be involved, right? So I'm the one talking, but there's many, many leaders that are making this, you know, happen and bringing it to fruition. So some of the key tenets and we wanted to be intentional about how we designed and go forward with the transformation and we wanted to really align on what those principles are. I'm not gonna drain the page and talk about all the tenants, but I did wanna highlight 3 or 4. So I talked a little bit about the augmentation of human performance. Uh, we really want to focus on this being a a tool for augmentation. I use the, uh, metaphor of the Iron Man suit a lot, you know. Think about that. Or if you took Steve Jobs. Famous saying when he said personal compute was the bicycle for the mind. This should be the rocket for the mind. This is about how we take people's capabilities and extend them and expand them in ways that we haven't thought possible before. So that's how we're really positioned in this across the organization. Uh, so then that goes in parallel with how do we strengthen our in-house talent. Skills are gonna have to change. Ros will probably change in terms of what people do, and we wanna give them the support mechanism to do that. So we have very intentional programs around upskilling, re-skilling, and even how we approach execution to make sure that's integrated, not just in terms of technology, but how do we bring our people along. And then the last thing I'll say is governance designed for speed. Uh, you know, if you're in financial services, which many of you are, speed is not a thing, especially with governance, but this tech cycle is moving so fast we have intentionally said we have to change because you can't take 4 to 6 months to go through a governance. Process to bring bring a new service into your environment that may be obsolete in 4 to 6 months as fast as this tech cycle is moving right so we've really been intentional to say how do we attack that how do we think about governance differently? How do we increase the metabolic rate of the organization. So we can keep up with this tech cycle and get more comfortable making decisions faster. How do we think about risk tolerance? How do we scope and use cases initially in order to kind of make them comfortable so we can test and learn and then understand the controls and get some pattern recognition about how we can apply the proper risk and governance mechanism and frameworks going forward. So those are the three things in terms of tenants that we're probably seeing the biggest change that we're really focused on. Thanks, thanks, Albert. So I have two questions, you know, first, we said, you know, you started early with your strategy. Initially it was fairly conservative. Uh, you wanted to be very, uh, risk aware, uh, knowing the technology is not mature as well. There is a lot of change in the space. But now we know Lincoln is significantly prioritizing, uh, Gen AI. So, why, how has that strategy evolved and why? Yeah, so initially we were all on a learning curve together. I remember when was it? Chad GBT was like 2020 mid mid 20, early 2023122, and you know everyone was worried if the data, if the models were trading in our data and had to answer that question like every time we talked about it, so everyone was on this kind of learning curve. So we did start out conservative. To ensure we understood kind of where the risk lied, you know, data leakage, PII, just regulatory requirements in terms of disclosure. So how do we ensure that controls we needed to have in place and where the risk really were and then also kinda begin to understand our level of risk tolerance now that we have more pattern recognition, we're starting to apply. Apply that pattern recognition to use cases and we're able to move faster. We have controls that we've, we've, we leveraged and put in place, even guard rails that even AWS provides, you know, you can identify PII and block it. Uh, you understand when the models are training in the data and when they're not, you know, you have certain agreements in terms and conditions built in place. You know how to do disclosures. So I think naturally right now we're starting to open up our risk aperture a little bit more. Initially, you know, we started with things of saying we, we, we didn't wanna do customer facing Gen AI capabilities, but now we are, right? So that's a good example there as we learn more and go up that learning curve is enable us to do more and to kind of expand and bring everyone along with us legal compliance, cyber, um, I'm don't think if I'm thinking anybody enterprise architecture which I own, but I have to bring them along. As well, um, and now we move together as a group and we understand what we need to do to kinda make sure we're boxing in the risk appropriately. Absolutely, absolutely. The, the second question I have, you gave the analogy of the Iron Man suit. So what are some of the things you're doing, uh, to train and educate your workforce, and are you facing any resistance and how are you dealing with that? So we, I have a very tight working relationship with HR which we call PCC people communications and corporate Services. I probably talk to my counterpart in the PCC organization every day. We've formed a program that we call Inspire AI where we. Do, uh, very specific training starts foundational about just basic understanding of GI capabilities. Then we get more specific with use case trainings based on where you sit. So let's say you're in a financial professional. We provide business champions specific use case. Is about how you can use Gen AI to help you build spreadsheets, check errors in your spreadsheets, etc. It's just one example, because we wanna make it as tangible and practical for people as well so they can then take that and extend, you know, the, the things that they can do with Gen AI and AI and, you know, more broadly. So we're very intentional about how we're bringing people along. We do, we did a whole program where we did in, uh, in office kind of training sessions at the same time, you know, we, we videotaped it and broadcasted it live, so we're trying to make everyone a part of it, and we call it crowd enablement. We'll talk a little bit more about that in, in the going slides. Absolutely. Yeah, so we talk about anchoring our AI kind of transformation journey and you know Gore have indicated it is a journey and I do feel that we're at the beginning stages of this journey, uh, so we continue to learn, but we're still at the very, I think, beginning in the precipice of what this is gonna be for us. We thought about why is this different or how should it be different than things we've done in the past, um, and what are the, uh, so how do we think about that? One is we wanna make sure this is top down driven. And so our CEO is very hands on and involved, uh, and she models the behaviors as well, and she's one of our probably power users of, uh, of our, uh, OpenAILM service that we have, and she expects everyone else to be doing the same thing. So she sets the tone at the top, and she's, you know, said this is a top priority for Lincoln going forward. So that, I think, has set the tone and, and for leaders beneath that to model that behavior. That's one. I think that the second piece you'll see is that when we talk about a centralized AI hub, you know, we're a data driven organization, you know, we don't make widgets, we don't make, you know, consumer product goods. Our raw material is data. In the past we've had fragmented roles around data science. It wasn't a competency that was as formalized and as supported. But now we feel that this competency is gonna be critical in order for us to really reach the full potential of how we can leverage capabilities like AI and ML and data more broadly. So we're gonna, we'll centralize a competency around that and really uplift it more and provide the tooling and the support that it needs to, to flourish. So that's a big investment that we're making. And then lastly I think in terms of the socializing of, you know, specializing of business expertise, this is really critical. Um, a lot of these conversations about agentic AI really rest on codifying and unlocking tribal knowledge that exists in the business today. Many times they're just in people's heads, sometimes sporadic documentation that is not well structured across the organization. And understanding the relationships between, you know, this, this tribal knowledge and the deci and how people make decisions so we have to unlock that we have to codify that and we have to make sure that that specialized expertise is lifted up and is recognized that it's very important for us to be successful. And then ultimately that's where we need the business to make sure that they're driving us and they're taking the front seat. Yeah, yeah, thanks, thanks, Albert. So one question that always comes up as we are thinking about G AI journey is, where's the ROI, right? How can I get ROI quickly? So how are you balancing building capabilities but also addressing the need of ROI? Yeah, so we even in the very beginning, right when we kicked when we kicked off the strategy session we brought business representation together at the table. And we aligned on what we call like a initial ROI framework. So how does, how do we think as a company around where's the value in the company? How will we measure it? How will we track it? How, what's the prioritization framework that we agree on, um, so that as we have these discussions for use cases that might exist in different parts of the business, but we have limited resources, you know, to invest across, how do we make a decision on where we're gonna focus? So I think having that initial conversation together was really helpful. You know, as we learn more, we continue to fine tune that ROI framework, uh, uh, more appropriately because, you know, we're learning about how much does it really cost to put Gen AI into production, how much does it cost to support it once it's in production and have that feedback loop. We'll talk a little bit more about that when we get this into some specifics of our use cases, but we're learning more, so we wanna make sure we keep that ROI framework, you know, fresh and updated. From that, from that standpoint, the other thing I would say is with that speed, that, uh, point I made about increasing our speed, that's helping us also kind of do the balance of placing big bets but also being able to test and learn because if we can do it faster. Um, and the other thing you're gonna hear me say is we do it in production now. We try to shrink the blast radius of how many people are exposed to what's in production, but we do it in production. We feel that's when we get the best learnings, and we can do it fast, and we can make faster decisions. Is this a capability that really has potential and can scale? And the last thing I'll say, Gore Gore is we've increased our appetite for refactoring, refactoring capabilities we put in place previously. The tech cycle is moving so fast, I think you have to be willing to say, you know, if I'm talking to my finance team, a 3 to 5 year amortization rate may not make sense anymore when we're talking about AI capabilities. You know, we're looking at things moving to 66 months sometimes, and we'll say, hey, we need to redo how we did that 6 months ago. So that's another key aspect of how we keep thinking about ROI short term versus long term. Perfect, perfect. So, as, as we move on, let's, let's talk about how you're structuring and what are some of the key foundational building blocks for your uh company. Yeah, so, uh, we continue to evolve, you know, the delivery model and operating model at a high level and. If you follow, if you ever heard of Ethan Malek, this framework might look familiar. We borrowed it. We tweaked it a little bit, but it really resonated with us, um, and what you'll see is like a triangle that has 3 kind of stools to the chair, for lack of a better 3 legs to the stool, I should say, for lack of a better phrase, and it starts with leadership, and I've mentioned that before. This is top-down driven. It's based on the strategic version of the strategic vision. This is not like the hammer looking for the nail. I'm not running around the company saying, you know, here's where you need to use AI. You know, it's starting with the business vision, and then we're seeing where AI can augment or extend or expand that vision. And it's not just AI, it's also automation. It's data. We're thinking about it in a cohesive and comprehensive way. So I would say that first, you know, in terms of the, the, the establishment of a data office is something that's new for us. But if you think about where the value really lies in an organization like Lincoln, a lot of times it's trapped between the seams of organizations and functions. And so we, we need to take that step from vertical solutions to end to end journeys. And a lot of times those end to end journeys are very hard for individual business units to execute because we don't operate that way naturally. So we're looking at establishing this function that can help us think across more in the end journey. How do you redesign, reimagine processes with a lens of AI and data to unlock that value that's often times, like I said, trapped between the seams of functions and organizations, and it'll help us then also build that competency I talked about previously around data science. So very important there and then the crowd, everyone has to be on this journey. You know, we wanna put the tools in the hands of as many people as possible. We also think that's a natural feedback loop because we learn as people actually use the tools and say, hey, let me show you what I did and share best practices. So in terms of crowd enablement, that goes back to the partnership I mentioned with my HR counterpart about how do we do that intentionally establishing business. As champions establishing training programs, modeling leadership behavior, real use cases that they can put in their hands and begin to use, um, and just create that flywheel effect. So I think the crowd enablement uh impact is a source of innovation for us. Then we can socket that back to whether that's the data office or back to leadership to say, hey, here's what's coming from the bottom up. Thanks, Talbot. So you mentioned about the operating model, the central AI hub or the and the central data office. We know Lincoln is a federated organization with business units having a lot of autonomy. So how are you managing building enterprise capabilities and getting buy-in from BUs versus how BUs have been traditionally very autonomous? Yeah, and I'm sure there's other organizations that probably sounds familiar. We have very federated nature, you know, and it, it's good in terms of it drives like an entrepreneurial spirit within the business units where it gets hard is when you're trying to coordinate and get people out of align to move it in the same direction, uh, but I think the thing that we've done from the very beginning is to ensure that we have everyone at the table. That's one, and you know, and you mentioned that at the beginning of, of our conversation. And then we looked at about how do we integrate AI capabilities into the organization and you think about in these three swim lanes. So there we have alignment on, uh, I would call them common user journeys or common foundational platforms that are utilized no matter which business unit we're talking about. And if we can build those in a way that's scalable and reusable and it's easy for the business to use it and we have alignment on that. That's for approach one, right? We're a margin-based business. If we can scale, you know, horizontal platforms, or horizontal uni uh, user journeys, it's a win-win for everybody. I think the common thing there is just how can you ensure that they can take advantage of it when they need to and that they don't feel like there's a bottleneck. So that's, that's really step one. I think there are still though deeper vertical, you know, still robust opportunities within the lines of business, you know, a good example is even in, you know, in IT we talk about using AI to augment the SDLC process, right? That's, that's a unit of journey in itself for us to look at in the end and say how do we reimagine how that looks with the leveraging and integration of AI, AI tools going forward. Other business units have similar end to end journeys that exist in their verticals. If you're in distribution, we have a strong distribution, uh, competency that we feel is a differentiator for us, and we talk about that distribution experience, you'll see us talk a little bit more about that. But that's the end to end journey that we feel like can really take advantage of these capabilities going forward. So that's the middle swing lane. And then when you think about the crowd enablement, those are the more the, you know. Generic tools that anyone can be able to use Microsoft 365, Go Pilot, or, you know, um, Anthropic Cloud or any name your LLM service du jour. We're probably all playing with all of them right now, but how do you put that in the hands of people in a way that they can integrate it into their work flows and get real tangible use out of it? And we wanna make sure that at least everyone has some exposure. To this way of working and so they can begin to feel a part of the journey and that's kind of the rising tide that we think will lift, you know, lift all the boats. Perfect, perfect. Thanks, Talbert. So, over the last few, few minutes, we covered, you know, how Lincoln is thinking about their strategy, their operating model. Let's now shift gears and move into some of the use cases and some of the work that we've already achieved and executed on and what is, uh, what lies ahead for us. Uh, as we, as Talbot mentioned when we started, we brought everyone together. When we started thinking about what are the business priorities, what are the use cases. We went through a strategic tensions exercise with Lincoln leadership, both business and IT. We looked through, you know, what's important to you. Cost reduction, growth, how much risk you are willing to take, uh, what's your capability and readiness now, which functions are ready, where you don't have readiness? And that really was very helpful for us to be able. To prioritize a number of use cases and when we started, we initially looked at three key areas. One was distribution and customer experience. Distribution is a key differentiator for Lincoln in the market. It drives their growth. It drives their sales. So that was one primary area. We also wanted to look at how we impact the entire employees. So we took an HR as a second priority area. And then workplace was another the growth area that uh LinkedIn has been growing over the last few years. So that was the third area we looked at. And as we started working through the use cases, we'll talk a little bit more in detail on the next page about the distribution use case, uh, but the HR and the workplace use case, we started with POCs and willing to understand, you know, what lies ahead for us, and very, very quickly we realized there is value. In using Gen AI from a solution perspective, but the decision was, do we build it or do we buy it, and that was very, very quickly apparent to us in a matter of 1 to 2 months that yes there is value, but there are better tools out there that are already doing it in the market and we don't need to build it ourselves. So that was really very important for us early on to make sure we are investing in the right places and we're investing the right way. So let's, let's move on to the distribution and info link use case around customer experience, yeah, and this was our first GAI application we, we took to production, um, and so as Gov mentioned for us distribution is one of our competitive advantages. The one thing we wanna make sure is that competitive advantage is not mitigated by AI. We wanna actually strengthen it and deepen it through the use of AI and utilization of AI. Um, and distribution also, uh, had data science expertise that existed within its line of business already, so that gave it, you know, an area that said, hey, there's expertise that we can leverage, um, the data that was available for this use case was external, so that helps when you're dealing with legal and compliance and data privacy, right? So that it made another good case for this to one to be to go first. And it had a clear business problem. And so when you think about our distribution team and where this use case applied, we get a ton of questions from our advisers, and I didn't cover that earlier. Now I'm thinking about it. When we think about who our customers are, we have two ways we think about customers equally. One way is we sell, you know, our products through financial advisors, independent agents, retirement consultants, etc. And it's very important for us that we provide a very great experience for them as well and and increase our ease of doing business. And of course we have the in policy holders, the in account holders who are calling our service centers and looking for service and support as well. This use case was really for our advisors and our and independent agents. They oftentimes call into our, our contact center or our investment, uh, support desk asking questions about our products, you know, our rate sheets, you know, these products are not the most, uh, layman term products understanding if you read the kind of the contractual language and so they can be pretty. Technical in terms of your understanding of them from a financial kind of expertise and lens and when they our investment desk gets these questions, it's a lot of manual work to go find the right document that pertains to a certain product or a certain rate sheet and find the answer and and answer that question. And so if we could build a knowledge assistant that they can enable them to bring that information to their fingertips by a simple prompt, there was a tremendous amount of value to reduce that cycle time but also improve the experience that that advisor is having when they're calling in, you know, I, there's a. Phrase I like, I, I borrowed it from someone where they talked about muck the muck versus the magic, right? So the muck is you're going through some knowledge database where it's not really well indexed and you're trying to find an article that's not really the value. The value is actually being able to have that rich conversation. Right, and that's where the magic is. So if we can reduce the amount of time, you know, our investment professionals are spending just trying to search for information but actually providing insights. That was the goal, right? And so that's how we developed this use case. And like I, we did this in production and we learned a lot from doing it in production, uh, both, and I say we, both IT and the business learned a lot because we did it in production. I think on the business side what they. Learned is what does it really take in terms of that integrated partnership to get to a level of quality of answers that makes them feel comfortable with using that for their people to adopt and use the system, right? Adoption is key. And so when we first started into production, we were probably at like a 70% quality ratio, but through that tight partnership of working with the business to set what the ground truth is. Uh, helping us really, uh, improve our document parsing and document ingestion pipeline and how the Gen AI system, you know, prioritizes and ranks the right context to bring back and then brings that answer forward and reference the right information, we've been able to get up to a 95%, you know, improvement in quality ratio, you know, accuracy ratio I should say, and that's the, and that's when we started to see the sweet spot of adoption and the comfort level of them using it. We wouldn't learn that if we didn't have that level, I think of doing it in production. I don't think it would have been as rich and tangible of learnings and adoption what it really takes. I think the other thing we learned from, you know, um, doing this is just building the skills. What does it really take to build a rag pipeline and it was kinda standard now when people talk about rag, but when we were doing, when we were doing this in early 24, it was relatively new and I'll, and I'll tell you the point that we've made before is where are we, we factoring it. So I, it gets back to that quick life cycle, that tech life cycle, but we've learned so much and the skills we our teams gain so much of actually doing it practically has been, you know, unimaginable. I can't, you can't really put a price tag on that learning curve at all, so that's really helping us. It's turned into reusable assets now, reusable practices and standards that we can extrapolate to other use cases. So we've definitely made sure we did that from that standpoint. Yeah, no, and I think as you, as you talk about, right, like what, what we learned building the AI infrastructure first, I think the technology was very nascent at that stage, as you said, you know, whether it's the rag methodology, Bedrock was fairly new, uh, we're looking at what are some of the guard rail tools for obser observability tools. Things were actually changing in the market leadership every day there was a new tool, so we learned a lot on how to go about even selecting the right, uh, parameters. What are the decision points that are relevant for Lincoln versus, you know, we're reducing the noise around the decision making process. That, you know, I think about how many times we, we switched our ranking model, you know, so that, that was, uh, uh, I think one of the capabilities when we put Bedrock in place and we have Sage maker. Um, and we started to evaluate which models were giving us the most effective output. You know, we, we came up with cohere as the completion as the embedding model. We use, uh, Claw 4.5 as our completion model. Well, we probably kicked the tires on the ranking model like 3 or 4 times, but that, but that's part of the improving that that process and kinda working with the business to understand what is preventing us from getting the level of quality of the answer for them to adopt. Uh, building observability weights and biases and things of that to find the traceability of where we can isolate where the problem is and make the improvement that we need to make and so that this thing, you know, has been a great learning experience for the team. Perfect, perfect. So core building blocks, uh, I talk a lot about this across, you know, not just in IT, but I think with critical business leaders as well, because this is, this is definitely a team sport. I'll call out, uh, a few of them that we talked about today. Do you reimagine the AI possibilities or your willingness to redesign a process, you know, yeah, I would say even beyond redesigning a process to change a decision making framework, how decisions are made. When we looked at processes in the past, and I've had continuous re-engineering teams, lean lean 6 Sigma teams report to me in the past, a lot of times it was a very linear thought process, you know, Step A, Step B, step C, Step D, you know, inputs to outputs, sippos, things of that nature. But when you're looking at it now in terms of what AI can do, a lot of things can be done in parallel and at a much larger bit of scale. You have to understand the decision where decisions are actually made, where are you comfortable with automation of certain decisions, or where you need human in the loop under what parameters. So that that kind of framework to reimagine a process to get the full value is critical, and that's not an IT. Thing only that's an IT business all hands on deck and you know HR talent roles, everything. So that's when I'll emphasize that and I think we're continuing to build, you know, a a repeatable playbook we can use to do that and we talk about that a lot, um, in terms of the AI governance I mentioned in the beginning we we continue to look at how we can speed that cycle of, you know, reviewing and improving. Yeah, I say to the team we gotta lay down train tracks, not speed bumps. We gotta provide guardrails that can enable us to move faster and have that pattern recognition. So we've actually made it a lot of process reengineering to how we evaluate risk, and that's a whole effort in itself. So we looked at that because we have to be able to do that differently than we've done in the past. The other thing I'll, I'll highlight is, you know, that on the technology architecture platform and operation side is modularity. You know, Gov said we've things are changing rapidly while we're implementing them, so you have to have a little a way, a modular kind of architecture that allows you to swap things in and out as you see the market changing and improvements happen that you wanna test and learn on us, I really think can provide uplift. And also you, I think you just have to have a different, you know, depreciation cycle, how an investment when you think about investment as well, because you're gonna have to refactor more than you would have normally in the past and how do you get your business, you know, comfortable with that. A lot of it is them understanding the uplift that comes with it being able to to make these changes and move from a business outcome standpoint. So we're very focused on modularity, reusability, but understanding we're gonna swap things out. We're gonna, we're gonna test and learn. We're gonna make changes. So, where are we going from here? I know we've learned a lot of lessons. Uh, we've done a lot, but there's still a lot to do. Still a lot to do. Yeah, I guess that we're at the beginning, so there's a long, long, long path ahead of us. Uh, we picked, we picked one area we thought about, uh, where we're gonna focus. So the first thing I'll say is as we go into 2026. We really wanna focus on maybe 2 to 3-ish in the in journeys to take to scale and distribution is definitely could be one of those in the in journeys and so as we work with our distribution leaders how they think about. And you know, distribution in the future and the technology and capabilities and the data to reimagine that process and so we started with Infolink and they got, they got a little taste of that and they're really, uh, I think really bought in now to, to expand that across the life cycle, you know, uh, of how they think about distribution. So intelligent prospecting, you know, dynamic marketing, uh, personalized experiences, I think it helps that, you know, when you look at all these capabilities we have on this page, they're very data reliant, and it helps that we made a, a substantial investment in data a couple of years ago when we built, you know, our sales IQ. And so now that we have that data foundation and we have good governance and control over that data domain, we can stand these more advanced capabilities on top and start to build them with a lens towards scalability, uh, with the right governance and controls, but also with the right ability to to see have line of sight to the value. And so this will definitely be one of the, I think, the in the in journeys that we go after. They'll be probably wanting workplace solutions because our, our group business is a growth engine for us and they have a target in the in journey and then you know we're gonna do something in IT with the AI S S DLC but the approach and the playbook, I think a lot of the frameworks and what need to be true for success are the same. You apply them in different, you know, businesses. They may be different data sets. Uh, I actually think with some of the foundational technology will be very reusable as well. Uh, and so we're building on this muscle and then we can expand it. We pick these three that we think are very high value use cases to start. Like every place else you get some momentum. You continue to get business buying and you scale. Absolutely, and I know we are also working through building an innovation platform or a data science platform where now with the comfort of how the technology works we're seeing value, we're now starting to bring in internal data sets where it's policy information claims information also integrating with the external data sets. So what's our vision there and, and how are we looking to leverage that? So the, the vision here is to speed up the cycle time of the feedback loop. And so if we do build a GI application, you build an AI application, and users are using it. Day one is not gonna be at the effective level that you know is really what you're shooting for and so you're gonna get feedback from those end users to say, hey, here's where, here, here's how it could be better. How you quickly you can respond to that feedback I think is a is, is critical in terms of how you will get adoption going forward. If it's too slow, people will stop using it. If you don't have adoption, then why are you doing it, right? You know, no value can be attained if you cannot, you know, drive the change in behavior that you need that's associated with the value. So I think primary one, building this innovation platform is really the to increase that cycle time. The second thing that helps us, we wanna make sure we can do is, you know, integrate controls and quality into the platform in a way that's systematic, uh, and seamless, uh, so we have the, the level of security frameworks built in, uh, and then it'll enable us to increase the complexity of the use cases we're doing, the sensitivity of the data that we, that we're, we're leveraging. So I think that's a, a critical and piece number 2. And then from a a scalability standpoint we're at the beginning of this journey, but we want, we wanna scale, you know, we wanna do things that have a tangible impact. So when I'm sitting down with the CFO, you know, at this point next year I can point to something that he goes, oh, I see what he's doing in the P&L. So ultimately we, I wanna make sure we can scale enough to translate it to tangible impact and, and a tangible bottom line and for if I'm sitting down with my leaders in marketing we wanna be able to say look at the experience, you know, uh, improvements we have you know we did a customer facing chat agent where we got really great experience MPS scores back and that really got the business excited. You know, I think out of a 5 score it was like a 4.95, and they, and they were blown away by that. What does that do also it increases their comfort level because yeah, some of the, the risk and I think some of the um. People who are worried about the solution, some of it is around security and legal compliance, but, but some of it is around, a lot of it is around experience. What if it gives a hallucination? What if it gives a wrong answer? You know, what does that, what does that mean for my relationship with the customers and the advisors I have? Can I trust it, right? And so the, the more that we can show that we can build things that actually drive great experiences, then that feedback loop is even greater in terms of the willingness to scale and invest and extend this forward. 100% agree. So with that, uh, that's the end of the presentation. Uh, we are open to questions. I'll also like to invite Ved who's been our technical delivery, uh, uh, and, and part of the journey for the last couple of years.