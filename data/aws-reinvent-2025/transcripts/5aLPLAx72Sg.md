---
video_id: 5aLPLAx72Sg
video_url: https://www.youtube.com/watch?v=5aLPLAx72Sg
is_generated: False
is_translatable: True
---

Hello everyone. Welcome, uh, welcome to the reinvent session, the next frontier in financial systems transformer-based foundation models. Before I get started, I would like to do a quick audience poll. How many of you ever had experienced your card being declined or getting a message from your bank that this might be fraud? Well, yeah, we do have a couple of raised hands. If you all wondered, this is coming from the AI model that understands your credit card history. Today what we're gonna do is we're gonna learn about newest foundation model and how it is transforming the financial industry and how NVDI is accelerating the innovation. So my name is Suddit Kaalindi. I'm a principal solution architect working with AWS. Uh, I do have the pleasure of joining the stage with Pahal and I'll let Pahal introduce. Hello everyone, my name is Pahal Patangia. I lead Nvidia's global business for the payments industry. Uh, I have been with Nvidia for the past 4.5 years and continue to grow the adoption of AI and accelerated computing, uh, within our within the broader payments ecosystem and our customers and partners. Looking forward to President Shat Sade. Thank you, Pahal. So in today's agenda what we're going to do is we're going to cover about why transform for payments and then we're going to look at how foundation models is using transaction data which is happening patterns at scale, and then we're going to see how it is unlocking personalization and also the pattern mining. Then we're going to see how we're going to deploy or develop on AWS solutions using Nvidia acceleration. And finally, the key takeaways and the call to action, how to get started and how to get deployed on AWS environments. So that being said, if you look into it, right, why payments need a new class of AI together? The world of digital transactions has exploded both in volume and complexity. Billions of transactions are flowing through these massive systems every day. So whether it is credit card transactions, QR, or it could be contactless, or it could be cross-border payments, right, all we see is they do generate vast, massive data sets. And these data sets, again, if you see right in your organization, you might be using rules-based or an AI-based model that may not even resolve. On top of it, the frauds are also becoming increasingly complex, meaning today they use chatbots, they use again what we see is genderative AI to simulate synthetic identities. So what we need to do is we need to see the payment transactions are happening in real time and also curbing the fraud. So what we are going to learn about is the newest level of foundation model. Which is transformer-based foundation model and how NVDI is accelerating and Pahal is going to talk about the next set. Why it transform our payments. Thanks Sudi. So if you, if you think about the world of financial services, the world of banking, the world of payments, it is largely, uh, uh, uh, an arena where there is a lot of structured data. There's a lot of tabularid style data, relational style databases, and that's where the ton of information. For the industry has been stored and so far the world has always relied on the structured data and combined it with some kind of algorithms, be it statistical or machine learning based to extract insights or make a prediction or ultimately derive more value towards it. And, and that is where that is where, where we have seen the opportunity that if we up the ante on the algorithms is, is, is where we would be able to extract that additional delta of insights, additional delta of prediction. And that is where we are seeing the world of Transformers being very crucial in changing that trajectory and that is, that is rightfully so because at the end of the day all financial institutions are vying to grab a piece of that personalized story of a customer. They want to know where the customer is spending versus they want to know what the risk profile is and how do all these things tie together so that they can give the customer the next best. Offer or they can predict what the next transaction of the customer be and that's that's that that context building is something which is very well encapsulated by the likes of algorithms which are transformers and that's been that's been the juncture of where we are in terms of in terms of using the technology which is fueling the generative AI and the unstructured data processes around it but also combining it. Into the world of structured data and taking it to the next level. So why they are powerful, the, the, the key reason is that transformers are able to capture not just the static features of a customer, but also the dynamic trends. So what, what have been my likings throughout in the, in the past 5 to 10 years of my history versus what are my recent interests? How do you capture both the information together? Until to date there have been. Algorithms that have been methods, but more and more, uh, but more and more uh effectiveness has been seen with the likes of Transformers, and that's that's the ultimate pull which Transformers bring for the broader industry and, and at the same time, at the same time, if you think about how we have been doing predictive models in the broader industry, this is how a typical. A typical pipeline would look like you have your you have your tabular style data sets and you would point them to a machine learning algorithm, uh, a target variable, a predictor, and ultimately you are predicting probabilities and this has been done very well. The likes of XGBost, the likes of GBM have always found. Uh, greater flexibility and greater accuracy and efficiency when it comes to, when it comes to pointing these algorithms at the data. But uh, now, now is the time where we are seeing this pipeline which you see, the canonical pipeline. At the top could be augmented with the likes of better algorithms and that's that has, that has given given birth to the rise of what we call sequential models into the financial services domain and the sequential models are not new but have been have been around for a while. Talking about pre-transformer era pre-2017, uh, the, the likes of RNNs, the likes of LSTMs have, have majorly dominated, uh, financial services firms because there's a lot of data which is not just structured but also say in a time series fashion. There's a certain sequence to it. There's a certain temporal pattern to it, and that is where the sequence models have, uh, have found value in one way or the other. Uh, and they were, you know, for a lot of time, kind of, kind of a notch up than the traditional machine learning algorithms. And what, what the industry and what the community was kind of grappling, uh, with them was problems like, uh, vanishing gradient problems or, you know, in other words, you're not able to capture context for a longer window. And those, those were the, those were the uh pressing points, if you will, uh, which led the industry, let the researchers think how can they, how can they leverage new algorithms and that gave this gradual shift gave rise to the emergence of transformers into this domain. And, and, and the other pieces, as I mentioned earlier, how do you capture temporal aspects of a customer's behavior or a sequence of transactions, both from a recent view and an old view, and then how do you connect it. With different entities, what are the connections it has with respect to your friends, your network, the kind of places you're shopping at, etc. uh, all of those could be more intricately captured by the likes of these sequence models and. And this is, this is the augmentation which I was talking about to the pipeline. So we have seen the world where traditional machine learning has found enough value. uh, XT boost is all we need. Uh, uh, the paper resonates with a lot of people, uh, so do, so do I, but where we are seeing more and more, uh, push into the new domain is the idea of embeddings when you, when you. Financial services is an industry which requires explainability to be at the forefront. There needs to be a certain level of transparency in the way decisions are made and in the way these algorithms are used, so you can't take a frontier model and just just put it in the front of your data and let it do the predictions. It will be a black box model. If you take that to a regulator, the regulator will shoot you down and there'll be penalties and so on. Uh, what could be done, which is a middle way that you could leverage the technology and the innovation that is happening, but also, you know, you satiate, you satiate the regulatory requirements from an explainability and a governance perspective is that you let the likes of Tree-based models be at the frontier, uh, of, of your decision making and at the same time you augment your features with embeddings which are built out of transformers. These embeddings would ultimately augment and aid to the predictive accuracy and power, which was not present in the vanilla tree-based models which we are doing. So, embeddings as features when being fed to these end models for prediction. Would ultimately yield that higher level of accuracy or lower level of false positives, whatever you're optimizing for. So this started with a very good premise on fraud as as a. As an industry which is uh which uh which is proliferating and ultimately it's AI's job to curb it and and and the way the way we are seeing more and more work being done in the industry is the emergence of these embeddings which help inform what are the entity connections. Uh, cluster, uh, cluster your, uh, transactions together and understand patterns more effectively than just looking at a row by row fashion, and that's the power of embeddings which is just an additive step in, in, in parallel to the in parallel to the traditional machine learning workflow which we have been seeing and. In in that spirit is where is, is where the value of uh Transformers being exposed. So, so the moment you expose Transformers to large amount of data and uh make it learn patterns, it's a large data problem. It's a large compute problem. And the way to paralyze those to paralyze those operations when your transformer models are learning the patterns in your data, not just your entity level data, but different modalities of data. And when I say modalities, I don't mean texts, videos, images, but different flavors of data, be it the merchant data, be it the bank data, be it the different event logs from. Uh, from a, from a click, a click through, uh, perspective, etc. How, how do you blend all these words together and Expose that to the transformer architecture that problem is massively compute heavy and that is where parallelization becomes key and that is where GPUs in general become very effective in paralyzing those operations when you are training those embeddings when you are fine tuning models to cater those Vanilla models to particularly your data and uh that is where uh that is where. What we have done is we have uh built certain libraries, certain toolkits, uh, which are available as open source, which are available uh on AWS's, uh, uh, managed services, be it Sage Maker, be it the marketplace, etc. uh, for developers to ultimately take value in terms of, say, fine tuning, in terms of, uh, uh, converging their, uh, uh, distributed pipelines, etc. So what, what, what we have seen is, so what, what we are seeing is a transition of the industry going from the traditional rules-based methods to more uh deep learning based methods, and this transition has been very. gradual over the years, uh, the earlier years in the industry were very much focused on heuristic rules which were so static that they were like a reactive approach that an incident would happen and then. Uh, the institution would apply those rules and then fraudsters would find new value around it, uh, uh, new ways to, uh, get through, get through those rules, and that's where the machine learning algorithms find, found value. And now more and more we are seeing uh moving from those machine learning algorithms and aiding them with these embeddings, uh, both from a transformer architecture perspective and also graph neural network perspective, um, you know, depending on how you want to structure your data, these embeddings continue to continue to become, uh, the foundational layers for these new kind of models and, and, and. That is, that is where you would see uh a lot of learnings which have been which we have been seeing from our customers and partners being involved uh into how can they shape their data now a traditional approach for a traditional approach for uh. Training these foundation models on tabular and structured data has been around, uh, you could take an open source model which is train on language, uh, like a lama or a queen like a model and then you fine tune that model base uh base your tabular and structured data. Now, these models, by nature, are are attuned to cater to text and unstructured data. But when you are when you, when you have the ability to fine tune them to speak the language of the domain, in this case, say payments, uh, they become highly effective. They are, they become the tools to predict what could be the next transaction or they also could become the tools to encapsulate all the behavioral patterns from a customer. And, and these learnings come from largely two worlds come from the world of large language models as we are seeing more and more progress in the domain as well as graph neural networks that how do you leverage these embeddings together to ultimately feed it into these frontier models and that is where uh uh. AWS and Nvidia have built like a fraud detection blueprint, uh, which you could leverage, which you could think of as GNNs as a service where you could expose your data and build GNN embeddings out of it and ultimately feed them into the models which you uh which you are using for prediction. And to take those learnings and feed it into a pipeline which is uh which is like a uh which is like a tabular transformer model training pipeline, I think the very big key step as we talked about is making that language model learn the language of structured data. And how do you make sure that it understands the patterns and nuances which are stored in cells and not words and sentences? The key part of doing that is writing your own tokenizers. The, the, there are, there could be as simple ways as just making your transaction in a sentence like Pahal did a $5 transaction at in and out on a certain date at a certain time and uh he has been doing it for uh you know, once every week, uh for example uh that is a very simple and rudimentary form of tokenization. Here, there are different approaches which we have seen from the industry and the academy and research communities. So as to how do they take the tabloid data and ultimately allocate say certain tokens for a particular cell and make it a fixated approach. On the other hand, you could also think about how do you take those merchant categories and uh tokenize for and tokenize for the most prominent categories, ultimately like a one hot encoding kind of uh uh kind of um uh uh structure. And like these are the techniques which uh which are which have been more and more prevalent so writing your own custom tokenizers has been the difference uh in finding uh accuracy with these models uh as compared to a vanilla uh large language model being exposed. The very, the very, there are, there are 22 approaches which the industry has taken. Uh, when it comes to, when it comes to pre-training these models, you could pre-train an entire model from scratch on your tabuloid data, write your custom tokenizers, build your own structure and architecture, and ultimately take it at scale. That's less prevalent. The number 2 method is take an open source model which is which which could most likely be a language model and then fine tune that open source model to ultimately uh to ultimately uh cater to the domain of the data you have and that is the method which is, which is more and more coming up to speed and fashion and uh the, the, the way you represent your tokenizers and your data in cells is becoming more and more important. And, and that is where, uh, that is where, uh, the techniques which we used, uh, the techniques which we talked about, uh, continue to, continue to, uh, uh, continue to come into fashion. Uh, it's about, it's about breaking those key important fields into, into, into what matters when it comes to, uh, when it comes to a transaction, what kind of amounts are, are those versus, uh, how do you break a date into a month or day or a day or week and then extract more valuable information out of it and ultimately how do you handle that long tail of merchants, uh, uh, merchant names which are not so popular. Uh, these are some of the considerations which we are seeing being done, and all of these problems are hard problems when it comes to computation time and, uh, uh, computation time and the accuracy of these tokenizrs, and that is where some of the libraries from Nvidia, like Nvidia Rapids, which goes very well into the likes of Sagemaker, uh, come into play and help, uh, developers, uh, build these tokenizrs at scale. And then, as, as you build your tokenizers, it's about, uh, it's about fine tuning, uh, your models and ultimately helping them make, learn the language of, uh, of, of your, of your data. And this is, this is where we draw the learnings exactly from how we fine tune large language models of today into taking all those steps but now applying into these new tokenized uh bits of data coming in from these tabularid data and uh the, the, the kind of steps remain the same, the fine tuning methods remain the same. Uh, and the kind of frameworks and libraries that go along also remain the same. Uh, at Nvidia we have a, a framework called Nemo Framework, which is like an end to end framework for, uh, customizing and fine tuning your models, uh, open source models, and ultimately making sure that your models converge effectively, uh, that your training runs. In a distributed fashion are running uh in the most efficient way and at the same time you if you want to uh up the ante with techniques like reinforcement learning for human feedback or supervised fine tuning you could do those techniques as well and that is all encompassed as part of this workflow. So this is, this is how the ecosystem and the community is thinking. About leveraging these payment foundation models or foundation models and tabuloid data. This space is super early, but we have been seeing more and more transcendence into new use cases that would emerge as a part of this effort, and Sudhir would talk further about it. Yeah, thank you, Bahal. So here, right, if you think about it, every customer interaction, whether it's a swipe or even a tap or even a transfer, right, that all forms a kind of a sequence as Pahal has already articulated what a sequence is. So and that sequence tells a story about the customer behavior, right, their habits, their intent, and their purchase. So on the left hand side as you could see there is so much financial data coming in. We have transactions, account information. And we do have digital behavior and banking. All these provide various signals for a company. So what we need to do is, and we need to build a foundation model as you could see in the center, right? It tries to build the relationships between this data and how this tries to build the relationship is because it has to understand the context and also the payment criteria. And just like a transformer, right, as a plain English language, it understands uh sentences. And then each sentence is like a word of tokens. Similarly for the payment transaction, what you see is it's more of a payment history as a sequence, and every transaction happens to be a token. And this particular sequence gets trained across millions and millions of transactions. On the right hand side, as you can see, once you build this foundation model, then you could be able to personalize for your fraud detection, AML, product recommendations, and even transaction monitoring. And once you build this, This can be built on Nvidia because they are superior in accelerated computing, and that can be scaled across millions and millions of transactions. So now let's look into how foundation models extend beyond fraud detection. As you see in the diagram on the left hand side, you do have raw data coming in. The first step what you need to do is you need to structureize. So that is where you have transactions and you do have user profiles and merchant profiles. And once you structure the data, the next step is all about enrichment. And here we could able to use Nvidia rapids, which can be run on Spark. And that in Media Rapids can do the feature engineering which does filtering, sorting, aggregation. And but what we have seen is it needs an expert intuition. So the next step is all about what we have seen customers is they're using RNNs which are nothing but recurrent neural networks. But again, they do struggle with long histories and also last time behavior patterns. So the next set what we are seeing is where Nvidia is working on Transformer and GNNs. What the Transformer does is it tries to learn from the history of your payment history, and GNNs, it tries to understand the relationships between users, merchants, and transaction data. So once you build this, this can be deployed on AWS solutions and again using NVID and Nemo and 5G and DGL, this is especially for GNNs. Once you build this transformer in GNS like a tablet foundation model, this can be fed into your training, but what it does is it was creating an embeddings, and these embeddings can be fed into your downstream model in case of fraud detection where most of the customers use XG boost, and here you could be able to leverage Nvidia XG boost on rapids. Once you, once you have this training part done and you have the model deployed, then that is where you're going to do the prediction part. And here you could be able to leverage Nvidia Triton. And this can be able to scale to millions and even billions of events in real time. And again if you see at the bottom part of the use cases, right, this can be fed into personalization and again for especially uh where you have authorization and routing and also for dispute protection again you're not stopping it here because the fraud keeps evolving and you do get a lot of fraud vectors coming in. And you take these fraud vectors, retrain, and this continuous life cycle keep continuous. So this is at a high level of how the domain level logic of how you build transformer-based foundation model. So in the next slide, we're going to see how you're going to put into action. So as you see, right, like you do get a lot of data coming in, which could be your third parties, SAS events, and your operational data, right? And where you could be able to leverage a lot of AWS services again, Afflow, Guinness' Data Firehose, you have Lambdas, and you have uh Amazon Athenas. Once you have all the data coming in, this is like your ingestion state. And the next step is where you need to. Structure this data and here where you could be able to leverage NVDDia's rapids where you could be able to structureize the data and again here you could use bass transform, especially for offline GPUs and you can also use FSX for luster. This is for high process storage for your massive data sets moving into the. Training part. So in the middle part what you have is more of uh your tokenization part where you're using NVID and Nemo framework where you could be able to tokenize your raw transaction data into more of a sequence-based. Once you do that, this is the important part. This is where we're training it and in the training here we could able to leverage uh Sage maker. On the SageMaker where you're gonna have NVID and Nemo framework with. Uh, Run on Pytorch with on the Sagemaker hyperpod. So once you have that, then the next step is if you wanted to have relationships, that is where you could be able to use GNNs. And here you can also leverage 5G and also uh DGL framework where you can have the relationship between the graphs. Once you build this tablet foundation model, you would be getting the rich embeddings which would be giving you a dense behavior of the customer. And this again where you can input into Amazon, Uh, vector database where again this embeddings, right, again, if you really wanted to see if I wanted to find a similarity search between customers, that is where vector search would help you. Once you have this vector search, uh, there is an ML flow where you could be able to train and intelligently would identify the best model, and that best model can be used. For your training, and here again as I said, an inference layer, Triton can be used for your billions and billions of transactions. And once you do this, this can be done for any kind of a downstream model. So rather than you building a unique model for each and every use case, here you're trying to build an aggregated model that can be used as a universal financial behavior for your entire organization. Sorry. Now, let's look into how we could be able to operationalize this ML workflow. On the left hand side, as you can see, you have the SageMaker Data Wrangler and SageMaker crossing job, which could be used for cleaning, transform and feeding into the raw data. Once you have it, you're going to push it into the feature store, and this feature store is going to give you an inference and also for your training purposes, for all the features. And the next step is all about training and tuning. In the training and tuning, what do you see the data scientists or even the data engineers, right? They try to do multiple Sagemaker AI experiments, and once you identify the best model for your need, that can be pushed into your model registry. And once your data scientist or elite data scientist approves it, that would be used for your deployment stage here in the StageMaker deployment where you can run multiple target experiments and you can deploy as a target runtime for your endpoint. And as you can see, this can keep evolving and once you see like a lot of model vectors coming in, that can be again input back into the StageMaker experiment and you can train, tune, and finally deploy. And let's see how do you convert into this. Sagemaker Airline pipeline in AWS as the left-hand side, as I said, yes, the data scientists, uh, they would be getting the raw data coming from your different streams, and here where you're gonna build SageMaker experiment, pre-processing, building the notebooks, training scripts and everything at the second step. And the third step is that is where you're gonna build SageMaker pipeline. Where you do the model evaluation, transformation, and then registry and also deployments. Once you build the pipeline, then you're gonna commit the code. The code would be again whichever resource deposit you would like to. That is where it would be triggering a code pipeline, and the code pipeline in turn, uh, would be generating a sage maker pipeline which where it would be deploying the endpoint, sorry, the model. So once you have the model deployed, uh, normally what happens is it goes to the model registry. And in the model registry, that is where it goes to the lead data scientist. Once they think this model is good enough, that is where they push it into the pre-prot stage. In the pre-prot stage again, you would be training against with your production data or even the pre-pro data environment to see how this model behaves. And that once that's been approved, it goes to the next state, and on the final approval, that is where you would be launching in the production state. And as and when you see a lot of transactions coming in, you would be retraining and the whole flow continues. So this is at the high end, how you could be able to build, train, deploy using a Sagemaker pipelines automatically using the Tabler Foundation model. Now again, in the financial industry, right, we always talk about security. Security is definitely number one criteria. And on the top you do have AWS managed services where you have Amazon inspector, especially for vulner uh vulnerability scanning, and you have cont AWS config for configuration governance and cloud trail for all the cloud activity, what you could see, and we do have services like AI, Sagemaker AI, Bedrock, and you have VCR and also ATI. This is for building and experimenting and training your. AA model securely and in the bottom part of the stack what you have is self-management where you have AWSKMS for key management and you have identity access management for identity controls and you do have VPC that is for separate network segmentation and WAF for again threat isolation. We also work with the third party solutions, and here we use Plank, uh, Krali, CrowdStrike Falcon, PrismaCloud, and Octa Identity Management. Again, this is for cloud security posture and also for endpoint protection and again for identity management, using all these services where we can help you build a state of the art secure financial tablet solution model running on AWS partnering with Nvidia. So in the next slide, uh, Pahal is going to talk about the early success and key takeaways. So these, as I, as, as, as, as I mentioned earlier, these efforts have been industry-wide, have been, have been prevalent. I think one of, uh, one of the most murky examples is from a payment fintech who trained their payment foundation model to, uh, uh, uh, detect card testing attacks, and their accuracy moved from 58% to 97%. Another large uh LA, uh, neobank has been, uh, has been training their payment foundation models and see, uh, uh, and what they are doing is they're building like, uh, uh, one size fits all model which speaks to these different use cases and, uh, cater them accordingly, be it fraud, be it personalization, be it dispute prediction, etc. and, uh. They are seeing an increment of AOC by 2%. Again, these are, there are no business outcomes associated with it yet because this is too early stage, but that's where the ecosystem is headed. We recently had a large payment network release a paper on transaction GPT. It's basically talking about the same things and how they're seeing value, uh, bringing transformer models to tabular data. And, uh, ultimately, another large bank presented at Nvidia GTC on building recommender systems uh out of Transformers. We have a library called Transformers for Rec, uh, which is, uh, which again uses the same principles as the concepts we discussed today. And they see uh 10 to 12% increase in engagement. So like really great examples and stories coming from all over the ecosystem and that's where every, every enterprise and every company is experimenting with it and as we thought, as we think about the session today, I think. The one big thing uh you should take away is that we, being in being in VR or AWS are not building these foundation models by ourselves. What we are rather doing is we are enabling you all, you developers and the enterprises and our partners to ultimately help build them effectively for your use case, for your own end data and your own end solution. And we have the right tools in the libraries, some of which were name dropped, uh, in these conversations, uh, definitely have a greater look at them and see how you could leverage those tools and capabilities to the best of your use. Ultimately, the idea is when we talk to customers, the biggest problems are models are not converging, accuracy is low, machine utilization is not enough, costs are high. How do I justify it to go from. A POC scale to a production scale, all those things being catered, these architectures, these reference blueprints are the recipes for you to, uh, uh, to take, take, take that step and, and remove these barriers if you will. Now, and, and in the pursuit of that is where you would see the two platforms coming in both from a, a performance perspective as well as scalability and trust is where AWS and Nvidia have been. Majorly partnering together and ultimately what we are seeing and we'll continue to see more and more that industry would be able to harness Transformers by pointing it to both transactional tabular style data and also unstructured data and combining the two worlds together. So our hunch is that there will be more one size fits all models coming in which will again dwarf. To small specific niche models, but those global embeddings would ultimately help build that greater context about a customer, about a product, about uh an offering, an entity, etc. that's, that's where we see the puck going. So if you are experimenting and building along, uh, definitely, uh, definitely get in touch with us and uh we'll be happy to help. We have our solution architects, we have our engineering. Uh, leaders, uh, we have our partner ecosystem to come and help you, uh, because all these best practices are being imbibed into our greater ecosystem and channel as well, and that is why we would invite you that, uh, if you are on the journey to augment your. Pipelines, uh, from traditional machine learning to the likes of GNNs and transformers, uh, this is our chance to talk. So, uh, please do reach out, have a really good reinvent if you want to, uh, if you, if you want to connect with us, uh, both of us are available on LinkedIn and, uh, at the same time, uh, we'll be happy to take any questions off, off the stage, uh, as we conclude the presentation. Thank you so much for your time. Thanks, Sudeir.