---
video_id: u199i6iMO6A
video_url: https://www.youtube.com/watch?v=u199i6iMO6A
is_generated: False
is_translatable: True
---

Alright, hello and welcome everyone. Uh, make sure you're all in the right room. This is IND 395 accelerating game design reviews with generative AI and LLM agents. So, uh, I am Sam Patzer. I'm a solution architect at AWS and I'm joined here with Christina. Hi everyone, I'm Christina, and I'm also a solutions architect at AWS. So I wanna just ask the room a few questions before we fully get started. Who here has ever built a game before? Awesome, we got a few folks. All right, who here has written a design doc before? Requirements, doc, you know, whatever name you wanna sign it, yep, yep, alright, I see a lot more for folks. Alright, perfect, awesome. Well today we're gonna kinda be reviewing kinda how that maps to some of the software engineering practices that you use as well as game design as well. So let's take a look at our agenda first off, we're gonna introduce to everyone what is a game analyst. What does that mean? What does that mean to the industry, and also how it maps back to software engineering as well. Then kind of we'll go over an overview of what are we building today. We'll go into what tools are we're gonna use in our toolbox and then actually spend some time doing some live coding for y'all. After that, we'll have some time hopefully for some Q&A. Uh, if not, we'll also be outside to answer any Q&A as well. So let's dive into what is the role of a game analyst. The modern game analysts serve as both an interpreter and a storyteller. What does that mean? Well, it means that they are able to take the business goals and the player goals and map that back to a story that they want to tell, uh, across the industry. So they want to translate things like player data, business data, uh, QA. Design docs, everything else like that into a boiled down doc and their role is to be able to translate any doc into the those different areas and be able to get feedback on it. So for those that have written requirements docs before, usually typically, uh, engineers are the ones writing the requirements docs, and then you have someone else come in and review it. Game analyst's job is to be the reviewer of those docs. So for those that aren't familiar with the game analyst, we've broken it down into four competencies. The first one is focused on lore. Lores, their entire idea is to validate that this update or this idea fits into the game itself. So think about like, oh, if we're adding, uh, a new race into New World, for example, uh, which we'll be using later on as an example, we're gonna be adding elves. Does that fit the lore of New World? Next we're gonna be looking at QA or quality assurance. Does this break anything in the game? Uh, is it valid to be in the game? Does it break? Is it a new powerpowered item? Does it break other play tests? Uh, this common problem that always has to be solved to make sure you're evaluated when you approach the problem. Finally, uh, the next one is game design. Is, is this update fun? Do players want to engage with this update? Uh, make sure that we design and build this game in such a way that players keep coming back to our game. And finally, game strategy. Does this fit into our business needs, right? Is this something that we wanna go forward and do for business and strategy? Uh, does it fit like our player retention models? Does it bring new players in? These are all questions that the game analysts should be able to answer as well. With that there's a lot of current challenges that have to happen with this. The first one is slow duration. During the back and forth process of that early proposal, there could be a lot of back and forth due to incomplete information that's unavailable. The designers don't have access to the same data sources as the analysts do. For example, the analysts might be, uh, have docs around the strategy that the game designers might not have, and they might know more than the actual designers. Finally, to take a game analyst to be available to be able to review docs, it takes a lot of time for them to, uh, learn how to be able to review these docs. So everything from learning knowledge about what the game is, learning how, what the content of that game is gonna be, and all the different silos that we talked about previously, as well as best practices and methodologies for reviewing those docs, it takes a lot of time and energy to, uh, bring someone up to speed. And then finally, subjective feedback bias do exist and so. When you have someone come and for example review a doc for New World, they might view that like oh this this new edition is not good for the game because I'm a purist and I think that all new updates are bad, uh, and that that might influence their decision making to be bad or if you've ever been in a design review and someone is like has an opinion about what code language should be written in when objectively something else is better, that's a bias that they're bringing into that doc review. So that's kind of what we're looking at when it gets that subjective feedback. So how are we gonna build it today? Well, we're gonna first start off by building 4 different specialized agents. These agents, their purpose is to only evaluate the dock that we're providing in its lens. So for example, with the LR doc, it's only viewing that dock from the lore perspective. So what that means is that we're not taking into consideration our strategy, we're not taking into consideration our gameplay, we're just taking consideration of our lore, and we're gonna do that for every single bot. Its whole purpose is to evaluate that that project with that idea. And with that we want to connect it to different knowledge bases. These different knowledge bases contain a lot of detail around how that lore exists or that gameplay exists, and it's gonna be very well specialized in that area. For example, in lore, we have uh the entire New World wiki loaded into it which contains all the different knowledge around New World inside of it. After we have all these bots review those docs, we then use an orchestrator agent, which is the game analyst agent that then reviews the entire summary and decides how to present it back to the, the customer. While the elves might exist as a great, uh, solution for the strategy doc, lore, it's against the lore overall, and so it has to evaluate how do we evaluate the strategy versus lore, and there's different ways to approach that and lore can then provide feedback on how to do that better. So some of the benefits behind this is that we get an accelerated innovation cycle because of the fact that we're continuously chatting with bots, uh, identifying how we can, uh, spin up things like for example, solve the lower problem or solve the strategy problem. We are getting collaborative intelligence by having these bots work together, uh, to be able to identify different areas and different gaps. And finally it's unbiased, right? You're literally reviewing the information right from the source. There is no bias saying that, hey, I prefer this or I prefer this. No, it's just straight from the source of the knowledge base and it's pulling those in. So with all that, let's take a look at our toolbox to understand how we're gonna build this lay. All right, so we have the what we're gonna build now. I wanna talk about how we can take this idea and make it into a reality, and we can do this by using a few different tools. Who here has heard about strands? Yeah, so strands is our open source framework that is designed for building agentic agents, um, AI agents for, um. Agentic applications. It was designed. It's an open source Python SDK that was designed for ease of use, so you don't have to spend a ton of time just trying to understand how to use the framework. It's supposed to be intuitive so that you can get started quickly for rapid prototyping. It has seamless integration with tools such as MCP servers. How many of y'all have used us, um, had the chance to use MCP servers before? So yeah, uh, you can use MCP servers you can have and it even is built to be seamlessly integrated with AWS services. It was also designed to be flexible, providing you with choice of models and tools that you can use. The next tool we're gonna be using is Amazon Bedrock Agent Core. Amazon Bedrock Agent Core is an agentive platform for building, deploying, and operating AI agents at scale and production ready. Agent Corre comes with a suite of services. The ones that we're gonna be specifically covering today include Amazon Bedrock Agent Corre Runtime, which provides you with a serverless hosting environment for deploying and running your agents on AWS without having to manage the complex infrastructure that comes with agentic applications. Additionally, we can enhance our agents and make them context aware using agent core memories which provides you with a fully managed memory infrastructure for your contextware agents. And finally for tracing and debugging, we have Amazon Bedrock agent core observability which provides you a unified dashboard where you can see all the things such as traces, uh, so. You can figure out the root causes of any of those errors that usually takes a really long time to find, um, and then additionally we can have we can see performance metrics to see how well our agents are performing, but you're not just limited to these tools in Amazon Bedrock Agent core. There are many other services that you can bring into your workflow to further, um, increase the capabilities of your agents. So just to list. A few of them we have identity where you can bring identity and access management into your agents and even integrate them with your own existing identity providers. We have a code interpreter for securely executing code in a sandbox environment as well as browser capabilities and we have the agent core gateway so that you can take your existing APIs and lambda functions and make them into MCP compatible tools. Pretty cool, right? So now that we've talked about the tools, let's talk about how we're gonna string these all together to build our agentic solution. So how do I do the. laser laser button. All right, sorry guys. So to start off we're gonna have 5 different agents that we're gonna be building and we can break these into 2 different types of categories. We're gonna have our specialized agents. These are gonna be representing our core competencies that a game analyst would need so we have a lower agent, a quality assurance agent, a game play agent, and a strategy agent and then as our like main director we have our orchestrator agent, and that is gonna be our game analyst. The first AWS service that we're gonna be leveraging, well, first, actually I'm getting ahead of myself, these agents are all gonna be written with the strands framework and the model that we're gonna be using today is we're gonna be using the anthropic cloud sonnet 4.5 model. With that, we're going to have all of these agents are going to be deployed to their own Amazon bedrock agent core runtime. And then with that. Each of our agents, they're gonna need to have access to various different data sources. We have our game design documents and business strategy documents. These are gonna be stored in an Amazon S3 bucket, and then we also have player behavior data which could be stored in maybe an Amazon Redshift data warehouse. With that we're gonna provide RAG capabilities since this is um at the basic level this is still a RAG application because we need to go and query data from our data sources and so with that we're gonna use the Amazon Bedrock knowledge bases. How many of y'all have used that one before? So it's a really cool tool because you can bring basically your enterprise data into your AI applications in a very easy way. Um, you, it provides you with the ability to create to easily integrate your AWS services and make them into a vector data store, and it does all the indexing for you. It's a really good tool, um, and so we have all of these data sources, um, use leveraging Amazon Bedrock knowledge base. And to provide us with our standardized way for our agents to be able to access these data sources we're gonna be leveraging an MCP server. This one is specifically designed by AWS for the demo, um, and it's called the AWS Labs Knowledge Base retrieval, and we're gonna be using that today for interacting with these data sources. And then finally, in order to provide a better performance with our agents and also a more personalized experience for our users, we're gonna be leveraging, we're gonna make these agents memory enabled by leveraging the Amazon Bedrock agent core memory. So for this we're gonna have, we can break our memories up into two different levels at. You'll see a little bit. This will make a little bit more sense when you see the demo, but we have, we're gonna have our project based memories and these are gonna be semantic facts and so each of these agent run times are gonna have their own set of facts that they're gonna think about it like as they're going through their knowledge base. They're gonna learn things about the data and so with that what the idea is we can make the performance better because we can reduce how often the um agent goes all the way back to the knowledge base to retrieve information so over time it'll get faster and better at responding. And then at the user facing level we have our game analyst, right? And so we have two different types of memory that we can add here. One is user preferences. This can make the experience better for users by saving preferences about about that user. For example, let's say someone prefers having, um, bullet points in the responses over paragraph forms. You can have the agent. Remember that so that it only is producing the outputs that it the responses in bullet points. Additionally, we want the agent to have awareness of previous conversations and with that we can also add a session memory to continue to um so like in the case let's say someone gets disconnected they don't have to start the conversation all over again or they can even refer to like a previous conversation so you have a more seamless kind of interaction with it. And then finally throughout this entire build process we leveraged the Amazon Bedrock agent core observability for all tracing and debugging and also monitoring the performance of our agents, all right. I know that was a lot. Now we're gonna get to see that in action, so I'm gonna pass it on to you, Sam. Perfect. All right, let's get this machine up real quick. All right. Go ahead and put the demo. Alright, so first off we're gonna have a, we're have a project portal that we've created for this this demo today this project portal we're gonna be demonstrating all the different agents and the different stages that she kind of talked about here. As I alluded, the first thing we have to build is obviously all the different specialized agents. So what we're gonna do is we're gonna create a new project. This project is referenced throughout the entire thing, and the idea behind it is that, as we talked about before with the memories, these projects are all different silos inside the actual project. So I've already pre-populated some of the fields in here. We're gonna be doing kind of the example I've referred to before. We're gonna be adding elves into our new world experience today. Uh, for those that haven't played New Worlds, it's one of Amazon's MMOs that really focus in on like a human during the medieval times. So elves should not fit that well, and we'll see some examples of that later on. So what we're gonna do is we're gonna go down here to the document for analysis. I've already prepared a document. I'll show that off to y'all, uh, here real quick, and inside this document. You'll see we have some different core features that we're gonna be implementing things like, uh, some characteristics for the, uh, elves, some cultural background, just like a little brief thing about lore and some racial abilities that come with being an elf as well. Uh, finally there's some customization options, some technical requirements, and a timeline for those that have ever written requirement stocks. This is missing a lot of details, and this should not pass anyone's kind of uh bar for writing a doc, but that's what we're gonna show today is that this was obviously not a great well written red doc, but that was the whole purpose behind writing this doc was just to show, hey, I'm a brand new game designer. I don't know what I'm doing. Tell me how I can prove this doc. So what we're gonna do is we're gonna go ahead and copy this on over to our wonderful tool that we have over here where we create a project and then we're gonna be prompted with our agent configuration. With us building out each individual agent separately, we've actually given ourselves the power to be able to select how we want to send this document to the agents. If, for example, I already know my strategy is a great strategy, we don't want our strategy to be evaluated, or if we know more than whatever else, we can just say, hey, don't, don't use that tool. Don't enable it. So for this, for the demo today, we're gonna start off with, uh, removing analysts and we're gonna send it to all three different agents individually. We've also disabled memory to kind of give us an idea of what the performance looks like without using any memory at all. So what we're gonna do is we're gonna go ahead and create that project. And then with some demo magic instead of waiting for this wonderful thing to respond, I'm gonna hop on over to our first page. What we have over here is an overview. The first thing it does is it will send over the entire doc we have over to each one of our agents, and each agent will automatically respond with a detailed, uh, overview of what it thinks about our strategy doc. And inside here we can then filter by gameplay, lore, and strategic. These are all the separate bot overviews, and it'll give us information around how it feels it should fit. So let me scroll down a little bit so y'all can see that. It will give us some details and tell us things that we want to see. For example, in the gameplay, it says, oh look, there's some unprecedented mechanics that I've added. Well, that's not probably great for other parts of the gameplay, like for example, PVP. It might introduce bad things. Inside the factory system might be hard to integrate, and so on and so forth, it gives us a whole overview of all the different areas that the gameplay might be impacted by the strategy doc. We're gonna do the same thing and see that also in lore, but one of the things that's interesting about lore is that lore, we definitely did not at all follow the lore inside a new world, so it's gonna call out a lot of bad things, being like, oh this is bad, no good, uh, and it's gonna go over and say that this is just a bad update. But then we go into strategic and strategic's like hm this is pretty good but then see that's the problem is that strategic likes this but Laura does not and how do we combine all those together and make sure that they're all valid and we're gonna get into that a little later but first I'm gonna hand off to uh Christina to talk about how we built these individual bots themselves. All right. All right, so all of this, so I have our IDE open. This is our lovely Quiro that we just came out with. Um, how, how many of y'all had a chance to play around with it yet? Yeah, I, I really like this one. So let me make sure that I open the right one. So we're gonna talk about how to start building our first agent and for this one we're gonna be looking at the gameplay agent. So at a basic level, when you're building an agent with strands, there are 3 key components that are specific to a strands agent. You're gonna have tools, you're gonna have a model, and you're gonna have a system prompt. Tools are gonna be things like they can either be custom tools that can be created through like custom logic with like think like a function in a class or something like that. Um, and it's designated with a tool decorator which you'll see in a little bit, or tools can also be your MCP servers that you wanna use for this code we're gonna be actually leveraging, we're gonna be leveraging the MCP server for the Bedrocknowledge-base retrieval MCP server that I alluded to earlier and so we can see here. That we have basically what we'll do is we would initialize the tool by initializing a MCP client and then we would state which MCP server we wanna use for this specific MCP server we have a few environmental variables that you can enable for this one we wanted to be able to. Increase the accuracy of our rag process and so we have re-ranking enabled and then the other piece we have is an inclusion tag key because basically the way that this MCP server works is you will tag your resource with a specific tag that you can define to say this is part of my grouping for like what I want my agent to be able to have access to for querying using this MCP server. For our model we have it up here and it is defined that we're using the Amazon bedrock model and we just have the model ID here. And then what I would say probably the most important piece of this entire thing and I'm gonna. Make this a little bit wider for you is this system prompt. This system prompt is basically like think of this as like the brain of your agent. It's gonna define its personality. It's gonna have its role and instructions on how it retrieves information and leverages their tools and what kind of questions it can answer. Most of the time when working with AI applications like this, when you're when you're using foundation models actually. The first thing you should check if you have any kind of unexpected output is your prompt because that's a lot of the time where you can start seeing the quickest improvements. So for this specific agent we're gonna tell the agent that you are a gameplay expert. Your job, your expertise includes any. Anything around game mechanics and we're gonna let it know that it has access to a knowledge base and we have specific instructions telling it how to leverage the knowledge base and to how we want it to query the knowledge base and return information whether it found sufficient information or not. Now, now this is a basic strands agent. Now, the next thing we have to do is we define our agent core to the next thing we want to do is to be able to deploy this to Amazon Bedrock Agent core. And so for that what we would need to do is one we need to use this Bedrock agent core application. Basically what this is is a wrapper to make it give it all the tools that it needs to be deployed to an agent core runtime, um, and the reason that we need this is think about how many of you have used Docker before. So in Docker, if you wanna make something web service enabled, you need to basically make it into some kind of web server using either something like fast API or like Flask. This is the bedrock agent core like flavor of that, but you can also use your own, the other frameworks like for example F API if you would like to as well, but this is a really easy way to get started. Um, and then with that, the last thing that you need for your main logic is you just need to have your app. entry point, and that's gonna tell your runtime of where your main function is, and this is, and that is how you create your first agent and it's ready to go to Amazon Bedrock agent core and so then when you want to. Deploy it. You have a few different options you can do it through the console or you can use our CLI for Agent Core where you would just configure it and in the background it's gonna create all the resources that it needs to deploy it to a docker container and then as a result it'll give you an endpoint and you'll use that endpoint to invoke it in your application. Now, the next, this with strands and agent core. One of the things I've really appreciated about it is the fact that since it's so like intuitive to use once you've created one good agent, you can really it becomes even faster and faster to create the other agents because you don't really have to change that much. And I'm gonna prove that to you by showing you a different agent. This one is our strategy agent. And so I mentioned before, this one we, for every agent, every strands agent, we have tools, our model, our system prompt. And we can see here that not a lot has changed. We're still use, we still have the same code for our bedrock model. We have the system prompt present. And then we have our MCP server and the only thing that's different here is we have a different tag because we're using a different knowledge base. And then everything and then our system prompt instead of the talking telling it that it's a gameplay agent we're saying that it's a strategy agent and it has access to strategy documents instead of game design documents but that's really all the difference and so now we're gonna talk about how we can use the tool Quiro to make making more agents even faster. Awesome. So who here has used Qiro before? And just me, OK, cool. Uh, so inside of Kiro, obviously it's just like an ID like anyone else's ID. You have obviously like your editor, you can change things down here like for example, uh, what type of language you have. You can also change like your auto completion rules, your formats, your themes, but the most coolest part. About it is you have this new chat feature over here on the right hand side. Now there's two different types of chat that you can have with Quiro. First off is the vibe section. If you wanna vibe with Quiro, that's great. If you wanna do like small code changes or even single file changes. But for me, I wanna build a brand new feature or in this case I wanna build a new agent. I obviously the agent is probably a little bit of overkill for spec driven development, but we're just gonna be using it as an example. A better example in the future would be instead of having me build out a single agent, it'll be building out all the separate agents and just me giving some specs around like, hey, here are the MCP servers that you should use. So inside of here I have a prompt already pre uh pre-populated that I'm gonna run um but I'll be explaining you guys that prompt while it's running. So inside of this prompt I'm telling you to only use the folder Quiro building so that way it doesn't get any of the wonderful code that we have prepared already for you today. I will only work in that side of that folder and place all the new code in the new Lore agent Basic. Uh, I've asked to just create 22 tasks just for demo purposes today and we'll see if it follows through on that today. Uh, other than that, it will just print. The prompt was, hey, build out a new agent, uh, based off of the old agents that we've already created, and then it will create a lower prompt for us and make sure to install any needed packages. For example, if we're using strands today, it'll go ahead and do that. So while that's uh running, I will review the new requirements that ours created. So the requirements document overview is that this is really if you were to design a paper that you give your executive, this is what our requirements doc would look like for them. It would really contain things like, oh, what is a lower agent supposed to be? What is a strands SDK? You would hand that off to your like. Your manager, your BD, and they'd be like, Oh, this is really helpful because they would not understand these terms. For us engineers we'd be like, alright, yeah, the the requirements looks good enough to me, and we move on to the more important section which is the design dock. The design doc contains all the implementation details and the requirements docs that we would typically look at as engineers. So inside of here you'll see the key components that we want to have, the different interfaces and, uh, interactions that we want to have with that. So it will give a little bit more detail, for example, what an app entry point would be, the lower agent functions, give different connections for MCP and give some variable names that we might want to utilize for. Example, I pasted in here what my, uh, knowledge base I wanna use and was able to keep moving forward. Uh, after that we like gave a, for example, the betterrock model we should use the system prompts, and it will kind of give us a, a detailed overview of exactly how the implementation of something like this would happen. Obviously with more complex projects this doc gets really cool and a lot of detail in there and to be honest, it is a great first step to documenting what you've built and the new feature that that you've built. After this is all done and it takes all this entire design doc in, it will then create a task list. Inside of this task list, it will contain all the different tasks that we have to do to build this thing. So you notice that we only have two tasks, but typically on a lot of other features we're gonna have longer tasks or even more tasks than this and even some subtasks. So we'll go ahead and get started on the first task itself, and it will go ahead and run that task at any point. Uh, if it has any questions or if it needs to run any commands, it will ask me before. For doing so, uh, obviously we just released, uh, today, uh, Kiiro Anonymous or autonomous, uh, which allows us to, uh, run things, uh, in the background or even sign, uh, GitHub actions or GitHub, uh, poll requests I should say, and, uh, GitHub, uh, feature requests inside of, uh, Quiro. So we'll do the entire thing behind the scenes instead of something like in the CLI. I personally like to use it in the CLI when I'm actively building something because that I get that feedback right then and there, and I can change things as I see fit. So the first task is now complete and while that, uh, second task gets kicked off, one of the really cool things it does is it gives you a nice little overview of what it has done. Uh, so inside of here it tells me it created a folder, uh, configured the bedrock model, gave me an initial prompt, gave me the MCP client that's supposed to connect, gave me environment variables and error handling. Billing entry points as well. Afterwards, the next task, obviously I've, I've made this very simple for Quiro is gonna be building out our requirements TIC. So it's gonna go through that file I created and validate that all the requirements that we need for Python are gonna be properly installed and configured for us to utilize, uh, for this project. So after that we are able to have a brand new agent spun up with Quiro. Saves us a lot of time and makes our lives a lot easier. Obviously that was a very simple task and there's a lot more cool complex tasks that you can kind of do, but just wanted to use that as an example of how you can use Quiro. Now that we've kinda talked about how we build the agents, let's dive into our next section. Let's take a look at what it looks like to combine all these things. So obviously we can go in here and go ahead and create a brand new project. Inside this project we're gonna fill out this just like we did last time. We're gonna use our same doc for. Alvin expansion we're gonna go in here and paste that document in and this time we're gonna configure it with the analyst thing. Notice how we're not able to select any other agents because the other, all the other agents are gonna be available in the analyst. The analyst is just gonna use them as tools. So we're gonna go ahead and create that project. Once again, using some wonderful demo magic, I already have done this ahead of time. So what we have here is this wonderful review of this document, and inside of here it gives us a high level overview. This gives us the three different agents that I called and give us a reason why or why we should not proceed with this project. For example, the lore, as I mentioned before, does not fit at all. Elves do not fit into the context of this game, so we're able to kind of look at that, evaluate that, and immediately see that. After that we can literally dive into each of the consistencies just like the original uh 3 prompts are, but this time it's all available on one side of one sided dock and so you can kind of see that each one of these areas, it kind of gives us a nice nice little overview and we're able to identify different themes and dive into it as well. One thing you'll notice as well is that uh it will give us the entire tokens that are outputted as well as the response time as well as the average cost for that document review. After that we can then ask additional prompts to back to this, for example, can we dive into, uh, how agents can be used instead of elves, which is a valid race inside of New New World. What this does is then it takes a look at all the different gameplay agents and stuff like that afterwards and is able to dive into, oh, this change that I've made is able to then align and move forward if I continue having. Conversation with this bot, it will continue to be able to create and design things for me. So next thing I asked is, hey, can you create a quest around this? And what it did is instead of calling all the agents, it started utilizing just certain agents. So now that we have strategy pretty much satisfied, this one, it just went ahead and called Gameplay and lore, and it built an entire quest around the game play and lore. And it was able to design an entire quest for me and what I can use this for is I can keep iterating and build a whole design doc around, uh, this and ask more questions. And as you are all familiar, if you've ever written requirements docs before, this is a collaborative process that usually when you give your first design doc, unless you've written it so many times or you've been like preparing it for months ahead, you always get feedback on your docs. And this is just that iteration and ability to ask for more details and to continue that iteration process offline or uh more streamlined. So after multiple different things you'll see that we have uh different token utilizations, response times, as well as estimated costs that are all outputted from the strands, uh, SDK, uh, that you can easily use through the metrics. With this, uh, we obviously are, uh, just trying to combine all the agents, so there's some areas that we, uh, can improve upon with, like, for example, memory, but we'll dive into that a little bit later. Let's first talk about how we build this with. Uh, strands. All right. OK, that's so good. So I like to I'm gonna bring up a picture because I think this is the best way to demonstrate how our orchestrator is built so. This one is gonna be a little bit different because we're not going this agent is not gonna be directly retrieving information from the knowledge bases but instead it's going to delegate to our our specialized agents so the way that we. Have this one designed is instead we're going to be having our main agent, our game analyst agent this is gonna be defined same as before except this time it's tools are going to be the specialized agents. And with that we're gonna be the way we'll be doing this is we'll be in for or it. Our orchestrator, what's gonna happen is when it when it gets our query like a game design proposal, its job is gonna be to break down the design proposal and to figure out which questions need to be answered to evaluate it across gameplay, lore, strategy, and QA. It'll delegate to those specialized agents to go and get grab that information for it and then those agents will return it to the orchestrator and the orchestrator will synthesize it and create a more concise response based off of all of its findings with full visibility across all of the core competencies. So now let's take a look at how that is built in the code. So go to the game analyst. So we can see here it is very, very similar to what we had before. We have our model that is still defined, still using anthropic clots on it. We are going to have our system prompt. Then I'll just make this a little bit bigger. But this time in our system prompt we're going to be saying you are a game analyst and your job is to take in game design documents or questions and you need to analyze all game designs for the core competencies. With that we're gonna tell it that it has access to the following tools that it we want it to use. So for anything around lore we're gonna have it send it to the lure agent for anything around gameplay we're gonna have it send questions to the gameplay agent and same on for same for the strategy agent as well. And then with that we have we're now leveraging the other type of tool that I alluded to earlier, our custom tools and that is all done by adding this tool decorator and so for each agent we're gonna each specialized agent we're just gonna have a function that invokes them using the Boto 3 library. And that is how we would use those agents and then with that we still have the same stuff we got our main function right here with our entry point and that initialization that was up at the top for our wrapper to make it ready for an aging core runtime. All right. Now, I can go to the, Uh, no, that's the next one part. So now that we have all, all our game analysts ready to rock and roll, now we wanna look at adding memories and why, why do you think memories are important? Well, first of all, they give us access to the ability to communicate with the, the thing and remember our prompts and everything else like that. So right now inside of my uh agent that I have built up today I've actually decided to send the entire conversation history every single time with the agent that includes that original document and all these original of the document feedbacks that I've gotten as well. So as I continue to iterate and continue to build on this doc, that number of tokens input tokens are gonna skyrocket every single time I send another. Update it gives me another prompt back or a big document back that means that I'm storing more and more detail back to each individual project, right? So that's not helpful. On top of that, each of the different agents underlying are going back to the knowledge bases and querying for more data. For example, if I asked to add agents into, uh, instead of elves and I had to go look up the ancients, it's gonna have to do that again when I asked it to. Designed that quest is gonna have to go back to the mortar and go pull all that detail because the fact that it pulled it at one time and didn't store it anywhere. So since I'm only sending the chat details, it did not remember that at all. That's where things like memories can come in handy, right? It can remember, oh, I already pulled that details from the knowledge base. I already remember what ancients are. It might need to go look up certain details around the ancients. Maybe it didn't store, you know, like. The details around where they, what region they live in, but it's stored like what they are. So instead of doing a deep look up into ancients in general, it might have to do lookups into only specific areas of that race. So what we'll look at here is that uh we'll do the same exact thing with the the project spin up this time we'll create a new project and this time we'll see that we can select an analyst and we'll actually turn on memory enabled once again I will go ahead and run this because I've already run this ahead of time and you can see that we already had uh a uh response back with the memories. We're then gonna be able to read through the entire response, dive into it, and then you can see the number of tokens between the two are very similar. That's because of the fact that this is the first analysis. It's reading the entire document for the first time, right? It's supposed to use everything it has available, but as we continue to call it, as I did today, uh. I'm not like that for some reason, uh. Let me go back to this real quick. So as it said, as I continue to ask the same exact prompts, I've, for this, the sake of this demo, I asked it the same exact prompts of designing a, a quest around it. I asked it to use the ancients. I've proceeded to go through the entire process. You'll notice at the very bottom here that our token usage has actually dropped. And that's because of the fact that we're using memories. We're no longer having to query our knowledge base every single time, we're no longer having to read the document every single time, we're just being able to utilize what we've already called in a different manner and be able to provide more and more things back and forth. The other thing that's also interesting to look at is that the response times start to drop as well because I start understanding the knowledge base. I start understanding the knowledge of what is happening. Obviously you can also improve that response times by using things like specialized agents, definitely like the Nova training models that just came out today. Those are all different areas that you also can improve that response times, but memories just dropped our response times by over 80 seconds, which is crazy. So by using memories we're able to provide a better response time as well as using less tokens which obviously drives your costs down as well. So I'll hand it off to Christina to talk a little bit more about how you can actually enable memories and utilize that awesome. OK. So let me go back to my pictures. I got another lovely picture for you guys. Um, So The way that we are implementing memories is at a few different levels. I talked about this a little bit before, but we have our project specific memories that are gonna be storing facts, and then we have our session memories which are gonna our session memories and our user preference memories to help out our game analyst agent. So for that. Our our orchestrator is very similar to the way before except for. We have a few we have a little, we have our memory integration so what that requires is we're actually gonna add a couple extra tools so we'll start with the specialized agents at the specialized agents level what we'll be doing is you would add to in order to integrate memories is you're gonna use tools to you're gonna define tools that are gonna be able to restore and retrieve memories. And then you're gonna have memory hooks. This is gonna be specific to strands. With if you think about it like this, in a strain in an agent, an agent has a life, a life cycle, um, and in this life cycle you have different phases from when the agent is initialized to when the conversation is over and the agent is done. With the strings, um, with strands hooks you can customize the logic of how your agents interact and how they function at each point in their lifetime. And so with hooks we're gonna define how it stores and retrieves memories, uh, with how it with the interaction between the agent and Amazon bedrock agent core memories. And so at the specialized agent level we're going to be leveraging, we're gonna be using memories like a cache so the whole idea is we want to reduce how often the specialized agent has to go all the way back to the knowledge base and look for the information and what we'll have it do instead is first check and see if it's already answered a similar question before or retrieved like the information that it's looking for before. And only if it can't find that information it's it's memories, then it'll go back to the the data source and go as normal. Additionally, we have our game analyst is gonna be leveraging session memories so at the beginning of its life cycle we're gonna have it load X number of previous conversations into its context to provide more um information on previous conversations and then we also have it leveraging user preferences to provide that more personalized experience. So now let's dive back into the code and see what that looks like. So I know I've been saying this a lot, but a lot of this is gonna be very similar to what we had before and the reason I keep mentioning this is because I what I really want you guys to get out of this is just how easy it is to work with the strands framework with this. We still have the model defined just as before and we have our our system prompt. This is actually spoiler alert this is gonna be the most different piece um you can see we've got some extra like notations in here and all of that um and actually this was an experiment that I had when I was building out this agent to see how much we can improve the agent's performance with just a prompt. Who all enjoys prompt engineering and feels like confident in their abilities with it. See, not very many of us. I'm in the same boat as you, but with tools like Amazon Bedrock, you don't have to be an expert in prompt engineering, and I'm gonna show you why. So I'm gonna go over into the console and show you a new tool that I love, which is our prompt manager prompt builder. So with that, I actually took the original prompt for from our original agent. And I create I pasted it into here and you can tell it what model you're using and it will optimize the prompt for you for that model and so I actually just took this and copied and pasted it into the code and I got immediate um. I, I got immediate results from it. It was really cool. So that's the main one of the main differences. And then the other piece that's different is we now just tell, so now we have memories, right? So all we have to do now is tell the system prompt that it has access to memory tools it's very straightforward and so we tell it hey go check the memories first and for like the information that you need and then if you don't have the information then go about your job as normal. And so we have that here and we can see we still have our same tools and then finally the last thing we have is we just are going to initialize we're gonna just basically say now instead of just. Leveraging the custom tools that we had before where we said we have that logic to go and invoke the other agents, we also are telling it to use the memory tools as part of it's like tool belt. And that's all you really need to do and then you go about as normal you have your agent core runtime initialized. I believe that is up here. And you have your entry point and that's like the entire magic behind the whole thing. So, With that, we'll go back to the presentation. So there's a few, well first I want to talk about I think I accidentally pressed a button earlier. So just a recap of what we did today. So what we did was we learned how to build specialized agents and orchestrator agents in strands, and we saw through our demo of how we can we by leveraging AI agents we can accelerate game designers at getting in their innovation cycles by providing them with a place to get quick and early feedback. So that they can basically have a more fine tuned design proposal for that human interaction with the game analyst so we can reduce all the back and forth. Additionally, we saw specialized agents working together and alongside the designers and analysts to provide that collaborative intelligence of the AI, um, information retrieval plus a humans industry experience and then finally because all of our agents are gonna have access to. A standardized knowledge base we're able to create a more unbiased standardized baseline assessment uh to kind of have a predictable kind of starting point for all of the design proposals. And then if there are 3 key takeaways that we would want you to walk away with today, these are the 3 I would say are the most important. One, Amazon Bedrock Agent Core simplifies the development of production ready agents by providing you with runtimes that are scalable and secure and all the tools that you need to continue to help your agents evolve in a secure and performant manner. Additionally, you can enhance your agent performance and user experiences with agent core memory by reducing by increasing the performance through, you know, lower latency by using it as a cache and providing more personalized experiences and. Additionally, and the last one is you can accelerate accelerate your design teams by equipping them with the tools to transform subjective feedback into objective insights, and this will enable more fast, faster and more confident game design decisions. So some next steps we've left some links for you here. Uh, be sure to look out for our GitHub repository. Uh, we do have it posted and we're con we're continuing to add to it. We have, if you are interested in having any more hands-on tutorials with Amazon Bedrock Agent Core, this is a really great place to start with the samples. I personally actually use them to learn how to use Agent Core. Um, and then finally, if you want a really cool hands-on workshop with another use case for Amazon Bedrock Agent Core, check out the autonomous live ops AI agents for dynamic gaming experiences session. I actually sat in on the dry runs for it, and it was really cool, so please do check it out if you have time. And Uh, I wanna open it up for questions and thank you all so much for being here.