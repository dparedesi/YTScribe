---
video_id: y7fuAOIVEGw
video_url: https://www.youtube.com/watch?v=y7fuAOIVEGw
is_generated: False
is_translatable: True
summary: "Ariel and Liron from Coralogix present \"Ollie,\" the world's first autonomous observability agent, positioning it as a paradigm shift from traditional \"passive\" observability to \"AI-native\" reliability. They argue that the current approach of simply adding more dashboards, alerts, and telemetry data (which has grown 100x in 8 years) actually decreases clarity and reliability, creating \"noise\" instead of insight.\n\nThey introduce Ollie not as a chatbot or assistant, but as an autonomous agent that actively investigates issues. Liron demonstrates Ollie's capabilities with a live demo where a product manager reports a \"checkout error spike.\" Instead of manually sifting through logs and dashboards, the engineer tags Ollie, who automatically analyzes logs, metrics, and traces, identifies a specific `TypeError` in the shopping cart API code (caused by adding `None` to a list), and even suggests a code fix—all within minutes. This reduces investigation time by an average of 94% (e.g., from 5 hours to 20 minutes).\n\nThe technical architecture of an agentic AI system like Ollie is broken down into five components: Model (commodity), System Prompt, Agent Architecture (orchestration vs. handovers), Knowledge (context engineering triangle), and Evals (evaluation). Liron emphasizes that \"Knowledge\" is the key differentiator—specifically how data is structured and queried for the LLM—and that rigorous \"Evals\" are crucial to prevent regression when prompts change. The session concludes with a roadmap moving from the current \"Co-pilot\" phase to a \"Proactive\" phase (automatic triage and actions like PRs), and finally to a \"Preventive\" phase where agents predict issues to prevent downtime entirely."
keywords: Observability, Agentic AI, Coralogix, Ollie, Autonomous Agents, Site Reliability Engineering (SRE), Root Cause Analysis, AI Architecture
---

All right, um, thank you very much everyone for coming. Uh, today we're gonna talk about a topic that was barely discussed this conference, um, and kind of under the headlines, uh, we're gonna talk about AI. Um, but seriously now I think one of the areas where AI can make the biggest impact is observability and, and, and observability is one of the closer areas to code where AI is already making a huge impact in the world and it's actually has the potential to, uh, not just make our lives easier as software engineers and platform engineers but also make our lives better with, uh, faster, better software delivery, better stability and security of software. When we thought about it in CoreLogics, we're trying to understand what is the highest value that we can provide. Is it an assistant? Is it a co-pilot? And we came up with Ali. This session is going to be both about how we built it and how we came to that conclusion of what exactly is going to be the design, but also a bit about how you guys can build it on your own and what are the fundamentals that you need to build, uh, uh, an AI agent, not just for observability, by the way, but for any data access. I'll start with a bit of background about CoreLogics and ourselves. Um, I'm the co-founder, CEO of CoreLogix. Laon heads AI for CoreLogics right here. I'll be the one talking more about AI. Um, we started CoreLogics in 2015, launched in 2017/300 petabytes of data ingested every day in CoreLogic, probably a much bigger number. We raised about $400 million so far running, uh, multi-cloud, but primarily Amazon. CoreLogic also was awarded this year, um, EMEA Partner of the Year for Amazon Marketplace. And uh we run on 8 different regions today, including the federal cloud on FedRAM moderate. I don't have to say much. Software is complex and it's getting more and more complex actually this thing sometimes you need to use something that you edited or something. This is like a small company service map. Uh, when we look at large scale companies, services got so complex today it's almost impossible for uh a platform engineer and, and I think it also reflected in culture of push things towards the engineering and the service owners just because it's impossible to understand and grasp, uh, software systems today. If we go back in time when we kind of started our engineering experience, um, maybe 1517 years ago, not, not a, not several decades ago, there were monoliths. You would go to the machine, you'd look at the logs, maybe a little bit of metrics, you get a good understanding of what's happening these days with microservices, different deployment models, um, multi-cloud serverless, your ability to actually understand what's happening in your software is very limited. And we think we have the solution to that. Uh, but we'll talk about it in a minute. Telemetry is exploding, so people think, OK, how do I monitor all these microservices? How do I, uh, get a better understanding of what's happening in my production systems? I'm just gonna add more telemetry. I'm gonna add a metric. I'm gonna add a log. I'm gonna add another instrumentation. In just over 8 years we see 100x, um, this is from public sources, 100x increase in telemetry data. What does it mean? It means that it's complex, it means that it's expensive, it means that it's difficult to manage, but we're not really getting better observability, and we'll talk about that. The solution is the all in one dashboard. One dashboard, it has all that you need, all the KPIs that you're tracking. I'm of course joking. The, the last thing we need is another dashboard. The last thing we need is another alert. The world of just dashboarding everything and alerting on every single possible scenario. Uh, removes a bit of the ownership from the service owners and the engineers towards the services that they provide and run in production. We think, OK, we solve the problem, we add a dashboard. What's our definition of done for a feature? We have dashboards and alerts set for it, but the definition of done for a future is that we can run it in production and keep it in high quality and high performance. So that led us to build more dashboards and alerts, and it, it feels like we got good coverage. This is an average of a CoreLogics customer. I'm talking only the dashboards that are accessed and alerts that are triggered were in the hundreds. Sometimes we come to customers that have 10,000 alerts, 20,000 alerts. Can you really comprehend the system? Do you really own your service if it triggers 20 alerts every day and has 100 dashboards on it? Probably not. This is what it'll look like. You go into your dashboard interface. You got multiple folders. Many things are just, you know, duplicated variables that you need to change to understand your specific, uh, component. Hundreds of alerts that are triggered, and then we built on top of these alerts we built all these, uh, alert aggregation tools and incident management management tools. So we're trying to figure out the problem, but we started with a very, uh, kind of low state and we're trying to improve it. There's a paradigm now where more dashboards, more data, more alerts don't mean more observability, don't mean more reliability. And intuitively you'd imagine. Um, on data analytics in general, you know, and, and a lot of us studied math or statistics, the more data you have, the more accurate you are. The more data points you get, the better insights you get. In reality, if you think about this for, for observability and software, what is easier? Is it easier debugging a service with, uh, one service, 1 gigabyte of logs every day, and 100 custom metrics, or billions of, uh, uh, metrics, terabytes of data for multiple services? Obviously it's easier when you have less. So how do we deal with that? Do we build more dashboards? Do we create more alerts? Do we trim down data, which is also a complete industry that's started now just for filtering data out? We've got so much garbage we don't want to pay for it. We start filtering out. We start optimizing. And The numbers I've given on dashboards and alerts, those are only the ones that are being accessed. 92% of dashboards only use, uh, are only used for one week. People will create kind of a standalone dashboard for a specific issue they have, then they neglect, neglect, neglect it. Now the funny part is. You know what's gonna happen in the next, uh, postmortem. The, the conclusions are we're gonna add this dashboard now an alert, so next time we'll know, but you just go back and add the same dashboard a week later, no one looks at it. So Engineers now have to spend close to half their time just triaging and understanding where should I even look, what dash do I need to look at out of 100 alerts, and every time there's a big incident in large scale systems, and you know we run one coincidentally. Um, when one service goes wrong, everyone starts firing alerts and errors. So where do I even start? A lot of the times we start triaging and we think, oh, it's that service. No, this is just downstream, this is just the upstream. So can we do something better? Is there a way to deal with this? And also a big question is how come this wasn't solved yet? It's a big problem, and the data grows faster than our ability to deal with it. Now there's an opportunity because something big happened in technology over the past few years and how do we leverage that? So let's start with the basics. Observability is a term that we have a problem with, uh, to be frank, and it's, it's a bit of a. It's a bit tricky because we're an observability company, um, but observe is a very passive term. Do I want to observe my systems? I mean, imagine a. Police officers will say that their job is to observe the street. We're, we're actually, you know, we need to keep systems up and running. We don't need to observe them. Having more monitors and dashboards and telemetry is a great way to observe what's the end goal. The end goal is to have reliability, to have stability, to create business insights, to have security. So we keep building better observability, but we kind of forgot why we're building it and you know I can say about CoreLogics specifically CoreLogics had invested over $250 million in R&D over the past 6 years. So imagine the the amount of engineering and R&D work that we put in just to try to keep up with how fast data and systems were evolving. So we were in that arms race against obviously a very crowded space. But it was very hard for us to go back to the essence of what companies want, which is reliability. Ease of management, being able to resolve issues faster, have performance that's measurable. Actually meet business KPIs that drive the revenue of the business and the customer satisfaction. Now how do we get there? Every software emits millions and trillions sometimes of data points. Core logics cover the entire suite, whether it's logs, metrics, traces, infra monitoring, APM, realism monitoring, SIM, AI monitoring. So we, we're, we have enough signals. We can get signals for every, every, uh, user action now. But the challenge is how do we stitch them together into a story and the cardinality of data and the variety of data and the multi-dimensional aspect of it. Create such anthropy that to actually understand what are the connections in a way that gives me high certainty to take action is really complex. And the compute that it required until now was just impossible to employ to solve that problem, and the data was growing so fast. So that frog leap we've made in technology now enables something else. How do we now create uh a story, not we, we don't have an assistant to help you build another dashboard or, you know, a, a co-pilot that gives you hints. How do we autonomously and automatically create that story from those bread crumbs? To create a a complete new interface, a complete new approach to observability, and go back to the basics of simple and coherent insights that lead to reliability, performance, business KPIs. And there comes Ollie, and I'll hand it to LeBron. Thank you, Ariel. So as Ariel mentioned, We've been focusing on the wrong thing for the past few years, right? We've been focusing about how we do better observability. And we forgot why we forgot that the key, the end goal we are actually aiming for is just to have reliable software. We built something. We invested in developing the software. We wanted our users to experience something. And we built crazy platforms just to observe them. And yes, we have very, very advanced tools, but how much do we increase reliability? How much have we increased reliability with all these capabilities? And this new paradigm of AI, right, like I think you can see it here on every single booth, on every vendor, right. There is a true and great power to it. Which allows us really to rethink what is observability or you know really reliability. How do we achieve it and how can we actually do it not the same way, not grow linearly, but actually how can we do it 10 times better than what we've done until now. And this is where Ollie comes into the picture, the world's first autonomous observability agent. So what all is doing, how it, how it actually works, right? So we talked about billions and trillions of bread crumbs, logs, mattress, uh, metrics, traces, um, let's say. I'm at home watching Netflix, watching Squid Games. I click on episode number 2. And it doesn't work, right, or it glitches for a few seconds, right? Quite annoying. Not a fun evening to have. But why it happens, so it means there's probably something along the way, along the software stack, right, maybe some microservice backend, maybe a database experiencing some issues, maybe actually another third party application that my app, the Netflix app is connected through API having some experiencing some issues. And all these pieces somehow result in me as a user experiencing not even a downtime, just a glitch or unexpected behavior that the original engineers actually wanted a name for. So this, all these pieces, taking each and every component, the microservices, the databases, the lambda functions, taking the bread crumbs of each one of them. Potentially should tell a story of what happened to Iran as a user, right? And the question is not only what happened to me personally as a user, but what actually happens as a trend for more users. Maybe there are hundreds or thousands of users experiencing the same issue, and what's common about all of them. So Back to what we see on the screen, we see all these billions of breadcrumbs, right? And usually how do we do it? We open the alerts, try to find correlated alerts, we deep dive into the dashboards, we filter out for logs, say, hey, I see there's some error logs, right? And then I understand, oh, I see like these services seems to be related, right? So I further analyze. So we have logs, we have, uh sorry, we have alerts, we have dashboards. And the third component is plenty, plenty of manual labor going into resolving, investigating production incidents, OK? So we've all, we have all this information available not only for us but also for Ollie, and the beauty of it. is that it makes everything go away. No, OK So Ollie takes all this information we can give them the task. And then it zooms in onto out of all these billions of breadcrumbs onto the specific most relevant pieces to the task at hand. And the next thing, it actually eliminates the noise, right? Remember, we have more and more and more and more. We lost clarity. We got more noise. On eliminates all this noise. And can actually focus on the signals that truly are important and hopefully. will tell my story as a user. And most importantly, it can connect the dots to one single story, what the end user is experiencing or the end users are experiencing, all the way back to what was the root cause of that. Sounds too good to be true. Yes, no. OK. So, uh, yeah, these are nice slides, but I think uh the proof is in the pudding, so let's see all the in the works. So, what I'm going to do now, you can see my screen, right? Awesome. So, nice to meet you. I'm Adam. I'm head of platform at um Spotify, a sport goods companies. And I manage the e-commerce website and the platform team, and I'm just writing code in the meantime. Doing my day to day tasks. And as I'm doing my work, Let's see. Oh, I got a message. I got a message from Sarah Parks. Let's see. So Sarah is our VP of product. She's very good at what she's doing, and let me zoom in a bit. Um, I got a message. Adam, this doesn't look good. What's going on? We're getting slammed with customers' complaints. Um, and it seems like she referred here to an alert we got on production alert channel. So let's take a deeper look into it. So we can see a warning about checkout error spike. So we're an e-commerce website. Obviously the last part is customers come in after they've chosen, they chose the items they can check out and make the purchase, and it seems like there is an issue, so. Instead of opening another dashboard, let me just ask Ollie, hey, Ollie, um, can you please take a look? OK, and as I tagged Ollie, Ollie immediately reacted, as you can see, and it starts conducting an investigation of what really happened, and he shared with me um this link so I can see really what's going on in real-time. Let me zoom in a bit. OK. So how will it work? What we are actually seeing all it works like yet another engineer in your team. So think about yourself, right? You would have looked on dashboard every time, let's say we would have filtered for error logs. We would have identified some pattern, actually, uh, let me zoom out, um. We would identify a pattern. Based on that, we would probably see another filter or go to traces or go to metrics. So in every given point in time I have one thing on my screen. So on the right side, what you can see is what Ollie is currently looking at. So for example, right now we can see Ollie is looking at the logs. OK, and it's, it's, it's writing to, it's writing to me, it's sharing with us what it currently thinks and what path it is exploring. Um, and it has many capabilities whether it's exploring logs, metrics, traces, correlating them, analyzing the code. And more OK, and we see that we got a result, so let's see what happens. So let me close this. And by the way, as you can see, this is a live demo. Decided not to go for a video. Um, OK, so let's see. I noticed a spike in failed checkouts over the last 2 hours. Dug into shopping cart API. This is one of our services, logs and found a new type error that correlates exactly with the spike. It is affecting about 2000 users. From here we can see what is the root cause. Failed checkouts are caused by type error exceptions in the shopping cart API service. Now one more thing you will see with OI is that every time OE provides you with information, it backs it up with evidence. Why? Because we know AI can hallucinate, can make up things, and we want to make sure we can actually trust OI. This is a key that we put a lot, a lot of effort and engineering resources into ensuring the quality of OLI. So I can see exactly the logs that are relevant. Let me open one of them. Uh, and we can see the message here is unsupported operan type and the trace back. This is a pipe service, so I can see really what's going on. Um, let's go on. So where it crashes. So Ali here actually opens the code file, analyzed it, and found the specific lines of code causing the issue. It explains that some function fails because it tries to add in. And none raising this error. Why have we gotten to this case from the get-go that we have none in this, in this list of items in Python? The reason is bug and removed from card lines 65 to 66. I can open the source file and actually I also got suggested fix from Ali. So it seems You know, not like really quite an easy fix we hopefully can apply. um, also an analysis of how many users were impacted and chart analyzing where the issue has started, how it looks like, and so on. Um, Ollie also share with you what happened, so. Just to understand how all it really got to this thing, it's not magic. There was a real in-depth investigation that happened in this one minute. So all the analyzed metrics. It tried to find relevant metrics. It looked specific payment errors, API errors, checkout error account. Rain query. I won't go into each one of them, but you can see just this, let's say we have one. 234. 4 queries Just think about how long would it take you just to realize you need to write this query and filter this out and make sure the syntax is correct. Realize what's the insights you're getting from it and how you can actually continue with the investigation. Um, usually, usually from what we see in our customers, this time being saved is about 94% on average, um, and to make it into numbers, that means an issue that would take me 5 hours to investigate with all it would take about 20 minutes. OK, Obviously this is average. There are things that. Finish up in a matter of 2 minutes. Some of them take longer than that, um, but all in all, this is extremely powerful. OK, so now we have we've identified the issue. Let me go back because I don't want Sarah to be upset with me. Um, so Nikesh, our engineering lead, hey Nikesh. We have a production issue. Please take a look on the reason. And deploy a fix ACP. please keep me posted. And I'll be sharing. Let me go to Ollie. Let me share this link. Awesome. And lastly, Sarah, we don't want to leave her waiting. Sarah Thanks for bringing. This up, we're on it in the fix. Is on the way I think it deserves some applause. This is real, OK? Like this is real. I have a video demo here in case the internet would, you know, make up some challenges, but what you just saw, the querying, the data, the use case, all of this is from a real environment, OK? No, no, not a simulation or anything like that. So What we've just What we just saw This is not better observability. This is how we can actually achieve 10 times better reliability than what we had until now. OK? Oli understands your stack. This is super important. The reason Ollie was able to do that in a matter of a minute and come up with valuable insights and not just made up ones. It's because like similar to any engineer you would hire that would go through training, when you connect and integrate OLIF for the very first time, it learns your observability workspace. It learns what, which applications you have, what subsystems, uh, what are, what fields are present in the logs, metrics. Has it occurred to you that you have 5 metrics, one with capital case, one with lowercase, they all mean the same thing. But they have different naming, so when you query, you may miss some of them, right, because you just didn't know that the casing is so important. So OLI is familiar with all these fields, and this is extremely powerful. This is what allows Ollie really to conduct smart and insightful investigations. So In a matter of minutes we found the root cause. We got a suggestion for a fix. I think this example is particularly interesting. Because the fix here was not. I don't know, increase CPU or anything like that or allocate more pods. This actually was a code issue. And think about an on-call, any on-call, any SRE, any developer. It doesn't matter how much, how much experts they are. With this, they could have gotten to the root cause within the lines of the code, even if they're not familiar and they haven't seen this code repository before. OK, so this is really the true power we're talking about, and I think most importantly, I didn't open even one dashboard. I didn't filter any query on my own. And this is really the true power and the true potential we have here to achieve 10 times better availability. Now for those of you here in the crowd and I see you and some skeptical guys here thinking, oh yeah, it's just chat GPT for observability. It's, you know, I saw that. It's not OK. And let me elaborate what OL is not. So it's not chat GPT for observability in the sense that OL is not just chat GPT-like interface for accessing your observability data. It's not that. All it has context, all he has knowledge, all is autonomous, meaning in every single, it doesn't have playbooks in every single point of time. It decides autonomously what is the next step. Ollie is also. Not an AI assistant. I'm sure you saw plenty. Every single company on Earth these days. Got directive from the CEO we need to add AI as much as possible, right? So we see that with AI assistant, this small cute chatbot you see on the bottom right usually, right, like, hey, let me help you. And these are great, like they do provide a lot of value, but the value they focus on is really making you and making your users more efficient within the platform, right? So if Oli, for example, was an assistant, It wouldn't do the investigation for you or with you. It would actually just help you build dashboard in core logics. Um, so it doesn't do that. It's not an AI assistant. The power, the difference in power is not becoming, I don't know, 20% faster in building dashboards or alerts. It's actually becoming 10 times better in reliability, which is a much broader, higher level goal for the organization. Um, and lastly, MCP. MCP has been around for quite a while now. Um, So Oli is not a server. We do offer for CoreLogics. OK, but all is not that. What are the differences? in a way is exposing API. For AI agents, OK. So if I want to take my, my information, for example, for, to expose CoreLogics information to other AI agents like cursor, I would take The CoreLogix MCP server, but it does not have any knowledge. It does not have any spatial context. Um, it, it is not an AI agentic AI system. So let's understand what OLI is and how OI works. So it all starts with data. For years we've been really focused on how we achieve, store better data. And finally, I think really over a decade that we're talking about data and the potential value of data, I think finally we can harness the true value and the true potential of data. So CoreLogix is really good as a platform for processing, harnessing tons, massive amounts of data, petabytes of data, terabytes of data. That we've really close to infinite retention. So all of this is being leveraged by Ollie. All he learns from this data, creating a knowledge base, like he would learn something in school, for example. And then with this knowledge, What you saw and saying that you may say, hey, this is an AI, an observability agent. It is not an observability agent because agent is singular. Behind the scenes, Ollie has a team of agents, specialized agents working for you. OK, so we have, for example, Logs Explorer. We have metrics Analyzer. Each one of them is powered by SLMs. SLMs are small language models. Think about 7 billion parameters, 13 billion parameters that is specialized in the specific role and specific task at hand, and they all collaborate together to produce insights, answers. Which altogether allow us to get this 10 times better reliability we were talking about. Now, let's go a step, uh, a step back and talk in general about any agentic AI system. I think this is, if you wanna know how you, what you can take back home to your offices, to your work, and how do you build your AI agent or your own only. This is all about that. So any agentic AI system has 5 main components. Obviously we can break it down to more, but these are the core concepts and the core things you must have in agentic AI system. So it starts with a model in the center. OK. Um, which is clear, this is like the engine for our agent or our agenttic system. We have the system prompt. So for example, you're an expert in analyzing logs for this company. So this is how we provide actual instructions to our agent. Then we talked about multiple agents, so we said logs explorer, right, metrics analyzer, let's say security analysts, and so on. The way they collaborate together. Could could be different and it is crucial. So for example, um, there are different design patterns for it. One architecture would be that I have one orchestrator that takes the task. For example, in this case we ask for investigation. So the orchestrator is familiar with all the other agents. And assign tasks. Hey Log Explorer, please find me relevant logs to this issue. Log Explorer will come back with results. orchestrator will decide on next steps. This is one type. Another type is handovers. We can decide, we can make any sub agent autonomous so they can play around together as a team. Like playing ball and at every given point in time someone else holds the ball, meaning they are the one leading the conversation from now on. So there are different architectures about how we actually organize our agents. Then we have the knowledge. Which talks about how we provide this, how we empower this agenttic AI system not only with tools but also the knowledge, the information that's very, very much needed to make smart decisions and smart actions. And lastly, but probably the most important one is the evolve. How do we actually evaluate the quality of our agentic system? So out of these 5. What you will notice is model is commodity. OK, so, um. Opus was released, I think 1 week ago, 2 weeks ago. We have access to it. All of you have access to it. GPT 5 got released, same thing. So there's nothing really unique about the model. We can, we all share the same models and same for system prompt. I can write something, you can write something. It won't be, it won't make a huge, huge difference. So that leaves us with 3 other main components. Which together compose what we call the context engineering triangle. The context engineering triangle, this is the key. What you see here, these 3 components. If you want to build a powerful, high quality agent. You need these 3 items and you need to understand them deeply. And focus on them. So we covered about agent architecture and the importance of agent architecture. I want to talk a little bit about knowledge and memory. So in Chat GPT I think we are all familiar with the fact that as we talk with Chat GPT sometimes it memorizes what we talked with him about it, right? And then it can be reused in other conversations. But one of the challenges With making the eugenticI system really, really good. With knowledge as well is the limitation of context, right? In theory, if I could take all my petabytes of observability data from the last week and just push it to all of them and wish them good luck, I would do that. But it's not technically possible as for today. And I think we can also take here that more is not more here as well, the fact that it will have more data. It does not mean I will get better answers. So what we do need, we need to have. Relevant data, we have, we need to have it in a structured way that is optimized for the LLM. And let me tell you that, it may sound that changing models is quite easy, right? Just within the API call, change the string, AGPT5. Now, I want to use cloud, very easy. In reality, changing model is difficult. Um, the reason it's difficult because different models have different preferences and you need to optimize the context you're building accordingly. So the knowledge needs to be optimized as well. So going back to we want to build this observability agent. We need to push relevant information to the context, and we already agreed we cannot just take all the logs, metrics, traces, and so on from the last week. So we need really the way to do it. Is actually the same way as we investigate and as we conduct an investigation. Is identify the key parts and the key elements. Store them at first. In the structure, it could be uh whether in vector DB, knowledge graph, or just rational DB depending on what information we store. And allow our agent to access this information and the querying part is also key here because the fact that I have this information stored somewhere does not mean I know how to query it in a, in an effective way. So we said, how do we, how do I store this information? How do I structure it, and how do I query? These are the three components when it comes to knowledge and of how we build this context. So let's say we've done these two really, really good. So we have model, we have system prompt, we have some agents architecture, we can do something simple for starters. Um, And let's say I saved some basic aggregations on my logs to, to start simple. The last Thing which is actually the most important one is evils. Els is short for evaluation. Now, in traditional software, we have unit testing, integration testing, system testing, right? When we build it, we test, we see, hey, thumbs up, nothing broke, all good, we can go live. A new future comes in. We implemented. We build some new tests, run it again, it all works well. Ideally we would have done the same with with AI agents. The challenge though. is that there is no structure. What are we testing? Right, like we have a conversation, the user can input whatever they want. The possibilities are infinite. Right, like, what do I test? What happens if the user just come in and say, Hey, um, what should I order for lunch? There are infinite amounts of possibilities to what the user would put in. So the question of how do I evaluate it becomes really, really challenging, um, and if not done correctly, or if you actually build an AI agent and you don't evaluate it at all, I can show from experience what happens is. You build something, it seems to be working, right? Even you, let's say you're very proud, you deploy it to production, you say to your friends, hey, I have it, you can start using it in a matter of minutes, probably you will get some slack message or someone will tell you, hey, hey, look, I tried this, it doesn't work. So, oh, no problem, I'm on it. You'll go back, you'll check this very, very specific use case, and you'll change the system prompt, and guess what, you will be successful in. Resolving this use case, so it seems like we made progress, but what you didn't realize is as you change the system prompt, you change the entire behavior of the agent. Two things that used to work before the test that you ran a day ago. This use case is not working anymore. OK. So this is why evaluations in AI agents are no less than crucial. I will say just one more thing about Evals. Another challenge here is not only Testing because it's not just one message. I got input, output, is it correct or not? We have an entire flow of conversations. So how do we test this? Um, so I, I'll leave it as a food for thought and we can talk this offline if, if some of you are interested. Um, so how does that translate into Oli? So the beautiful thing you saw before, this is what happens behind the scenes. Onto the left, OK, and this is really sharing with you the internals, how all it works under the hood. On the left, OLI is powered by CoreLogic's extremely powerful observability data platform. OK. So notice. Remember I told you OLI is not an AI system. That's exactly that. O is not an expert in using core logics or using its features to build dashboards. It is an expert in helping you with achieving better reliability and investigating issues, optimizing production, and so on. So it leverages the large scale data observability platform by CoreLogics. From it, it takes all these signals and creates knowledge. To be honest, like if you were to ask me to choose one thing or the key to all these success and quality lies, it's within the knowledge. And then we have the collaboration of the multiple agents. Again, each one powered by small language model, which allows us to get really invaluable production insight as we saw. So how can you take it back home and how can you start to build your own ollie? So, first thing first, beta. We need to store all of our data somewhere blogs, metrics, traces. The more we have, the better. Um, but we also need to make sure that we build in a structured way that the agent can actually optimize in that. The second part is what we call semantic layer. OK, semantic layer is. Really, uh, more, uh, we call it the basic understanding at least of the data. Let's say I have a field that has the name CPUSG 70. What does that mean? So maybe we can assume it means CPU usage over the last 7 days or CPU usage over the last 7 days, but it's it's important for the agent, for the, for our engine. To have this information. Because this is what allowed the agent to make smart decisions, say, hey, I have a problem with CPU. This metric is actually the metric that would is likely to be relevant, so maybe I should check this out. OK, so semantic layer is, I would call it kind of the first building block within the knowledge of the agent. And the third part is building your first AI agent MVP. Take whatever model. Build it. They are actually great tools. It's OpenAI agents at Python SDK, Linkchain, and others. Today it became quite easy to come up with an MVP, so I do recommend really just getting your hands on. And try to build this thing. So and this is just the core concept to start with. So from here We've seen one Demonstration of how Oli works. Um, Oli has been in beta for the last few months with with some of Corologic's customers. I do want to share with you how it actually works, how it actually looks, um, in real life. So this is a story of one of our customers. I was actually fascinated to hear that, to be honest. So they had a notification service, OK. It's a microservice run on, uh, running on Docker. That had random latency spikes. Every now and then, it was really unclear what's going on. They look on the logs, they look on the metrics, they look on the traces, they try to analyze them. But they couldn't find any correlation to these latency spikes. They're really puzzled of what's going on, and at some point they say, you know what, we have no idea. So let's, let's leave it this way. Um, after trying out only for a few weeks, they say, you know what, let, let's give it a try. We have nothing to lose, right? So what happened? OK. So the first thing they, that Oli helped them realize is this notification service is actually connected to another email service that's connected to authentication service that's using RDS. Now that's interesting because until that point in time, they didn't even think or consider checking for RDS. Because the service itself has nothing related to RDS. OK, so they were looking in the wrong places. Now not only that it is connected to RDS, the reason it is interesting, um, as identified, all the correlated spike, spikes in the CPU, CPU spikes of the RDS, amount of connections, as well as latency spikes of the RDS, and this was correlated with the spikes of the notification service. So for the very first time after 4 months, they got some clue of. We have at least more understanding of what's going on. No, the investigation didn't stop there. It was not a one shot. It was not just one prompter. It was 4 or 5 messages in the conversation. So the next thing that Oli shared with them. It has found another lambda function. That also works with this RDS function, uh sorry, with this RDS database. And the interesting thing is that This lambda database apparently this lambda function. Apparently has also correlation when it activates with the spikes of the progress. OK. Um, And looking when Oli looked on the queries that were running by this lambda function. It realizes this, that the queries were running against unindex tables, right, which obviously makes, uh, it creates loads on the RDS and as a result identification service. So from here the, the fix was quite obvious. Um, Oli Ollie also recommended it, but I think it was quite clear that the, the fix is really just building indexes for these tables, um, but this is fascinating. Like think about how much time. would take to find and investigate and get to the root cause of something like that. Right, we have 3 services, 1 lambda function, each one running and managed, being managed by a different team. RDS they didn't even think about it as something that could be related, um, and in general, something tricky about lambda functions, right? They start, they end, nothing happened, like they never existed. Um, so this is a real case, super interesting. 4 months, this, this issue was unsolved. Tripoli, less than 10 minutes, they realized what's going on. So we talked about the power of reliability. We talked about the fact that our focus was in the wrong place. We're running after observability. We forgot to chase after reliability. So how do we move from here? OK, we're still in the early days of AI. And what you see, so in the last decade, I would say observability has increased and has Kind of evolved in a linear way. We've added more features. We've had user sessions, RAM, more capabilities, obviously and more features, so we have improved but not exponentially. This is the first step what I'm just presenting you. This is just the first step in the paradigm shift we're experiencing. The first milestone is co-pilot. So just to clarify, it's not here to replace anyone. It's here to help us make our work, make our job much, much better, and to allow us to focus on what really matters. The next step for Ollie. And, and by the way, it's like a few weeks from uh getting the first release of it. It's being proactive. What does that mean? It means two things. One, it means when an alert fires on Slack, only automatically triage and provide you with initial understanding of what's going on. The second part of being proactive. Is really allowing OE to take actions, all right. Maybe it's just pull request, maybe it's just changing our AWS environment. So this is the next step. This is the proactive phase and the holy or holy grail. Is getting to preventive observability. OK, now think about it. This is An ideal. Think about an agent, and I think we've been talking about this vision for years now, but for the very first time it seems possible to get to a point where we have an agent that constantly watches our production, whether it's from a product standpoint, whether it's from a cost standpoint, operations, infrastructure, and so on and so forth. It is familiar with the way it works in general. It is familiar with seasonality and it can say, hey, listen. We expect to see an increase in loading in the next few days. We have 80% utilization. We have to increase the amount of pods and take action and actually increase this amount of pods, preventing a downtime or delay for users. So this is really where we're going with that. And with that, I'm really happy to share that OLLI is now generally available for all of CoreLogic's customers. Um, we started rolling this out and by December 15th, will be publicly available for all of our users. So I think we covered a lot of points, um. Whether it's the fact that the software stack has changed. Um, I really remember, you know, 10 years ago when you build an app, it was just one single application. It was a web application. You were running it on one web server. And that's it. And maybe you had some blogs written somewhere to help you debug it, so it was much, much, much easier, much convenient. But now we have microservices and we have server loss and we have so many text tech and so many moving parts, all of them creating so many breadcrumbs. So we have to find a better way to achieve software reliability. This is not a feature All the autonomous observability. This is not the future. It's not just something cool. It's really a paradigm shift. It's really a change of the way we think, the way we work, the way we act, and the time is now. It is not something futuristic. What you saw is already working. Proactive is really, let's say 2 or 3 months away, so the time to adopt this solution, the time to implement those solutions is really now. We have this opportunity to make our software much, much better, to make our work better. And with that, I want, I wanna thank you. I'm happy to take questions afterwards. I'm not sure if we have time. Uh, yeah, just, uh, we, we got like a few minutes for questions if anyone wants. Um, we're very happy.