---
video_id: 5L5mfUMsXgA
video_url: https://www.youtube.com/watch?v=5L5mfUMsXgA
is_generated: False
is_translatable: True
---

You've probably heard of Werner Fogel, who's Amazon CTO. His famous quote, Everything fails all the time. Now, if you've been to reinvent a couple of times, you may remember last year, he said that he's often misquoted. The quote is actually, everything fails all the time, so plan for failure and nothing will fail. And so that's exactly what we're gonna talk about today. I'm gonna tell you how to plan for failure so that your workloads won't fail. My name is Mike George. I'm a principal solutions architect with AWS and I work primarily with nonprofit organizations. And the, the tips and tricks and techniques we're gonna talk about today are things that we use to help public sector organizations be successful through disasters. So, unfortunately, humanitarian disasters are all around us, you know, you can think about the war in Ukraine, the Uh, you know, you know, floods in the Midwest, uh, there seem to be problems all around the world. And so this is something that's just gonna be continually happening. Uh, but you know, if you're stranded on the roof of your home and you're calling for help. When you call, if, if you can't get through, knowing that everything fails all the time, isn't very comforting. You want things to work when you need them to work. And so that's what we find with most humanitarian organizations, that when things are at their worst is when your workload needs to to to be successful. So what I want to talk about today are the major ways that you need to think about resilience in the cloud. Now, when people think about resilience in the cloud, they, they typically think about the N + 1 problem, right? I have 1 E2 instance, so 2 must be better. I have 1 availability zone, so 2 must be better. I have 1 region, so 2 must be better, right? Well, not always. There are actually 5 principles you need to think about to have resilience in the cloud. And so the first one is, is really managing those single points of failure. So this is what I, you know, what most people think of. 1 is good, so 2 must be better. The second thing though is excessive load. And excessive load is all about having enough resources to support your workload. This also includes things like making sure that you have appropriate service quotas and other things to support your workload. We think about excessive latency. Excessive latency is all about how does your workload handle latency, or what happens if a downstream dependency of your workload has high latency. We think about misconfiguration and bugs. And misconfiguration and bugs is all about making sure that you have the right CICD processes and automation so that you can effectively deploy workloads to production, and make sure that, you know, you're, you're not making manual changes. If you've got manual changes that you're making in production, you may not have a resilience problem today, but you will one day. And the last thing to think of is shared fate, and shared fate is really about reducing the blast radius of our workload. Think about blast radius, or think about a shared fate where I have one database that supports two or more workloads. If there's a problem with that single database, then those other workloads could be affected. So again, it has a larger blast radius than I may, may want to have. Now when I think about these different categories of failure, you can see that we use the acronym SEEAMs to help you remember it. Where the S stands for single points of failure, the E is excessive load, the next E is excessive latency, the M is misconfiguration and bugs, and that final S is shared fate. So when we think about these different categories, you know, we think about single points of failure. Think about how is my, my workload architected? Is it architected for redundancy? What happens if those components fail? When I, I think about excessive load, you know, I wanna think about what could overwhelm this component. How can this component overwhelm other downstream components? What happens if this workload takes so long to succeed that people stop waiting for it? Is it possible to to throw away work that's never going to be returned? Could this workload, could this particular workload experience bimodal behavior? In other words, does it operate one way under normal conditions, and another way under a failure failure scenario? Are there quotas that could be exceeded? And how does this component scale under load? You know, when I think about excessive latency, I, I, I think about similar things like what happens when a when this component experiences latency, or what happens if a downstream dependency experiences latency. How does my workload behave? When I think about misconfiguration and bugs, again, you know, we want to think about, can I automatically roll back a failed deployment or a bad deployment? Or can I shift traffic away from a fault container, or from an AZ where maybe a bad deployment has happened? Do I have guard rails or other things in place to prevent operator errors? And are there things that could expire in my workload, like certificates or credentials? And finally when I think about shared fate, I'm, I think about, you know, when I deploy this component, how big of a change is it? Is it a large change? If it is, that could increase my risk of having a problem. Does this component share user stories with, with uh with other workloads? And, you know, again, or, or are there things that are tightly coupled with his workload? What happens if this workload experiences a partial or a grave failure. So these are all the different kinds of things that are worth thinking about. Now when we think about resiliency, it's worth having a mental model. And the mental model that I want you to keep in mind is that we have high availability, and high availability is how I have built my application to react to certain kinds of failures that I would anticipate. You might think to yourself, you know, what are, what are the things that I would expect to fail in my workload? Maybe an easy one would be if I'm spread across multiple AZ's, I should expect over time that I might lose an availability zone. That seems like it's something you should plan for. And you could think of other types of failures that you might want to anticipate. And then on the other hand, we have disaster recovery. And disaster recovery is all about, I have failure scenarios that I've anticipated, but I've decided not to mitigate. Or maybe I have things that I just haven't anticipated. But it disaster recovery then is a way for me to resume my operations. And then below both high availability and disaster recovery is this, this mental model of continuous improvement. What can I do to continuously improve my workload, because resilience is not a one and done type thing. This is all about implementing good CICD processes, uh, you know, introducing resilience testing to not only test your workload for failures, but also test your team for failures. OK, so we've spent a lot of time now talking about resilience. Let's actually now talk about generative AI. So, let's look at a typical generative AI application, where I have uh users that interact with an agent, that agent interacts with a foundation model, and a set of tools. And those set of tools you may have heard of as MCP, OK? Now, one thing that I think would be interesting is, I've talked about resilience. I've talked about those five different areas that I'm interested in testing my application for. I'm interested in determining, does my, is my application vulnerable to shared fate, high latency, uh, sufficient capacity, misconfiguration and bugs, and uh and uh single points of failure. It would be interesting, wouldn't it? Could I use generative AI to automatically look at a workload that I'm running in the cloud, and tell me if I have problems in any one of those five areas? Well, so that's what I've done, and that's what I want to demonstrate for you today. So similar to that diagram I just showed you, this is an agentic resilience advisor that I want to demonstrate for you today. The resilience advisor acts like, well, first of all, I built an agent. That agent interacts with a large language model through Amazon Bedrock. And it interacts then with a set of tools. These set of tools give that agent functionality that it doesn't natively have. And there are 3 sets of tools that I've given it access to. The first is a use AWS tool. This is a tool that's built into the strands agent SDK and the use AWS tool allows my agent to go into my AWS account and inspect the resources that I'm running there. The next tool that I've created is a calculate letter grade tool. I wanna be able to have a simple letter grade, so I know the resilience of my workload. You know, given. Given a shared fate issue, is this an A, B, C, or D, OK? So this is just a very simple piece of Python code that I've, I've decorated with an identifier in my strands agent to let it know that it's a tool. And finally, I'm, I'm using the AWS documentation MCP server. This is a publicly available MCP server that allows you to get detailed documentation on really anything you want as it relates to AWS. And what I wanna do is I wanna run this agent. I wanna ask it questions about a specific workload in my account, and get the resilience posture back so I know whether I need to fix anything or not. So what I've done now is I've got two consoles here. On the right hand side, that white screen is my agent that I'm actually running. Now, in the real world, I wouldn't run this through a console, but I'm doing this here just so you can see what's actually happening under the hood. And on the left hand side is my client who's actually interacting with my agent. So I'm gonna start out with my agent, well, I'm gonna start out, starting my, my agent up. You can see it's beginning to run. And now on my left hand side, I'm gonna start up my client. And I'm gonna start by asking, by answering a few questions. I'm gonna give a tag of a workload that I'm interested in that's running in my AWS account. I'm gonna tell it the RTO and the RPO of my workload. And you can notice that. On the agent side, it's identified that it's looking at the food agent workload, and it's looking, it's gonna analyze that workload with an RTO, a recovery time objective of 24 hours, and an RPO or recovery point objective of 12 hours. It's going out to my AWS account, pulling back those resources, reasoning about them, and then it's pulling back documentation to, to look at what are things that I could do to improve my workload. And you can see now on the left side, that it's returned results to me. And it gives me those 5 different categories that I mentioned earlier. And if you look really closely, you can, you can see that it gave me pretty much B's and C's for letter grades. So given an RTO and an RPO of 24 hours and 12 hours, my workload is OK. There's probably some room for improvement. But now I want to ask it another question. So, how would my letter grades change if I changed my recovery time objective from 24 hours to 2 hours? And what if I changed my RPO, my recovery point objective, from 12 hours to 30 minutes? How would, how would that change my resilience posture? Now, you can see on the right hand side that the agent, it doesn't need to go back to my AWs account, it already understands my workload that's running. It's, it's reasoning about that workload further, and it's pulling back some documentation to justify any claims that it's going to make about what I should fix. And you can see now that on the left-hand side is what I get back in my client. And if you look carefully, you can see that whereas before, most of my letter grades were B's and C's, now most of my grades are D's and F's. So it definitely, this workload definitely does not support an RTO an RPO of 2 hours and 30 minutes. All right, well, the next thing I wanna do. You know, how many of you have, have workloads that have been running in production forever, and you've just never updated the, the architecture diagram, it gets out of date, you know. Now I wanna ask this, you know, build me an architecture diagram based on what you've already pulled out of my account. Count. And so the agent reasons about that, and if you look here on the left hand side, what it's done is it's generated an architecture diagram for me in mermaid format. So if you check that into Git, you'll have a a nice graph that will look something like this. And you can see, so this is the workload that it's been analyzing for me. You can see that I've got a bedrock agent. That interacts with a lambda function. It reads and writes from an S3 bucket. It uses secrets and Secrets Manager. It's logging out to cloudwatch logs, it's got some IM roles, and it's got some encryption through KMS, OK? The next thing I wanna do, alright, I know that there are problems with this workload. I, I've seen that. There's definitely things I want to improve, but I don't want a list of everything that's wrong with it. I want to just start out with observability. So, let's start out by asking, OK, what are the top 3 things I should focus on from an observability perspective? And the agent on the right hand side is going to reason about my workload, it's gonna pull back some documentation, and if you look on the left hand side, it's now given me my results. And you can see that it's identified that, OK, I need to have comprehensive cloud watch alarms and dashboards. Fair enough. I don't have any alarms and dashboards currently. It's then saying I should enable X-ray for tracing. I don't have any sort of distributed tracing, that's a good next step. And I should have set up enhanced logging and log insights. Again, you know, I do have a log file, but I'm not really logging that much, and so that's another good next step. And you can see for each of those recommendations, it's giving me links to specific documentation on how to accomplish that task. Notice, you know, when it's talking about setting up logging, it's not dropping, it's not giving me a a link to the cloud watch. The, the Cloud Watch main page on AWS, you know, it's giving me a link deep in the documentation. All right, last thing I wanna do with my agent. Nobody wants to have downtime, but I wanna be prepared for a bad operational day. So I'll ask it now. Build me a run book that I can use to recover from an operational event. My agent is running, it's reasoning about my workload, it's going to some documentation, pulling that back, and you can see now that it's generated. Oh, a run book for me. So I can, I can see some incident severities here. When I scroll down, I can see recovery procedures for my lambda function. Recovery procedures now for my S3 bucket. For my bedrock agent. For Secrets manager. And now I also have ways to validate my service overall, so I can make sure that it's back up and running. And if you look here at the very bottom, it then gives me steps on what I should do to complete a post-incident analysis. All right. So I think we've answered the question, could we use generative AI to help me improve the resilience of my workload? The answer is absolutely yes. So, I've got a couple of QR codes here if you're interested. The first QR code, that's strands agent. I mentioned that I built this agent through the strands SDK. If you're interested in learning more about strands, scan that QR code. That middle QR code, the resilience agent. If you're interested in the code that I demonstrated for you today, scan that middle QR code. That middle QR code will take you directly to our GitHub repository, uh, which is our non-profit samples repository. And if you're interested in seeing how to actually build this code, scan that QR code on the right hand side. This is a QR code for AIM 336. This is happening on Thursday, where I'm gonna be walking you through how to build this exact code. As a next step, you know, if you're a nonprofit organization, the, the QR code on the left-hand side are more sessions that we're doing this week related to nonprofits. The QR code on the right hand side, if you're a nonprofit and you want to know who your account team is, scan that. Let us know who you are. We'd love to talk to you more about what you're doing. And with that, thank you for your time, and I'd ask that you complete the survey in the session app. Thanks, everyone.