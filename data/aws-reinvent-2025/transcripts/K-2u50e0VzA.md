---
video_id: K-2u50e0VzA
video_url: https://www.youtube.com/watch?v=K-2u50e0VzA
is_generated: False
is_translatable: True
---

Good morning everyone. Welcome to Reinvent 2025. My name is Pratik Sharma. I'm a principal solutions architect at AWS. Uh, I've been working very closely with Netflix for the last 6 years, and over the last few years, Netflix has shared their journey with you. Um, Netflix has been a pioneer in building resilient architectures that leverages AWS's scale and flexibility. Uh, previously we talked about how Netflix, uh, efficiently scales its compute infrastructure, how they do service capacity modeling, and today we're going to peel more layers here and dive deeper into how Netflix balances efficiency and reliability. Um, I have the pleasure of hosting Arga today. Arga is a staff software engineer at Netflix, um, who's currently responsible for ensuring Netflix's, um, scalability, at, at for, for live operations. Uh, over the years he has worked on, uh, ensuring Netflix does fantastically, uh, when it comes to scalability and availability at the Netflix edge. And recently, he along with his team has been redefining how Netflix does uh compute scaling and capacity modeling effectively. So let's dive deep into it and welcome Argon stage. Um, thanks, Pratik. Uh, morning, everyone. Uh, my name is Arga. Um, like Pratik said, I'm an infrastructure, I'm in the infrastructure org. Uh, for most of my time at Netflix, uh, I've focused on, um, what we refer to as the end to end streaming architecture. So this goes from right to the global devices to the Netflix edge, which is our CDN into our cloud edge, and then obviously our unique microservices architecture. And, uh, I've mostly been focused on the network side of things, but in the past year, I got dragged into solving some hard problems on the compute side. So, uh, we'll, we'll look into what that, uh, entails. So what are we gonna discuss today? Uh, I'm gonna warn you that I've been told this is a pretty dense talk. Um, it's intentional because I think, uh, we are putting together almost like 2 years of investment across many different facets that we've spoken about, including at previous, uh, years, last year's reinvent. But, uh, the main takeaway is for us to, um, get a, get a, get a closer look at how Netflix approaches efficiency. But without sacrificing reliability. And what that means is we're gonna look into things like what, what does it mean for us to plan for compute supply? How do we understand supply? What does demand look like to us? How do we look at our workloads? Uh, and then finally, to the favorite, my favorite part, which is what I'm tasked with is how do we balance what the supply side looks like with what demand actually looks like from a resource usage. However, it would be incomplete without, um, sort of getting into. In, in, amidst all of this balancing act, how do we actually use reliable techniques? Like how do we manage and mitigate risk? So by definition, as most of you know, we're a global product. We are all across the globe. We are everywhere except that, uh, you know, like non-red part of the globe, which you can guess where that is, which means people at any given time really like, and, you know, could wake up and want to watch and binge binge-watch like uh K-pop Demon Hunters or like our latest Stranger, uh, Stranger Things season. And they can do this across a whole fleet of devices. These are mobile devices, these are PCs, these are iPads, these are TVs. Now, what that means for us is we cannot assume that, uh, all of these share like a single stable network. We cannot assume that they have the same compute capabilities on the device. And more importantly, we cannot assume that they have the same resolution. So this has implications for, you know, how many byte streams we encode, uh, how much storage it entails, all of that. So, the way we serve our global audience is we run what is called like you know a 4-re active, active global control plane. And like the active active piece is very interesting because as you can guess, like very, I think few companies at our scale choose to run in active active mode. And we are in like 4 AWS regions, US East 12, US West 2, and EU. But that there is a good reason as to why we chose and like stick with this architecture. Um, and part of this is we want to be able to serve users from across the world, any part of the globe that you saw on that map with low latencies, extremely low latencies. More importantly is when things go wrong in a region like we had most, some of you might be familiar with the rec uh recent AWCU at least one outage. We do not want to be trending on Twitter. You should not hear that Netflix is down. Like, it's a big part of my job and part of what my colleagues do. And what this enables us to do is do something that we talked about previously called failover. Uh, there are detailed talks on how Netflix does it, but this is why that active active mode is very important. And finally, we complement the global control plane with our actual, you know, global data plan. Um, it's, it's our homegrown CDN, uh, called Open Connect. Uh, we have a lot of dedicated talks that go really deep into the CDN architecture. Um, but this is how it all comes together. It's often referred to as metrics as, uh, crown jewel in terms of engineering. OK. So, this is the start of what we're gonna, uh, you know, dive deep into. Um, and the emphasis here is I'm gonna talk about a lot of efficiency techniques and how we approach efficiency, but I'm also gonna talk about how we do this while staying highly reliable and available. So, how many of you here, you know, have been asked to make your systems more efficient? How many of you, of you have you, you know, have been asked to sort of reduce your annual cost, compute spend, network spend? How many have you, how many of you do, do get a chance to explain to your bosses that, well, uh, that's all good, but what is the cost? What are we trading off in terms of risk? I would have guessed that that's a very, you know, very small number. So the reason this is important is we cannot have one without the other. Like, you know, we, we can be very efficient, but if we do not have a reliable product, the business has a huge cost. So we're gonna level set on some definitions because, you know, by, you know, by definition, these mean, uh, these two terms mean different things to different people. I'm mostly going to focus on what do we mean by efficiency at Netflix, what do we mean by reliability at Netflix, and how do we measure both. So, let's start with efficiency. The reason workloads exist, or the reason your services exist, is they must, and they do create some business value. Like there's no point running services that, you know, are not additive to the business. Like for every single service, no matter how, you know, tier 0, tier 1, they have to be creating business value. That's the end goal. Now, the workloads that do create business value, they come at a cost. The cost of those workloads, like the simplest way to understand, is the amount of resources you need to run that workload reliably and efficiently. So, when we talk of resource usage, it's, it's not just the work that the service itself does. It's, it's, you know, in totality, uh, the amount of infrastructure needed to support that work. And now, you have to move from a single workload to supporting a set of workloads at scale. Sorry. And finally, um, this is something that's often underlooked, is failures come at a cost to business. I, I want to dive deep, uh, dive, uh, deeper on the cost of failures a bit, but, uh, this is something important. You cannot have workload estimation or cost without factoring in what it means to the business if that workload goes down, and this is the risk aspect of it. So this is probably one of my favorite graphs uh that I use uh with leaders, and you will see, so this is essentially like a posar distribution. And what it shows you is like towards the tail, like this green line. There's a very interesting number. Um, and that number is not just interesting in terms of the probability, but just the scale. So, to put this in perspective, and I'm gonna assume a really big business like Netflix, Amazon, etc. If your revenue is, you know, ARR is $100 billion a year, What would you think is a good estimate for like a minute of downtime? 5K 100 50K. I'll answer that for you. The cost of, uh, downtime, a minute of downtime approach, approaches 2000 a minute. And I let that sink in again. 10 minutes of downtime for a business like ours, and I'm only taking 100 billion of revenue. Hopefully, we do more. Uh, 10 minutes of downtime is equivalent to $2 million in revenue. This emphasizes the point that as you, as your business gets more important, you get bigger, you really, really need to start caring about the tail costs. There is an exponential increase in the tail risk of failure, and this is what I mean, um, by factoring in this as part of the cost. And I'll dive, uh, deeper into how we visualize this. So, let's look at what efficiency means first without going into systems in, like to the business. What is the business value of efficiency? So, so let's understand the value part better. So, I really like this visualization that talks about, you know, it's, it's simple enough that, you know, you have a workload, uh, that produces some value. Uh, you have cost for that workload, for running that workload. Hopefully, the value that it provides is greater than the cost of running that workload. But then one important part gets missed. And we referred to this, we came up with a term that is called risk-adjusted net value. What is the cost if that workload fails? And To prevent failure as a business, how much do you have to pay or invest to buy down that risk? You know, this is what we refer to as cost uh risk-adjusted, uh, net value. So that is why that cost shifts left. And your ROI, like the, you know, the, the true value of that service is much, much higher when you look at risk-adjusted net value versus just like a, you know, cost versus value comparison. Now, when I expand this, uh, you know, for my teams, we're running thousands of services, batch workloads. Uh, what does this look like at the fleet level? So, the top, the first one is, you know, what we refer to as a moderately efficient workload. You know, the cost is lower than the value. There, there's a good net ROI. Uh, the one in the middle is, you know, a, a, a set of workloads that are generating a lot of tremendous amount of value for the business, but are, you know, we, we are able to run them at relatively lower costs. So, it's like a huge ROI to the business with few dollars. And finally, this is what we want to avoid at all costs, is we have a set of highly inefficient services, where the cost to the business, you know, greatly undoes the value, the aggregate value that's uh provided. OK. So, we'll, we'll, I, I'll come back to business value in a bit, but let's also look at another aspect of efficiency, which we care deeply about, and this is, you know, how, how do we understand efficiency at a system level. And I promised minimal math in all my talks, but there is some math here. Um, this is probably one of the most used, uh, mechanisms, at least at Netflix, for how we think about system efficiency. And it's, you know, it, it's pretty simple in that it says that, uh, you, you can think of PRF or performance engineering in general. Through what we refer to as Kingsman, uh, Kingsman's approximation, and I highly encourage you to read the paper. It's linked out if you haven't already. But in simple terms, it basically says you can predict the performance of computer systems by approximating how fast a system responds to new work. So when you keep adding load and work to the system, how fast does it respond? And in the graph, you pay attention to the right, what's happening is, as you push utilization higher, which means you, you know, keep adding work, Your arrival date doesn't really change. You're approaching this point where this is, there's nonlinear, almost like a hockey stick spike in the amount of time it takes to, you know, service new work. And in simple terms, it's essentially like a queuing delay. The reason this is important is for at least 22 key takeaways. One that means, one is it means that if you, you know, focus on this equation, because we have system load and service time and arrival rate, Playing around with any of these should actually change the outcome. So you can be like low efficiency, which means like, you know, your, your load number goes down and your service uh rate will be high or stable. You can play around with load balancing and other techniques, and you'll, you'll still have very predictable uh service times. And finally, is for you to know that if you keep pushing your system higher, uh, you know, far towards the right in terms of utilization. Without any other mitigation techniques to buy down that risk, you are approaching the tail which risks congestive failure. This is the law of physics. There is no way to get around that, you just have to design around it. Now, what it also means is it, it, it points to what my colleague likes uh to refer to as that utilization alone is a lie. And the reason they say that is there are 3 examples in this graph. All 3 services actually report on a sheet that they are running at 30% utilization. But the behavior of these services are, like, you know, is radically different. Focus on Service B, the line in yellow, where if you see how much it spikes, we would refer to this, this is like service time. We would refer to this service as a highly unreliable service. So you cannot just look at 30% efficiency if that's your target and say, OK, we've achieved it and it's good. Service C, on the other hand, Does have some variance, but it's, you know, much more within the thresholds. A is just flat. So this is another, uh, point to emphasize where we cannot and should not just look at pure utilization, uh, as a way to measure efficiency. We should use other levers like arrival rate, you know, spreading load to truly improve system efficiency. So now we've built up mental models of, uh, at least at a high level of how do we understand, you know, business value of a workload, How can we measure system efficiency? What does it look like when we actually apply it to the fleet, to a fleet of services. Now we're talking thousands of microservices and batch workloads, etc. So, this is a very, very interesting visualization. In this graph, we are basically saying that, you know, you have 4 different classes of workloads. The first one is what we care about deeply. These are tier 0, no fallbacks. If that thing is down, your, you know, product is down. You have downtime. The second one is degraded, which means if you, if that, if, if those set of workloads are down, you have some degraded experience, but by and large, the product still works. The final piece is best effort, and then batch. Batch workloads are, you know, they will run to completion, but they can wait. You can queue them up. What's pretty interesting though, is like we're running this, this utilization says that we are running all of them, all these 4 classes of services, very close to 50%, roughly. Now this is typically what compute spend looks like for a business. Let's assume this is $50 million or $100 million or whatever the number is. So we've, we've allocated $100 million to run these four class of services at 50% efficiency. Everyone's happy there's a report that comes in and like, you know, people think the business is doing great, but is this efficient? The answer is no. For Netflix, this means or reflects a highly inefficient fleet. What does efficiency look like? An efficient fleet for us actually looks something like this, where we are actually intentionally choosing to run our most critical tier zero workloads at a lower efficiency, not 50%, but 30%. And of course, this is a median or an average. We're choosing to run services that we, where we can accept, you know, fallbacks or degraded at 50%. But more interestingly, we are choosing to run a whole cohort of services, which is our, you know, uh, best effort and batch at much, much higher utilizations, close to 60% and 70%. Now, can you guess what is the total dollar spent, uh, of running the fleet with this ship? Anyone? The interesting answer is this is also exactly $100 million. We are not adding more dollars to be more efficient. And this is, while this is a visualization, this is really, really true for how we've approached running services at Netflix over the past two years. And especially with some of our efforts like live ads, etc. This has necessitated, like, you know, us to sort of redefine what efficiency means for us. And again, I'll re-emphasize, is between the last slide and this one, the compute spend is exactly the same. OK, so metrics. We really like numbers at Netflix. Um, so, to summarize how we think about efficiency, what do we like to measure? The first is, where are we spending our dollars, obviously, um, because that matters to the business. This next one is very interesting. It's like the, where, where, wherever we're spending the dollars, do those dollars mitigate risk for us in some, you know, in some form. The 3rd, which is where like Kingsman approximation, etc. comes in, is what levers can we pull? To reduce costs, including that other efficiency, uh, distribution that I showed, while managing risk. And this is sort of like a very, very, you know, I'll keep re-emphasizing this, is we never at Netflix think about efficiency without also considering reliability and risk. And the final takeaway, this is sort of my add-on to that, is there is this ah fascination for the right reasons on, you know, myopic focus on just raising utilization, which means that we can all just be efficient if we just, you know, run things hotter. Our, our sort of counter to that is yes, but it's just one lever. We can pull other levers which do a better job of not just managing uh efficiency, but also doesn't ignore risk. So Before diving deep into reliability, I think the key takeaway is if we had to articulate how we think about efficiency and reliability, we look at reliability as actually a complementary construct, not a thing on its own. Complementary construct to efficiency. And what does it mean for us? So, at minimum, it means, uh, like I had in my graph, that the services that we run, especially tier 0, tier 1, they can respond with predictable latencies. And, the low piece is, uh, obvious at high scale, but when we throw more load, when we accept those hocket stick spikes during a live event or like a mad rush of a new season, we do not want degradation or, you know, wild spikes in latencies. That's the first tenet. For batch workloads, which we do have to run a lot of because we do all these massive encoding jobs for all our content we have, we want to schedule with low latencies, which means it's OK for the work to sort of take X amount of time to run, but we do not want wild spikes in scheduling delays for our batch workloads. These are non-transactional. We do not believe that zero failures are a thing, or we do not believe that we can prevent failures altogether. But we do care that failures are infrequent. We also care deeply that when things do fail. They recover fast. And finally, another often underlooked, uh, piece of reliability is we care deeply about isolating our failure domains. We have to be excellent, all of us, in articulating what a failure domain should look like. And when that piece um of software, of workload of the system fails, what else is affected? How can we minimize that scope? So again, for reliability now, what do we measure? So instead of just measuring lines, which, I mean, as an industry, we've spoken about a lot and continue to do and it's important, what else do we measure at Netflix? We care about impact to business, like the blast radius. When, when one system or a set of systems fails, what, what is the cost to the business? We care, like I said, about how often does it fail. So this is a very good metric for this is meantime between failures. We care about recovery time, which means we measure mean time to recovery a lot. So now, with an understanding of, you know, efficiency and reliability, This is the part where I dive into techniques that we actually use, but I want to pause here for a bit and re-emphasize that everything that I'm going to talk about is not a mere focus on efficiency. It will, by definition, include how can we be efficient while managing risk. And for this to be viable, the first piece is, we, we will talk a lot about demand later in the presentation, but the first piece is, we need to understand what our supply side looks like. And in, in, in my case, that means compute supply. And thanks to AWS being our vendor, um, you know, we have to be very, very, um, we have to be very, very precise about the amount of compute that we want in every single region, uh, what does that get allocated to? How do we use it efficiently. So, the first part, um, as many of you are familiar with, is this is something that most organizations do, hopefully, at scale, uh, it's called capacity planning. And this essentially talks about how do we allocate our dollars for compute spend, specifically. Now, this is pretty interesting, uh, as a visualization, because, um, well this is simplified, this essentially shows us that at Netflix, for example, we have at minimum two very, very different sets of services. The yellow is the state full layer, so this is our like persistent stores like Cassandra, EVCash, uh, like our KV, uh, you know, stores, and they are interesting in that. They don't really have wild, uh, patterns of auto scaling, you know, which means we have to run those nodes very predictably with a certain amount of quorum. We have to pre-reserve capacity for them. They cannot scale fast by definition, right? When nodes come up, they need 30 to 60 minutes to reconcile. And which means how we think about this for capacity allocation is we have to reserve compute for them all the time. That's simple. But there's this other set of services, which is, you know, these, which are, you know, actually like the microservices fleet. They are stateless by definition, and they have very, very different scaling patterns depending on the load. So this is just diurnal pattern, but it looks very, very similar with the peaks being slightly different when there is additional load. Now, the trouble is, We can't reserve capacity. For, you know, this peak here. I mean, we could, but that would be highly inefficient. So what do we do? So what it looks like for us, at least, is, in practice, the answer depends on how, how, you know, of course, it's, you know, based on your pricing, uh, with Amazon, how well can you articulate? What percentage of your dollars must go to what it's called reserve spend, so this whole area in yellow, which can include and should include not just state full, but a percentage of the stateless fleet. But then on top of that, which is very important. is how much can you or should you be paying on demand. Because the interesting thing here is if you spend too much on, you know, for reserve, you're being inefficient. If you spend too much on on-demand, which is arguably higher priced, you're not being efficient. Now, Netflix on top of this is also smart about doing what we call reusing our reservation trough. Now, because our, like workloads are diurnal, like global traffic pattern, etc. uh, we over the years have invested in shifting capacity. So we really like to move these large batch workloads, uh, that get scheduled on this capacity, which means we have reservations that we're already paying for, might as well use that capacity to run our batch when uh services can't. The other piece and very important input to capacity planning is actually the availability of the hardware. So, in, in that reservation graph, I, I'll go into families and hardware shapes, but we can't just assume that everything that we want will always be available on demand, as much as AWS tries, and they do a fantastic job of being great partners for us. So what this actually means for us is we have to account for compute availability. And this is a very interesting graph, probably one of my favorite ones where If you, if you look closely here, and I've color coded this for simplicity, is, this is probably like a year or so back, I guess. But this is essentially saying that when you have stable generations of hardware, and here we're looking at uh 5th-gen and 6th-gen mostly. You, you know, capacity availability is guaranteed almost, right? Like you have, you know, deep supply because your hardware vendors, whoever they are, have done a really good job of like racking and stacking. But then when you get to this other, like new generation hardware, this is 7-chin, AMD and Intel. Now, this is where we want to be. We want to keep moving our fleet towards the more newer generation efficient hardware, but then we are approaching what we refer to as volatile frontier. Now, it's interesting in that we can't assume that the hardware vendor here will be able to guarantee the same capacity that they can in this pool. Now, the way to do this is like we want to depart tools, uh, we want to sort of adopt the new generation hardware, but incrementally, like parts of our fleet, and we're really good about this. And the other piece is, uh, you need to be benchmarking new hardware, and this is one of my strongest learnings is even if you do not dip your toes in the pool immediately, you need to have solid references of benchmarks of how these, uh, different families compare. And a big shout out to our performance engineering team who do a lot of this heavy lifting for us. The other piece, which is, you know, complimentary to my previous graph, is it's not just about the family. It's also about shapes. So, what this graph is showing is on your X-axis, you have uh shapes going right from Excel to 16 Xcel. Now, this is great because the more we move towards the right, we efficient we are more efficient by definition. Uh, we can push those instances higher, we can run, uh, much more hotter on those instances, that's nice. But then we are also taking on the risk of the supply. And the risk here is only because that as you go harder, uh, as you go higher or bigger on these instances, Your vendor needs to do like a, you know, much, much more complicated job of bin packing. And, you know, some of you might know here is like my joke is the wind packing is an easy problem, right? What can go wrong? Wind packing is a really hard problem. So it doesn't matter who the vendor is. If you keep pushing on on towards the larger instances while they're efficient, you are also, you also need to be cognizant of the availability risks here. It's yet another trade-off for capacity planning. And the third piece is very unique to what Netflix, um, I believe, has done, is we have added a 3rd construct to our approach of understanding supply. And we refer to this as buffers. It's essentially a way to understand headroom. And buffers, by definition, mean that, uh, you know, you have given a service, you have two territories within your services headroom. One is what we refer to as success buffer. This is the part in yellow. And that part means that, you know, beyond a certain point, so beyond here where your service is humming along nicely, if you throw more load in this area, How much additional load can the service still absorb without any impact? To predictable uh latencies or otherwise. There's no degradation. That's what we refer to as success buffer. Now, if you push beyond that yellow line, you can still go higher until you fall over. And this small area there is what we refer to as failure buffer. Now, the reason it's important is like these constructs help us capacity plan when we do allocation of, you know, workloads to a hardware. We must account for success buffers and failure buffers. And the other reason, which is probably the most important reason. is as you can see that I'm not using constructs of like CPU cores, memory, efficiency, clock frequency, hyper-threaded or not, but that is included in the map. I'm just not diving deep in this presentation. But what this allows you to do is when you have a high-level construct like that, you can transform like those shape details of the family, of the threading of the clock frequency in these constructs which shift across both instance families and shapes. So it by definition makes things portable. So, I really, really like this concept of success buffer and failure buffer, and we'll look into, you know, how Netflix uh uses that for capacity planning. The, the, the second most important thing, however, is buffers. Again, by definition, uh, if you stick with that concept, also vary by workload. And how many people in this room have heard stories of like the center clusters going down because they were running compaction workloads, for example. So what this means is, you know, stateless services, uh, we are very JVM heavy, are easier to reason about, you know, they, they take, they run with a certain amount of load, they take on more load, and beyond that they degrade. When you go to state full services like Cassandra AV Cash, they have their own oddities. Like you need to, and, and you need to plan for those oddities. So this background buffer here. Is us planning for what would, what would it look like if a set of Cassandra nodes like run compaction? Like how do we plan for that as part of the headroom, which means like the math looks very, very different. We do not pin the success buffer, uh, of a Cassandra workload at the same set of thresholds. These are CPU numbers for simplicity, but you can see how they vary. Like we are you know, computing this additional buffer factoring in factoring this in for capacity planning. And finally In addition to workloads, buffers also vary by hardware type. And this is, these are numbers from some of our, you know, internal benchmarks, and what this looks like in practice is, uh, you both have a 7-G AMD and Intel here, but also different shapes. And what this shows us is that I can push like a 7-G8XL much, much harder. Like this is 88% CPU versus, you know, what I would push like a, you know, hyper threaded uh uh Intel. But also that, you know, the larger the shape, you know, you can push them harder. What it also means is that depending on your service, so tier zero again is like no fallbacks, tier 1 is degraded, because we are planning for buffer, like, you know, different amounts of buffer, like I showed in my utilization sort of distribution, is we are choosing, we choose to intentionally run them less hotter. Like the tier zero runs at 35%, tier one at 46% in this example. And that's yet another thing that we need to sort of keep in mind when we plan for buffers in capacity planning. OK. So now that we've understood, uh, sup, you know, the supply side, we also need to understand what compute demand actually looks like. Where does this come from, to have this mental model before we can even get to the balancing piece of it. So, the first piece of understanding demand is we need to understand, uh, at a workload level, what resources they need to do their job and how do we approach, uh, sort of estimating for that. Now, this is not complicated. Uh, this is basically us using a CPU memory network, like the three most common, uh, ways to estimate work. Um, and the CPU side is much, much simpler, for example, is regardless of whether you're running on VMs or containers, you hopefully have a metric that tells you utilization. The memory gets slightly more interesting. For example, one of my most interesting times in the last year or so was like figuring out That, oh, because we are a very JVM heavy fleet, we can't just estimate for memory based on life size. I also need to care deeply about the allocation rate, and that changes wildly on depending on the garbage collector that you're using, is it a piecing allocator, etc. But you, you know, you need to come up with your own math of estimating, uh, memory based on the workload. And the network is also interesting where for stateless services, you know, they're really like network bound, but for stateful we see very interesting patterns. It's like depending on what that workload is doing, while you have a, you know, baseline, you do need to account for these like bursts of network. And what all of this means is we need to, you know, have our mental model for workloads, but we also need to observe them in production. Now, this is a real graph of, you know, us looking at existing production workloads and trying to fit in, in this example, like a CPU to RPS ratio. So how much efficiency or how much utilization is it running at for a given uh RPS. It obviously assumes, you know, fair load balancing, etc. But what this allows us to do is like if we fit this line here. You can see that it allows us to fit cert certain targets, scaling targets, and we'll talk about this in the later part of our presentation. But the takeaway here is the best way, you can have your models, but the best way to also do this is observe your workloads in production. And finally, this is another often ignored or underlooked piece, is you, you need to care deeply about service startup times. Because while you can add capacity and hopefully add capacity fast, you do need to factor in the amount of time it takes for your JVM to start up, for your OS startup, for your scheduling. Because if you don't, you will not be compensating for, and there might be regressions. If you don't, you might, you will, you will, you will not be compensating for how much buffer. You need to preserve before like new capacity is added. So this is the 3rd element of service startup time that we model in our workloads. So now we've, you know, talked about, uh, quickly workload profiling or understanding usage at the workload level. Let's look globally, what that looks like, uh, in terms of, uh, Netflix traffic. There's a very simple or simplification, I want to say, of our microservices architecture, we have devices, they come in through the front door, which is our open-source gateway called Zo. It goes to a federation layer called services. They call their data stored through data gateways. Now, what this means, however, in terms of load, is when there is a 4X spike in load through the front door, it doesn't automatically translate to a 4X spike to all the services. And this is very interesting. It might mean like it's a 2X spike for a tier zero workload that is critical to playback. It might mean there is a 3X spike for something that is critical to, you know, discovery, which is when you're browsing your rows or finding your shows. And then a 1 1.5x spike for something that is like uh ranking, what show would you like to watch next? What this means for us is we cannot apply the same math in terms of buffer and capacity allocation across the board. So the summary is that microservices call patterns, your unique architecture has a tremendous impact on how you must estimate and understand demand. And this is, you know, from production, this is what it looks like for us. Uh, you can see that because we run in 4 regions, there are some very predictable elements to this, which is the good part. Uh, there is this 24 hour periodicity, which is like, you know, because it's a diurnal pattern, like the same region within 24 hours will roughly have the same amount of, you know, usage every day, week on week. There's gonna be like a 10x delta, however, between a regional peak and trough. Now, the reason this is important is it would be highly inefficient if we were to reserve compute for this peak. We cannot. And when you multiply that 4 regions, that is a lot of dollars that we don't want to be spending. However, there is value to this predictability. It allows us to, do things like failover, which I talked about. It's like when US East 1 has an issue, we get ourselves out of that region, and this predictability allows us to say that oh, if we move X amount of traffic out of US East 1, how much additional compute do I need to balance this traffic in 3 other regions. And then this, this, this is the non-predictable or unpredictable demand, uh, which we, uh, also need to account for, which is when a title gets wildly popular, like a live event, uh, like the Jake Paul fight, like, uh, the Stranger Things launch. The demand looks very, very different. So we also have to by definition account for that. All right, so now we've hopefully have, ah, you know, decent mental models for understanding both the supply and demand side. This is the fun part of my job, which is we are now tasked with perfectly managing demand and supply. And I often joke is like the task is that you're asked to predict the future. And it's similar to asking like, can you time the market? Uh, you know, how do you know which stock is going to go up or down, and what could possibly go wrong if you, you know, if you are incorrect. So, let's dive deep into what we do. But first, tenets. So when we talk about balancing, We need to be very, very precise that the efficiency that we talk about needs to be a global versus a local optimization, which means that you often can end up with very, very locally efficient, uh, optimization decisions, but they add risk at a global level. And that risk can be for a service, it can be for dollars. The second thing is, so buffers or headroom, as I just explained, they need to exist for our most critical services when it matters. So, the emphasis here is it's not just enough for buffers to exist. They need to exist at the right time and for the right set of services. And finally, you can imagine for everything that I'm talking about is this heavy lift of managing both the supply side and demand side. This does not scale at a service team level. So you need infrastructure teams like ours to do bulk of this heavy lifting, both on the supply side and demand side. So we've uh talked about balancing. This is us getting into seeing how do we do the management of supply. And the first piece is that while, while we discussed, you know, hardware availability and uh applicability and workload profiling, we need to do the matching game for which workload should run on what hardware. So let's look into how we do that. So this is a visualization of uh what we typically refer to as service capacity modeling. And at minimum, a good simplification is there are two kinds of modeling patterns that we need to account for. One for stateless, 1 to, uh, 1 for stateful. Uh, we do more than that, but that's the minimum. Now, what it shows is that you need to account for 3 key pieces of data. One is the hardware data itself, you know, what family, what generation, what clock frequency, hyperthreaded or not, what course. The second is pricing data, which is very unique to your, your relationship with your vendor. And the third is obviously workload data that we just, you know, saw earlier. Now, when we put all of those together, we want to end up with a set of recommendations for every single service that we run in the fleet. That says, you know, what is, what could be the most optimal hardware to run this on. And what it looks like is something like this. So, in this case, this is like a stateless service, one of our stateless services. The model is essentially saying that based on all these three pieces of data put together, we believe that you, you should be running on C7A2 Excels as the most optimal hardware. Now, bear in mind we persist this for all our services in a central depository, and because it's a C, my guess is like this is some compute-bound workload. Which means, you know, it would be inefficient for us to run that on an R. It's a very important takeaway. So What is that second row in the recommendations. So, it's interesting because we can just, you know, have a list of preferred hardware types across the services and call it a day, but that doesn't really work in practice because, like I said, you cannot, you're not guaranteed hardware availability. So when you're not, what do you do? You also, we also recommend a bunch of fallback shapes that this, you know, each workload can run on. But the key part is these also need to be validated, you know, in production through, you know, canneries, and we talk a lot, a lot about testing in production. Um, but, but you'll see the consistency here is like, because it's compute bound, we are recommending both SCs, the difference being 6-gen and 7G. And my guess is like the 6th gen is also cheaper, uh, versus like us moving to a 7th gen, but a different hardware. And finally While we've understood hardware supply and, you know, demand at a workload level, we need to see what it looks like after the recommendations. Like, how do we apply it across the fleet. Like we have thousands of services. So we refer to this as fleet shipping. Now, what this means is that at any given time, if you look at 3 different pieces of context, which is, you know, the performance side or cost. Capacity in business, we can come up with an approximation of, for every single region, what is the distribution of hardware shapes and families that we would want. Now, this is what, what we're showing on the right. It's like, OK, playback, discovery, personalization, tier zero, tier one. This is the rough split of M7As and R7A, C7A, sixth generation Intel. But what it doesn't do is this. It doesn't factor in that, oh, we might want X amount of capacity in a region, but what happens when we are short in a given family? Quite possible. And like 7th Generation, for example, we really, really went through this. It's like, you know, we were very early. When those pools are tight, what do you do? So, one interesting thing here, you will note is, I don't know that the highlights are clear, but while 7-gen AMD M7As are short in capacity, we do have the option to find opportunities for other things, other hardware ships we could run on, slightly less efficiently, maybe, but with much, much more robust availability. And this is where the alternative shapes come in. Now, what that looks like in practice is in that example, We're going to start this fleet-wide shaping loop. That's gonna say, without touching tier zero services, because those are our most important ones, what other workloads can we touch that are currently running on M7A, but we are, because we are short 10-K CPUs, VCPUs, what does it take to start moving them to safe fallbacks? So R7. Now this obviously assumes I'm oversimplifying that, you know, you're talking mostly about memory-bound workloads, but if you do find that cohort of memory-bound workloads, we can drive on demand this fleet shaping loop of moving them from M7XL to R7 XL. And our vendor can tell us whether they have enough capacity for that. The other piece is, now that we have estimates for how much capacity we need in the region, we've done the shaping. How do we actually account for traffic coming in through the front door? This is something we refer to as fleet shipping, uh, sorry, pre-scaling, which means um We need to do something like this. It's both, a, you know, science and an art, I like to say. It by definition, uh, involves things that I like referring to as crystal ball predictions. We have to do this job of like estimating what does, what would our peak viewership look like on that launch of, you know, Stranger Things season 56, whatever. Now, what that means is we also need to convert that from peak viewership to peak traffic estimation globally. And then we have to split that traffic estimation into regional traffic, and from there, we can go to compute demands. But finally, we also need to scale the fleet up because we are likely running, you know, colder, ah, sorry, hotter than we would want to when such a thing happens. And in practice, this is what it looks like, um, where we know that traffic is going to arrive when we launch a certain thing at 7 a.m. here. We will use our math and the modeling to pre-scale the fleet. We refer to this as min pinning. So you see, we scale up the fleet here, traffic arrives here, we handle it well. But then my favorite piece here is not this part, and I'll go deeper into this. It's called auto scaling. The reason for this is we do not want to scale up the services so much that we run inefficiently. We do want this like auto scaling at the end. And then finally, after the traffic has gone away, we want them to scale down, all automated. Service owners shouldn't need to do anything here. You can do pre-scaling in many different ways. You can do pre-scaling very inefficiently, as I call it, which means overscale. You're so far below your target, and when you add this up across the fleet, it's millions, hundreds of millions of dollars. You can also do this, you can underscale, which is like, you know, push services so hard that they risk falling over, especially tier 1, tier 2, and tier 0 is running like near capacity, which is kind of pretty bad. But we would like to be sure. We would want to pre-scale in such a way that tier zeros are running efficiently, close to the target, a little below. Tier ones are running hotter, but not so hot that they fall over, and same for tier twos. Now, what is efficient pre-scale is if you've done a job well, um, what does this look like? It's in that same graph where we had pre-scale, you see the traffic arrive here. And you, this like green line spikes up, which means your like average RPS goes high. But then we have another tool, which I'll go into slightly, ah, is we have load shedding kick-in, but importantly, it's very brief and minimal. And then immediately as new capacity is added, we have RPS normalization. So we, we are humming along nicely again. And then like I said in the previous part, like there is this like brief period of auto scaling. So, this is a graph that I really like because I like pointing to this as a reference is like I need all the elements of this if we are doing pre-scaling right. It's not right if we did not have any load shedding, which means we overscale. It's not right if we did not have any reactive scaling, which means we overscaled. And if we had sustained load shedding, that means we under-scaled. We don't want to be there. So how do we actually achieve this? Um, I refer to this as dynamic traffic shaping, and it involves two key things. One is we have to take our existing traffic that is already in the regions and find a way to redistribute them across our four regions. What this looks like is if I didn't do anything at any given point in time, uh, you will have this weird pattern of regional deltas, which is, you know, one region like EU West probably is going into trough, and the other region is running at a very different capacity. Now, this, the reason it's a trouble is it makes it very, very difficult to plan for capacity. However, I could do this. And if you look in the middle here, is we have actually managed to balance this traffic. So this wild delta is now reduced. It's important because doing that is a precursor to us also figuring out what do we do with the new traffic that hits our data centers. And there are, you know, deep talks from Sergei, etc. on this, where, you know, Netflix uses its own, uh, DNS and authoritative resolvers to, you know, uh, steer traffic into regions. This is what it typically looks like, uh, without any intervention. You can see here that US East 1 is taking a lot more traffic. West 2 and East 2, not so much. This is not balanced. So for the new traffic, we also like to publish shaping rules and I force uh in, in this graph, it's like uh you, you force steering, which can be suboptimal for latencies for a bit, but results in balancing. So you're pushing people who would otherwise have gone into US East one into the two other regions and thus achieving balance. So this is about the new traffic. This is, so these two steps are complementary. You take out new traffic, redistribute them across regions, and then you re-steer the new traffic which would have, which would otherwise have approximately landed in some of your most loaded regions. Now, the effect of this is when new load comes in, uh, This is, you know, that those wild deltas would be amplified so much that almost invariably I could assure you that we would be getting iced, as we call it, which means like we would have not planned for the capacity, it's impossible, or if we did, we would have like the compute spend would be so large it'd be highly inefficient, but with shaping. This is what, what we're able to, uh, you know, achieve. Like we, we do this, like we've tapered down the wild, uh, spikes, and we can account for capacity a lot, lot better. So the peaks do get amplified, but you know, they're well within range now and it enables. So, traffic shaping is, is a, is a massive, massive to, uh tool to have in your toolkit. So now that I've, you know, talked so much about um balancing and perfectly balancing supply and demand, Uh, we all know here that, you know, it's, this, this, this is like exceptionally hard in practice. And we can hope that that's the case, but, you know, we also need to have strategies for how to do this or how to manage risk when that's not true. And I'll quickly go through some final levers that we pull to do, you know, what what I refer to as managing risk and these are the reliability techniques. So the first one is, when you do not get, and you will not get capacity planning perfectly accurate, you need to account for reactive scaling, which is how fast can you add new capacity. So for this, we need 3 things. So at a service level, what we do is we come up with scaling targets, and at least 3 different numbers. We refer to something as target tracking, where this is the threshold when you're rich service hits this. We want to start injecting new capacity, but slowly. We have another threshold where when which, when you hit this is you, again, it's called hammers, very aptly named, I think, where you start adding more capacity, but much, much faster, uh, both in volume and, uh, you know, scale. And finally, if you're still sort of not able to keep up with all the scaling, we want to shed traffic, and I'll go into this very quickly as well. So what does this look like? At a cluster level, this is all automated. You'll see our systems fire events, which is called as like CPU based, you know, scale-up. So, it's basically saying that, oh, because the target tracking in my previous slide fired, I need to bump up like the fleet shape, like the compute, spin up new instances, um, and this, this, this is basically where the desired compute should be, this is where it is at the moment. And finally, the other complementary side of it is we cannot hold on to capacity forever. So this is also very automated. Uh, essentially, when the traffic, uh, has gone down, we want to, you know, enable, uh, uh, sort of giving that capacity back. We refer to this as scaled down. Now, what's interesting is, um, while I talked about the fleet loop previously, this, that was in the, that can only operate in the order of months because you can imagine it's like hardware planning, procurement, driving that across the fleet. From there, however, we just talked about reactive scaling and what it's done is like we have another lever now. But that reacts in the order of minutes, not months. So this is that complementary, uh, sort of reliability technique where we cannot just rely on like, you know, monthly or quarterly change of fleet shape. We need something that operates in the order of minutes. This is nice. But, again, ah, I want to be very honest, like order scaling can be slow. You know, you can have delays in your control plane, you can have delays with your OS startup, you can have, if you're running JVMs, you know, there's jit and JVM startup can take a while. So I really recommend watching Ryan's talk from last year's reinvent. He goes a lot deeper into how we fix some of those issues. But the key takeaway is we cannot rely on auto scaling alone if we truly want to mitigate risk. So what do we do? When we cannot add capacity fast, we need to shed load. What that means is we can shed load in two ways. We can shed everything, which we refer to as bulk load shedding, or we can be much more opinionated and smarter about which load do we shed. We refer to this as priority-based load shedding. And in, in the same graph, what this looks like is, uh, you know, your, your service is in a good mode, operating healthily, and then new traffic comes in. Additional traffic comes in and you're still in that success buffer territory. Everything is green. But then further load gets added. And what happens is we start approaching what I refer to as failure buffer. And you can see, this is where we must pay attention. Now, this is where, at this cut point, as we refer to as, this is where we want to start shedding load. And the, you know, thing that we want to avoid is if we did not do that, we would end up here. Now, this is us failing over, uh sorry, falling over, and you can see that this is mayhem, if you take the analogy of like an actual, like, you know, traffic highway. So For bulk paste, uh, or, you know, bulk load shedding, um, the important thing to keep in mind is You know, while you can drop traffic, there is impact like there's gonna be customer impact if you just drop drop traffic indiscriminately. So what we actually wanna do is we only want to shed this traffic here and this is my friend's analogy here is that, you know, on a, on a, on a very packed highway if you're trying to add people and you have an ambulance which really needs to get in that get onto that highway and then you have this other person out for like a weekend spin with his Porsche. Ah, that person can probably wait. So how do we do that? We shed that load here. This is what we refer to as prioritized load shedding, and when done well, this is what it looks like is we wanna move towards the right incrementally as more as we hit higher utilizations. So we first start with bulk traffic where there is minimal degradation, best effort, and only later do we even touch shedding critical traffic. Now, this is our way to ensure that you have minimal impact, minimal customer impact, even when you're like underscaled or can't keep up with our scaling and we want to shed load. And this is what it looks like from, uh, you know, real production graph. It's not three things here that we start non-critical shedding much, much earlier. Critical shading, uh, shedding waits until much later. But in all of this, while these errors grow up, you know, go up because we are shedding traffic, we have successful, successfully maintained our RPS rate, and this is key. That means by definition, we had minimal, very minimal degradation to our customers and service experience. Now I'll bring this back to fleet loops. What does this mean? So, we had months, we had minutes for reactive auto scaling, and then finally, when that wasn't good enough, we were able to add another layer, which operates in the order of seconds. So the hammer rules, which is like, you know, adding bulk capacity fast and load shedding, these operate for us fleet-wide. In the order of seconds. Now, we have a full picture of like what techniques, ah, you know, ah, add up together to sort of mitigate risk in addition to just pure compute spend and capacity planning. So, I'm gonna quickly summarize with lessons and key takeaways that I think are valuable. The first is, whenever you, you do an exercise like this, you must think about both proactive and reactive levers. And because I have a network background, I like to think about this in like pre-inggress and post-inggress. So, pre-inggress is the, are the things that you can do before traffic gets into your data centers. This includes like shaping your fleet, predictive scaling, traffic shaping like I showed. And then you have to also account for things that, oh, what happens after you already have absorbed additional traffic into your data centers. These are the reactive techniques. This is the reactive scaling, this is the load shedding. And second one, I think this is obvious, but I feel like this gets lost a lot, is you need to have end to end systems thinking. You cannot look at pure compute and pure spend as the only way to buy down risk. The other thing is that if you operate at scale like ours, one great thing is like you, you know, efficiency means compound by definition. And finally, you need end to end traffic management. And the final piece is my uh favorite statement, which my, ah, you know, dear colleague Joey appreciates is like math is our safety blanket, as I like to call it. And if you take some basic concepts from economic and apply some math, you can solve some of the hardest problems. I do want to give a shout out to Joey, who's been my partner in crime for the past year or two. and you can please check out our Oasis library, which is Service capacity modeling, which we have a talk on too. Thank you very much, and I hope you enjoyed the session. Have a good rest of the day.