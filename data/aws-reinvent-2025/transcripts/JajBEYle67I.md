---
video_id: JajBEYle67I
video_url: https://www.youtube.com/watch?v=JajBEYle67I
is_generated: False
is_translatable: True
---

Got awful quiet in here all of a sudden. Um, I'm Bill, uh, product manager for AWS DevOps agent. I'm gonna be joined here on stage later by David, uh, who's a senior principal engineer here at AWS. And a little bit about David before I start, uh, before he comes on a little bit later, uh, David's been part of the teams that have built, uh, AWS services like Dynamo DB, Lambda, uh, IOT, CloudWatch. Uh, he's also as a senior principal engineer and, and been at Amazon for nearly 20 years now, um, been one of the people that's really shaped the DevOps culture. Uh, at Amazon and AWS, so he publishes weekly inside of AWS to all of our AWS and Amazon builders, uh, a blog called This Week in Observability. Um, he's also been, uh, a primary contributor to something called the, the Amazon or AWS builder library. Is anybody familiar with the Builder library out there? A few of you. So that's where we publish in-depth pieces on how Amazon approaches building and operating software. And I checked last night, David, uh, David's authored 11 of those articles. I think he's the most prolific author on that site. So I do encourage you to look at those, um, uh, a lot of, a lot of, uh, really interesting and helpful information there. And so I say all of that. Um, because we've been really lucky to have David be part of the team that's helped us build the AWS DevOps agent over the past year, so his fingerprints are all over it. A lot of the culture, the DevOps culture that he's tried to infuse into the AWS and Amazon development, uh, and operations teams, uh, has been poured into the, the DevOps agent itself. And so the other thing I wanted to say, I said, David's a senior principal engineer and at Amazon. Some of you might be wondering what is that? What, what, what does a senior principal engineer at Amazon really do and it's actually, it's, it's kind of a big deal, um, but like what, what's, you know, what's the day in the life of David? And so if you're one of those people, hopefully there's a few of you that had that question, uh, we got some undercover footage of a day in the life of David from a couple of weeks ago. So let's take a look at this. So there he is. So it turns out that the first thing you need to know about a senior, um, uh, uh, principal engineer is they still get paged, uh, and they still have to scramble. They have to get back to their office. They have to drop everything. They have to get their laptop out, get their favorite playlist going to help them, you know, work through this troubleshooting scenario. But what's new about this case or what's new about this week is David has a new teammate. He has the ADOS DevOps agent to help him. So his first thing he does is he checks in and before he got his laptop out, metrics, analytics, logs, traces, analyzed, check for, uh, you know, deployments that may have impacted this situation already found root cause identified. It's all before David got his laptop. Open. So let's see what else it can do. Can help us get out of this alarm state, uh, resolve this incident. So sure enough, uh, following has been, uh, clearly reading David's blog about this weekend in observability, so it knows how to generate an effective what we call an MCM or a change document, uh, that's gonna include pre pre-validation steps, action steps, post-action steps, uh, and then, uh, you'll see here that it's actually even showing off a little bit. Uh, the agent is providing David with code to go ahead and actually fix what was in this case, uh, a bad deployment, uh, the code itself. So the last thing to do here is to actually, uh, perform the rollback as instructed and fix the issue. So fingers crossed, uh, we'll check the validation steps. The errors are going to. Wait for it. Go away. All right, and so we should have a happy senior principal engineer. So that's just a, a little bit of insight into David, but also really what the AWS DevOps agent does. And so we launched uh AWS DevOps agent in public preview yesterday. Uh, and let me ask, who here in the audience has ever been on call, carried a pager, been on the other side of that page? Is that it really? Only about half of you? OK. Uh, one other question just out of my curiosity, is anybody here actually on, on call right now? OK, well, we got a couple of dozen of you, so we, you know, if you get up and run out, I won't take it personally. So what does the AWS DevOps agent do? So it performs two fundamental tasks. So the first I just showed you helps you resolve incidents faster. Uh, it's gonna analyze your, your metrics, analytics logs as traces that are related to a particular incident, look for related deployments. Uh, it's gonna share with you its findings. It's gonna generate a root cause, and it's gonna provide you with mitigation steps to resolve the issue. So all with the goal of helping you reduce MTTR. Uh, the second thing which you didn't see in that video is the agent, uh, is always there, it's always available, it's always sort of lurking in the background and it's designed also to help you prevent incidents before they happen. So it does that in a couple of ways, but you know what it's able to do is it will periodically scan all of the incidents that have been investigated and managed by the agent. Uh, it's gonna take its understanding of things like AWS best practices or well architected best practices, it's understanding. Of your application environment and it's gonna suggest changes, changes to fix what it detects as maybe underlying problems that created the context for a particular set of incidents or other, other opportunities it sees to improve the, the posture of your application. So, how does it work? Uh, 4 things that I wanna highlight about how the AWS DevOps agent works. Uh, so the first is that we've designed it to operate as a member of your team. Uh, if you saw the keynote yesterday, Matt Garman talked about frontier agents, and an aspect of a frontier agent is it, it can work autonomously as part of your team. And so in this case, what it can do is it can respond just like an on-call engineer would respond. To support tickets, pages, alarms that you configured uh to trigger the agent to perform an investigation of a particular incident, uh, it can then uh uh write its findings, communicate its findings back into say the comment uh in a Service now ticket. Uh, it can also, you know, if you're using something like Slack for your team to coordinate during an incident, it'll provide. Uh, just as any other engineer would, it's findings back into the Slack channel itself. Uh, we've also designed the agent to not just work with the people on your team, uh, and I'll talk about this in a, in a little bit more depth in a moment. We've also designed it as an agent to work with other agents, uh, that you may be deploying in your environment as well. The second characteristic of the DevOps agent that I want to share with you is we've designed it to be a telemetry expert, so we've built in integrations with not just AWS CloudWatch and telemetry that AOS provides, but, uh, with our launch partners Dianatrace Dataoc, Neelli, and Splunk. Uh, we've given it access to the telemetry and the telemetry signals that you all have told us that you, you use and you all have told us that you use other things too, uh, and a very common thing we've heard is open source stacks with grafana, Prometheus, something like Loki, for example, and so we give you a way to tie into those self-hosted telemetry systems or really any other telemetry that you may have through something that we call BYO or bring your own MCP server. Um, and so with that in place, you know, the, what it gives the agent the ability to do is what you saw in that video, which is rapidly and quickly, you know, scanned through, you know, the, the the important or related logs, metrics, alarms, and traces that you have in your infrastructure. Uh, but because it has access and because it's also oriented to prevent future incidents, uh, it will provide you with recommendations on how to improve your observability posture. So that might be adding what it perceives to be missing metrics or perhaps tuning what might be flapping alarms, those kinds of things. So the third characteristic of the AWS DevOps agent that I wanted to highlight is we we've worked very hard to make it what we think of as a pipeline pro. So often it's the case that um when an incident happens, it's a result of some kind of change. It might be a change to the application code, it could be a change to the infrastructure code. Uh, and so we've given it access through built-in integrations to things like GitHub and GitLab so it can understand your deployment pipelines, uh, and the repos that sit behind those pipelines so it can detect these changes. And again, it's got this dual personality of being able to react to it. Incidents but then also provide guidance on how to prevent future incidents. So in this case, may make suggestions to improve your pipelines themselves, perhaps to add tests or other things that will help you prevent incidents to begin with before they happen. Fourth characteristic and the last one that I wanna talk about is it was important for us to be able to construct this agent so that it really knows your apps and your organization. So, um, knowing your apps, so what it will do is it'll automatically Uh, generate what we call an application topology. You can think of it as a knowledge graph or a map of your application and your environment. What are the important entities? What are their relationships with each other, uh, that'll help guide the agent as it's performing its tasks, uh, the. The other thing that we recognize is that there's some things that are going to be unique about your organization, your practice, uh, so you can provide those to the agent in the form of two things, uh, what we call steering files or you might think of them as run books that you can directly provide to the agents that you create. Uh, you can also give the agents access to existing, uh, run books that you may have, so these might be coming out of your ticketing system, uh, or, uh, for example, we have customers who told us they maintain the run books in something like confluence. And if that's something you want to actually see in action, I understand that the, the Atlassian booth at 5 o'clock they're gonna show you how to connect AWS StevOps agent uh through the Atlassian MCP server into a tool that'll give the agent access to the confluence run books. So, uh, having said all of that, the one thing we've also sort of acknowledged is that it's important, you know, to, to know that at some point there's a limit to what the agent can do, uh, and so that's where we give you the ability to bring in an expert, a human expert, and so at any point, for example, during an investigation that the AWS DevOps agent, uh, is, you know, in the process of completing as it's working through findings plan, you know, the hypothesis generating. Uh, root cause analysis or mitigation plans. If you wanna have an expert's human set of eyes, one click you can pull an AWS support engineer into an investigation. They'll be provided with all the context up up to that date. You can be chatting, uh, with not just the the agent but also an AWS support engineer to help you refine the root cause or maybe validate mitigation steps. So I talked about topology. I wanted to double click on that uh just for a second here. Uh, this is a picture of what an application topology looks like, uh, in the AWS DevOps agent web application. Um, so, um, what is it? So, you know, how this is created starts from the ground up when you Set up your AWS DevOps agent. You create what's called an agent space, and within that agent space you give it access to one or multiple, uh, cloud accounts, and we'll start discovering the resources, uh, that you've given us access to through IM roles and their relationships with each other. Start the agent will then start to, it will continue to. Maintain that we'll look for additional relationships if you've given it access to a telemetry source, it'll bring in their service maps. It'll map like say log groups, metrics, alarms into entities in your topology if you've given it access to your CICD pipelines, it'll map deployments into entities, so it builds up this structure or this map of your architecture. That helps it, you know, during an incident narrow in and refine, say, log queries or metric queries that it needs to do to to determine what's going on in a particular incident and we'll also use this application map and compare it to patterns, uh, say for example in AWS. Best practices patterns that it knows are well architected principles as well as any guidance that you've given through the steering files for your own guidelines and policies and I'll look for deviations from that and use that to form part of its recommendations or what I called earlier the prevention recommendations. So the last thing I wanna talk about is, you know, how this is, you know, the, the success that customers have had, uh, with the AWS DevOps agent so far, and then I'm gonna turn it over to David to, to demo the solution to you, uh, but the first thing I wanna say, you're probably wondering, we released it yesterday, how could I have customer examples of this, but we actually started. Back in August, we gave AWS DevOps agent, we made it available to internal Amazon and AWS teams uh to use in their on-call rotations to help them improve the posture of their applications and services. And so, so far today we've, the AWS DevOps agent internally has handled over 1000. Uh, real live Amazon and AWS, you know, application and service team incidents. Uh, and we carefully track each of those. We, we work those, uh, manually with the teams that have, uh, the actual on-calls that have been responding to those incidents, and we've determined that the AWS DevOps agent to date has been able to achieve an 86% success rate on its root cause analysis. So two things about that, um, 86%. Uh, it's a B, right? And so, you know, we're certainly after an A here and so we're continuing to work on improving this every day. Um, but one thing that, you know, was pointed out to me by some of our customers is that don't beat yourself up too much over that 14% because even when the agent is getting it wrong or maybe has an incomplete RCA, it's gone down pathways, right, and shown its work, uh, in terms of the log analysis that it may have done or the metric analysis that it may have done. And those are pathways that I would have gone down, dead ends that I as an on-call engineer would have gone down and so you saved me a lot of time, even if in the end you didn't get it right. So, um, a couple of other examples here. So we did open it up then, uh, in September, uh, to external beta customers. One of the first customers that we made it available to was the Commonwealth Bank of Australia. Uh, and the first thing they did, they, they got it set up, and they said, um, let's give it something tricky. Like they said we had a network outage a couple of days ago, or not a network outage, a network incident of some type, um, that took them and their team a little over 5 hours to root cause, uh, and so what they did is they gave, uh, the AWS DevOps agent the same context that their teams had a couple of days prior and asked it to figure it out, uh, and, uh, what came back in 15 minutes, uh, was an accurate RCA analysis. So, uh, really happy with what we're seeing, what our customers are seeing in some real world scenarios. Last thing I wanted to point out to, to you all, you know, as a result of the customer experience and the beta, as I mentioned earlier, that we're making the AWS DevOps agent something that works with not just other people, other on-call engineers, other SREs, DevOps engineers on your team, uh, but also with other agents. Uh, and so we worked with some of our launch partners on this, and here's an example of, of one of them, Dynatrace, uh, where we targeted jointly a couple of customers. You see, uh, Clarence, a chemical manufacturing company, Western Governors University, if you haven't heard of them, they have about 200,000 students, uh, United Airlines, who many of you probably flew here to get here, um, these were joint Dynatrace and Amazon customers where we worked with them to set up AWS DevOps agent to work in concert. Uh, with Dynatrace and particularly Dynarace's Davis AI capabilities and so Western Governors is a, a great example that I'd like to highlight in, in less than a day they were able to get, uh, both of these systems, these solutions, these agents up and running and working collaboratively together in their production environments to help them, uh, investigate in this particular case real world incidents. So that's the introduction. Uh, it's time now, uh, to turn it over to David who's gonna walk you through in detail, uh, what the agent, the AWS DevOps agent can do. Alright, so yeah, pictures 1000 words. So let's just look at a whole bunch of, uh, examples of the DevOps agent going and doing things. So I'll actually show you 4 different demos. Uh, we'll start with the another kind of slow motion version of that fast motion video where I was had started with the pager which I still have. Hopefully it doesn't, uh, doesn't go off while we're having this presentation. I might have to investigate something. Uh, it's not going to. I'm just, OK, but then, uh, we'll also look at, uh, custom customizing DevOps agent with bringing MCP servers. Uh, we'll, I'll show you how you can steer it and nudge it in a direction or ask questions of it with chat, and then I'll show that future issue prevention where it scans past incidents to look for patterns of things that you can improve long term to prevent future issues. OK, first demo time. Now throughout, we're gonna look through uh a set of services that I built that work together called robots as a service service. This is, you can imagine this is a set of web services that talk to each other that make it so you can control robots like in the world, so you can send them commands, schedule them to do things, see where they are, and what, what, whatnot. Um, I actually created this. It's a set of microservices like a gateway service, the bot service that talks to a schedule service that talks to the action service to go and, uh, use AWS IOT to, uh, send commands to the robots, and then a forge service that you would use for provisioning. These all this is simple, uh, but it's also each of these are internally complex. There are a lot of different things or security groups, load balancers, auto scaling groups, all apples, I don't know all kinds of stuff that actually make up these things. I am rolls. So lots of uh things that, uh, could potentially go wrong. So let's talk more action. Let's get into it. So, let's start by implementing a bug and pushing it to production. Here I'm in Quiro. I have this caching logic in the forge service, and I'm gonna say let's add a timestamp field to the cash records, just to, because maybe I need to measure how, how effective my cash is, how long things are cached for. Harmless change, we're just adding data, it's not gonna cause any problems. So let's just push that, fantastic. Uh, and in the distance alarms. So, OK, so things break. So in this case I'm using Dynatrace for my kind of application level, uh, monitoring and alerting, and sure enough, uh, it goes and it detects the problem automatically. Uh, it's pretty nice. These Davis alarms, so you just, I didn't have to set anything up, finds, sees the failure rate increase, and sure enough, yeah, problems, uh, bot service is seeing errors. I can see overall that there are problems. Kind of throughout the stack all the services are kind of lighting up with with bad failure rates, so not good. Um, so in the meantime, uh, Dina Trace has called the web hook that sent, uh, this notification to the AWS DevOps agent. So before I, I didn't necessarily even need to be to show up here and look at it, it already kicked off an investigation. In this case, this, this is the uh operator app within the AWS DevOps agent. So I go here and I see all my ongoing and past investigations. Let's go ahead and look at this one to see kind of the details. So here we can see um oh you don't need to use the chat for right now we'll come back to that. So here we see the, the issue. This is what uh Dinah Trace Davis sent uh the AWS DevOps agent. You can see the details of the problem, where the problem is, and, uh, extremely precise time stamp. Of when the problem occurred there, so that's really helpful. And so let's go from here. Let's just like jump to the end from like a storing standpoint. Let's go right to the root cause. So at the end this investigation is already complete. Like by the time I made it back into the office and opened my laptop, the investigation was done. And so what is the root cause in this case? Well, it says a code deployment introduced a cache serialization format incompatibility in the botforge service. Uh, and this is exactly what you saw me implement. Uh, the, I added a new, it says I added a new time stamp field, uh, that's with the code change, and the deserialization method, so it understood the surrounding code, that the deserialized method didn't like to see this new field, so it was, it was failing. Um, and so it also talked, talked, said that, well, actually the requests that were cache misses, the ones that went right to Dynamoti B, succeeded. So that's actually an interesting kind of nuance that it noticed while doing the investigation that some requests were succeeding. Uh, it wasn't obvious, but it, it's useful from, uh, figuring out, uh, useful evidence to prove that this, yes, this code change was the problem. So great, let's go back to the top and see how the agent got there. We're gonna scroll through slowly here. So it started off, it said, OK, I'm gonna investigate. Let's start by just kind of figuring out where we are in the world. It's gonna go read some run books. These are all MCP tool calls. You notice since I'm using Dynatrace, it's doing uh DQL queries. I could, if I want to inspect and see what it's been up to, I can, I hear I'm clicking on one of the details and it's showing me the inputs and outputs of each of those MCP calls. Uh, to, and then, so fantastic, it runs a bunch of queries just to get figuring out what's going on. Now it has between calling Dynatrace and, and querying our own kind of internally figured out version of your topology, it says, OK, here's what we're dealing with, here are the services, here's what talks to what, let's keep going. So next it's going to continue to triage, kind of understand the health of each of those services, and it says, OK, yep, the, the front end service is seeing errors, it's having errors talking to dependencies, so it's kind of going to look for a needle in a different haystack. OK, let's go investigate those services. Those services are also seeing errors, but. Talking to the bot Forge service, which is seeing errors. So let's go move, search for a needle in a different haystack and says, OK, fantastic. BotForge seems like the most dependent service. Let's go let's now zoom in on BotForge and really interrogate that to see what's going on. So it's bought Forge. Like it's, it's calling AWS APIs through MCP. It's calling, uh, its own topology, and now it has figured out the AWS resources and other, other just everything that makes up like what pipelines are deploying to this service, uh, what are, where are its logs. Let's just figure out how we zoom in and explore this thing. OK, so it's it's from there, now we have a lot of investigations we need to do. So he comes up with a lot of ideas. OK, let's search the botForge deployments and infrastructure changes to see if we just changed something. Let's look at the EC2 instance health metrics to see if maybe there's, we ran out of something like file descriptors or CPU. Let's check other components like the bot config Dynamo DB table that this uses to see if it's running out of capacity or something. Let's look at application logs and zoom in to see if there's any, any hints. There and actually let's look at traffic patterns to see maybe there's some incoming shift that's causing some problem here, some new, new robots being added or asked for, who knows? These are kind of I find from my experience that these are the the the kind of key beats to look for when you're trying to figure out what to mitigate because each of these can map to a mitigation. If we're running out of CPU. Let's add more servers. If we deployed something, let's roll back. So these kind of map to things that can be mitigated. OK, so, but that's a lot to look for. So the agent is actually has listed all these things it wants to do, but it's gonna do this in parallel. It has a lot more hands than I do, so because it's, so instead of me doing one thing at a time, it's just gonna investigate everything. So uses subagents. Now we're not gonna look at the output of each individual subagent as it goes along. We're just gonna look at the observations, the key things that it has figured out, OK, so and these. These are the types of things that get pushed to like a Slack channel. So it says aha, there's been a cloud formation stack deployment with a new launch template version. Uh, this was created and it's interesting like it the agent likes to do a little bit of extra credit sometimes and it says, OK, well, and it found during this cloud formation stack update, here are the instances that were added and replaced. I, the way I do deployments in this fleet is I replace my instances kind of blue-green with deployments and it noticed that it says, OK, here's an instance that started here, here when it had errors. It's pretty neat. OK. What else does it notice observation wise it actually is looking at a lot of things and giving me the evidence that it has ruled things out. This is what Bill was talking about. It's useful that it kind of also tells you what it looked at that it thinks is not important. Um, and it's actually kind of interestingly clever about that. For example, it noticed, uh, here below it just found that there's a CPU spike on one of the instances, which is suspicious and correlates in time, but irrelevant because this is an instance that just started up. So of course it's gonna use a lot of CPU as it does whatever installation of software and everything. So it's nice, it's just OK, we saw this, we're gonna ignore it. OK, so this is just all the, all the evidence that it found. It also found, um, looking at log files, wherever the log files were, it found them and noticed that some stack traces. This is where it starts to get some hints of the root cause of this, here's, here's the line of code according to the log. That seems to be having some problems. OK, it's more, more observations. It's pushing these slack and stuff. Uh, what else happened here? Uh, it's found, uh, the rolling instance replacement. Uh, see, so it's, yeah, these are the instances that started up, um, and were shut down. Um, it also noticed this is where it observed that from the logs that the requests that were cache misses succeeded. So it's just really interesting details as a hot. There's definitely something wrong with the cash interaction here. OK, and then, uh, sure enough, here's the code deployment. It is connected to my GitHub actions that I'm using to actually drive the code changes, not just the cloud formation part of the infrastructure change, and it, this is where it really kind of did this fusion of, OK, based on what I know about the logs and based on the code that changed, here's what I think is starting to really happen. So great, all the sub agents are done, and it's going to come up with, uh, just kind of pull all that data together and think it we come up to come to the right conclusions from that, and and then we're back at the root cause, which is what we talked about before. So great, I figured it out. Now there's actually, if I were to really just jump to this from Slack, the first thing I would look at if the investigation is done is the root cause. This is where it's just formatted a little bit nicer. I don't have to scroll through like a ton of, of raw, raw findings and things like that. So it just says that here's the impact, there's a failure rate increase. Uh, it then talks about, uh, here's that same root cause text that I showed you before. Here's the root cause, cause there we already saw this. It's that the cash serialization issue. And then it shows the kind of key observations below it. The observations, these are the key findings that that kind of prove that this root cause, there could be multiple root causes, in this case it's just one. It says, OK, yep, there's that evidence about the failure rate of the cache uh hits versus cache misses. It talks about the code that was changed, and it talks about the cloud formation stack update that was part of the code change that actually made it into production. Great. So there's that root cause, OK, but your root cause is great, but we need to fix the problem. So here's where we can generate a mitigation plan, we ask it to, OK, like uh what should we do? In this case, uh, it's gonna be pretty simple because it's a bad deployment. So what do you do when there's a bad deployment? Well, you roll it back, um, so it's just rollback, uh, but I hadn't actually until I was using this, uh, doing this demo, I didn't, um, really know about, I hadn't used GitHub actions before. So this is teaching me for the first time how to do a rollback. Now it breaks things down into how I change whenever you're gonna do anything in production, you wanna do things. Very carefully, you want to prepare, make sure you have everything in order first, pre-validate, make sure the system hasn't changed in the meantime when you kind of have this first idea about what to do, you need to prepare and make sure, did somebody else on my team roll back already? Did we learn something new in the last 5 minutes? So you just do all those careful preparation, then apply. This is where I didn't really know how to do GitHub actions rollbacks before. This taught me and it it showed me what to expect. Um Then, uh, after you do that, it says here's how, here's some commands you can run if you want to uh to make sure that the change is rolling back and that you're seeing the recovery you expect. And of course if that goes wrong, then you want to be prepared ahead of time to undo the thing that you just did, in this case, rolling back the rollback. So put the, put the bad, potentially bad code back in place and then figure out what to do from there, which will be a real exciting journey. Um, now I would look at this later, but this is the long term solution. I don't wanna be distracted by this. I need to kick off the rollback, but the long term solution, it prepares the spec of what I can do to, uh, hand to, say, a coding agent or just do myself. It says, here's how you should actually fix the code, um, you wanna, you wanna make sure you have good tests and that you update the deserializer to match the serializer. OK, but let's focus on rollback, um. So let's go over to GitHub actions where uh I can do that um and this I'm just kind of following that run book uh it says, OK, this is actually this is the bad deployment though adding the time stamped uh was the problem, so let's go back, um, and so the problem was that or the thing we wanna roll back to is the actually last that's last known good deployment. So click on that and. Uh, rerun all jobs. That's so this, there's nothing special in the agent that knows about GitHub actions. It just kind of knows that because it's a, because it's an LLM, frankly, and so it's just, uh, it teaches me how to use GitHub actions to do the rollback, and this is just one example of a, of a thing that it would figure out. And so to confirm, I can go to whatever observability tool I'm using. In this case, Dinatrace, hit that refresh, and then we are back to healthy, so. There it is. That's that's going through an investigation. Uh, so now, um, oh, and the, and it shows that the Davis problems have been cleared, so that's great. Now, uh, let's do that follow up, uh, let's fix the bug so that we don't, um, you know, come into this problem again. So I copy the spec. Which is at at the bottom of that mitigation actions page and then I like to use Quiro for the using its spec-driven development. It, it does kind of production changes pretty well without the uh without and while keeping the uh the fun of vibe coding. And so I paste that in. Quiro is gonna generate its own specification, its own, OK, here are the full requirements. I'm gonna go spend some time understanding your code. Maybe it's a code I didn't have to author with Quiro. It'll just like learn about your code base, come up with a plan. I'll read the. Come with detailed requirements that say, say how I should, uh, how I should make the change. Come up with a design and I can review the design. I'm not showing you all this, and then come up with a timeline, uh, excuse me, a task list of things that it's going to do one thing at a time, and I can say, OK, implement it. Um, I actually used Quiro to implement a lot of the things here, so, uh, it works really well for this, uh, and then it also generates really nice tests because you have that, uh, that, uh, specification that says the requirements of things that that you wanna make sure that you have. Uh, Quiro generates property-based tests to make sure it's not just kind of, uh, no op unit tests. It does very thorough tests on because it understands the behavior that you wanna have through the specification, so it can generate really nice tests, so. That's the, that's the end to end fix on a typical incident for me is you react to it, you you mitigate it, and then you solve the root cause. So the flow of this, I, so why didn't we build this before? Like why didn't we just build Dev DevOps agent like a decade ago? It's because like DevOps is messy. It's a mix of so many different tools and systems together that were never really designed to interact interface with each other. Like, sure, they all have APIs. Like when APIs, web services became a thing, like you can make anything talk to anything by like coding the integration. But there are just so many, it's like end to end, like there's so many combinations of tools that you might use together for deployment, observability, logs, metrics, traces, who knows? But and so the, the, the key unlock for why we can do this now is thanks to LLMs and MCP and that kind of thing. So you saw the, the bad code change in this case flow through git. Hub, it's a cloud formation. EC2 service uses Dynatrace, uses cloudWatch, Slack for to talk to me, ServiceNow to page me, all these things that all need to play together and so you, I'm sure you can kind of map, you might use some of these tools. You might use a different set of tools for a different type of thing. So that's the, the flexibility is key. OK, so how did it actually do this? Like under the hood, what's going on? Well, the the objective of incident management, uh, is to find some hidden gem, the root cause. Like that's you've got to find the thing that something that you can do something about. So the agent gets just kind of plopped into a desert, says, hey, go, go find that hidden gem with really no instruction. It doesn't need any instruction at the beginning. So this is where it starts by building that topology. It needs to understand what you have in your system, like what is the universe that I might need to look at. I don't even know where in the desert to look. So, OK, let's, let's understand your system. It learns how do I find metrics and how do I find logs about each of these things, how do I find out how you're deploying. So it surveys the landscape. And then it needs to understand the links between and so this topology is the connectivity between different resources, different systems, and it uses everything available to build this topology map. It looks as, well, uh, are you using traces? Then I can follow traces through the system. Do you have IAM permissions on this lambda function to talk to that Dynamo DB table? Oh, OK, let's draw a line there. It doesn't necessarily talk to it, but I should, it's, it's worth looking at during an investigation. And then it learns how to drill down. It says, OK, do I know how to zoom in on this service and break it down and see it, look at all the host metrics and everything, all the little things. So, um, that's ultimately what it's doing, uh, behind the scenes, uh, as pictured here. Um, OK, let's look, so, so what we saw, we saw, uh, an investigation happen. It interfaced with a ton of different tools. It provided all of its evidence, uh, maybe more than you needed, and it gave safe mitigation steps really thoughtfully because you're about to mitigate something by doing a production change kind of on the fly, so you need to be ready for that and review that and, and break it down. So that's that. OK, demo two MCP servers, bring your own MCP server and customization. Like I said, you probably use some combination of these types of tools or totally different tools. Let's, let's do another investigation where instead of sending our traces to Dynatrace and logs and metrics to CloudWatch, let's say you have some your own log store. So let's say you have a log store maybe. In S3 or something, um, and then you send your traces the cloud watch whatever it's just let's mix it up, use different observability tools. Well, how would the agent know how to, how it doesn't know about your bespoke logging system, so how would it do that? Well, the answer is to just add in and pop up an MCP server. And then configure the agent to use it. Um, I did this in like 20 minutes with, uh, I actually implemented this, and you'll see it. I'm using, uh, Amazon Bedrock agent core MCP gateway. I just set up a gateway, talks to a lambda function, and queries my S3 logs. Uh, used Quiro specs to generate it, and it was done. Maybe let's say 40 minutes just end to end. Um OK, great. So here's another failure. Let's, let's, let's cause another problem. So here I am, same botforge service set up. Let me just log into one of these instances. I'm gonna SSH into one of them. Maybe I'm like troubleshooting some latency issue. I don't know. Let's say, oh, I'm seeing, I'm gonna, I'm gonna pick on the cash again. Let's say there's, uh, I'm seeing something weird with the cache. Let's just go into the cache and, uh, clear the files. Like I think that might, that might fix the problem. Just remove the cache. Yeah, maybe that's a good idea. I don't know. Uh, and again, maybe not a good idea. It depends on how it works. So it wasn't a good idea, um, and so here we have the investigation that triggered off of a cloud watch alarm. So in this case it was triggering off a cloud watch alarm on a load bouncer. So now it's the tools are different. The the flow is the same, but the tools are different. It's getting, looking at X-ray. It's looking at cloud watch metrics. Um, it's using the topology again that's similar, um, and it says, OK, yeah, we have a load balancer showing errors. Let's dig in from here and figure out where to do that investigation. Goes in, looks at stuff, looks at stuff. OK, now it found quickly that these are the SQL lite cache table is missing, causing HTP 500 errors. I just figured that out. How did it figure it out? Well, it saw it in the S3 application logs. This is that random, you know, MCP server that didn't exist until 40 minutes before this that I configured and it just knew how to talk to it to query logs. Pretty great and use it contextually in the investigation for this service. I know how to get to its logs, so it's great. And I figured it out Um, more just investigation, uh, it's coming to observations, the same idea, uh, it just, uh, investigates and figures out, uh, what's going on, uh, more metrics, more metrics, more metrics, more metrics, uh, all sub agents are done. So let's synthesize the findings. Um, great, uh, it actually noticed I was busted. Uh, manual SSM session on bot or forge instance caused SQLite database corruption. D Yanniuk, who's that, uh, started an NSSM session and, uh, maybe messed with some stuff. And so sure enough, it found through Cloud trail that I had logged into the host before, and it's like, oh, that's suspicious because that's the same host that it saw from the logs that had the errors about the cache. So great again I'm just showing the, the interesting thing that I'm showing here is that how similar the investigation is to the past investigation. It just happens to be using different tools. It's still successful. It's just using this custom bespoke, uh, MCP server, uh, that, that I made that, you know, if you have your own system, you can also plug that in. And so here it is with the root cause manual SSM session, uh, maybe messed up the ASQL like cache, so. What should, so let's see like how what it comes up with. Uh, the earlier mitigation plan was simple. It was just a rollback. This is a little bit more exotic of a scenario. Um, what does it figure out? It says, well, what you should do is just, it's happening on one instance you should just replace it and that's absolutely what I would do in this incident. Um, I would just first just get the bad thing out and then quickly like follow up with more, more figure out, make sure we understand why, and we actually do have the evidence as to why it's because I logged into it. So similar thing, prepare. Let's make sure that the auto scaling group that we have all, all of the right arms figured out. Make sure these are the right things we should be messing with, uh, prevalidate, make sure that, you know, more instances haven't come up bad. Make sure that the healthy instance is still there and healthy, you know, you maybe the situation has changed, um, that's, and then apply. It also gives the. You know, conveniently it gives the right, uh, I would, this is how I would terminate the instance with auto scaling command instead of an EC2 command, OK, just because this does the graceful shutdown. OK, fine, a little nuanced but still useful, and then post validate, make sure that you have a healthy fleet, and then, um, and if, uh, rollback steps I guess are just to try to put the fleet back together in a, in a good way, um, great. So that's about that So what did we see? We saw bringing a random MCP server that can do some part of your ops, whether it's how you do deployments, if you have your own deployment system, if you have your own knowledge base that you wanna wire in, if you have your own observability tool for something, if you have like. metrics maybe you measure customer impact using your own tool. I don't know whatever it is, just bring it in an MCP server using agent core gateway. I find it to be really easy to do that securely like the this agent is the only thing that has permission to talk to that MCP server, so it's not like it's just open. It's great, um, and this particular root cause was interesting. It was a great failure that, uh, we did, and so it's great. Now let's see how you can interact with investigations and with the DevOps agent um outside of passively watching what it's up to. So you can ask questions about investigations that's one thing you can do is to say, let's say I had some investigation and OK, say what is the start time? Well, it's a simple question, but maybe I didn't wanna read through and scroll so I can say what was the start time and just answers questions about what this this chat is about what did the investigation already find. So it's not gonna do big research projects, it's just gonna answer questions based on what was already revealed in the investigation. So for example, I could say what alarms triggered during the event it says well this is one alarm. And it says, by the way, the investigation didn't look super thoroughly, so if you want, you would have to ask me to go look for other alarms. And so, OK, great. So I might say, OK, let's do a little bit more of a research project. Let's say, can you summarize the customer impact? Uh, maybe I wanna just have a nice report that shows, uh, shows a little bit more about what the impact was. You can see it sends that request, uh, to the agent, and then the agent replies with this report. Maybe you had to do a little more research and just compile things. And you can see just it's a nice report. It says here are the services, each of their failure rates um that happened during the incident, um, and just a bunch more details. Uh, about like the duration, the timeline, uh, customer facing impact, it describes things in terms of, of what it understands my functionality is. OK, and now I wanna ask a deeper question to say, OK, but how many robots, like this is the bot service robot servers, how many robots were impacted during this? How I can maybe I could later on figure out which robots. OK, how many robots were impacted? Um, so it's gonna have to do more queries. It didn't query that during the investigation, so it's gonna take a little bit of time to go and do a bunch of queries. You can see it doing, uh, more, uh, DQL queries, the Dyna trace, because spans are the part that are really rich with this information, the labels of like this is the robot that you know is working on and this is the status code and stuff. Spans are really rich with that information. So it queries the spans. And then, uh, and logs, it's kind of again just figuring out what it needs to pull to answer my question of like which robots were impacted and then it answers. OK, we found 91 unique robots that had, and here's the way in which they were impacted. They saw 500s. They were impacted on certain operations. It's kind of breaking that down by API all great because this was all in the span data. Uh, I don't know how I figured it out, but I just happen to know. Um, and then, uh, yeah, it just gives a nice, a nice overall summary. Uh, this isn't something it was specially coded to do. I just thought it would be a nice thing to show off. Well, what should I ask it? Let's ask it customer impact because it's something that I often do during during an incident. I wanna see kind of what my customers saw when there was a problem. It's really an important question, uh, in the moment and then later. OK, you can also, uh, when an investigation is going on, you can nudge it, you can steer it to say, hey, I think you should actually go look at this because you might not just be sitting there if you, if you do open your laptop before the investigation is done, you might wanna go look at things and and be active in this, uh, so when you're looking at things you might share things you found or if you see it get off track you might wanna put it. Back on track. Here's a real example of it getting off track. Um, during when preparing this demo for all of you, I noticed I was doing an investigation, but I was breaking things like every 30 minutes, uh, and letting it recover for a short time and breaking something again, and so I was confused. It was like, oh, there are all these different things happening at the same time. And so I said, can you show me the incident timeline? And I said, OK, well 1800 and then 1900 this thing happened. And then at 2100 this thing happened and then like uh uh the 2200 it's it's finding all these different things that I had broken um I I don't remember what I had prompted the next day this other thing happened. I'm like OK like agent like can you relax and just look at I said can you look at the most um the most recent impact only? I said, yeah, the, these, uh, these deployments are way too spread out. Can you ask the investigation to focus on the most recent, uh, impact to see what might have triggered that? I think for this prompt initially for this investigation, I didn't actually ask it about a specific, you know, alarm time stamp or something. I don't know. So that's why it kind of got a little creative and said, great, OK, I'm gonna, and it actually like. Improved my prompts to the agent too. It said actually let's get this time stamp. I didn't paste that time stamp. It it figured out the the start point in the chat and then it pasted that to the agent to have it look at the most recent wave. So pretty nice, uh, way of, of steering investigations, but, uh, through chat. Um, I can also just kind of learn about my environment. Um, I don't have to ask about a specific maybe operational issue. We haven't really tuned to ask about like explain an alarm, explain like why there's a problem right now, but you can actually ask it really whatever you feel like, um, for example, sometimes I might start an investigation, um, you can tell what alarm to look at here, but I can say maybe don't investigate anything. I just wanna ask you some questions, um, great, and let's start an investigation, an investigation about. TBD. And so the agent responds with, OK, I will just, um, I'll help you. Let me know what you want to do. So I might ask, can you investigate to see what you see two instances I had a few minutes ago, and this is a real thing I was doing when preparing for this. I just thought, what would be a neat thing to ask and show all of you. I didn't really realize it was gonna be kind of neat. So it said, OK, um, I'm gonna describe instances, look at the topology, figure it out, spent a little bit of time and it said, well, here's a summary, and it said you have 54 terminated instances in the last 30 minutes, and I didn't know this was going on. It said you have instance churn happening in your auto scaling group. Instances are coming up and getting shut down right away. I was like, oh, oops, OK, so I didn't actually realize it was kind of actually a kind of fun question, but so it didn't just literally answer my question, it did, but then it also got a little bit of that extra credit and said, well this is funny, maybe you wanna know why. So it has that nice helpful curiosity to help me, oh yeah, OK, this is a problem I need to go fix that. So I, I from customers who have been using this so far they've also said just in different things whether they're glancing at that noisy topology map or they're looking at uh anything during an investigation or asking these questions we keep hearing that people are always learn a little bit. Something that they didn't realize they would learn that is interesting and relevant in some other way. So it's it's interesting that it does that little extra looking around corners for you. There's so much telemetry data out there, so much going on in logs, that's just, but you don't have time to go look at everything all the time, but the agent does. Another way you can influence the agent around future investigations are with runbooks. Now run books are sort of like uh the things that will uh the agent can use and reference when doing an investigation to speed things up or or steer it in a certain way. Um, they're kind of like, uh, today we released uh in Quiro this thing called Quiro Powers. Uh, Quiro powers are when you're using the Quiro IDE they're a kind of a combination of MCP servers and steering files and hooks and things that just like in a nice packaged format that help the agents specialize. In a certain tool, uh, that it might not just your agents are generally good but like broadly good but to help to specialize in something it's nice to package all of that up and so this is the same idea it's, uh, it's run books that that just little pieces of guidance that are just loaded on demand whenever the agent wants to look at it. Let's look at something you can add one. Um, or let's look at a couple that I find are useful. So I decided to write one for this that, that kind of just helps the agent, uh, speed up, maybe shave off a minute of an investigation. Actually, I think, I, I think I disabled the wrong book, and I don't think it was helping, but it doesn't matter. It's, it's, uh, I just for, for, for demonstration purposes. I kind of gave it an overview. And said, OK, here's where I'm storing all of my stuff. I was using logs in an MCP server. I was, I'm doing code deployments, um, through, uh, maybe through GitHub actions in this case, just to kind of give it a little bit of a head start on, on figuring it out. It'll wander around and figure out what I have, but this kind of, I just feel like it, it sometimes it helps ground it. So that's one kind of run book you could write, just give it nudge it in the right direction. I gave it a little bit of a run book for that bespoke MCP server for logs that I've been talking about. Um, I found that the agent, it was using the MCP server OK, but it, it kept querying for the current minute of logs, like the right now, like if it's, if it's like 2 o'clock, it was looking for logs from. 2 o'clock and I said oh actually I only push my logs like with a 2 minute delay so only look for logs 2 minutes ago and all that does is save a little bit of a back and forth. It figures it out but it's still I just give it a little bit more information about how to use the tool. Often that information is in the MCP tool documentation, but whatever you can, it doesn't matter where you put it. I also said, oh yeah, I, I, a, a good logging service like Cloudwatch logs will like will just package up stack traces and everything into the same line. Uh, in my case, uh, you could my MCP server will let you grip for for errors, but it didn't return any data after, uh, that, that line that. They gripped and found, so I said, hey, don't forget to ask for extra lines after, uh, so that you can actually see the stack trace. So I just gave it some tips. The details aren't super important. This is just how you can give the agent a little bit of hint if you see in the details like why didn't they get to this thing fast enough? Oh, let me help it next time. So great, uh, that's, uh, you can use Runbooks and, uh, to, to influence future investigations. You can learn about your environment and chat, and you can, um, yeah, and you can, and you can actually just learn about your environment by just talking, talking with it. OK, let's now talk about preventing future incidents, which sounds impossible, but it is. So, uh, one thing the DevOps agent does is prevention. If you go to the top of the ops council, there's a prevention of the operator app, there's a prevention tab, and what this does is just in the background, there's no real hurry, but in the background it's looking at your past incidents to look for patterns of like what, what seems to be going wrong that I can recommend you change that could prevent this class of issue from happening again. So in this case, I've been injecting a bunch of failures into this environment and it looked at the last, in this case 19, and gave some recommendations. The recommendations are categorized. They're like governance things, uh, infrastructure recommendations, observability. Notice it has no observability recommendations. I used to work on cloudwatch, so I'm pretty good at that. So I guess I passed the test for, uh, observability. Uh, great. So, uh, what, what, let's look at some examples of what it did find. Uh, just a bunch of stuff. I found, uh, some health checks stuff I should do. Let's, let's just explore a few of these. Uh, one is configure the min instances in service. They found some auto scaling setting that I had screwed up at one point in time. Uh, it said, hey, there was this change you made where you configured your auto scaling group in a dangerous way. It didn't immediately cause a problem, but you made it so in certain cases it could replace all of your instances at the same time. I don't want that, so this might have prevented me an outage from happening. It's like you should change it to to be the safer, safer configuration of your auto scaling group to avoid that if it were to happen that way it would keep instances in service all the time. OK, that's great. I could avoid an outage with that. I'll take that, and it gives some details on what I should do and how to make that change and where and that kind of thing, um, where to, where to look in your, it knows about the CDK stack, and here's the, here's where you should change it. I can paste this and give that to Kiro or some coding agent. All right, another type of thing that I found, um, in this case was. Um It noticed, oh yeah, this, I didn't show you this failure mode, uh, but I had kept messing with my Dynamoti B table permissions and it was finding this just fine, so it was saying, OK, you, somebody 10 times has keeps, that was me just testing this over and over again. Uh, has, uh, keeps breaking your Dynamo DB table policy, denying permissions and messing up everything, and so the recommendation for this that it gave was actually, hey, you should, um, well, he gives all the affected incident, and it was just telling me, hey, like lock your permissions down. Don't let this Denachek person keep breaking your permissions on your Dynamo DB table. So, uh, then another one, it said, OK, implement post post-deployment health validation with automatic rollback to prevent defective deployments from causing service failures. This was, um, it noticed that. I had broken the ping health check, um, so, uh, I just had a syntax error in my ping health check logic, and that was making it so my instances were launching and getting terminated, launching terminated, and said, well, you should fix that, uh, and you should also, um, uh, you should add checks to make sure if you break it in the future. That it will get rolled back automatically. So it's not only is it saying fix this problem, but also, uh, prevented this class of problem by having automatic rollback. Great idea. This incident now this remember when I said my instances were churning like they were getting replaced all the time. That's what this it was actually recommending the long term fix for that, uh, for fixing that. So it's very, very nice. It kind of links all links all up, uh. Great. So let's look at an, oh yeah, yeah, implement auto rollback, um, and this is, and it knows about my code. It says here is what you should change your, your launch script to to do, uh, and, and here's how to make it do the auto rollback. So pretty, pretty convenient. Last thing was it uh it just found this whole class of problems. I kept pushing those cash breaking bugs. It said somebody has been, somebody keeps pushing bad code changes, they keep making it to production. Can you write some tests and like have good responsible CICD going on here? It's a good idea. OK, so sure, but it doesn't make for as good of a demo, but uh yeah, that's the thing. So it says let's put some, some blockers in your pipelines, your deployment pipelines. Let's add some tests to make sure that you're on, you're testing your gamma environment, your pre-prod before prod, and if you do break things in prod, make it auto rollback. So just give me some nice uh recommendations there to just have better operational posture. So the prevention part, it, it kind of is finding improvements to my operational posture, uh, based on what it notices about the things that seem to keep breaking. It's all that is when it looks at it, it looks at all the past incidents and investigates and looks for patterns of things that you should improve. Great. So Bill, you wanna wrap us up um and, and, and talk about how to, how to get everybody started with everything? Yeah, so thank you, David, for that, uh, and, uh, and for all of you to, to be here. So we, as I mentioned earlier, we launched some public preview yesterday. And so what that means is it's available as a service in the AWS console. You can go there, get it set up, search on Google, uh, you'll find the docs, you'll find there's some links there that I've provided for you. Um, and try it out. Public preview means it's free to use. You might have some charges like if you're, you know, the actions that the agent takes to query some of your logs and traces, if those generate expense, you may have some expense, but we won't charge you for the agent itself. Uh, there are some limits, so you get, uh, 20 per account that you set up per set of agents that you, you deploy. Uh, you get 20 hours of investigation time per month and 15 hours of prevention time per month, so plenty to give it a try. So it's super easy to set up. I hope you saw that it has a lot of power already. Uh, I'll anticipate the first question. Are we thinking about giving it the ability to actually take actions in your environment? The answer is yes, that's something that we're working on, but we want to step into that. Uh, closely you should be able to see that there's a lot of promise there, uh, but give it a try. You can get it set up, uh, uh, running in a couple of AWS accounts literally in just a couple of clicks. Uh, start asking a question, so you know what it can do. Throw alarms at it, throw questions at it. Love to get you trying it out and give us your feedback. So again, thank you for your time. We have a, a, a few minutes for questions and we can take some questions on the side after.