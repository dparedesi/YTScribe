---
video_id: xDgGBSyZG3g
video_url: https://www.youtube.com/watch?v=xDgGBSyZG3g
summary: "In the session \"Accelerating AI innovation with NVIDIA GPUs on AWS\" (AIM251), Dwij Bhapa and Shrikar Reddy (AWS) join Ersin Yier (Adobe) to explore the infrastructure powering the next wave of Generative AI. Dwij opens by marking the 15th anniversary of the AWS-NVIDIA partnership, contrasting the early days of \"Gen AI 1.0\"—which prioritized massive pre-training—with the current landscape where inference compute is exploding. This shift is driven by three emerging workloads: Reasoning Models (which generate \"chains of thought\" requiring significantly more intermediate tokens), Multimodal Generation (video/audio production), and Agentic AI (where models execute API calls and interact with environments). Shrikar details the hardware innovations designed to meet these demands. A major focus is Liquid Cooling, now deployed at scale in the new P6e Ultra Servers to manage the thermal density of NVIDIA Blackwell GPUs. He introduces the P6 family, specifically the P6e GB200 Ultra Servers, which connect 72 GPUs in a single NVLink domain with over 10TB of unified memory, crucial for training multi-trillion parameter models. For scale-out networking, AWS has redesigned its EC2 Ultra Clusters with 51TB switches and a \"rail-aligned\" topology, enabling non-blocking petabit-scale connectivity across tens of thousands of GPUs. He also highlights P6b instances, optimized for distributed training with 50% more memory and 2x networking bandwidth compared to previous generations, and the G-family for cost-effective inference and agentic workloads that don't require massive GPU-to-GPU interconnects. Ersin Yier then takes the stage to reveal how Adobe leverages this infrastructure for Adobe Firefly Video, a commercially safe generative video model. He describes a sophisticated three-stage training curriculum: starting with static images (high efficiency), moving to low-res video for temporal dynamics (requires innovative parallelization), and finishing with high-res video for texture and alignment (extremely compute-intensive). Adobe’s inference pipeline, serving over 1 billion generations per month, is heavily optimized. They \"disaggregate\" the model, splitting heavy components like Diffusion Transformers (DiT) to run on powerful P-series instances, while lighter encoders run on G-series instances. Instead of a push-based scheduler, they use a \"pull-based\" global queue where nodes claim jobs only when ready, maximizing utilization in a heterogeneous cluster. Ersin concludes by showcasing Adobe Firefly Foundry, a service allowing studios to customize base models with their own IP. He demonstrates this with \"Cosmo's Laundromat,\" an open-source animation. By training on just 2 hours of clips derived from the 12-minute short, the Foundry model learned the specific characters \"Victor\" (a red-headed man) and \"Frank\" (a sheep) and the unique physics of their world. This enables creators to generate consistent, on-brand content using simple prompts—something impossible with generic foundation models."
keywords: NVIDIA Blackwell, EC2 Ultra Clusters, Liquid Cooling, Reasoning Models, Disaggregated Inference, Adobe Firefly, Agentic AI, Interconnect Topology, Model Customization
is_generated: False
is_translatable: True
---

Hi everyone and welcome to AIM 251. Today we'll be talking about the GPU platforms we're building at AWS and how these are powering recent breakthroughs in generative and agentic AI. My name is Dwij Bhapa and I'm a principal product manager at Amazon AC2. I'm joined today by my colleague Shrika Reddy, senior product manager at Amazon, and we're really excited to be co-presenting with Urson Yer, senior Director of Engineering at Adobe. Our agenda today is in 3 parts. First we'll talk about customer use cases in Gen AI, diving deep into the compute requirements for emerging use cases like reasoning, disaggregated inference, and agentic AI. Next we'll talk about the major investments we're making in our AI infrastructure to deliver the next level of GPU performance for these use cases. And finally we'll hand over to Urson to learn about how Adobe is leveraging GPU infrastructure at Amazon to develop new capabilities and generative AI for creators all around the world. But before we jump in, We'd like to commemorate the fact that this year, 2025, marks the 15th year in our partnership with Nvidia. Our partnership started way back in 2010 with the launch of our CG1 instances featuring Nvidia Tesla GPUs. You can see the diagram from our launch blog back in 2010, showing the main architecture differences between CPUs and GPUs. CG1 was mainly focused on use cases in graphics and high performance computing. But in the early 2010s, there are also major breakthroughs showing the benefits of GPU accelerated compute for machine learning. In 2012, a paper called AlexNet showed that neural networks trained on Nvidia GPUs could significantly outperform the state of the art in machine learning use cases like image recognition. We launched our first P series and G series instances in the early 2010s to support these emerging use cases in machine learning. The next turning point in our portfolio came with the launch of P3 instances, which featured NVLink, a high bandwidth, low latency interconnect connecting multiple GPUs within an instance. This meant that customers could train bigger models and more complex use cases. Around this time, the transformer model architecture was developed and quickly became the industry standard. Researchers discovered what are called the scaling laws, which showed that model performance in terms of model accuracy, improved consistently and predictably, the more data and the more compute that you dedicated towards training. This was a major inflection point in the industry because it showed that you could get consistent returns to your model performance by scaling your compute. And so we quickly saw that customer workload scaled from 10s or 100s of GPUs out to 1,000s or even 10s of thousands of GPUs for training. In 2020, we launched our EC2 ultra clusters to support this massive scale out on P4 and P5 instances. But of course the industry was just getting started. In the past couple of years we've seen the emergence of 1 trillion parameter models, context lengths out to a million tokens or more in production, and compute intensive use cases like reasoning. To support these use cases, we launched our first EC2 Ultra servers based on Nvidia GPUs earlier this year. To provide a bit more context on this present moment, let's dive deeper into the specifics of how customer workloads have evolved over the past couple of years. Looking at the industry two or three years ago, what we'll call Gen AI 1.0, our top customers spent the vast majority of their compute on large scale pre-training. Researchers used parallelism techniques like data parallel and model parallel. To scale workloads out to thousands of instances, leveraging the scaling laws to improve model performance. The next step in the process is fine tuning and then model distillation, where the size of the model is reduced before it's deployed to production. Smaller models are easier to deploy, and they cost less in production because they require less compute for inference. And so in inference, customers focused on single node or even single GPU inference to optimize the economics of inference at scale. So simplistically, customers spent the majority of their compute in training and fine tuning to optimize the compute required for inference. And of course this general focus continues in the industry today as well. But we're starting to see this paradigm shift in certain important ways. One thing we've observed over the past year is that the compute requirements for inference have increased significantly. This is partly driven by the fact that demand for inferences is increasing, but it's also driven by the fact that inference use cases themselves are becoming more compute intensive. There are 3 use cases in particular that are driving the need for increased compute in inference. The first is reasoning, where a model takes a problem and breaks it down into intermediate steps before formulating its final response. The model therefore needs to generate a lot more intermediate tokens to get to its final answer, which requires more compute. The second is multimodality, where a model doesn't just respond with text but can generate a broader set of outputs like images, video, or audio. Urson will talk a lot more about recent developments at Adobe in this area. And the third use case is Agentic AI Where a user no longer simply uh interacts with the model through prompt and response, but the model can actually take action on the user's behalf, calling APIs or tools, or even interacting with other models in multi-agent configurations to solve complex problems in production. So all of this is driving the need for more compute in production. But we're also seeing customers dedicate a lot more compute towards reinforcement learning or RL. Customer scale RL pipelines to ensure that models continue to stay aligned with user intent. As they operate in a broader set of environments and generate a broader set of outputs. So Shrika, let's dive deeper into the compute requirements for each of these use cases. Yeah, so the first major trend that we want to focus on today is the growth in reasoning use cases. Several of our customers are increasingly implementing chain of thought reasoning. During inference where a model breaks down a complex problem into simpler steps in order to improve the accuracy of the model across a range of benchmarks. Reasoning models. Reasoning models set a gen um generate a set of intermediate responses that are iteratively fed back to the model for it to generate a final output response to the user. As such, the number of intermediate tokens generated during reasoning is significantly higher than that of a traditional LLM. In the last year or so, we have also seen that train of thought, train of thought reasoning has been evolving into tree of thought reasoning, which closely resembles how humans think. In tree of thought reasoning, a model explores multiple different paths in a structured manner in order to figure out a solution to the problem. Reasoning models also generate an interesting trade-off between training time compute and test time compute. As we're all aware, The accuracy of a model can be significantly improved by having it trained extensively. But the same can also be achieved by using significant compute power during inference, enabling the model to take time to think, validate its response, and, and refine its answer. An interesting trend that we've seen from reasoning is that customers' compute requirements and G2 memory bandwidth requirements is continuing to grow to optimize for these reasoning use cases. The second major trend that we want to focus on today is the growth in multimodal LLMs. A couple of years ago, customers used dedicated models for vision, text, audio, and video. However, in the last couple of years, we have increasingly seen customers train and deploy multi-modal models that are capable of crossing input and generating output across a variety of models. Depending on your use case, you can have a range of input and output sequence length. For example, if you are trying to summarize a 200 page document into one paragraph, you're likely going to have a very large input sequence length. On the other hand, if you are trying to generate a video from a simple text prompt, you're likely going to have a very large output sequence length. Scaling input sequence length, where the input tokens are processed one at a time, is usually very compute intensive. And scaling output sequence length is usually more GPU memory bandwidth intensive. In order to optimize performance across a range of input and output sequence lengths, you're seeing, we're starting to see that customers are increasingly disaggregating the two major phases of inference. By running them on dedicated GPUs. In the first phase, called prefill, which is more compute intensive, the focus is on processing tokens all at once and then generating the first output token along with the initial KV cache. And in the second phase called decode, which is more GPU memory bandwidth intensive, the focus is on generating the subsequent tokens using the initial KV cache generated during the prefill phase. A key bottleneck that typically surfaces during disaggregated inference is the transfer of the KV cache between the prefill and the decode nodes. As such, a key compute requirement that is that is arising from these use cases is the need to have the best inter-GPU connectivity to optimize performance for multimodal use cases. The next trend that we want to focus on is agentic AI. Several of our customers are increasingly focusing on Agentic AI as the next frontier of AI of, of, uh, of AI across enterprise and consumer applications. In agentic AI, a model not just interacts with the user, but it also interacts with the environment through either querying a database or through APIs and tools, and also interacts with other models before performing an action on behalf of the user. Some of the actions performed by the agent are CPU intensive, such as querying a database or data preprocessing. Whereas some of these actions performed by the agent are GPO intensive, such as generating the output tokens. As such, a key compute requirement that is arising from these agentic AI workloads is the need to have heterogeneous compute of CPU and GPU cores co-located in order to optimize performance for these use cases. As customers want agents to take actions on behalf of the users, they also want models to better align with user intent. And this is where reinforcement learning comes into the picture. Reinforcement learning works, it includes inference in the loop and works by having a model learn by trial and error. In reinforcement learning, a model observes the environment, takes action, and gets feedback in the form of either a reward or a penalty. The model then uses this feedback to update its model weights and model policy so that it can make better decisions in the future with the goal to optimize or maximize the cumulative reward over a period of time. For example, let's say you're trying to train a robot to navigate through a maze. When the robot starts off, it makes random movements. It may hit a wall, and then get up, get penalized. It then knows that in that state, it is not supposed to take that action. And when it makes progress towards a goal, it gets rewarded over time. The model figures out, or the, or the robot in this case, figures out what actions under what states lead to positive outcomes and what actions lead to negative outcomes. Through multiple trials and errors, the robot. Um, optimizes its strategy to make sure that it, it can navigate through the maze in the least amount of time, through, uh, the least number of errors. RL RL algorithms also work efficiently in complex environments that have multiple rules and dependencies. RL algorithms can also um quickly adapt to ever-changing environments by constantly updating their strategies to optimize results. An interesting trend that we've noticed from um, from the recent past is that customers are increasingly using the same cluster of nodes for pre-training, for inffeerencing, and for reinforcement learning workloads by dynamically scaling up and down as needed. Now that we've talked about some of the trends in the GAI industry and the compute requirements these are driving, let's talk about some of the investments that EC2 has been making in the past year or so to meet these evolving customer requirements. Thanks Shrikar. So emerging use cases like reasoning, disaggregated inference, and agentic AI require more compute and memory in production, but they also require better interconnectivity between GPUs, both scale up and scale out. Let's talk about some of the major investments we're making in our AI infrastructure to deliver the next level of GPU performance for these use cases. A major area of focus for us over the past year has been liquid cooling. As GPUs have gotten more powerful in recent years, they have also gotten a lot more power hungry, generating a lot more heat and requiring more efficient methods of cooling. Liquid as a conductor is a lot more efficient than air, meaning it can more efficiently dissipate heat away from GPUs. We launched our first at scale liquid cooling platform this year through our P6E Ultra servers. So let's dive a bit deeper into how we actually implemented this technology at Amazon. It all starts at the chip level, and as you can see from the picture there, we have a GPU cold plate that fits right on top of the GPU which carries cold liquid through the central tube, dissipates heat away from the GPUs, and then carries the warm liquid out through the tube on the side. We actually co-designed a custom cold plate with Nvidia with a focus of serviceability and reliability in production. These liquid cooling plates have quick disconnects so that our our data center technicians can quickly replace compute trays that need maintenance and production while maximizing uptime and availability for customers. We also carefully selected every material in the liquid cooling loop to reduce the probability of corrosion causing liquid cooling leaks in production. The liquid then flows out to what's called a coolant distribution unit or a CDU, which is constantly measuring metrics like temperature and pressure and also other metrics like turbidity and conductance. This means that we can continuously measure the health of the system in production. The liquid then goes to a heat exchanger to get cooled down. And at Amazon we actually invented a new solution called the INR heat exchanger, or IRA Checks, where we can co-deploy our cooling racks alongside our compute racks in our data centers. This means that we can a lot more dynamically and flexibly leverage existing data center capacity to scale our GPU compute in response to customer demand. But what are the main benefits to customers from liquid cooling? The first main benefit is higher compute density. With liquid cooling, we can fit a lot more GPUs a lot closer together, meaning that we can pack more GPUs in stand-alone compute products like EC2 Ultra servers. We'll talk more about these in the next slide. The second benefit for customers is that our focus on serviceability and reliability means that customers can enjoy higher uptime and availability through our design. And finally, solutions like IRHX means that we can scale quickly in response to customer demand without having to constrain ourselves only to those options that can support facilities-level liquid cooling. So liquid cooling is a foundational technology that provides better GPU performance for customers. Let's now talk about how customers actually access this compute. Through EC2 products like Ultra Servers. An EC2 Ultra server is a collection of EC2 instances that share a high bandwidth, low latency GPU interconnect, meaning that a mesh of GPUs across instances can operate as a single powerful stand-alone unit of compute. This means that we can deliver an order of magnitude higher compute to customers. Our P6E ultra servers provide more than 20 times the compute under NVLink compared to our prior generation P5EN instances. So Ultra Servers represent a new tenancy model and a new customer experience in EC2, and we launched a dedicated set of APIs to help customers manage every aspect of this customer experience, from capacity provisioning to monitoring and observability to other aspects like topology and auto scaling. We also implemented a custom nitro-based design to managing the envy switch trays in the ultra servers so that we can securely and dynamically partition the envy link domain, providing greater choice for customers across ultra server sizes. And finally, we had an emphasis on ensuring that every aspect of this Ultra Server customer experience integrated seamlessly across the GAI stack at Amazon from EC2 services like topology and auto scaling to orchestration services like Elastic Kuberneti Service and Parallel luster, all the way to fully managed services like Sagemaker Training and Hyperpods. Our primary focus was to ensure that this powerful compute product was as easy for customers to use right out of the box. So what are the main benefits for customers? The first is that customers can train and deploy bigger models at higher context using ultra servers. Our P6E ultra servers have over 10 terabytes of GPU memory under NVLink, meaning there's plenty of GPU headroom for customers to scale into the multi-trillion parameter range. Customers can also use the higher compute within the Ultra server to optimize performance for use cases like reasoning, as we spoke about earlier. And finally, customers can leverage the higher aggregate memory bandwidth under NvyLink to optimize performance for decode heavy applications like video generation. So Ultra servers provide up to 72 GPUs connected over NVLink, but as we know, training workloads can scale to thousands of GPUs, and that's where our scale-out network or our EC2 Ultra clusters come in. An EC2 ultracluster is a petabyte scale network that connects tens of thousands of GPUs in a data center. Ultra clusters are non-blocking, meaning that each GPU can drive its maximum bandwidth without causing contention within the network. We completely redesigned our ultra clusters this year with 51 terabyte switches and 400 gig links, up from 12.8 terabit switches. What this means is that we can connect more GPUs with fewer networking components, meaning bigger GPU clusters at lower latency. We also implemented a rail aligned topology with our ultra clusters, which provides optimal performance for collective communications that are used in machine learning. One of our main design principles for ultra clusters is resilience at scale, and so we implemented design features like tor overprovisioning and backplane redundancy, as you can see in the diagram there, to ensure that customers continue to see consistent performance, even if there are events like links or switches down in the fabric. At the instance level, customers provision instance networking using elastic fabric adapter or EFA. We launched our 4th generation EFA this year with our P6 instances, offering higher throughput for customers at 20% lower latency. EFA leverages our scalable, reliable Datagram or SRD protocol. Which is purpose built to optimize scale out in a cloud scale environment. SRD uses techniques like intelligent multipathing and out of order execution. So that we can deliver a networking protocol that can dynamically adjust to events like congestion or links down in the network. So what does all of this mean for customers? Firstly, with our new ultra clusters, customers get bigger GPUs at lower latency from the new networking switches. The new hardware also means that we can provide higher bandwidth at lower cost. The rail align topology offers optimal performance for common collectives in machine learning workloads. And finally, design features like tore overprovisioning, backplane redundancy, and SRD mean that means that we can deliver an end to end solution that is resilient to congestion and hardware failures, providing consistent performance for customers as they scale out to tens of thousands of GPUs. So these are the major investments we've made in our scale out network, in our EC2 control plane, and in foundational technologies like liquid cooling. Let's now learn more about the Nvidia platforms that we announced at Reinvent this year that leverage these capabilities to provide the highest GPU performance for customers. Yeah, we made a couple of really exciting announcements in the last month or two, including one exciting announcement just yesterday with the P60 GB 300 Ultra servers. The P6C GP 300 Ultra servers are powered by the Nvidia Ultra Nvidia Blackwell Ultra GPUs and offers 72 GPUs within one NVin domain. They offer the highest GPU compute and the highest GPU memory. With an EC tool for customers to train multi-trillion parameter models. They are ideal for use cases like Reasoning and production, as well as for training and inferencing multimodal, uh, large multimodal models. The P60 GP 300 Ultra servers offer 20 terabytes of GPU memory within one unveiling domain. Just to put that in context, this is 30 times the GPU memory offered on P5 instances, which only launched 2 years ago. We're also starting to see customers use FP4 for inference, and the P60 GB 300 Ultra servers offer 50% higher FP4 flops compared to the P60 GB 20 Ultra servers that we launched just a few months ago. Also want to highlight a few design aspects of the P60 GB 300 ultra servers. The Ultra servers feature the Nvidia superchip architecture in which the GPU and the CPU are co-located within one compute domain. For customers, for uh for G A workloads, um, to provide heterogeneous compute for GI workloads that can take advantage of CPU and GPU cores in parallel. The, the Ultra servers, um, or the various compute traces within the GB3R Ultra servers are connected in a tightly coupled mesh using Nvidia NVLink. We leverage our nitro-powered head node to manage the NVLink switches, which means the NVLink network is completely managed within the trusted nitro domain. We also developed a set of dedicated APIs for the Ultra servers for customers to be able to provision capacity using capacity blocks, as well as for them to monitor the health of their nodes, to monitor the health of their unveiling fabric, as well as for customers to determine the instance topology within the Ultra servers. And finally, um, we remain extremely focused to make sure that the new APIs and the experiences that we developed for Ultra servers are seamlessly integrated across the generative AI stack. And just last month, we also announced the Amazon EC2 P6B300 instances. These instances are also powered by the latest Nvidia Blackwell Ultra GPUs and offer. 8 GPUs within one Nvili domain. These instances offer the highest performance within an NVA configuration at AC2 and are ideal for training and inferencing mid and large scale G AI workloads. There are three important customer benefits of the P6P30 instances that I'd like to highlight. These instances offer 50% more GPU memory compared to the P6B2R instances that we had launched back in May. Giving customers the ability to deploy bigger models within a single GPU or within a single unwilling domain. This reduces communication overhead and model sharding. The P6B3R instances also offer twice the networking bandwidth compared to the P6B2A instances, supporting up to 6400 gigabits per second of EFA throughput, making them ideal for large scale distributed training workloads. And similar to the P60 GB 300 Ultra servers, the P6B300 instances also offer up to 50% more FP4 flops compared to the P6B2100 instances. These instances are also the first, not the first, but these instances also offer a dedicated. Nitrocard for north-south north-south traffic supporting up to 300 gigabits per second of ENA throughput to access remote storage services like S3 Express one. They also offer twice the CPU to GPU bandwidth compared to the P6B20 instances. Further improving latency for large scale uh training and inference workloads. At DC 2, we prioritize giving customers the broadest set of NVIDA GPU instances, but we do acknowledge that it can sometimes get quite overwhelming to look at a bunch of specs and figure out what the right instance is for your workload. One way that we think about segmenting our portfolio at EC2 is on the left side, we have the G family instances, which mainly comprise of the G6 and the G6E instances. These instances are ideal for small and mid-scale training workloads, for agentic AI workloads, graphics intensive, and spatial computing workloads. The G family instances are also ideal for models that can fit within a single node or even within a single GPU. So customers don't really need high inter-GPU connectivity or high GPU to other component connectivity. This enables us to optimize the infrastructure or simplify the design and remove the PCI switching layer. Giving customers a cost optimized option, enabling them to strike the right balance between performance and cost. In the middle, we have the NVL 8 based Pfa instances. These instances are ideal for training and inferncing mid and large scale generative AI workloads. They offer up to 8 GPUs within one and willing domain. And come with the most powerful Nvidia GPUs. They also have the MLA chip interconnect at the bottom, which provides high GPU interconnectivity between any two GPUs in a single server. With P6B3 instances, for example, any two GPUs can talk to each other at 1.8 terabytes per second. So if you have an expensive communi communications, um, uh, call in your algorithm, these instances become incredibly useful. And then finally, we also have the switching layer in the middle. In our design, the GPUs and the EFA devices share the same switches, which means the GPUs can talk to the EFA devices and over the network without having to go through a CPU, which further improves the overall network performance. So overall, if, if, if your workload needs the latest and greatest GPUs, need the needs the highest inter-GPU connectivity, and you want the ability to scale out, the NVL8-based Pfamily instances become an ideal option. And on the far right, we have the P family EC2 Ultra servers. In the Ultra servers, we have several compute trays that are interconnected in a tightly coupled mesh through NVIDI in wheeling switches. These Ultra servers provide the highest performance in generative AI and are ideal for training and deploying multi-trillion parameter models. To summarize, we looked at some of the recent trends in the generative AI industry and the compute requirements that these are driving. We also talked about some of the investments that we've been making at DC2 to help meet customers' evolving requirements. I'll now hand it off to Urson to talk about how Adobe developed Firefly on AWS. Thanks guys. Thank you. The first part was, uh, quite a bit technical, and these guys have been preparing for a while, so I'm just gonna take it down from here, setting your expectations. Um, the, uh, the way the compute evolves is, is already fascinating, and we have been hearing that, uh, throughout this conference, uh, here at Rayen. I'm gonna talk about the application layer about and and particularly 11 thing that we care about at Adobe which is the video models of course and it's not only the uh the AI slop that you see uh on the web today but more so that how do we actually product. Auctionize and and make it useful for our customers and our customers especially at the high-end media and entertainment companies studios um we bring we bring this product um Firefly Foundry very recently in about 6 months ago. And the way Firefly Foundry works is we take our base models which are commercially safe and we really run them through training on on the customer's IP so I'm gonna go into all of that and I'm gonna talk about a little bit referring back to the to the talk in the last half hour, um, how the infrastructure impacts what we do and what we can do, right? OK, so we're gonna start from the training of our base models just to set a level set with everybody. The way we do base model training is really what we call commercially safe, and that means that we don't only have the, um, training right rights to the data that we train on, but we also moderate and we, uh, eliminate any trademarks and, um, any brands from the from the content that we use, uh, and then of course we we curate the data. Uh based on safety, fairness, robustness, um, which then results in, in, uh, commercially safe models when you think about that type of a data scrutiny, you end up with a model that's not necessarily extremely powerful, but you know that it's commercially safe and when our customers want to be able to generate something that's in their IP space, then they have to customize so we will touch on that in a bit. Let's walk through how we train and what type of architecture we use for our base models. So when we do curriculum training, uh, we generally walk through in our pre-training pipeline, right? 33 steps that those 3 steps is after image, uh, based model is trained on images where computer efficiency is high, better utilization of available data, we have a lot of, uh, commercially safe stock data on the imaging side and basically it then stabilizes the model, especially the diffusion layers, the IT layers, um. That allow the model to learn more and more of the special patterns early on and then we move to the dynamics so then we, we start adding uh low resolution video to the uh to the training that helps the model to capture the large scale temporal dynamics, right? And so, uh, not only that, but also it's especially, uh, efficient on the compute side. You guys have just seen the, the GB 300, uh, PCE, um, you don't have to go to that architecture for this step because you're using a lot less compute and, and you're very efficient. But then as soon as you start going into the, the medium resolution videos and you start. Adding them then um on the model side you're starting to learn uh facial mimics you're starting to learn fluid dynamics but on the infrastructure layer when you look at how you need to do parallelization uh HSTP FSTP comes into play and really wrangling all of that is is uh uh quite a bit of uh both time consuming and, and, um. Uh, resource consuming, um, structures, so I'm, I'm really looking forward to getting on, uh, on the larger, uh, setups of the future. And then finally when we add the high resolution video this is where we do post training if you will, and that's where we do, um, texture quality, uh, and, and prompt alignment, uh, reinforcement learning. Um, as well as specialized even forks of the post training because we do from the same base models that might have been trained until mid-training we fork off and we create, um, image to video applications, video extension applications, and text to, um, text to video applications as well. We, we do all of this training and, uh, image and audio and other media modalities on, uh, primarily our AWS infrastructure. I'll walk you through what we use. We use, uh, at the very low level we start from bare bones EC2, but we actually. Add the managed Kuberti layer which is um the EKS we are on EKS with AWS um for our training instances completely on uh on top of that that tooling we actually build uh our own training um. Job management system and and that particular job management system allows us to be able to do three things very efficiently. One is what we call the day box sessions. These are, uh, training, um, uh, sorry, uh, these are developer sessions that our developers or researchers use on the same machines right before starting and kicking off a batch training we can do day. Uh, uh, batch, uh, offline jobs that when, when the computer's empty and then of course the priority jobs are the larger, larger training jobs so to double click on that a little bit we, we are, we're in a unique state where, uh, we have, uh, more than maybe thousands of, uh, folks that touch the cluster of, um. Hundreds of thousands of GPUs and, and these are researchers, these are engineers, these are service engineers and product teams, and what happens is really you need an an efficient job, uh, management system, right? so projects and quotas and, and how do you manage that and how do you create priority jobs so that, uh, uh, top, um. Very important large scale training jobs don't die is basically what that what that box is the multi uh multi-tenant training scheduler that's something in-house that we built, um, and we can talk to you about that, uh, later if, if you're interested, but basically what that allows us to do is we have a heterogeneous set of compute and, and the jobs are distributed into that compute based on those priorities and, and, and quotas. Um, on the. On the scaling side, uh, we, uh, I really wanted to bring something into perspective, right? If you look at the, the video models that we have today, uh, Firefly video models, it's really when you go to the highest resolution of the video you're talking about 10% of the iterations maybe and maybe not, so that's really the last set of the, the post training side, but it actually from a flops teraflops perspective. You, uh, is the 90% or more of the training, so, so that's where the, um, uh, GPU failures, the hardware failures, the node failures, networking failures, everything comes into play, right, um, and you really need to consider auto recovery on the software level. You have to solve that because essentially you're building a, a, a rocket from, um, hundreds of nodes. Uh, which is consumer grade hardware and then, uh, uh, failures are, are inevitable. So how do you actually make that, uh, rocket keep flying while hardware is failing every now and then, quite, quite often actually. So we use auto recovery. This is also something we have built in house. Um, we do pre-flight checks on the hardware and then while the, the hardware is running, a job is running on that hardware, uh, we actually consistently drop nodes and bring nodes back up into the job, uh, if necessary, when necessary, the, the, uh, the hardware fails and then of course postflight checks just to keep the, the san sanitary, uh, state of the cluster, uh, in a good state. Uh, on the parallelism side I talked about this, uh, when I talked about the high resolution video which is when you go up to thousands of GPUs and more, then you start, uh, thinking about like HSTP which is something that you wouldn't necessarily think about at the earlier stages, uh, on the smaller set of set of GPUs, FSTP works, uh, uh, a lot better is, is generally what we do, um. The on the data side. In general, the FB 8 has always been the goal, and this is where you see generative media forking from, uh, from where the LLMs and the state of the art in LLMs are, um, large scale LLMs you can go even, uh, even lower, or, um, uh, bit level, but. Uh, for us on the media side, the, uh, the bed representation is really important for high resolution when you think about movies that you see on, uh, on TV or, um, or in the cinemas, what you're really seeing is over 4K and 1416 bit depth, so that depth resolution needs to be represented and, and that's, uh, today we use a mixed precision with FB 8 and BF 16 to be able to, uh, to get, to get there. Um, just a few interesting, interesting bugs, and, and, uh, this is more of the, uh, visual form part of it, but when you're training video models, these are the four that I'm gonna show you is, is most of the time what you see. There's almost always this, this confetti bug, uh, that happens with fast motion. Whether that's like raining uh or uh the uh opening of the wings, um, dancing of course is is uh really really tough and uh when you look at people in front of the camera for a while they actually start they they actually should be blinking of course but they don't blink, uh or uh initially and. And those are the bugs that you need to catch. You can, you can see how hard it is to catch this bug, for instance, from a metrics perspective, automation perspective, right? So, um, it's really not, uh, at the quantity, uh, quantizable level. It's at the qua uh, quality level, and then, of course, exit, uh, um. Expressions generally end up being like exaggerated. These are early levels of training, so that's what we fix at the post training side of things with really, really high quality data that focuses on this type of data. OK, so on inference, um, I'm gonna talk about a little bit of the anatomy of a, uh, of a general inference pipeline, right? So what you really have is not just the model that represents the particular, um. Uh, video generative model, but you add a lot of additional models and, and business logic around it, so text and, and images go in if this is an image to video model and the video comes out, but, but along the way we actually split not only the business logic itself but also the model itself to be able to optimize. How we use the the machines in the cluster, uh, in the cloud, what I mean with that is if you think about the the encoders at the IT block which is the the heaviest part of a video generative model generally and then if you think about then the decoder and what you do from a business logic perspective on the safety side, every single box here that you see on the screen. Requires a vast amount of difference, uh, different, uh, different, uh, compute, so the DIT blocks will be your, will be the ones that you would put on say P6s or even, even more, right? And then you can of course for some of the encoders and decoders get away with even, uh, uh, 810Gs which are which are G5. So the way we structure our. Uh, our inference pipeline, the in-house built the inference pipeline, is each and every single, uh, module that we break out has its own, uh, set of chews and machines dedicated to them in the cluster and. Those machines that are queued are picking up uh jobs only for that particular box, right? So the DIT block would be going into the say P6 queues and then P6s would be crunching on the DIT block and then we do auto balancing across the board as well so an entire inference pipeline could be split at any given time on 20 different machines. Um, and, and on the IT block specifically I already talked about the, the fact that this is the most uh compute incentive side of it, but, um, when you look at. The unoptimized. Model that's coming from Pytorch on on training generally where we where we work you're thinking you're looking at one step taking about maybe even 3 or more seconds right? and 40 steps would take. Maybe even 2 minutes or more, uh, with a medium sized model and if you don't optimize this really you're burning a lot of compute that's number one and uh and the customer experience uh is not great so we do heavy optimization, um, and distillation TRT run time optimizations kern uh specific in-house built kernels and then we switch to mixed precision as well at this point to even further push the push the gains there. Uh, from an architecture perspective, the way we have built our, uh, inference, um, stack is similar to what we do on the training side. We build on top of EKS, but we have what's called ethos, um, that builds on top of that which, uh, which brings us to compliance and across the, um, the, the customer facing side of things. It also brings a lot of, uh, additional safety and security. So, uh, on top of that, that ecos, uh, ethos distribution, we build our, um, uh, ML compute frameworks essentially and then finally we do, we do ML inference on top of those frameworks. So depending on the model type and how it's optimized, the, the particular framework will be, um, will be different. So few things to talk about here uh I, I already mentioned that the set of compute that you need for particular parts of the pipeline even in a single model is different and that's how you get to really high um uh utilization of your compute. Um, but communication ends up being a, a, a bottleneck if you don't design that particularly well. So what we do is we really create a global queue for each of those, uh, those different broken out pieces of the model, and then we pull them. From that, uh, the machines themselves pull them from the global queue when the machines are empty and ready for the next job so we have seen that work a lot better rather than the pushed based method where a global master pushes into each node thinking that that node should take the job, um. Uh, and, and the custom auto scaler that we have that, that particular set of broken out network that we distribute on the inference clusters, that's only one of the, one of the networks at any given time we serve. We have about 100 of these services at any given time with many different models. ities many different applications, so it becomes very quickly, um, not really easy for the uh uh an SRE team to go in and optimize those those uh number of machines that should be assigned with each service and then assigned to each of the. Uh, the working, uh, worker groups, right, so that's why we built an auto scaler that basically handles that entire distribution of the resources towards the services and then down to the worker groups itself, uh, automatically and then the only job for the SREs end up being pulling in more, uh, nodes if needed from, um, if you're tapping into too much on demand that we reduce that by, by bringing in more reserved. Alright, so that's the base model training and base model services. Remember at Adobe we have, um, more than a billion generations every month. So the scale of that model and, and the reason that we optimize it at especially on the inference side thinking about bottoms up from the hardware is because every single optimization we can squeeze in matters a lot, right, due to the volume of, uh, of generations that we serve. Now on the, on the foundry side, and I'm gonna talk to you about what Foundry is in a, uh, maybe showing it in an in an Adobe stack, so what you see at the bottom is our foundational AI models, right? And the ones that you see in the dashed is basically the ones that we have brought in from third party but think about the ones that are Adobe and specifically for the video model that I showed you, um. They are serving a specific need as a base models and and commercially safe, but if you are um willing to put in your IP and you really wanna still stay commercially safe but use your IP as well, you didn't have, uh, anything other than what we call custom models before, which was essentially how you would bring in maybe 20-30 images and uh uh self fine tune and Laura. Uh, image model on top of our base model and would be able to represent a, a small, um, new, um, character or maybe a background. What Foundry does is Foundry takes that that customization to the levels of full on, um, foundation model customization, right? So we walk back in the in the training timeline of the model, start from mid training or maybe, uh, post training. And then add a lot of uh IP which could be coming from a studio it could be over petabytes of of data so it's really a very rich high resolution high quality data that that the the studio potentially owns the IP for and is able to use right? and then we we are able to then create video models for them that um. Represent the high quality demand that uh that they have for production and makes the model not only production ready from a quality perspective but also production ready from a legal uh perspective as well because it's the commercially safe data that Adobe has and as the studio customer the IP that you have so I'm gonna show you one example here so what you see here in this video is uh is an IP that's generated um. Using both hand drawn figures and then uh. Is, uh, replicated across the board for 3D representations, um, and further down a, a full series of, um. Motions motion characters and, and voices and sounds are generated for all those characters so it's just the video is quickly really summarizing a typical um IP generation character design character development and then further on generating um content for the environment and really content for uh for the episodes of this IP so when you think about all of that IP. And bringing all of that into many episodes of this this regenerate franchise into the model and then also the artist's drawings, the artist's textual descriptions of what and how the environment should feel like, what is the look and feel of. Characters and their motion and uh and all of that it's it's really rich data and it's not only just scrape data off the video parts of it right? and I'm gonna show you if you take this data and mold it into our models with the um. Uh, with the, uh, commercially safe data, uh, what the results are, so the first few seconds of this video you will see is going to be, uh, regenerates franchise fully generated from a foundry model, and then it will walk through even making that IP, uh, into different styles as well, which is the power that's coming from the base model. Taking selfies in front of the hideout, what could possibly go wrong? Who is this guy? We got to get out of here, fate, Quick, take another selfie. I'm not enjoying this. What? So this is the original style of the, of the franchise. And you can see the, the, the power of training, uh, a foundation model on your IP this way because, um, this completely enables the creative team to be able to create shoulder content to be able to revitalize their content and, and post it onto social channels, um, and, and even go to the levels of, uh, uh, user generated content UCG to bring to their users as interactive, uh, interactive content, right? Um We'll go into one more example. So this is Cosmo's laundromat. This is something actually you can find online. It's a, it's an open source blender project that was, uh, done 10 years ago. Uh, it's an animation short on YouTube, I believe it's like 12 minutes or so. So we took that. The team took that data and, and put it into clips. Uh, we, we created, uh, I believe in total about 2 hours of training data. From the 12 minute um uh uh original by, uh, clipping in different ways and, and, uh, cutting it, cutting the video in different ways as well and then train the foundry model with that and these are the results that you're seeing that are typical generations basically from that foundry model, right? And I'll show you one with a with a particular prompt for instance so Victor. Is the um is the red headed man and Frank is the uh black sheep and they are in this mystical world of floating islands right and uh and what you're seeing here if you really read the prompt and and look at that motion you're controlling the camera you're controlling the actual characters that's in franchise uh and it's extremely powerful to be able to create clips like this um the these video models are good. Enough from a from a com uh base model perspective but even even if you look at the strongest open source models you wouldn't be able to create these results exactly for these characters and that's because when you when you learn on an average basis and not necessarily the specific franchise, you're not really thinking about Victor and Frank, you're thinking about the entire world of knowledge and, and that's good for the base model but not necessarily for high quality IP, uh, content generation. So here's another example where uh Victor and Victor and Frank again are prompted by just um uh by just their names as well so it's a lot of like um uh a lot less sorry um. Mental load for the creative that's working with the system as well. All right, I'm gonna stop there. um, thank you for showing up to the last session of I think the last, uh, day, right? Uh, and we can take either questions or, uh, or we can meet with you as well offline after the, uh, after the talk and please do fill out the survey that should pop up on your mobile phones to give us feedback. Thanks everyone. Maybe we can take. Questions.