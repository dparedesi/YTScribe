---
video_id: Y2eyW6o_yV4
video_url: https://www.youtube.com/watch?v=Y2eyW6o_yV4
is_generated: False
is_translatable: True
---

Good afternoon everyone. So let me tell you a story. We've all just now finished Thanksgiving. You remember that evening after you're crawling into bed, full of turkey, stuffing and stuffed. And falling asleep, absolutely cashed out. And there's that little buzz that starts to like wake you from sleep. And it gets louder and it gets louder, and then it's an alarm, and you wake up and you look at your phone, and it's a page and it says engineering response needed. It's 2 a.m. How many of you guys have been there before? Same. So, You think about this. There's a customer that called that, right? Latency doubled. Their requests are dropped. And the customer is furious. And now you're online, sleep deprived, desperately trying to figure out what happened and why. We've all been there or have enough experience to know about that. We're gonna talk about the groggy engineer today. And we're gonna talk about finding the noisy neighbor. Customer performance patterns at scale. I'm Nick Hefty. Uh, I've been at Zendesk 7.5 years, and I've worked in the customer support portion of the company and moved over to software engineering. So I've kind of got a unique bridge between being uh, having being the one that calls that incident. And now, occasionally one that jumps on those incidents, to try to help see what's going on. And I'm gonna talk through a little bit today about how Zendesk's journey. Through observability, has found a way that we can give the engineers a little bit more rest. So, I'm gonna step away from that for a second. We are at Reinvent. Let me briefly tell you about some of the AWS stuff we have, just as I go through all of this. Um, we're an Aurora shop. We use elastic cash, EC2 instances, um, a potted infrastructure. Uh, and we heavily leverage AWS. We are an AWS partner, and we really, really do like working with them. They're fantastic. We moved from self-hosted to AWS. A big reason being scalability. And they're very good for that But We've got 100,000s of customers. That leads to challenges. How many of you have an infrastructure map that looks like that? Uh, um High cardinality. Because we have so many, oh, back up, Cardinality. A whole bunch of stuff at the same level, high cardinality being a lot of those things, at that same level that you're trying to look at. Just so that we're all caught up. We have high cardinality, and we struggle with that from time to time. We're trying to look at parts of an infrastructure that a small subset of customers are interacting with, and we need to be able to figure out when one of those goes sideways, what we can do to help the customer and protect those other customers on a shared infrastructure. And solving a problem for high cardinality. Comes with a bunch of other challenges, costs being one of them. So this is. It's extremely high level. Kubernete's infrastructure. For us And We need to balance out those loads across our infrastructure for all of those customers on our charts. So when you're thinking about. Managing for high cardinality. I promise I will get back to the noisy neighbor in a minute, but I gotta set the stage. Balancing for that, you need to look at what things. It explodes along with those hundreds of thousands of customers. So, how can you mitigate some of this, retention. How long you retain what you're seeing. How much you sample of all of that, right? Is it a process that you really need to see a lot of, or is it just something that you want to track over time and is 1% enough? And data sensitivity. Is this something that's a security risk, you need to capture all of it? Is this something that is a process you, again, you need to track over time. And so We've all been in a situation similar to this, where you're balancing cost and observability. It's kind of a big teeter-totter, right? And the more you engage with it, sometimes the scarier it is. I'm hoping I can tell you a little bit of our journey through that, and make it a little bit less scary and frustrating. It, it, a wise engineer learns from their own mistakes. It takes a fool not to learn from the mistakes of others. Hopefully I can tell you some of my mistakes, and you can take those away so you don't make them yourself. So, one more piece of things to set the stage. What is a noisy neighbor? Max did a great job of describing this initially. But I'm gonna use an apartment complex, kind of descriptor here, and we'll talk through it a little bit in that sense. So. A noisy neighbor is just like Max said, the hotel neighbor, the upstairs neighbor, that's causing noise, keeping you from sleep, we will come back to sleep a couple of times. But keeping you from sleep. And they're affecting everybody else that's around them. That apartment complex is a shared piece of infrastructure. It's the thing that everybody is using, and one person is messing it up for everybody else. Back to our groggy engineer. Jump into that incident. And this is what you see. This is the customer traffic pattern. Yeah, it doesn't tell you a lot, does it? It's pretty straightforward that they're hitting an endpoint. Well, we kinda knew that already. But a lot of times this is how observability starts. You don't know what you need to see, until you suddenly need to see it, and the worst time to suddenly need to see it. Is your under-caffeinated brain woken up from a deep sleep, it's the worst time to troubleshoot. So we started to solve that by tagging across our tenants. Tenants go back to the, the analogy I used of apartment complex, tenants are. The buildings, the people in those buildings. And If we're just showing that previous image of just that one part of the call, I've no idea where resources are being starved. I have no idea where traffic is coming from to be able to see why the customer is having the problem that they're having. But if I start early on, by tagging some of that infrastructure, when I wake up in the morning or get woken up in the morning. I now can see something like this. This is APM traces from Data Dog. And They helped develop tracing alongside Zendesk. It was for a lot of use cases we had, it was it's fantastic. But it allows us to tag every step of that infrastructure. Same call, look at all what I can see. So much more. So let me talk you through a process that we use to try to go through this. To try to iterate on what I just showed you, now adding tags to tenants, but that's not the only way to see more information. The first part is recognize, what do you see or need to see? Do I need to see hot partitions? Do I need to watch our backlogs? Do I need to see latency spikes? So you can do that in multiple different ways. We can look at APM, that application performance monitoring I mentioned before. But we can also look at things like logs. Infrastructure monitoring and metrics. Excuse me. All of that allows us to use different vehicles to recognize the thing that's causing the problem. Now we need to examine that, so. Great. Woke up in the morning, had to solve the incident, couldn't see what I wanted to see. OK, what did I need to see? Recognize what you needed to see. Now let's examine that, let's. Figure out how to be able to see that. Do we need to see top tenants? Do we need to tag those tenants like I showed initially? Is one of the issues, the cardinality that we're running into at that level. We have a bunch of microservices that, you know, hundreds and hundreds of microservices, now we have exploded cardinality there. Is that where you need to see something? Or is it slow database queries? And again, we can see that through the same setup, traces, infrastructure monitoring metrics. So we're looking at the thing you need to see. You need to recognize it, you need to examine it. OK, now once you get all of that information together, what do you do with it? So this is like In that retro that you have after the incident. Once you say, was it caused by a bug escaped defect, or was it caused because we couldn't see it. So we couldn't fix it. We couldn't get there in advance of the customer sending us in a ticket and saying, hey, something's messed up. This is the part that in an incident, in an incident retro, can get skipped. Can just be a line item, right? It can be a, we couldn't see it. Assigned the Jira. Let's talk about not just can you see it, but once we see it, what do we do with it? If that alert fires, and you get woken up, if there's no discussion on what to do with that alert. You're still back in the same problem, right? You're still tired, you're still confused. So you might as well talk about it in the retro when you're awake and caffeinated. Than in an incident in the morning when that customer's mad. So shaping is where you start to leverage long-term thinking. So, in this case, a dashboard is what these are from, but they're ways to see a limit versus traffic. So I know what's happening. I can boil down to who's doing it. And now I can see. What it's running into. So, OK, let's say, let's say a customer is frustrated. And that's their traces. OK. Theoretically you could raise the limit a little bit and give them some more, or you could look at that and go, well they're hitting an endpoint or a rate limit that they really shouldn't be. Maybe they have a runaway integration that limits doing what it's supposed to be doing. It's protecting them from more issues. Do we need to lower the limit even more? And the last is test. So this is outside of now that retro that I talked about. This is. During, I don't know, let's say a cleanup of your backlog once a quarter. Maybe once a month if you want to be proactive. Is it working? Are all those limits that I put in place, not limits, sorry, all those alerts that I put in place. Or all the ways that I now just spent money, right? To see stuff. Is it working? Are these alerts going off and they're meaningless? Who's got a Slack channel that's got a lot of alerts that are meaningless. It happens all the time if you don't iterate upon that process. And it does take time, I get that, and I'm sure all of us are strapped for our own time, but. This is a way to keep costs down, is by going back and going, well we, we, we decided to ingest. 100% of the logs for this service. Well, did we actually need all of them? Does it never fire? Well then, maybe let's not ingest 100% of the logs. Let's take a step back and change our ingest. And we look at these things through service level objectives or scorecards. We go back and go, what's our up time? Are we actually doing this? Are we looking at the right stuff so that we're engaging in the right places. And that we're preventing the right things from happening. Rest. See, told you I'd get there. Recognize what's the issue and who's impacted. Examine, how do we see it. Shape, what do we do, and then test it at work. And again, this is iterative. So it don't just do this once and then walk away from it. This is something that can be continually a part of a conversation that you're having with your engineers during incidents. So that You can not only help your customers, but what I'll get to in a second is keep costs down because that's a pretty important thing for companies nowadays. I mean, I know it's companies all the time, but. Cost is, is a vector that. If you only talk about it when a contract comes up for renewal, or every fiscal year. It's blindsiding, right? I mean, I've, who's been in meetings with, you know, your finance team where they're like, uh, you spent too much on logs again. Could you not? It's frustrating, because now you're like scrambling to get back to a place that's, that's sustainable. But if you're continually having conversations around that, you're not stuck in those conversations once a year or once every 3 years, or whatever it ends up being. We have this conversation a lot more frequently than we used to. We've been able to drive down certain things like query performance. That's directly related to infrastructure cost, right? Queries perform better, you lose less compute. Save money. Active data sets, same deal, less to index. Means less infrastructure cost. You can move it to a colder tier of storage. But again, this is cost as a vector. So You can do it. You can keep costs flat. But it's a level of engagement that. I don't want to say it gets lost, that's not fair. But it's a level of engagement that we didn't always do. In an iterative process like we should have. I've 100% had conversations where a budget conversation comes up and they're like, you need how much? And that's a bad conversation to have for everybody in the room. There's executives who are trying to balance being able to let the engineers see what they need to to be able to do their job, and there's a finance person on the other end going, well, but the industry standard is this. But if, as an engineer. You're engaged with a process that. Iterrates upon that, that has conversation around, not just, we need to see this. But we need to see this. Here's how we should go about it. We've managed that over the last few years and it's really transformed how we respond to incidents, but how we continue to work through new and emerging challenges is because it's now a part of our DNA. Because we have those conversations continually. Nobody's blindsided. You're not waiting in fear for the next time you have that financial conversation and that your observability is gonna go away. So let me talk A bunch more about how we did that. So hopefully I've set the stage a little bit. I've talked about the problem that we have kind of all experienced. But It's a clever acronym. It's not necessarily actionable. Let's talk about the actions that go along with it. So, logs. This is kind of where most people start, most places start. I think Zendesk started here as well. Big verbose logs. Big whack ton of JSON information, right? Stuck in there and then maybe formatted depending on the UI you're using. DataDog's UI formats are pretty well, but. They're big They're very expensive. And Do they actually give you the information that you're looking for? I mean, I was an advocate. Customer support at Zendesk advocacy. And I've looked at our logs plenty of times. I still look at our logs. They're useful. But I don't need every one of them. I don't really care. About the successful request that somebody sent. To get a help center article. It worked Not really relevant to me. And we have audit trails if we really need something. Why, why would I record that? Why would I even ingest that information? It's a conversation worth having. And again, part of that rest is having that conversation, testing, do you really need that information? You probably care more about the 422 the customer's getting, because they're sending you garbage payloads. That's an issue that not only customer support can help resolve, but it also could potentially down the line cause incidents. So you're managing things. Like your ingest and your index pipeline. Do we need to intake it, how much does that cost? Do we need to index it? How much does that cost? 10%, 20%, just the errors. Again, it's different per company, but if you don't have that conversation, and you just ship everything in, and you've got hundreds of thousands of customers, that's a lot of data. It's a lot of data you don't need. The next is 8 p.m. It's a less expensive tier. But you can take it. I showed you that graph way back in the beginning, that flame graph where it had all of the little pieces of the puzzle that. Were happening on the call. And this is where you can get at. Traffic over time So let's say Back to my example for grabbing a help center article. Customer goes to help center, gets an article. Retrieves it from the database. Perfect. 200. Well, Well, so they do that 100,000 times a day Do I need all 100,000 of those? Or do I need 10% of those? Again Your workflows are gonna vary, but having that conversation means you can go, oh, well. Maybe we do only need 10% of that. Now you can still see what's happening to the customer. They suddenly have a flood of requests and you're called online. You can still see it. You don't see every single call, but that every single call wasn't important, because we discussed that in. Our testing part of rest. The next is metrics. So. If I care about all of the things within my infrastructure that are happening to, let's say my EC2 instance. One of my clusters. Just for an example. Well, I don't need to see all the customer information. To determine which one of my hundreds of clusters. Have high CPU load. It's batter. I can drill down into that. I can decide that we need to do that if we don't have the visibility when something comes up. But I don't I don't need to plan for that. I just need to plan for seeing an infrastructure on a high level that's perfect for what metrics are used for. Again, a less expensive way to still be able to see the things you need when you're in that incident and. Very frustrated. And I've talked about this kind of throughout this whole index and ingest strategy. Is cost lenses and keeping that a part of the conversation as you move forward. It's kind of uncomfortable, isn't it? When you have those conversations internally. I'm not usually very comfortable in finance conversations. I'd rather avoid them. Just give me a problem to solve and I'm perfectly happy. But if I can't see what I need to see in the problem. Then solving the problem Not, it's not just frustrating, it's almost impossible. So I've had to learn over time that it's OK to have those conversations. It's OK to ask those questions. And again, every organization is different, but. Developing that level of openness within your organization. Leads to huge dividends. I mean, again, I showed you the evidence of it a few slides ago. There's no finance person that is happier when they see. Big flat line straight across for as long as that's underneath the barrier that they set. If it's over, then a different problem, but. If you're having those conversations, it's a lot less awkward. So Need a quick drink. It's a desert out here, quite literally. So this is what we saw. After, again, I said I've been Zenda 7.5 years. This isn't something that happened overnight. I'm, I, I'm, I'm showing this in a, what we saw now, knowing all of the struggles that we had getting up to here. And I'll go into detail a little bit more about those pitfalls in a minute, but. We saw a much faster response time. Being able to get online, to jump in an incident, to see what's going on, to have the alert fired, to know exactly what you're supposed to do with that alert. Means a huge difference. And because we can see where that. Silly person with a really loud stereo, we know that they're in 21B. We can put guardrails on them to stop them affecting everybody else. Soundproof the walls, whatever you wanna use for that analogy, or kick them out. But we're also able to do things that are on a much larger infrastructure level, that by, by paying close attention to being able to see really granular parts of our infrastructure on the customer level. We can right size things like our cloud resources. We can keep consistent EC2 instances across our entire infrastructure. And we can align our observability costs, ingestion costs, are aligned to the value that they provide internally, and to our customers directly. Speaking of customers, we also have measurably hap happier customers. Not every customer is happy. That is the reality of the world. But they are measurably happier because we're responding to their issues and, and getting them meaningful information much faster. I mean, we've all been on the other end of incidents, right? Like we, we, all of our companies own or or purchased a piece of software from somebody else and it's had an incident. And when it's the, we're working on it, and we'll get back to you. 30 minutes after they've been down is mind-bogglingly frustrating. But if that company can see better about what's going on, they can be like, hey, this part of our infrastructure is having an issue. Our engineers are working on it, we'll get back to you soon. Makes me feel so much better, even though they've, they've still not done anything. I still feel better about it, having that information. So let me talk about the issues. Don't do freeform tags. Having any level of governance over how you do observability internally, matters a huge amount for cost. If everybody can just put in what they want, it can be the Nutella team can be putting in for cookies, and it's the, the, the other team whose carrots is looking at cookies as well, but they called it cookies 4. It doesn't Do anything, you're looking at the same thing, but because you tagged it two different things, now you're looking at it twice. And more frustratingly for the finance portion, you're paying for it twice. Might have done that. It sucks, and. It's even worse to untangle once it happens. Because If they're not tagged consistently, then how do you know that cookie 4 is the same as cookie? You've gotta get those teams together, you gotta have a bunch of meetings, and you gotta talk about it some more, and then you gotta put a test in place. You've all been there. I don't need to go through it. But leads to high cost, and. The, the even trickier thing is. Because it explodes cardinality. It gives you a false sense of cardinality. So it makes it look like you need a lot more observability on one layer than you actually do. Because From the outside, from like an SRE perspective, my job, um, it, it looks like it's all different. And you don't know any different just on the surface, you gotta dig pretty deep. So I said before, I was in customer support. I. Loved catch-all logging. It was an audit log, and an engineering log all in one place. It is freaking expensive. And to be honest. Not actually really worth it. I'm sure there are workflows out there that it is, by the way. For us, it wasn't worth it. And it means you're, like I said before in the freeform tags, you're doubling, tripling, quadrupling what you're recording. Cause if you have now this information in multiple places, like you have it in logs, and then you have it in APM and you built a metric around it, and you stuck your email logs in like an S3 bucket, well now you've paid for it 4 times. And that's useless. So having some. Governance or some conversation around how that stuff is used. Keeps those index and ingest. Levels correct, so. You can ingest a lot of logs, depends on the platform, but, it depends on the observability of platform you're using, but you can sometimes ingest logs costs very, very little. But you're not, cause you're not spending any index time. And that's a good way to balance those two things out. Is, well, I'm only indexing these, the 400 range errors and the 500 range errors and. OK, for this one customer who's had issues on this endpoint before, I'm indexing those. But the rest, I'm ingesting. I can go back and rehydrate, if I need to. But I'm not spending that cost upfront. By the way, when that went away for uh advocacy, I was not happy. But. In the end, it did really work out. So there are some decisions you make that will ruffle people's feathers. That's the reality of change for some people. It just, it, it's different, it's a new thing to learn. But it will be OK in the end. As I said, having that conversation and making sure that's a part of your DNA at a certain point, that really helped soften the blow here. Across all of our, our customer support teams, is being like, hey, cost matters to our company. We're we're here to make money, not burn it. And this is why we're doing that, and that really did change the narrative in a very meaningful way. This one is hard, especially for a big company, but no owner map. If I don't know who is pumping all these logs in. Then Whose desk do I sit at, to go, hey, could you not? Because if you just nuke them from outer space, you will find out really quick who it is, but the problem is, again, you're gonna find out at 2 a.m. when they jumped on for that incident. And that Leads to customer frustration and all of these other things and. Again, we've gone through all that chain of events, but having a service map, having ownership over your service map, and making sure it's continually up to date. Is An extremely hard task, but one of the more valuable tasks that I have ever seen. A company undertake. I cannot tell you how nice it is to be able to be in an incident, look at a set of logs and go, I need to page this team. Under a minute. There is no better feeling. The worst feeling is when you've paged 15 teams, and they're all in this Zoom meeting yelling at each other because you paged 15 wrong teams. Now everybody's pissed off. So Again, hard, but extremely worth it in the end. This last one, I alluded to it earlier, but that Slack channel where it's all these warnings and alerts, and nobody's reacting to any of them because it's the same alert that's been firing for the last 3 weeks. Signal to noise really matters in alerting, and curating your alerts. Routinely Is I know, not all teams have time for that. There's deadlines for releases, and there's deploys, and there's customer bugs and all of these things. I, I do get it. I know this is kind of idealized and doesn't always take into account the real world, but. It It leads to a level of alert fatigue that isn't just an issue for incidents. It becomes, it, it's that if everything's critical, nothing's critical, right? I mean, I think we've all heard that phrase at some point. It, it ends up being that. It's not that you're just ignoring the alerts that are flapping. You start to ignore other things that are also alerts, because in your head you're like, well I see this ping every 5 minutes, and it's pointless. So now other things might be pointless. So it, it ends up like kind of ballooning into an attitude that changing the direction of that chip can be difficult. Um, it ends up with issues ignored. And again, back to my customer experience days. Customers get really mad when you ignore their issue, and it's hard for an executive, say a big customer had an issue they had to write in about. You had a flapping alert that told you about this issue, but you ignored it because it's always been flapping, or it was buried in another flapping alert. It's a really hard thing to talk to a customer about. It just is. Hey, we, we actually saw it 2 hours before. But we can ignore it. So. That one's a hard one to curate, and I know it's hard to make time to do that, but it's another one that's really like. It has a lot of cascading issues other than just what it seems like on the surface. So You're all gonna leave here in 20 minutes or so. What can you, I, I want you to take away a couple of things, because I did say a lot of things, I put up a lot of slides, but. I, I, I'd really love for you to take away some of this, you don't end up in some of the pitfalls that we did. They were frustrating, we've come out of them. But they're frustrating. So, start with a tag before a fancy dashboard. Figure out how you're gonna record all this stuff and stick to a standard. Having a dashboard is nice, but if you have no. Like rules, no governance over those tags that inform that dashboard. You're doing style over substance, and that just. It burns down quickly. Invest early in per tenant KPI and heat maps. I know that's really big wording for a takeaway, but. If you're not talking about performance on your tenants. Then you're gonna run into a scenario where. You can't Focus resources in an area appropriately, you're gonna be kind of flailing. And if you spend time early on, this whole slide is kind of about governance and like company culture. If you're investing in this early, SLOs, saying, hey. Mic tickets endpoint, they should be created in 3 seconds or less. Great Set an alert Set an SLO when it doesn't happen. Talk about it. Because if you don't, you end up hiding problems because you're never looking for them to begin with. Treat costs as a first class dimension. I, I'm sure you're tired of me hearing, or tired of me saying, you hearing. The word cost over and over and over and over again. I get it gets old. But without a culture that Is accepting of that. It, it causes so many. Other issues, like I talked about that flapping alert, like it caused other cascading and, and issues that kind of go alongside of it. Being afraid to talk about cost can mean you're afraid to talk about other big changes at the company or big governances at the company. Cost is a dimension for every single company, at some point. And If teams internally aren't comfortable talking about it. How can you drive profitability? How can you drive sustainability? If everybody's scared of it. It's really hard to do. And the last is bacon context. Essentially, fix your metadata. If it's not there, you can't see it. And that goes back to the rest that I talked about. Iterating on a process, whatever you wanna call it. I called it rest because. I get sleep deprived and a groggy engineer needs rest. But you can use whatever mnemonic you wanna use or whatever process that works within your company structure. To go through the process and make it. Just a thing that you do. We are going to do rest today. That's what we're gonna do. Great. Everybody goes through and does it. It's a great exercise. But if you don't do that, to then bake in that tenant context, to then have it available when that incident is called, you're always reacting, and you're not being proactive. And the thing customers love the most is when you're ahead of the issue before they even see it. This is where you wanna be, right? Latency decreased, requests retained. You want customers happy. All of this that I just talked about is all trying to drive those things. Customers Are how we make our money. And if they're pissed, they're gonna leave. They're not gonna pay. They're not gonna, you're not gonna have a business. So keeping customer context in mind, while it drives all of your other observability decisions, is a really healthy balance to have. Not saying it's easy. But hopefully some of those things are good takeaways that you can use to try to help drive those conversations internally. I know it's high and it's conceptual, but I'm happy to give more information around that. I really appreciate you all listening to me today, talk about this stuff. Customer experience is very near and dear to me. It's a problem that always exists and it's always there to solve, and I like solving problems. I'm gonna open it up for questions in a second. If you have any that you want to ask to the group, feel free. But if you wanna come to me afterwards and don't want to talk in a big group, I know not everybody's comfortable with huge groups, feel free. I'm more than happy to answer any questions you might have, or if you need details on what we did. I wanted, I do want to give a huge, huge thank you to, uh, Max from Data Dog who helped prepare this presentation, and to Zendesk as a whole for giving me the opportunity to, to speak to everybody here today. Um, they are at booth 165. There's my pitch. Um, but yeah, I appreciate all your time today. Thank you so much.