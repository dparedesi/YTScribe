---
video_id: rMPe622eGY0
video_url: https://www.youtube.com/watch?v=rMPe622eGY0
is_generated: False
is_translatable: True
summary: "This session, titled \"Building multi-agent AI SRE: from root cause to vibe debugging\" (AIM394), presents a comprehensive look at Resolve AI, a platform engineered to transform Site Reliability Engineering (SRE) by automating the \"toil\" of production operations. Speakers Mayank Agarwal, founder of Resolve AI, and Claire, a research engineer, argue that modern software complexity—characterized by microservices, siloed observability tools (logs, metrics, traces), and fragmented \"tribal knowledge\"—has forced engineers to spend disproportionate time managing production rather than shipping code. To combat this, they introduce Resolve AI, which does not merely act as another tool but plugs into existing infrastructure (AWS, Slack, Datadog, etc.) to build a dynamic, time-versioned dependency graph of the entire system. This allows the AI to \"think\" like a human engineer: starting an investigation at the narrowest symptom and working outwards, rather than ingesting infinite data.\n\nThe presentation details Resolve's multi-agent architecture, which is designed to overcome three specific challenges: the complexity of constantly changing systems, the difficulty of using heterogeneous tools with proprietary query languages, and the need for cross-team coordination. Claire explains how Resolve uses specialized agents to run parallel investigations, iteratively refining queries to tools like Splunk or Prometheus just as a human would, while a \"grumpy critiquer\" agent rigorously challenges the findings to prevent hallucinations. A highlight of the session is a live demo where Resolve autonomously diagnoses a critical PostgreSQL deadlock. The system detects the anomaly, correlates it with a recent deployment that introduced a \"batch update\" feature, identifies a non-deterministic lock ordering bug in the code, and even generates a Pull Request with the correct \"transaction wrapping\" fix, all while explaining its reasoning in a clear, causal timeline.\n\nThe session also features a compelling case study from Chris, a Lead SRE at ZScaler, a zero-trust security platform protecting 50 million users and blocking 9 billion threats daily. Chris describes the \"middle ground\" incidents—those that aren't catastrophic but consume hours of valuable engineering time—as the prime target for AI automation. He shares how ZScaler evaluated Resolve, noting that unlike simple dashboards that just present data, Resolve provides \"working theories\" and root causes. He cites a specific example where Resolve correctly diagnosed a DNS registration failure two hours before a human-led incident bridge was even established. Crucially, addressing security concerns, Chris highlights that Resolve operates without ingesting sensitive customer data, using local redaction and a \"sidecar\" deployment model to ensure privacy. The talk concludes with a forward-looking discussion on the future of AI in operations, predicting a shift from reactive troubleshooting to proactive tasks like automated capacity planning and synthetic testing, ultimately aiming to reduce incident resolution times by over 75% and return engineers to creative work."
keywords: AI for SRE, Resolve AI, Multi-agent Systems, Root Cause Analysis, Production Operations

Good evening. It's great to see so many of you here today. We are here to talk about uh how we build Resolve AI. Resolve AI is an AI uh for production. Uh, we can automate incident troubleshooting on-call, and a number of other SRE uh. Actions, but resolve is not just limited to SRE work. Resolve can build a deep understanding of your production systems and automate all kinds of production activities that span from reactive work around incidents, uh, continuous optimization, and even helping you become more and more proactive around your reliability and your operations. And so today I'm very excited to share uh some of the lessons that we learned as we built Resolve AI, uh, and, and help you, you know, avoid some of the pitfalls if you're trying to build such a system yourself or help you decide, uh, as you are trying to partner with someone, uh, to help you resolve your own production issues. So I'm Mayank. I'm one of the founders and CTO at Resolve. Uh, I've worked in the observability space most of my career. This is my second startup together with my co-founder Spiros. Uh, uh, at our previous startup, uh, called Omniition, we helped create open telemetry and built a distributed tracing product around it. Um, we got acquired by Splunk in 2019, and our product, uh, is now called Splunk APM. Um, I was the lead architect for Splunk's observability product lines. But what I, uh, you know, was most surprised by was that as our engineering team grew in size to hundreds of, uh, engineers, uh, most of engineering time was being spent on, uh, toil, uh, and grunt work related to production rather than shipping new features. And that was part of the motivation to start Resolve AI. I'm joined by my colleague Claire, who joined us earlier this year from Google DeepMind. Uh, and, uh, with Chris, who is a lead SRE at uh ZScaler and one of our favorite customers, he'll be sharing about his journey with Resolve AI later today. Um. So it's a very exciting time to be a software engineer. Definitely my favorite time in my career. Uh, Gen AI is really changing the way we build and we run software. Uh, AI is allowing new kinds of software applications to be built, including Resolve AI that was not even possible to be built, only just a couple of years ago. At the same time, AI is upleveling our engineers by allowing them to offload most of the grunt work that that fills up the vast majority of their time today. Last year we saw the power of models and how co-pilots around them can help to offload some of the boilerplate code creation. This year we have seen the power of agents that are able to bring in more complex tools on top of the models and reason about them in complex reasoning loops that that have automated even more grunt work around entire PR creation or debugging. incidents, um, but I believe the, the biggest gains are still ahead of us as, uh, we, uh, work towards even more sophisticated agents that can orchestrate, uh, very, very complex tools in, uh, you know, complex reasoning loops, um, bringing, uh, you know, uh, a lot of domain knowledge, you know, general purpose reasoning, but also very domain specific reasoning. Uh, requiring minimal or no human guidance to automate entire workflow spanning multiple teams. Some of these are, uh, uh, you know, agents that we are building at resolve we are that, uh, happy to share more later. Um, so what is all of this toil that, uh, you know, burdens our software engineers, uh, that takes up the most amount of their bandwidth? So, you know, modern software systems are, uh, very complex beasts. They are composed of large moving parts. At Splunk, uh, we had. Uh, our system composed of hundreds of microservices and code repositories. In fact, we, we had more moving parts than we had engineers. We had legacy components that were not even being maintained, but we're still providing business value in production. Uh, these systems can run on a variety of infrastructure all the way from multiple clouds, on-premise data centers, packaged in multiple form factors, uh, containers, uh, mainframes, and so on. And we manage these systems with a large number of tools including telemetry, which might be further siloed into, you know, logs, metrics, traces, and a long tail of other tools like profiling, real user monitoring, and so on. And finally, unlike code, which is, you know, consolidated in a single repository, production knowledge is scattered across many different data sources. So that's just one layer of complexity that creates toil. On top of that, we require multiple teams to come together and operate production all the way from application teams, uh, infrastructure or platform engineering teams to networking teams. And then another layer of complexity on top of that is that software engineering is not just development, but we execute multiple workflows that require multiple teams to come together operating multiple tools to achieve the end result. And the toil of these workflows. Can manifest in many different ways. So, uh, of course, you know, many of you must have been on call by a show of hand, how many of you have been paged, uh, you know, uh, unexpectedly during an incident or, or triage, uh, an issue. You have definitely come to the right talk, uh, so you know, inefficient workflows can manifest as incidents that take a very long time to figure out, to resolve. Oftentimes we have too few or too many alerts at the same time, uh, and customers might be reporting issues because either we did not have the right alert in place or we had too many alerts and engineers did not have the time to investigate them. And then when incidents do happen they can pull in a large number of engineers, creating a lot of impact on productivity. And then, you know, because incidents can, you know, because changes can cause incidents, it takes, you know, we slow down changes because we are worried about breaking production or we might overprovision infrastructure because we are worried about underprovisioned infrastructure causing. and then finally we, we, we buy a lot of tools to help us manage production, but really the, you know, the costs outweigh the ROI that we achieve because all of the burden of troubleshooting still falls on our engineers. In fact, only a handful of engineers might understand production fully, and they are constantly getting paged, um, and you know, AI is making the problem worse because we are creating a code a lot more faster than we can deploy in production without the help of AI. And you know this is myself and you know many of you getting constantly paged when we are trying to have fun or when we want to get other work done, but we have to bring the system back online because that you know demands uh you know our our priority. So this is how we might want to think about software engineering, you know, two sides of the same coin. We create code and we deploy it in production, but because of all the toil and complexity, what we have found frequently is that engineers spend most of their time managing production rather than shipping new features, and that is what we are trying to solve here at Resolve. So to bring AI to automate these problems, what, you know, what we have found is that we really need to understand what makes production hard for humans and then backtrack from that to make it work for AI. So what makes production hard for humans? The first is, you know. Production is composed of many systems. They have dependencies with each other. We as humans start by investigating the issue at the narrowest point where we can see the symptoms and gradually navigate from there to the root cause that might be 4 levels deep because we have a deep understanding of production structure. And on top of that there is tooling that's completely siloed. None of the tools work well with each other. There is data that lives only in specific tools. They have their own specific query languages. We as humans don't go and start operating all of the tools at the same time. We know what data lives in which tool and go. Create precise queries against those tools and for AI to work well, it needs to operate in the same way. AI cannot go broadcast queries against all the tools because that will just pick up a lot of noise from many different systems and take you down the wrong path. AI needs to understand where data lives and how to operate these tools like a human. AI should not have to suck out a lot of data and treat it like language because that's not how we as humans use these tools. The next layer of complexity is that any particular workflow requires coordination across multiple different teams. An incident may start at a gateway service requiring an engineer from that on-call rotation, but may actually progress 4 levels deep to the underlying database requiring the database engineer to run very sophisticated PG stats or other commands to exactly figure out the hot query. And some of these people might be living in different geos. It takes a lot of overhead to pass the context between these teams and and to coordinate and maintain the shared context. So for AI to work well, it needs to be able to take the expertise and skill set of any of these engineers at different steps in the workflow, but you cannot shove all of that expertise in a large prompt because that's going to confuse the model and and make it pick up a lot of noise again. And finally, unlike code, production is never well documented. Information is scattered across multiple different data sources. Um, some of these, uh, wikis and runbooks might be even out of date or, or incorrect because production has changed a lot, uh, and a lot of knowledge is never written down because it's only in the head of senior engineers, uh, and can only be learned by watching them work and, and, and learning from them. So these are the principles, you know, with a deep understanding of these problems, we built Resolve AI, which I'll show to you in a minute, but Resolve, you know, what we realized is that you cannot build a very point product to just solve one narrow problem like incident troubleshooting because that's not how we as humans operate. We build a deep understanding of production and then we are able to use that understanding to solve a large number of production problems. So that's how we build AI Resolve AI as a platform that can understand production and then it can help you resolve incidents, but it can help you with all of these other activities as well. So you know there are 3 pillars around which we have built Resolve AI. The first is that Resolve plugs into your production tools. Resolve is not yet another tool. It plugs onto these tools and offloads the toil that humans have to take on on top of these tools. So Resolve builds a deep understanding of production just like a seasoned engineer, and then is able to operate these tools like humans operate these tools. The second is that it is always able to bring in the expertise of all of your teams within an entire workflow, so it's able to take the skill set of, you know, a platform engineer or a database engineer or a network engineer or an application on-call engineer as and when needed depending on how the investigation is progressing through your dependency chain. And finally, No AI can root cause all incidents all by itself because there is never complete context written down in systems. So Resolve learns from humans, is able to collaborate. It's not a binary product, it's, it's always able to collaborate and then learn and get better every time. So Resolve is a is a tool that you can use every single time to get to the right answer faster than doing it yourself manually. And then as you interact with the resolve and teach it, it gets better next time and then over time it can become as good as your most seasoned engineers. So with that, I would love to show you a demo of how Resolved actually behaves in real life. So this is what Resolve looks like. I'll show you how you, you get started with Resolve and then show you a couple of actions of Resolve in, in, in, in, in real life. So Resolve plugs into your tools. This is an environment where we have plugged and resolve to our AWS environment, to our Gate Reaper. To our monitoring tools, to our infrastructure, and then to our Slack. Resolve can plug into a large number of other integrations, and you can customize Resolve through your own MCP servers to capture any long tail of tools that you might have very specific to your environment. And once you do that, resolve builds an automated understanding of your environment that looks like this. This is not something fit for human consumption. This is just to demonstrate that even in a small demo environment, there is a very large number of resources and complex dependencies that come together and real world environmental. Can be composed of hundreds or thousands of clusters running in many different regions, but it makes more sense when you look at it from the perspective of one specific component. So this is a front-end service. We know exactly what services call into the front-end service and what services does the front-end service call out to, including databases and external dependencies. This dependency map is constructed from multiple different data sources, combining data from infrastructure, from distributed tracing, from code, from tribal knowledge, and maintained in a time version manner just like we as humans think about our infrastructure. Resolve knows what workers back the front end service. If you go to a specific worker, Resolve knows exactly the underlying infrastructure topology. It knows which node runs the worker, and then it also knows what other workers run on that node because oftentimes we have issues coming from noisy neighbors. So once you have that, uh, then Resolve also brings in a very powerful natural language interface. Uh, so I will, uh, show to you. Actually, maybe I'll, I'll start a live chat first, uh. Let's say I'm new to this environment. Tell me about my AWS infrastructure. And my services format as a table. So now at this point, you know, I don't really know much about the environment. I want resolved, so please tell me uh what consists in my environment. So Resolve is now able to is, is now going to go explore my AWS infrastructure. It's going to look at the graph that it has already curated. It's, it's operating a lot of tools. So, you know, let me show you what it did. It has access to your AWS environment. It ran commands in CLI, um, it's able to see the AC2 instances, the RDS instances, uh, what lamb lambda functions run, what are the S3 buckets, and now it's, you know, able to give me this answer. It's printing it out. You know, about what are the ECE2 instances in my environment, what are the RDS databases, uh, what Kafka brokers do I run, what are the S3 buckets, uh, and then it gave me what are the application services, uh, and then, you know, it has, uh, citations for everything that it got. There are a total of 25 services running in this environment, 267 deployments, pods, and so on. And earlier this day I was actually running this uh chat so I'll show to you, uh, you know, some of the other cool things that I was able to do with this. So, uh, because this was a pretty long session, uh, so, you know, it gave me this infrastructure then I wanted to know about my RDS cluster, you know, what services read from RDS what services write to it, draw me a mermaid diagram, right? Just because I can. Uh, so, uh, you know, resolve went into the database details, uh, for this, you know, it navigated my infrastructure graph, told me, here are the services that the ad service, uh, you know, connects to Postress as well as a recommendation service. Here is a config map that maps the configuration details of my database, uh, and then, you know, only two of the services use this RDS database. Uh, both use the same config map. OK, now I want to know, you know, what is the health of my RDS cluster? Do you see any anomalies or spikes? Now, this requires going into RDS performance insights, uh, you know, it's, it's a separate data source, usually separate from, you know, your typical metrics back end. Resolvent and you know created a number of chart queries, metric queries. It's able to explore the dashboard that already exists in your environment as well as able to craft live queries based on inspecting the metrics, um, what is available, or even able to go into code and see what are the right metrics to monitor this system. So in this case it came back with a health report. Here are the key metrics. There is some concern about CPU utilization. There were some spikes. In fact, Resolve has already analyzed all the data, detected some anomalies. For example, here's the database load. It really spiked high last day. And then in fact Resolve went and proactively identified what is the root cause. What is this resource intensive query? This itself takes a lot of effort to go look at the hot queries, uh, correlate them back to the right workload. Resolve identified this resource intensive query and also why this is problematic. There are multiple MD5 hash calculations on every row, and there is a full table scan with Countstar. And uh then you know I asked it what service is making this query. Can you make a PR to fix it, you know, because now I have the full context, uh, can you just fix it for me? Uh so Resolve actually went and created a PR that now I can go submit to the right team uh and, and please uh fix this full table scan that's hogging up my RDS usage. So within a few natural language commands, you know, I started. I learned about the environment. I saw there was an RDS database. I was able to identify a hot spot, correlate it back with code, and create a PR to fix it. So that's just, you know, one of the use cases. Next, I want to show you resolve in action, uh, you know, during a live incident. So this one is pretty close to my heart because I, this is actually in our own environment. This is not a demo. I was on call for this one and I was very appreciative of resolve, uh, doing the root cause, uh, by the time I acknowledged the alert. So, the alert was, and, and, and, you know, in resolve, uh, we have the, the full thought stream of the agent on the left, and we have the report on the right. So I'll first show you the, the thought stream and then we'll, we'll look at the report. So in this particular case there was a post-Cres deadlock. Postgress is a database that backs our services, uh, and a post-Cres deadlock, as you can imagine, is a very severe, uh, problem. It can, it can bring down production. Um, so Resolve uh automatically starts investigating alerts, does not need any human intervention. Um, and, and, you know, Resolve is on call for your services. In this case, Resolve said, I'll investigate the post grad red lock alert. Uh, it searches for knowledge, search, searches for in run books or artifacts. Unfortunately we have none, uh, as is typical in most environments, but no problem. Resolve goes ahead and creates a, creates a plan and continuously refines it. So this is the final plan. But let me show you how it came up with this plan. So it starts by uh validating the alert metrics, so it looks at the alert condition, creates a query based on those alert metrics, in fact, creates many queries and identifies, you know, when the deadlock exactly started, you know, has fully analyzed that data. Then in parallel it starts checking logs. We use low key, and then what Resolve does is that it. It starts with a query and incrementally refines it until it finds the right query that gives it the smoking gun around what kind of deadlocks are happening. So in this case it finds there is a deadlock message and the affected module is event reconciler, and here are the event IDs that. You know, um, surface the error, and this is very interesting because this required, you know, crafting the right log queries which required multiple iterations, then fetching the logs, analyzing them, and figuring out the right evidence. This in itself takes a human a long time. Resolve did that in parallel with understanding the metric condition. Um, and then it also navigated the infrastructure graph to understand exactly which database is impacted, what name space it runs in, what are the services that talk to it to understand where is the impact. Based on that, it sees a clear pattern emerging. It updates the plan, but now it knows that it needs to understand what triggers this concurrent processing condition. So it has already identified the kind of error. Now it goes into code and we and and figures out exactly where in code is the module that is seeing these errors. And now, with the understanding of code, it goes back into metrics and logs. So it's going back and forth between multiple tools. And finally it says I have a complete picture. And I have a high confidence in my root cause analysis. So resolve comes with a very sophisticated critiquer that's criticizing the work of the agent to see if there are any gaps in causality analysis, in evidence that might be missing or driving false conclusions from the evidence, and this is very important, as you all have seen, you know, if you ask a Claude Sonnet, is this how it should be done, it will say you are absolutely right, but that's not the behavior we want here. We want someone to be a very grumpy critiquer or to say you have holes in your theory. So finally, after all of this, Resolve created a very high confidence working theory. There was a deployment trigger deadlock from lack of lock ordering in a batch update logic. So this is very interesting. A major scaling operation deployed 27 new pods, dramatically increasing concurrent event processing. The newly introduced batch update alert events resolved at method lacks deterministic lock ordering allowing multiple concurrent requests to acquire row locks in different orders. Uh, so there was a bug in our code, uh, that lacked deterministic ordering, and multiple batches could come together and try to acquire the row locks in different orders that created a deadlock, uh, and then this was only triggered because there was a major scaling operation that dramatically increased the concurrent processing. And so Resolve has, you know, backing evidence for each of these claims. There was a pod creation that, you know, greatly increased the number of pods. This increased the number of database connections. The transaction rates nearly doubled, and then the deadlock started, and this is when the alert triggered. Resolve creates a precise timeline of how the event happened, an impact assessment of exactly what was impacted. Uh, and, uh, then, you know, um, you know, uh, uh, the exact details, but that's not it, right? It's not a one and done, uh, you know, I saw this and then I wanted to understand who added this logic, so it told me the exact commit. Shangy added this commit. Shangyu is a very smart developer, you know, but every one of us, uh, commits bugs. So, so no shame in that, uh, but then, you know, I asked it, what is your suggestion for the, what the logic should instead be. Uh, and, uh, you know, it gave me a threefold, uh, suggestion which all of which we implemented at deterministic lock ordering at transaction wrapping at deadlock retry, um, and then, you know, meanwhile someone else also jumped in. Can you tell me the exact change, you know, explained its work, and then, uh, you know, another engineer says, oh, how about this query? Resolve says yes, that is the offending query. You have shown me the actual code. Uh, we use an ORM, so it, you know, joined those. And then meanwhile Nichol said, hey, I touched this logic recently. Could this have been my change? So I asked Resolve, could this have been caused by Nicholl's recent change? So it explains why I attribute it to Shangyu's change. So I didn't quite figure out the commit ID, you know, because AI doesn't get everything right. So I told it, look specifically at this commit ID, and Resol said, look, Nicholl's commit could not have caused a deadlock because the deadlock alert fired. Before Nichols commit merged, but actually Nichols Change has the same vulnerability, uh, so he did refactor the same method but did not fix the lock ordering issue. Neither did he introduce this. So this is very important because during an incident time you can have multiple competing theories, uh, but only one of them will be right, and it's very important to also say no to the wrong theories. So this hopefully gives you some flavor of how, uh, Resolve AI works and how you can collaborate with it during an ongoing investigation. The final thing I will say is that everything you can do in our web UI you can also do in Slack because that's where most of the engineering collaboration happens. So Resolve automatically replies to alerts in Slack, uh, and then you can interact with Resolve in Slack the same way that you can do it in Web UI and both of them are kept in sync with each other. So this is a multi-user system that's constantly learning and as you saw here it was able to go and explore between multiple different tools, bring in the persona of the on-call team of the database team, uh, and able to diagnose a very, very sophisticated incident with the human collaboration. So that's it for my section. Next, I'll hand it over to Claire. Thank you Mayank for the very cool demo. Uh, so my name is Claire again. I'm a research engineer at Resolve AI and like Mayank said, I think this is a very exciting problem to work on at a very exciting time. I think this was just not feasible just a couple years back, so it's really cool to see how model capabilities have come so far. So now that we've seen a bit of Resolve Live, I'm gonna walk through a bit of the architecture and how exactly we built Resolve. So like my unc mentioned, there were really 3 key challenges that we wanted to address, and these are things that real-life engineers have to deal with in a production system. So the first one is that production systems are very complex and they're always changing, right? It's not just understanding your code base at a snapshot, it's really being able to correlate across all of these different layers, all these different services within your system, so you can correlate across the signals across your system. And because of this understanding, Resolve is able to operate all of your tools that are siloed just like an expert can. The second challenge is that a lot of this knowledge in a system is fragmented. It's scattered across teams and people, and oftentimes it's just not documented at all. Organizations develop this knowledge over long periods of time, right? And it's also constantly changing, so Resolve encapsulates all of that knowledge and all of the updates and it's also collaborating live with your engineers, right? It's getting live feedback. It's learning how to operate just like an engineer within your system. And finally, a lot of these investigations often require coordination across multiple teams, you know, experts across multiple domains. Because of this, resolve combines all of the expertise, all the skill sets, and all the knowledge of these experts within your system. So I'll walk you through a bit of how we architect architecture resolve to really understand your system while addressing each of these challenges. So again, the first challenge is production systems are very complex and not only that, but they're also constantly changing. Right, while code is inherently self-documenting, we find that a lot of these tools in production are not. And we really have to understand, you know, not just one vision of your system, but we have to map between your code, your infrastructure, and your telemetry. To really know how to maneuver through all of these tools because maneuvering through all these, you know, millions of logs and all these complex query languages is very hard for humans and also models themselves. So we find that traditional approaches, you know, just following these static run books, following these, uh, documentation that become outdated, just doesn't scale in these modern production environments. So Resolve operates in these environments by building this really comprehensive understanding of exactly how your production system works. Right, it's connected to every single layer, your code repositories, your infra APIs, all of your telemetry, all your observability platforms and your knowledge bases, so that it has this comprehensive view of your system. And by plugging into all these tools, it's able to build this really deep understanding of your system and how it works and it's not just this inventory of your tools and your platforms, but it sees how failures propagate, it sees how errors occur, it sees how data flows, and it also captures these dynamic changes. And this model of your system is very key, right, because when it comes to real-time investigations just flagging all of these signals, you know, gathering all the data related to every signal and try to correlate between them just doesn't scale because these large production environments are very noisy, right, and very complex. So at time of investigation, Resolve isn't just exhaustively going through all of your tools, going through all of your data. It's able to start at the narrowest point within this model of your system. Start at the most precise origin point to get to root cause. It's not asking, you know, what's abnormal in my system right now. Instead, it's asking, given these symptoms, what is the most likely origin point for what's going on? And once Resolve knows where to look in your system, it's able to operate your tools just like an expert. And this is very complicated, right? All of these tools have their own proprietary languages querying one logs uh service looks very different than another one. All of these take its own set of skills and expertise. And on top of that, every team uses their tools differently, right? They have their own naming patterns, they have their own schemas, they have their own output structures. So it takes a human a lot of trial and error, you know, getting the syntax right, filtering out the noise, but Resolve is able to operate your tools and really craft these precise and efficient queries to get the right data at the right time because it knows the language, it knows how you use these tools, and it knows how your output's structured, and it has the context of what's going on right now. So the second challenge is that investigating these production issues often require expertise that spans multiple teams, right? Investigations don't respect these organizational boundaries, and we find triaging these new incidents really hard for new engineers, even for tenured ones, because it's pretty uncertain where to look sometimes, especially when your system is constantly changing and there's so much noise going on. And this expertise silo creates a lot of coordination overhead, right? You have one engineer who's on call. They tap into another engineer and they tap into two other engineers from another team, and they're all waiting for this domain expert to become available to help them out. And we find that these organizational boundaries and these expertise boundaries really fragment the process and it leads to not just inefficiencies but a lot of context being lost, you know, during these very crucial times. So Resolve is able to synthesize all of this expertise across your system, right? It begins by creating this comprehensive plan, uh, and it's based on the context that it has now. It's not just some predetermined workflow, and it's able to go through and operate all of your tools in parallel, go down all of these hypotheses, right? Because a lot of times these root causes are not very obvious. A lot of symptoms can arise from different causes across different services within your system. And through this process, it can use this cross-organ team collaboration to build a causal timeline of what it thinks is happening. And it links every single layer of your system, right? Code changes, infra events, telemetry signals. And as new evidence is coming in, resolve is dynamic. It's adapting and it's listening to this new evidence, you know, new telemetry might suggest some new hypothesis while eliminating existing ones, strengthen other ones just like an experienced engineer, it's adaptive to this new information, uh, to decide, you know, like what new paths to investigate, what tools to query, and what components to analyze. And finally, the last obstacle is that knowledge is very fragmented. It's scattered and it's oftentimes just never documented. If you think about where all of your operational knowledge lives, right, it's in these run books, it's in these documentation that are often going stale, uh, it's in these engineering discussions, these chat channels. And fragmented knowledge. Becomes really hard to use at time of investigation, right? New engineers are trying to gather all this context across all these different sources, decide what's actually relevant given the current context. And again, a lot of important information is lost in the process or the wrong context is chosen. So even when you do have these documentation, systems are constantly evolving, but the issues that these new investigations really need to lean on learnings from previous ones. So we find that stale documentation, stale runbooks, don't just become useless, they become harmful, right? They're leading to inefficient investigations and outdated practices. And if you think about how new engineers learn, right, they don't accrue this knowledge just by reading through run books or reading through documentation. They're really collaborating with other engineers to get feedback, to correct themselves, to improve their techniques over time. So the question becomes, how do we store this tribal knowledge and how do we capture its updates? And the answer isn't just better documentation, it's building a system that's able to learn the way that engineers do with real life feedback, uh. And So Resolve is able to gather knowledge that's scattered across your system by connecting to all of your knowledge bases, you know, your Slack channels, uh, all your documentation, and it's all this explicit knowledge, right? Run books, but also the implicit ones, the best practices, the operational behaviors. And it's really reactive. It's live collaborating with your engineers and it's able to build new learnings constantly. Again, if you think about how a new engineer learns, they come in and they shadow a tenured engineer. They ask questions, they see where they began, they see which tools they start with they go down the wrong path and they get corrected. So when engineers are providing feedback, you know, they modify how an investigation is done, uh, they validate findings. Resolve uses those as signals to improve future investigations, make them more robust. So then when an incident does happen, Resolve has this context-specific knowledge that it can pull from. It knows which run books to apply, you know, which, uh, techniques to use, what's, what's worked in the past, maybe what hasn't, what investigations might have a similar flow. And it's working alongside your team again it's observing how they investigate what tools they're using, incorporating their feedback live. And this drives resolve to have more context aware and nuanced investigations. So Resolve can tackle these problems by combining model capabilities with the expertise that's scattered across your system. And through this, it's able to really understand your production system, right, and operate all of your siloed tools just like an engineer does to build this model, this constantly updating model of exactly how your system works, how things depend on each other. And again it can do this multi-agent orchestration to run all these tools in parallel, run all these workflows, you know, that mimic like teams working together, engineers working together so this combines the expertise of all the engineers across your system. And finally, it can capture all of the scattered tribal knowledge, the explicit and the implicit knowledge that's never documented. Through this learning and memory system and it's collaborative, right? It's getting smarter over time and it's learning from your engineers. So we really built resolve to encapsulate a real world engineer to face these real life problems that live in a production system. And here are some of the lessons that we learned while we built Resolve. So first of all, it's not just a model problem, right? At first we thought maybe as these models get better, this problem might get easier, but that's not the case because again, production systems are super complex. We really have to encode this understanding of, you know, how to use these tools like an expert, all these cross layered dependencies, all this domain expertise within the architecture itself. You can't really prompt engineer your way into understanding how to use all these tools, like query all these tools, uh, get the right context at the right time and know how all these dependencies work and, you know, keep the knowledge up to date at all times. The second lesson is that tool use is non-trivial, right? These raw APIs are just super indigestible for humans and models. And again, we have so many tools to learn. And each has its own proprietary language. It gets very complex. So an AI really needs to be able to handle each of these tools, uh, you know, know it's syntax, know how to handle errors, uh, know how each team uses each of these tools, etc. And the third problem that we see is context windows don't solve everything because data in a production environment is theoretically infinite, right? It's there's so much noise and you just can't fit these hundreds of millions of log lines into any context window. And becoming more intelligent isn't just getting more context, it's about knowing exactly how to start at the narrowest point within your production system, right? How to use the right tools to get the right context at the right time. Fourth problem is that you know evals are really hard. They're just as hard as building the product itself, and to really trust resolves reasoning quality across, you know, these very long and complex investigations. We really have to replicate that production system level complexity within our evaluations. And finally, Resolve is connected to so much data, all of the customers' data, and we really need to protect the customers' data, right by at run time, uh, redacting all the PII, redacting all the sensitive information, and making sure that everything stays on prem and safe for them. So now that you've seen Resolve Live and you heard a bit about how we've architectured it, I would love to introduce Chris from ZScaler. I'm here to represent the customer side of the conversation. So that means out of the three of us, I'm the one who is always right. Not them, um, but. Z scaler, I assume probably most people know what it is that we do, but I'll give you the brief nickel tour real quick, um. We have about a little over 9000 different corporate entities that we consider our customers. Translates to about 50 million individual human users. And as a zero trust platform, what our job is, is to take their internet traffic and stop bad things from happening with it. The number here is every day we stop about 9 billion bad things from happening. That could be someone trying to download a virus, someone trying to go to a phishing site, someone trying to accidentally exfiltrate credit card numbers, whatever it is. Our job is to stop it, and we do it globally. What the infrastructure to do that looks like is hundreds of thousands of systems spread throughout the world. A lot of that is honestly bare metal. 160 data centers, we have gear stashed in throughout the world, but there's also various services that run on all major hyperscalers, notably AWS as well. And the idea behind this is, is not to try to impress you with scale, because scale isn't necessarily the hard problem here. It is to tell you that this is not a crud application that we're trying to keep running here. This isn't hello world, this isn't Spring music. It's more like a very large distributed next generation firewall router firmware kind of thing that spread throughout the planet and has to run or the internet simply doesn't work for our customers. And I'm interested in in trying to dispel to you any notion that oh this agentic AI operations, that's only gonna work in hello world on Cub, and I'm here to tell you actually, you can do it even in, in complex, very bespoke systems like we have at ZScaler. So, the numbers here I'm gonna talk about, um, these are really in practice, only a slice of what we as an organization care about. But me in, in incident command at ZScaler, this is what, what I kind of deal with on a daily basis. There's about 150,000 alerts that occur in any given month. I'm defining alert very liberally here. But some of these are important, they're important enough that a human addresses them. Some of them are important, but super easy to automate a response to, so automation deals with them. And then there's an additional pile that are really useful in aggregate looking for patterns over time. But out of those alerts. About in a given month, 120 incidents are going to occur. What that incident is, is something that actually has to get people onto a bridge, all into a zoom, to work together to resolve the thing on a ticking clock. And in any one of those bridges, generally me, as an SRE, my job is to pull all of the right people in to make sure that we get to the resolution as quickly as possible. That could certainly take 30 or more people. The class of incidents that actually I'm kind of the most interested in personally, aren't so much the super high severity ones, those we actually can address, we have a a real low SLA on those because we have to to keep things solid for the customers. But a lot of kind of the middle ground, a lot of these incidents just chew up time. They can easily take over an hour to resolve. I am pulling in an awful lot of additional disparate teams to make it work, and I just want to move faster and waste less people's waste less of people's time to get to a resolution. There we go. So what, what it actually looks like, and I keep talking about the number of people that are involved. Something like you see up here, way on the left is me. I'm in incident command. I'm the person managing the people, I'm the person managing the Slack channel, trying to page people in, and I have to combine human resources and technical resources to get to a resolution. Up at the top, those are some of the, the human resources. I'm always going to be pulling in different engineering teams. Maybe start with one. Start with the one where the noise started. We might learn as an investigation progresses, actually with some downstream service instead, so I have to go to a 2nd engineering team, and depending on the nature of the incident, I might have to bring in. Networking infrastructure teams, systems engineers, the support team that's actually been working with a customer. I could probably actually add boxes all day long onto that, but it is, suffice it to say, a reasonable number of people, and every additional person that I have to add there is time waiting for them to show up or someone I'm taking away from doing their creative day job that adds value for, for our customers. But we're of course also integrating all of those sources of telemetry that that that Claire was talking about. You know, for us we happen to be a cloud view shop, but we're going to be looking at logs, we're gonna be looking at dashboards. Our observability solution is key to the incident management process, but you're, it, it isn't, it isn't something that exists in isolation. We also sometimes have to go to Jira to figure out, well, what actually did get deployed last week and what did it mean? Um, and because we have a bespoke custom infrastructure, we also have bespoke and custom observability tools that we use to manage things. Um, so all of these join forces to solve an incident for us. So if I'm going to take an AI agent and I'm going to try to At least help all of those teams as much as possible. What are the boxes that I need that agent to check? Well, I definitely need it to easily integrate with all pieces of my observability stack, logging, dashboard, metrics, etc. as well as the tools that I use to manage the incident, like Slack and Zoom, what have you. I'm going to want it to act autonomously, at least in some instances. That's where we were talking about the uh automation of investigations around alerts. If it can kick that off and have an investigation solved before I even get into a bridge, that is an absolute win. I would like it to be done before I get there, that's just great. Um, but also, and this is absolutely critical, it, it actually isn't hard to put together an agent that's just gonna give you more information about your system status. That doesn't really help me. I don't need more numbers or more data. What I need is a root cause, or at least really good working theories as to what the problem is, given my context. That's really, that point number 3 is really how I'm going to be grading the performance of that agent. And of course, I needed to work at scale in my environment, which is bespoke and interesting and very unique to Z scaler, and certainly isn't hello world. So, what does this actually look like? What did we do? Well, this is sort of a, a goal state. In a couple of slides back, I had a whole bunch of bubbles with a whole bunch of different teams. What my goal state is, if everything goes as planned, I can be that incident commander on the right, and rather than vast teams of people that I've taken away from their day jobs to help me resolve something. I instead have a robot engineer in the middle there that is resolved, that can navigate all of those data sources for me to get to a resolution. So I collapse all of those teams and rather than paging them in. I can just page resolve, and if it gets too thick, well then, then I'll go to 2 additional human teams. But why, why resolve? Why did we at Z scaler select resolve? Um, so there's, there's, there's a handful of things that it, that, that became very obvious during our evaluation of different products. One of which is, you know, like dashboards are fantastic. But you, they can proliferate and sort of grow on you over time, so that you have hundreds of them, and it can be difficult to understand per dashboard. What dashboard do I need, given the context of the incident that I'm in? What panel do I need on what dashboard? What metric, on what panel, on what dashboard do I have to worry about. And it turns out robots are pretty darn good at figuring out that answer for you, and then putting the metric into context and actually interpreting it just like a human would. So kind of the same data sources, that dashboard, it's certainly great for people. Turns out it's also great for robots and Resolve does well with that. Log queries we were just talking about, the one thing I really like that Claire said was that she mentioned that it's an iterative process. In practice, like think about your own incident time log querying. That iterative process is actually kind of frustrating. You're sort of hammering on it, trying to figure out which field that you actually needed there, and all of that is time that you are spending fighting with a bit of an arcane query language that you could have spent doing something else. Turns out agents are really good at doing that. Um, One thing human beings are absolutely terrible at, at least individually, is working in parallel. One thing that AI agents, and particularly Resolve, are good at doing is working in parallel on you. So while you're sitting there, it's spinning down multiple paths trying to find reasonable working theories for your problem in parallel. And again, what I always want is the thing that resolved as well, which is give me at least working theories, don't just give me information. I don't want you to tell me what went wrong, I know what went wrong. I need you to tell me why. So here is a heavily redacted, as you can see, it's largely redacted to make me feel important if I'm honest with you, but there's a very large redacted, um, screenshot taken from an investigation we did during our POC process with Resolve. One of the reasons that I picked this was, this was an auto investigation of an alert. The alert name is up top there. It was kind of a fresh alert, it was, it was it wasn't really being used yet, um, by our SREs, but it happened to have fired. And 2 hours after that alert had fired, a real life incident did spin up that this alert was essentially indicative of. And after getting into the office, looking at what Resolve had going on, it turns out that it actually had a very good analysis of that result 2 hours before we even created the, the incident. Um, and it's not just a description of what happened, you know, the alert and the notion that an alert occurred, that is obvious, I can see that. Registration failures happened, but why? Well, it happened because of DNS resolution failures. It happened going to this domain. Uh, and that's the kind of thing that's actually helpful for me to resolve the incident. But life, like I, as an SRE I kind of see the world through incidents. It's kind of a miserable life, but it isn't the only thing that is, is that, that agents are useful for. It just doing your basic planning, your basic operations, keeping the lights on, capacity planning, it can actually be quite helpful for that as well. You know, here's, this is actually sort of a generic screenshot. But the idea is if you, you're, you're trying to deploy a very unique workload, doesn't quite fit into your usual capacity capacity planning scenarios, and you just go into chat and say, hey resolve, I'm trying to deploy this really weird crazy thing. Here are its characteristics. They don't fit like anything else I have. When's the best time to schedule this thing? Um, that, you know, you could spend some time trying to figure that out in Excel or write some software to do that for you, or you could just go. Ask resolve and be done in 15 minutes. The goal for us as we move forward is to reduce that hour-ish time for those middle severity incidents. Reduce that down to 15 minutes, cut it by 75%. Ask me in a year if we got there, but that's what we're trying to accomplish. Personally, I'm trying to get at least, I'd like to see that number way higher, but get 30% of the people that we page into incidents out. Not because I don't like them, generally I love them, but every time I page people in, they could have been doing something better with their time. And there's sort of a, a, a, a before and, and crisp prognosticating what the after is going to be viewed here. But what we can do with resolve today. Is investigate alerts autonomously, that's great. That's something that is in production, it's helpful, we can do that now. We can also invoke AI or resolve AI on demand through chat interfaces to help us debug on a daily basis. But what is our general use of AI? Say resolve and, and beyond that, what's it gonna look like for operations at ZScaler going forward? You know, I, I firmly believe. That we are going to see, uh, we are going to put AI in place for, you know, far beyond just incident resolution to actually taking an active role, say, in, in scans, synthetic injection of tests throughout our network, get more active maybe in terms of supervised, responsible human in the loop, automated remediation. And with any luck, what we're gonna get from that is engineers are gonna get back to engineering and spending less time in my bridge, and that's what I'm hoping to accomplish. So, thank you. So Chris, you are not off the hook yet. I have a few pop quiz questions for you. Uh, so my first question for you, Chris, is that, uh, I mean, you are one of the smartest engineers that I know. You have a number of engineers at your disposal. We are in the GEI era. Why not wipe code something like Resolve in-house? Why try to partner with the vendor, uh, for this problem? Sure, so. You know, honestly, in, in isolation, in bits and pieces, you know, we have done small experiments that I would consider to be somewhat successful, at least in terms of giving, giving you a surface level view into what's going on with the problem. Where the rubber meets the road is, is really, you know, we were talking about sort of that service dependency graph. If I really wanna get to what a root cause is, that's the kind of thing that you need the professionals to do if you want to happen to happen reliably over time. Uh, so that, that is something that I felt that I was incapable of doing on my own, and was able to get really good results from resolve with. That's the thing the first. Thing the second is just that the speed at which the space of aging technology, and also even, you know, model technology changes, requires people doing this full-time in order to keep up, and I already have a job, right, so I'd, I'd rather give you money to do that for me. All right. I'll, I'll take your money happily. Uh, uh, and, you know, very excited to work with you guys. My second question for you is, you know, ZScaler definitely knows a thing or two about security, and there is, you know, a lot of important data, even more important than source code in some of the telemetry systems. So, you know, can you share a bit more about how we were able to win your trust to, to serve you in these kind of use cases? Sure, a couple of things. Like 11 thing that is not, um, it, it, it, it certainly isn't glamorous, but it starts with the simple things like sock 2 certification, right? Like you, you at least were able to check boxes that I could go to security folks and say, hey, like we're, we're cleared to go here. But that's the easy thing. It, it's, it's obviously far more than that. One thing that I think may be somewhat underrepresented from the architecture described thus far is that the, the way that this is, this is all deployed, I'm not handing over the keys to the kingdom to resolve. I'm not even giving it complete access to all of my telemetry. You wouldn't want it if I gave it to you because it's just too voluminous, um, it's. Uh, a very small deployment that we put aside, you know, beside our, uh, telemetry sources that means that I don't have to expose any customer data whatsoever to you. I don't even have to expose any telemetry that you don't want to. I just can expose the bare minimum, but I can still at the same time get like the ease of a SAS solution. It, I can have my cake and eat it too, basically. Amazing. And then my final question, you know, this is a fun one. So if you could look into the crystal ball, and if, you know, uh think about what we'll be talking about here next year, what kind of outcomes are you most excited about driving with AI, even, you know, beyond Resolve AI in, in your team? I think the, the one thing that's probably very obvious to everyone, including myself, just the, the, you know, the nature of the tools available to developers right now, it's, it's kind of mind boggling how much easier that's made our life. I think it, it, the market's been a bit slower to see what. Agentic AI can do for operations. Obviously you're on top of this, but I, I think a year from now it's going to be just as talked about, um, as, as the dev side of it, and that's, you know, we're just not quite there yet. Amazing. So on that note, you know, thanks a lot to everyone for joining us. Uh, you know, please come see our booth. There's a lot of, uh, good swag. Uh, try out Resolve AI. This is a QR code. You can scan it and you can, uh, you know, try Resolve out yourself and, uh, happy to stick around and, and answer any questions.