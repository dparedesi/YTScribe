---
video_id: igZT3m39azE
video_url: https://www.youtube.com/watch?v=igZT3m39azE
is_generated: False
is_translatable: True
summary: "This session, \"Anatomy of a Cloud Ransomware Attack: A Live Simulation (HMC206),\" is an immersive tabletop exercise led by Rubrik, simulating a sophisticated ransomware attack on a fictitious company, \"Horizon Retail.\" The simulation unfolds in five scenes, demonstrating how a seemingly routine morning escalates into a catastrophic business crisis. The attack begins quietly with anomalous S3 API calls and EBS latency, orchestrated by attackers using compromised credentials (via social engineering) and exploiting a forgotten, unpatched QA server with overly permissive IAM roles. The attackers leverage \"living off the land\" techniques, using the victim's own cloud services (like S3 batch operations and KMS) against them to encrypt data in place. Key takeaways include the critical distinction between \"operational recovery\" (where data/identity are trusted) and \"cyber recovery\" (where they are compromised and untrusted), the failure of versioning as a backup strategy against malicious encryption, and the dangers of \"shadow data\" (untracked PII in dev buckets). The session culminates in Horizon Retail being forced to pay a $6 million ransom because their backups were compromised and a clean rebuild would take too long, ensuring financial ruin. The ultimate lesson is that true cyber resilience requires immutable, air-gapped backups (credential isolation), complete data visibility, and executive alignment to prioritize security over feature velocity."
keywords: Cloud Ransomware, Cyber Resilience, Tabletop Exercise, Rubrik, S3 Security, IAM Misconfiguration, Cyber Recovery vs Operational Recovery, Shadow Data, Backup Immutability, Ransomware Simulation
---

Can everybody hear me? OK, show of hands. All right, great, thank you for joining this, uh, this unique experience, um, uh, we're calling it here in the context of reinvent anatomy of a cloud ransomware Attack live simulation show of hands, how many folks in the audience have been through a tabletop exercise before? All right, so about half the audience, cool, um, I'm gonna give you a little rundown on how this goes down, but first I wanted to introduce Zero Hour. That's what we call this event. It's an immersive tabletop experience that Rubric has created to help our customers. Understand uh what what folks go through during a complex cyber attack. Um, a lot of what we've built in this is based on real cloud tactics, techniques and procedures that threat adversaries use to infiltrate cloud environments. This is not a real company. This is not a real scenario, but it is based on real TTPs. So, uh, just want everybody to know that, uh, what you're gonna see here can happen, uh, and hopefully will not happen in your organization. We, um, let me introduce, I guess first the, the organization, the fictitious organization that we created, Horizon Retail. It's a cloud forward retail organization with a multi-country physical store footprint. With mission critical deployments in AWS. There's 3 things that we ask as a part of these exercises. The first is observe the gaps in horizon strategy. And how that led them to this no win crisis situation at the end, right? The second that we ask you to do is just reflect, reflect and bring back some of those learnings to your own organization, right? There's gonna be pieces here that you're gonna be like that could never happen to us, and we've got that covered, but there may be other pieces here that you might look at and be like, hm, you know, I might wanna bring some of that back to my security team. So please use this as an opportunity to reflect it's a safe space we're gonna try to leave questions at the uh leave time for questions at the end. Uh, I can't guarantee we're gonna get to questions, but we will try to leave time for questions at the end. That's the participate part. Um, when we do this in a much smaller setting, it's about 200 people in this room right now, when we do this in a much smaller setting, we usually will have the participants read the parts. But unfortunately we're in a very large group here. uh, we will try to pass around a microphone at the end so that you can participate and when I say participate, I mean either ask questions again and this is a safe space or lend some of your own experiences, right? I am sure there are folks in this room that have been through an attack before. And it's a scary experience and you think if you think others in the room could benefit from your experiences, we would love to hear it because that we need more of that, right? That's ultimately what uh what we try to do as a part of this exercise is to foster that discussion. So these will be the participants, uh, these are folks that I work with day to day, uh, at Rubric, and, uh, they will help me with the script and the read through. How's this gonna go down? It's about a one hour event and usually it's a one hour event because we foster discussion after every scene, very big room here, lots of people we're not gonna be able to have a very deep discussion so what I'm gonna do is I'm gonna summarize the key takeaways after every scene. I will have a key takeaway slide we'll go through it and again at the end hopefully we can pass around the mic and have time for questions. There will be 5 scenes. Scene 1 will be a video. Scenes 23, and 4 will be read-throughs. These folks behind me are playing parts. You could maybe read these signs, and if you can't read these signs, I'll give you some idea of the type of personas that will be involved in this. People like an ops analyst or a head of cloud ops or a CIO, right, those are the kinds of folks that are gonna be involved in a mem call that gets called when these things go down, and they will play those roles. Again, sorry for the font size. Sorry for that these signs may not be big enough for the folks in the back to see. If you wanna come up and get an understanding of who's speaking, we can do that. There are more than these roles. There are 4 more roles. They will be done virtually, and my friend Aud here will orchestrate all of it flawlessly, right? Uh, we've, we've had a lot of discussions about how we orchestrate this thing, so it's kind of an inside joke, um. As I mentioned, normally we facilitate discussion after each scene, and it's folks that are just trying to either vent, right? Because again, if you've been through it, it's, it's a hairy experience so we have folks that just like to vent and that's cool, but we also have people that just wanna lend some expertise to the discussion or ask questions or ask further uh clarification again we'll try to do that at the end. I am going to give you key insights after every scene and the learnings that. Our customers hopefully get away, uh, takeaway at the end of every scene, and as I mentioned, hopefully we'll leave time for an interactive audience discussion at the end. With that, let's start with scene one. The modern enterprise is a marvel of efficiency. Every transaction, every shipment, every customer interaction is a seamless flow of data orchestrated in the cloud. The business is the application, the database, and the cloud services that run them. But what happens when the cloud that powers it all. Balls. We are inside the IT operations center, a room that is usually quiet at this hour. The team is starting a shift that is about to become anything but routine. The first sign of trouble appears not as a single critical alert. Hey Alex, But as a chorus of discordant data points. Can you look at this? Yeah. The right latency on the primary EBS volumes is through the roof, but Red IO is completely flat. That's weird for a Sunday morning. Well, yeah, you're right, it's not a DDoS attack. External traffic is normal. I mean, we haven't even pushed the new coach since last Friday. It's like a single process just keeps writing over and over. What about, what about the S3 buckets? Checking now. Oh wow. I'm seeing thousands of API calls per minute against the critical buckets. Put object, put object, delete object versions. It's just the object names. They're all being rewritten. There, there's a new suffix locked. This is not a runaway script. This is hostile. This is an active attack. You're calling early. What's going on? We are under active attack. I'm seeing what looks like mass encryption across our core S3 buckets. I'm also getting high IOA alerts from the primary RDS instance now too. I mean, the customer facing websites, they're throwing 503 errors. I mean this feels coordinated and it is moving incredibly fast. To find coordinated, what do you see in cloud trail? Is it one IAM roll being used? Multiple? Is activity coming from a known IP range or something external? Cloud trail is flooded. It, it looks like thousands of unauthorized SSEC encryption calls originating from within our own VPC. I mean, they aren't trying to pull data out. They're encrypting everything in place. Guard duty is lighting up with alerts for unusual activity from a single EC2 instance, a QA server I've never even seen trigger an alert before. This is bad. You're inside. Start a log dump to a secure, isolated account now before they can purge them. Use the out of band management network, get the CIO on the line, and initiate a major incident management call for the entire crisis team. I'm on my way in. By 6:30 a.m. the first members of the crisis team are. Give me the situation report and blast radius is not an answer. I need application names, business units impacted, and a preliminary assessment of potential data loss. It's a sophisticated ransomware attack and it appears to be centered on our US East one region. We've confirmed the active encryption on the EC2 instances, hosting the web front end, the S3 buckets that hold all our process data for BI and analytics, as well as the primary RDS database that manages inventory and logistics. We've also found a ransom note in the root directory of a dozen servers. Let me be clear about what that means. If that RDS instance is compromised, the stores are operating completely blind. The handheld scanners in our warehouses that associates use for picking orders are effectively bricks. The system that optimizes truck routes for daily restocking is offline. The point of sale systems can't even validate gift card balances or process returns correctly from in-store staffing to deliveries and inventory intake. We have to assume every single digital service that touches a physical store is either down or about to go down. Yeah, this, this isn't just a website outage. This is a direct decapitating strike against our core retail operations. I mean, the East Coast stores are scheduled to open in less than 90 minutes. I, I'm, I'm activating the full incident response plan. Get the entire crisis team assembled. I need the CISO and the general counsel on a call in the next 15 minutes. We need to decide if we even open the stores. Just to reiterate, we can't just reboot the servers. This isn't a hardware failure or simple outage. This is a hostile takeover of our environment. Every move we make has to be deliberate or we could accelerate the damage. All right, very dramatic. The modern and we have uh a star in our midst. Our application support manager is right here who has now changed roles. He's now operations analyst at Horizon Retail, so yeah, he's got a new job. Um, so I guess the first takeaway here, right? Modern attacks begin as a whisper, not a bang, a chorus of discordant data points, right? That was, that was mentioned in the scene. Um, you know, it's, it's cloud attacks are complex. It, it usually involves multiple services. It requires specialization to troubleshoot. You've got EBS latency at play here. You've got S3 API calls at play here. You've got guard duty that's lighting up. It requires specialization and coordination and most importantly, it requires a visibility. Right? Visibility into how all of these services interoperate with each other within your environment, so that when you do have to, uh, you know, troubleshoot and ultimately recover, uh, that you are, um. Um, that you're doing so in a way that that is measured. The second thing, cloud attacks, and, and this was again very dramatically said in that scene, direct decapitating strike on our retail operations, they are business attacks, right? Your business runs in the cloud. An attack on a core data service like a database or object storage, it, it, it can paralyze everything, everything. In your environment and most organizations haven't mapped the dependencies between their critical business functions, right? Taking a true inventory of what you have in your environment, ensuring that it's all mapped in a CMDB, how many folks have a CMDB that they feel is up to date and is accurate, 100% accurate? I see one hand in the front here. Uh, I see another one as well. That's cool. I'm glad that you guys have that confidence, right? That's really, really important to ensure that you have an accurate inventory of what you have. If you don't, it ultimately is you flying blind when some of these services go down and when that business impact happens. The last thing When ransomware strikes, it strikes at machine speed. It will encrypt very quickly, uh, code finger, the code finger attack from the beginning of the year. Who remembers the code finger attack from January? Has anybody heard of it? One in the front, maybe a couple. It was, uh, essentially encryption at rest of S3. It was using customer managed keys, SSEC, as a matter of fact, um. AWS will be deprecating SSEC for S3 in April, uh, as a result of the fact that there were these vulnerabilities that were causing impact in their customers' environments. And that once the attacker gains a foothold into the cloud environment, they use the same services that you use to maliciously impact your data, right? It can encrypt hundreds of thousands of objects in minutes. And even if you have versions, it's not necessarily gonna help you here because those versions are gonna get encrypted as well, right? If you turn on immutability, that's an option, right? You can turn on immutable versions and you can have immutable versions, that's gonna cost you, right? So it becomes a cost-risk play. And something that you're gonna have to determine in your organization, how secure do you wanna be. In the impact in case there is an impact, um, and again versions are most likely gonna save your bacon when it comes to operational recovery, not necessarily from a cyber recovery event, and we're gonna, we're gonna talk about the differences between operational recovery and cyber recovery in a future scene. So let's hop over to scene 2. And we're gonna start with the read-through. The time is 8:00 a.m. Eastern. I play the narrator, by the way. I, oh, I have, I do have one. I'm the narrator. The time is 8:00 a.m. Eastern. The first stores on the East Coast are attempting to open their doors amidst chaos. The full crisis team, including the CISO general counsel and the line of business leader, are now assembled on the emergency MIM call. The atmosphere is thick with tension as the team tries to understand the full scope of the attack. Our forensics analysis of the logs have given us a much clearer picture. The initial point of entry seems to be social engineering. They called the help desk and somehow got credentials for a standard employee account. A standard account? How did they escalate from there? Our internal segmentation should have contained that. It should have, but from that account, they scanned our network and found a single unpatched QA server. They exploited that to get access to a misconfigured IM roll attached to the instance. This is a piece of technical debt that we never cleaned up. And that misconfigured role, what did it allow them to access? Enough to compromise one of our cloud admin accounts, and with that, they took over the identity provider. It was a chain reaction of human error followed by technical oversights. Once they controlled the identity provider, they could move anywhere. They basically had the keys to the kingdom. So with the control of our SSOs they could get to the retail ETL role, the RDS databases. They did. That role gave them complete control over the RDS databases. They also deployed scripts using SSEC server side encryption with their own keys across our most critical S3 buckets. It's a nightmare scenario because it makes recovery from versions or replicas impossible. The forensics also show they were in our environment for weeks exfiltrating data low and slow the whole time. They didn't just lock our data, they took a copy first. I just got off the phone with our regional manager for the Northeast. Customers are abandoning full shopping carts at the register, because the manual checkout process is taking over 20 minutes per person. The media has picked up on it. We have local news vans outside two of our flagship stores. Our store managers are asking for permission to close. The damage to our brand is happening in real time on live television. Based on our hourly revenue rates, we are losing hundreds of thousands of dollars an hour. This is an absolute catastrophe. The time is now 11:00 a.m. Eastern. The situation continues to deteriorate. We have an update. Our external security consultant is now engaged based on the attack tactics and techniques, as well as the specific ransom note, our external consultants believe with high confidence that this is the work of scattered spider. And I can confirm that after consulting with the FBI we have established contact with this group through channels they provided. The demand is $4 million payable in Bitcoin. The deadline is 3 days from now. We are not paying. This is what our disaster recovery plan is for. This is why we have backups. Head of cloud operations, reports on our recovery posture. The EC2 instances are recoverable from AWS backup. What about the S3 data? Well, that's the core of the problem. While we do have backups for a portion of our S3 data, for a critical share, we've just been relying on versioning and cross-region replication. As you can imagine, versioning's not a backup, and it specifically does not protect against this attack vector. Since the attacker used server-side encryption to re-encrypt every version of every object with their own key, the objects are there, but they're indecipherable to us. The replicas in the other region are just copies of the same useless encrypted objects. This is unacceptable. Who signed off on a policy that left our new stores data so vulnerable? It was a decision made during last year's capital allocation review to reduce cloud storage costs across the board. The risk was documented and formally accepted, with the understanding that versioning would mitigate the accidental deletion. Nobody anticipated an attack of this sophistication. How much data are we talking about? How bad is the S3 data loss? It's a severe loss. All operational data, POS and sensor logs, all our BI data sets for our entire fleet of stores opened in the last fiscal year. A full year of our company's growth has been effectively wiped out. It's almost like the stores have never been in operation. The main call ends. The operations analyst leans over to the head of cloud operations. My team filed 7 tickets over the last year about inconsistent backup validation and coverage gaps in our cloud environment. They were all deprioritized in favor of new feature development. Those tickets, document them and keep the files ready. All right, great job, everybody, thank you. Um, yeah, a little CYA being played there on the horizon retail part, right? Uh. I, I filed those tickets on those tickets, document them, keep them ready, right? How many times have you as an organization deprioritized quality of life and security for new features, right? It happens all the time. It happens across our customer base all the time. So let's talk a little bit about the attacks and how they got in, right? Um, it was a human and process failure in this particular instance and one of the things that was fairly unrealistic about this scenario, you guys probably some of you, if you've been through a tabletop exercise before have already caught on to this, it was fairly unrealistic that Horizon has that much of the attack chain understood. In scene two. Right, That usually takes weeks, it could take months. Sometimes there's never root cause analysis of how the attackers got in, of how they infiltrated the environment, and ultimately the scope of impact, right? So, uh, because we have to keep these to an hour, we accelerated things a little bit and Horizon Retail and scene two, bam, they got it. So, what happened? How did they get in? unpatched QA server. OK, that, that could happen. Misconfigured IAM role, an overly permissive identity. How many of those do you have in your environment, right? Sentinel One in their uh cloud security risk report we we our friends on the perimeter side Sentinel One CrowdStrike. I'm gonna reference a lot of their stats here as we go through this IBM um we, we use a lot of their stats on, uh, uh, trying to understand the cloud security landscape. They basically say that almost all cloud environment failures can be attributed to some form of human error in this particular instance. It was a socially engineered credential that they got a hold of. Once they got a hold of that socially engineered credential, they found the retail ETL role, which was, uh, uh, essentially an overly permissive identity that gave them cross account access. Once they had cross account access, read write cross account access, they had the keys to the kingdom, right? Crowd strike in their report. That abuse of valid accounts is the primary initial excess vector for cloud breaches. It's about 35% of all cloud incidents come from, uh, abuse of valid accounts. Attackers are not hacking in. They're logging into your environment. They are compromising credentials and they're logging in. Maybe they're finding AWS keys that got leaked, maybe they're socially engineering. A, a, a, some identity within the environment, but they're logging and they're not hacking in and as a matter of fact they look like an authenticated valid user on your network. Bottom line is, if you had to count how many times you found a misconfiguration or a forgotten server or some shadow IT that was sitting out in your cloud environment, uh, you're basically playing a bad game of whack a mole. Because you're gonna find those things all the time, you're gonna tamp them down, you'll probably find them again. 80%, according to Sentinel One, of data breaches can be attributed to some sort of misconfiguration. Second, operational recovery, versioning, it's not a backup. Versioning is not gonna save your bacon. If you had to recover hundreds of thousands of objects from versions, and you didn't know the point of time you had to go back to, how would you do it? You're gonna write a script to do it? I'd like to be the person who writes that script. It's fine for operational recovery. It's fine when your data and your identity are still in a trusted zone, right? But in a cyber recovery scenario, your data and identity are inherently mistrusted. So because of that, you wouldn't know what to recover from, right? How would you potentially recover hundreds of thousands of objects from versions? The 3rd Today's breach was yesterday's budget cut. Right? How many times in our careers have we been faced with hardening efforts or quality of life things being deprioritized for new future development? It happens all the time and that's as much a management failure as it is the support person's credentials that got pilfered, right? Everybody's gonna blame it on the support person, but ultimately it was a management failure at the very beginning to deprioritize disheartening over new future development. So again, two critical things brought into focus here. The first, prevention alone isn't enough. The attacks started with human error, exploited the chain of vulnerabilities. Those vulnerabilities could exist anywhere. Second. If we adopt an assumed breach mindset, the ability to recover is everything. Right? If you assume that at some point you're going to get breached, you need the ability to get that back. In this instance, their recovery plan was based on a flawed assumption that based on operational recovery, right? And not based on cyber recovery techniques. All right, let's hop over to scene 3. The time is 9:00 p.m. on Sunday evening. The teams in the IT operations center have been working for 15 hours straight. The initial shock of the attack has worn off, replaced by a grim, bone-deep exhaustion. The mood in the room is heavy with the weight of finding a path to recovery, as every attempt so far has ended in failure. I've mounted the 10th EBS snapshot from our backup vaults. I'm running a deep forensic scan on it now, but it takes nearly an hour to scan each 1 terabyte. At this rate, it will take us days just to validate the backups for a single critical application. We're looking for a clean needle in a mountain of infected haystacks. I know it's slow, but we have to be certain. They were in our environment for more than 1 month. They knew our backup retention policies better than we did. They waited until their malware was replicated across all of their recent restore points before they launched the main attack. We're not just defining encryption, we're fighting their 30 days of reconnaissance. And this is a completely manual hit or miss process. Our backup tools have served us well for operational recoveries, but I'm only realizing now that they're greatly lacking in dealing with such sophisticated attacks. They were never designed to withstand a direct malicious assault by an attacker with admin credentials, and they can't even tell us which backups are safe. Wait, I think I have something. An EC2 snapshot from two weeks ago, just before we believe the main malware was planted. The initial scans, they look clean. The rootkit signatures aren't there. It's a risk, but it could be contain a time delayed payload that our scanners are missing. But it's the best lead we've had all day. It's the only shot we have. Let's do it. Kick off the restoration process into a new completely isolated VPC. What's the ETA? Uh, if we push it and if everything goes right, we can have a test environment of our core application up by midnight. The time is now 1:00 a.m. on Monday morning. For the first time in nearly 24 hours, there is a flicker of hope in the room. On a monitor, the core application is initializing in a clean environment. We are online. The app server is up and the database is connected. We are running the first test transaction now, uh, and it worked for about 5 seconds. It worked. Uh, we were cheering. Wait, wait, wait, no, uh, the dashboard just died. CPU utilization is pegged at 100% and the file system, I can see it happening. The files are being re-encrypted right now. How's this possible? We scanned that snapshot. It was clean. The malware must have been dormant, it was waiting for a specific event, a connection to a database, a specific API call. It was a time bomb and we just triggered it. Shut it down now, terminate the instances now. The time is 2:00 a.m. The brief moment of hope has been utterly extinguished. On the video conference, the faces of the CFO and the general counsel appear. They look grim. The CIO has to deliver the bad news. I'll get straight to the point. The financial consequences are severe. Our financial modeling for Sunday shows a loss of over a million dollars. Our stock is projected to open at least 8% down. That's half a billion dollars in market cap. Are you now telling us that our only viable backup has just failed and we're essentially back to square one? Time is of the essence. It was a significant setback. The attack is more complex than we anticipated. The team is now working to analyze much older restore points. I appreciate that we're all understandably tired and that your team is working incredibly hard. While I value your technical explanation, my primary concern is that the company seems to be in a financial freefall, and that's worsening every hour. To put it bluntly, what is the plan here? While you formulate one, I must inform you that we have received another email from the attackers. They have seen our failed recovery attempt. They're raising the ransom to $6 million. They feel they have the upper hand and legally they might. The story is now being picked up by national news outlets. We are losing control and control of the narrative. Yeah, tough spot that keeps getting tougher uh for, for Horizon, um, so the primary objective of the attackers, um, first is to go after your backups, right? If they can go after your backups and they can infiltrate your backups or poison your backups, you have no recovery and their chances of getting paid just incrementally went up significantly, right? They use that long dwell time, the time that they're doing reconnaissance in your environment to study your recovery policies and to ensure that the malware is planted, um, and across any, any of the restore points that you have and. You know, ultimately our own research has, has found that 93% of organizations reported malicious attackers attempting to impact data backups during a cyber attack. It happens in every cyber attack basically that they're looking for the backups and to poison those backups so that, um, so that, you know, obviously you have no recovery. But that clean backup, if you really don't find clean, that clean backup could be your ticking time bomb, right? In this particular instance, there's one thing that Verizon Retail did right here. First thing is that they had it recovered in an isolated VPC. So they had an isolated account in a separate VPC where they instantiated these backups, right? And that is important. Everybody in this room should have an isolated recovery environment. Uh, or accounts that they recover from that should be separate from your primary identity domain. I can't stress that enough. Credential isolation is really, really important in the cloud. But just because you have that backup, it doesn't mean that it's clean, you know, the reconnaissance, the lateral movement, uh, they all can involve planting malware and in, and delayed detonation techniques that ultimately the attackers use here and that was the needle of the needle in the mountain of infected haystacks, uh, problem that that that was articulated in, in this scene. One of the things I think that this brings up is the stark difference between an operational recovery and a cyber recovery, and I already mentioned this a little earlier. In operational recovery, your data and your identity are trusted. You just need to go back and rewind to a point in time, it's easy. Cyber recovery, you're dealing with inherently mistrusted data and identities. You need to ensure that everything that you bring back into the environment is clean. Forensic work needs to happen against all of it. And really, if you're not ensuring that your recovery and that these, that the, the data that you're recovering into these environments is clean, your recovery plan turns into a lottery ticket and, and that, that's just not gonna be acceptable, right? Those repeated recoveries into an IRE, right? Let's talk about that. They went and in a separate VPC, which was the right thing to do, started a recovery that turned out to be bad. Those repeated recoveries into an IRE and the forensic work that has to happen after the fact. Takes time and time is money. And your systems are down and the CFO is screaming that the stock just lost 8% yesterday and you have to. You're under pressure. You have that time pressure in your uh in, in this scenario, right? Time pressure is going to be a factor here. So what are the really two key, the key takeaways here? Cyber recovery, different than operational recovery, right? I think we can all kind of all agree on that. The second thing I think we kind of learned here is that traditional tools tend to tend to fall down in a in a cyber recovery scenario. They were built to protect against operational issues and not with not protect you against a malicious adversary who is authenticated into your environment as a valid user. They're acting as a valid user. So really you have to know which ones are compromised, which ones aren't before you start the recovery and doing that ideally against the backup chain. Is, is what's gonna save you time. Hence why one of the announcements at AWS at Reinvent this year was guard duty integration with AWS backup, right? May have heard it. This is because of this. All right Let's move on to scene 4. It is now Tuesday afternoon, more than 48 hours since the attack began. The company's stocks plummeted by more than 10%. The recovery efforts have been a brutal, demoralizing slog. The team now knows the attackers were inside their systems for at least 36 days. The cyber incident response manager has called a critical meeting with the IT and legal teams to address the scariest question of all, what exactly did the attackers take? We need to get to the ground truth of our data exposure. The integrity of our S3 inventory is now in question, especially considering the lost data. The legal team needs an accurate inventory of the compromised S3 data, specifically what type of data was in those buckets. We must know if we're dealing with a data breach. I've been running a deep scan of our S3 environment trying to reconcile our inventory with reality. I'm running a regex search for PII patterns across bucket manifests and object names. The problem is many of these buckets don't have data classification tags, so I'm flying blind. What are you seeing? I'm seeing files named Q3 rewards members full dump. CSV and loyalty test data prod sample.json. The file names themselves are screaming PII and worse, our S3 inventory is a mess. It shows 920 S3 buckets in our production environment. My deep scan has found 965. There are 45 buckets that are completely untracked, unmonitored, and unbacked up. The group reconvenes late Tuesday night with the general counsel and the CIO also on the call. The mood is grave. Yeah, those untracked buckets, they appear to have been spun up for the customer loyalty program that was in development last quarter. The customer loyalty program, you managed that project. What was the data schema for those development buckets? It, uh, it was a development environment. It shouldn't have had production data, but to properly test the new predictive models, the development team may, uh, they may have pulled a sample from the production customer database. I can confirm they did. I've decrypted a single file from the proof of life sample the attacker sent us. It's a CSV file from one of those untracked buckets. It contains over 1 million records with full names, email addresses, mailing addresses and birth dates. This is all personally identifiable identifiable information. This is officially a data breach. I need to pause everyone for a moment. This discovery is fundamentally changing our legal position. The moment PII was confirmed, we moved from a business continuity crisis to a major regulatory and legal event. We likely have 72 hours to notify the California Attorney General under CCPA. We have other notification deadlines in every other state and country which we operate. We need to retain outside counsel specializing in breach notification tonight. We need to establish a budget for credit monitoring for every customer in that database. The cost of this incident just grew by an order of magnitude, and that is just before class action lawsuits begin. Does everyone understand the gravity of this situation? Yes, we understand. Oh Yes. Uh, couple, couple, few, a few takeaways here, yeah, you can't protect what you can't see. Shadow IT is real, right? Thank you guys for, for this. I appreciate it. We'll give them a round of applause at the end. Oh, thank you. Thanks, thanks. It's hard to get up here and talk in front of this many people. Thank you guys. We appreciate that. Um, can't protect what you can't see, um. Shadow IT is real. 965 buckets, 920 in the manifest, 45 buckets untracked, unaccounted for. Turns out one of those buckets had dumps from databases, production databases that weren't obfuscated, that weren't masked because product because the development team needed production data in order to develop an application that happens all the time, deal with it all the time. And it's endemic in in large cloud of states. Our friends over at IBM in their 2024 cost a data breach report, which by the way there is 2025 report out. I would ask you to read it. These data breach reports and, uh, state of data security reports that the our our friends on the perimeter, um, release are most definitely worth a read. If you don't read them, uh, they are most definitely worth a read. 35% of breaches involve shadow data, untracked data. Residing in unmanaged data sources and those breaches have a 16% greater cost on average, right, so the stats prove it out. On track data is real and it costs more when it gets breached. Second, production data and development environment is an absolute disaster. Right, I understand and we, I think we all understand that our development teams need production data to develop their applications, but that data has to be properly obfuscated and masked. And that's a really difficult thing to do. It just is to keep the data integrity intact, that's a very difficult thing to do. The third thing I think we learned here um is that uh when a data breach is in play, when PII is in play, it changes everything about an attack. It turns it from a short term catastrophe that I need to get my systems back up and running into long tail costs that exceed this event by months. Most of the costs in a cyber attack are associated with the long tail costs after a if a data breach is involved, and those long tail costs include brand damage, customer loyalty being impacted, regulatory fines. Class action lawsuits. LifeLock subscriptions. How many of you folks have gotten a data breach letter that says yeah right it says oh we'll give you a free year of LifeLock unfortunately they don't stack them. I've got a stack of them at home they don't stack the LifeLock subscriptions. If so, I would probably have a LifeLock subscription until I die. That's how many of those I've gotten. Right? So, to recap this scene, PII, that's what the attackers are after, right? Primary objective, find and exfiltrate sensitive data. I can monetize that regardless of whether you decide to pay me or not. They may give you their word that they're not going to release it, but are we gonna take an attacker's word for it, right? Data sprawl is real. That problem of shadow data is real, and when you combine that with the increasing regulatory version and uh regulatory burden and financial penalties that are associated with that, it leads to massive long tail costs. And event ultimately IT tools need the ability to proactively reduce the risk of sensitive data exposure through automated discovery and classification, right? You should be doing automated discovery and classification across your environment. If you're only doing it in S3, you're not covering the whole estate. I understand most of your data estate probably sits in S3. If a breach happens, those teams need to be able to quickly respond and understand the data. That was impacted. There are breach reporting requirements. In California that's 72 hours that was modeled after GDPR, which is an EU standard. The EU has a standard across the board for reporting, uh, after a data breach. California created their own data privacy law. There are data privacy laws on the books in most of the states in this country. Which means that if there is a data breach, you have to you have to find counsel that specializes in reporting that to the state's attorney general. Across all of the states in which your organizations do business. And if you're talking about. Europe, if you do business overseas, if you're a global organization, you are beholden to their regulations as well. Those regulations have teeth, significant teeth. GDPR, I think, is 2% maximum fine, 2% of overall revenue. It's massive Right, so again, PII changes the entire landscape. All right, let's move on to scene 5. It's a video and. It has been 3 days, 3 days of failed recoveries, mounting losses, and public humiliation. The company's leadership is now assembled for a final, agonizing decision. So a full clean rebuild of our environment from the ground up will take at least 3 weeks, given that the sophistication of the malware in our backups, that is the most realistic estimate. Direct revenue loss currently stands at $3.4 million. Our projections show that we'll continue to lose nearly a million dollars for each additional day of significant outage. We're hemorrhaging money by the day. I must advise in the strongest possible terms not to pay these criminals. It's rewarding their actions and funding their next attack on another company. We get a decryption key that might not even work for data that we're gonna have to scrub and validate for weeks. This doesn't get us any of our data back and it absolutely doesn't remove the attackers from our environment. It paints a giant target on our back for every ransomware group on the planet. We have to rebuild. It's the only way to be sure we're clean. Rebuilding is a fantasy of control that we can no longer afford. Your clean rebuild will take a month, and in a month there won't be any company left to secure. We will have lost massive shares of our revenue, our brand reputation will be in tatters, and our stock will be worthless. Look, I get it, principles are important. But they don't keep the lights on. From a legal standpoint, our situation has fundamentally changed. We're no longer just managing an outage, we're mitigating liability. The single biggest quantifiable risk we face is the public leak of that PII data. RCSO is correct. Paying the ransom doesn't guarantee that the attackers won't leak it. However, refusing to pay it makes it a certainty. They will leak it to maximize our pain and make an example of us. The reality is paying the ransom. Is our only leverage, however small, to prevent that outcome. More importantly, we must be able to demonstrate to the courts and regulators that we took every possible step, however distasteful, to protect our customers' data after the breach was discovered. The general counsel's right, from purely a financial perspective, paying the $6 million as painful as it is, is the only move we've got, you know, it's an ugly but necessary transaction to ensure that we stop the financial hemorrhaging. It's the only business decision on the table. They were right, the security team, the operation analysts, they told us our backup strategy for the cloud was flawed. They told us we needed better visibility into our data. We deprioritize the projects and the funding to meet the deadlines for new features. We made this choice months ago, not in this room, not today. Now we're just paying the bill, so I agree with the CFO and the general counsel. We have to pay. Then it's decided. Authorize the payment, and the first non-negotiable line item in next year's budget will be a complete overhaul of our data security and cyber resilience strategy to be led by the CIO's office. This will not happen again. For the record, I'm formally objecting to the payment, but I have advised on the legal rationale for doing so. All right. Spoken like a true lawyer, by the way, kind of, kind of, kind of a weird way to end the scene, but you know, it is what it is. Uh, and the CEO being so confident this will not happen again while stats prove him wrong. Uh, 80% of people who are impacted by a ransomware attack are usually re-impacted after 6 months. Um, that's because they either didn't do a good job of finding the initial entry point or unfortunately how the attackers got in and ends up getting sold. Right, Attackers are just looking to monetize, uh, and they will monetize their TTPs that they use to infiltrate your environment to go after someone else in your set industry sector. We're seeing what scattered spider today tearing through industry sectors, retail, airlines, insurance. Right? And it's because a lot of the industry sector, a lot of the organizations in those industry sectors use common tools, right? And they have common vulnerabilities, and those common vulnerabilities can be shared amongst the community of bad guys. Nation state Bad guys. Who would have paid Nobody? Come on, come on, based on, based on what was faced here, who would have paid? OK, thank you. One brief soul says he would have paid a couple now. OK, I, I don't really think they had a choice here. I mean, let's be honest, if it's gonna take them 3 more weeks to rebuild, right, which is what the CF CIO is saying, um, there may not be a company left after the rebuild, right? Really don't think they had a choice here. I am not advocating that you pay, as a matter of fact, um, that may be illegal at some point to pay. It is considered, um. Uh, it is considered, um, paying, uh, essentially. Um, a hostage fee to a nation state actor who are enemies of, of, of, of the US, so it may become illegal at some point. Ultimately paying is a decision, is a business decision. I really didn't think Verizon had a choice here just based on what they were faced with. Uh, when a clean recovery is impossible, the focus shifts from financial survival to mitigating the legal liability, right, of a leak of a public data breach. Second thing, and I thought that's really like I love, love this statement that choice was made months ago, we're just paying the bill now. Right, that is ultimately what happened here. That choice was made last year. Um, they deprioritized security investments. They accepted those risks. This is a management failure as much as it is a failure of the person whose credentials were were lifted, right? And that's the typical push pull in any organization. That constant trade-off between investing in cyber preparedness and velocity. Right. Ultimately we believe that true cyber resilience is the power to say no, the power not to pay, and in order to do that, you have to have certain things in place that we're gonna go through on the next slide. According to our own research, 39% of IT and security leaders believe that the board of directors or C-suite have little to no confidence in their organization's ability to recover critical data. That's a pretty sobering statistic. Like I said, cyber resilience, true cyber resilience is the power to keep your data safe and your business running, assuming that you have executive alignment, which again was not, did not happen here, and proactive investment. Some things have to be foundational. The first survivable backups. Immutable and air gapped, you might say, how do I air gap in the cloud? It's all about credential isolation. That's how you air a gap in the cloud. Right, every single time you write your backup data to an immute not only to an immutable bucket, you should be writing with object lock as well. Right, immutable and air gapped, survivable backups, that's number one. If you don't have backups, you don't have a recovery strategy. And ultimately the ability to be able to guarantee that you can recover that back. Now the other things that I think are really important when we talk about resiliency is understanding the type of data you have and who has access to it if you're doing proactive, um, you know, uh, uh. You know, proactive data governance in your organization. You should be ensuring that um you adopt a least privileged model across the board. I Folks in your organization should only have access to data if they need access to it. It has to be on a need to know basis. I know that's easier said than done. I know that. Trust me, I've been through it. And then having that complete sensitive data visibility and management is ultimately going to. Limit the amount of data and limit your risk surface area, limit the amount of data that the attackers ultimately have access to. So I think we're gonna go for any questions. Do we have? I actually, before we go to questions, I do wanna put this up here. I noticed some folks were taking pictures of that last slide. Please take a picture of this QR code. Go and visit Rubric's website. Please stop by our booth. We've got a massive booth, um. On the expo floor. We've got lots of folks there that are demoing rubric, rubric protection, uh, we've got lots of different innovations we wanna share with you. This was not a rubric commercial, so I hope you found that really valuable, right? But I have to put this slide up here because we don't do this for free. Uh, so thank you. I really appreciate everybody attending.