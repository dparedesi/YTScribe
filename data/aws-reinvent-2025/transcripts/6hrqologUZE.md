---
video_id: 6hrqologUZE
video_url: https://www.youtube.com/watch?v=6hrqologUZE
is_generated: False
is_translatable: True
---

Good afternoon and uh thanks for joining this session today on building, fine-tuning, and deploying AI models with SageMaker HyperPod, CLI and SDK. I'm uh Giuseppe Angello Porcelli. I work as a solutions architect in the SageMaker AI uh service team. Um, in the past few years, I've worked with multiple customers, uh, being training, um, fine tuning their LLMs on Hyperpod as well as running inference at large scale. And I'm here today with Arun Nagarajan, who is a principal software engineer, um, who has been deeply involved in the design of Hyperpod and the features that we are going to discuss today. Let's walk through the agenda for the session. We're gonna start with the introduction to SageMaker Hyperpod, just to set the context, what is Hyperpod and what are the key benefits of the product. Then we will look at the uh Amazon SageMaker Hyperpod CLI and SDK and why we have built this uh CLI. Um, then we will get into a series of demos with live coding on getting started with hyperpods, so how you create a cluster and how you connect to a cluster, then how do you run training of AI models, how do you do the deployment, how you optimize resource utilization using hyperpod task governance, and finally we will also. Demonstrate a feature that allows you to run your IDs and notebooks on SageMaker hyperpod. This is something that we launched a few days ago. And then we are going to share at the end some resources, so all the code that we're going to run today, you will be able to download at the end of the session. Uh, great, let's get started with the introduction to SageMaker Hyperpod. Amazon Sage Maker Hyperpod is a service that allows you to manage and operate, uh, persistent clusters to train and deploy foundation models. Um, you can scale up to thousands of accelerators, and Hyperpod comes with improved efficiency. As I mentioned, Hyperpod has functionalities which help you optimizing the utilization of the computer resources on the cluster. Um, you reduce overall the time to train because Hyperpod comes with advanced resiliency capabilities which help you recover from failures. Um, you know, when you run training or fine tuning at large scale, failures can happen on your hardware side, on the networking side, GPU side. So hyperpod has built-in features to recover from these failures. We'll talk to that shortly. Uh, and, um, overall, the objective is for sure to lower the cost of foundation model, pre-training, fine tuning, and inference. Let's get into a bit of details of the benefits of Hyperpod. Hyperpod is resilient, as I mentioned. The service runs continuous health checks on the instances that are part of the cluster and in case failures are detected, I don't know, GPU failure, networking failure, the service takes appropriate remediation actions like replacing an instance or rebooting a distance depending on the type of failure that gets detected. Uh, the service is hyperbole is highly scalable. Um, we can deploy nodes with a single spine node topology to get low latency. Um, the nodes come with preconfigured EFA for high networking connectivity across the nodes, which we know is extremely important, especially when running collective operations. And um overall, um you can also uh implement scaling, uh, scaling, scale in and scale out of your clusters, even automatically thanks to managed auto scaling support in Hyperpod. Um, it's extremely customizable, uh, you can control the entire stack on Hyperpod from the hardware side to the, um, you know, libraries and frameworks that you use for running your workloads. Uh, you can even SSH into the cluster nodes as needed and do whatever, whatever you need to do on the nodes. Um, and, um, Applepod is an efficient, efficient service, very efficient thanks to the integration with, uh, task governance capabilities which help you allocate the computer resources across teams, defining priorities, and we're gonna demo that, uh, as well. Um, it also integrates with, um, observability solutions like, uh, the managed Prometheus and Grafana stuck on, on AWS. Um, on the, uh, possibility to customize hyperpothus, you can see, you can, uh, really. Um, um, modify the configuration of your cluster and do customization at any layer from the hardware side with the choice of computer options, your choice of storage if you want to, I don't know, train directly from S3 or maybe you want to use a high performance distributed file system like FSX for Cluster, um, you can choose the, the framework of choice from, you know, Pytorch, Tensorflow, Jacks, whatever, whatever you prefer. And um you can even connect to your preferred experiment tracking solution. We offer managed ML flow on AWS, but if you want to use, for example, um third party solutions like weights and biases, you can definitely do that. You have full flexibility. Uh, what is important to note is that you have a choice of two orchestration options on Hyperpod. On one side, you can use SLEn, uh, for those who prefer to SLE, uh, experience. On the other side, you can, uh, integrate. So Hyperpod automatically integrates with the Amazon EKS to provide Cubern-based orchestration for your notes, and that's indeed the focus of this session. So we're gonna. Focus on EKS orchestrated classes for which we have built the Hyperpolicy Alliance. That's the key. At high level, the architecture for a hyperpods orchestrated cluster looks like what you see here on the slides on the right hand side you see the hyperpod managed nodes, so the service takes care of managing the compute nodes. Then once you deploy a Hyperpod EKS cluster, um, you are also going to create an EKS cluster in your account which acts as the orchestrator for the Hyperpod managed nodes, and then. Uh, users will use the Kubernitis APIs, uh, provided by this cluster to, uh, interact, uh, and, um, the interaction can happen from any client. You can use Sagemaker Studio or you can even use your own terminal, uh, on your own machine, which is what we're gonna, we're gonna do today. And, um, uh, for administrators, uh, on the other side, you can still use Cubbernatis APIs to configure the cluster, set up the cluster, uh, as well as, as I already mentioned, you can establish a. Uh, AWS System systems manager, session manager, secure connections to cluster nodes so that you can kind of SSH into the nodes and configure the nodes as needed. Uh, on the storage side, the storage will be, um, for sure configured in your accounts because that's where your data resides, and you can use S3, you can use FSX for Luster or as I already mentioned, you can connect to any. Any file system that might be running in your account, when the hyperbolic cluster gets deployed, elastic network interfaces are created in your account, in your VPC, and then from that VPC you can connect to your own solutions for storage as needed. Great. Let's move on to the hyperpo CLI and SDK and why we have built another, I would say CLI and SDK. Why another is because to integrate with, uh, so to work with the Cuber Natives cluster, you, uh, I'm pretty sure you're familiar, you can use Cube Cale, you know, the, the Cuber Natives CLI and just run commands there. But the feedback that we have received from our customers, from data scientists and researchers, is that. Using, um, you know, Cube Catle is definitely fine, but it's very complex, you know, you have to manage YAML configuration files. You have to edit those, those files and then submit the workloads. And while this is extremely, could be extremely easy for those who are familiar with this process, sometimes, you know, data scientists were looking for a more abstracted interface which is also closer to the type of workload that they are running, which is indeed a GAI workload. It's not like, you know, a generic deployment of Kubertis. Sources on the cluster in addition to that, some users prefer a more kind of a programmatic uh interface with, you know, with an SDK based interface. Others prefer command line interface. So that's why we have built this new solution which brings together CLI and SDK. And for sure, you know, to also leverage the observer, to better leverage the observability capabilities that are available on Hyperpod. The Hyperpod CLIDA is is an open source project which is available on GitHub, on the SageMaker, Hyperpod CLI repository. Um, you can see an example of running the Hype HYP command, uh, and CD on the help, uh, that, you know, guides you towards the, um, actions that can be executed with the CLI. On the architecture side, um, um, I mean, I hope this is, this is readable, uh, what the way it's structured, um, it's, what's important to, to, to get from this slide is that it's a layered architecture. On the top layer you see the user experience layer which is either the hype command for, um, running, uh, commands with the CLI or the uh import Sagemaker hyperpod essentially. To work the Hyperpot package to work with the SDK and then you find the core modules of the CLI and the SDK, core modules of the CLI implements the actual CLI commands. The models of the SDK are the ones who are responsible to build the request that then will be submitted to the Kubernitis client. Um, it's important, uh, to see how we're reusing the SDK modules for the CLI commands. So the, the CLI has been built on top, I would say, of the SDK itself. So it uses the SDK to execute the commands. Um, the SDK, as I mentioned, uses Kubernitis client to then, um, run the commands against the Cubernitis APIs and on the right hand side in pink you see. The uh templates, uh, the templates are, um, you know, the representation in the CLI of the custom resource definitions that are defined by the various operators that are installed in the cluster. So, um, these templates are versions so that in case the cloud, the, the, um, the, the operator gets updated on, on our side, you can still continue to use the CLI with the previous version of the operator. So you, you don't necessarily need to upgrade to the latest version. Um, how to get started? Um, sure, you know, create your hyperpod cluster. On the cluster creation experience, um, you know, as you can imagine, setting up such sophisticated environment requires, you know, to look at the networking, to look at the storage layer, security, as well as, you know, the actual compute. Um, and in order to simplify this process, we have recently launched, um, uh, simplified the cluster creation experience from the AWS console. Um, this, this cluster creation experience allows you to use quick setup which comes with opinionated kind of defaults on the infrastructure side, how to set up your VPC, how to configure the cider ranges for your clusters, so that depending on the scale of your cluster you are sure that you, you have enough IPs available for your machines. All, all these things are pre. Figure there you can just easily spin up your cluster, but if you want to tune the configuration, you can move to the custom setup and then in the custom setup you can decide what to enable or what to change on the configuration of the cluster. For example, if you don't want certain operators to be installed by default or if you want to change networking config storage, and so on. Uh, great. um, and then you, you know, for installing the CLI and SDK you can just run people install SageMaker hyperpod in your environment. Great. Let me move to the code and Here we go. So, uh, before we get into the actual examples, I just wanna show you the cluster that we're gonna use today. So I am in the, uh, SageMaker AI console, and then I can move to the hyperpod clusters, and I have 3 clusters running here. Two are EKS orchestrated and one is FLRM orchestrated, and, uh, we're gonna, uh, use this cluster ML cluster AIM 371. And uh here from the console screen you can also see uh the corresponding EKS cluster which is working uh which is acting as orchestrator for the hyperpod nodes and then you, you, you can find here all the configuration including the observability configuration so how, uh, which metrics we decided to export to Prometheus and and Grafana from the cluster. And what is important to look at is also the configuration of the instance groups. So here we have two instance groups, one with G5, 12 Xcel instances which come with 4 810 GPUs on board for, for each instance. And then we have um a few uh compute CPU nodes as well in the class. All the nodes, as you can see here, are running from the details. So 4 G5, 12 XL, and 6T3. Um, uh, to Excel instances. Great. Now, um, let me move to VS code, um, and, um, the, the code that I'm, that I'm gonna show right now, you will be able to download later. Um, it's, um, a modular example. It starts from getting started, then it goes into training, inference, task governance examples, and then, uh, spaces. So running uh IDs on hyperpod. Let's start from the getting started in the first module. Uh, you will see how to set up the cluster and all the things that we have discussed so far. I don't wanna spend too much time on this, how to install the, the, um, the CLI, but then, you know, we can just check, uh, I am in, um. In this directory where I've created a virtual environment where I've installed the upperpo CLI, Indeed, if I run the hive command, she, I, I should see as output to the various, uh, the help essentially on the CLI. Great. I can also do something like hive minus minus version to check the version of the CLI that is installed and as I mentioned that the versioning of the templates that are available uh with the CLI. Great. Um, let's, uh, take a look at the various clusters that I have, uh, in my account. So the list cluster operation will list the EKS orchestrated clusters that are available in my account, and as you can see, we have the AIM 3371, and then we have also the um. The uh ML cluster, yes, the other one that I have. Great. um, now, um, what you need to do to start working with this cluster is setting the cluster context. So essentially updating the local cube configuration so that, um, you can, um, uh, execute API calls against the humanities API correctly. So when I run this command, you will see that it will, it will update the local cube call. And um yeah, so we are ready to run commands and if you want to see uh you know what is, oh sorry, sorry about that if you want to see what is the current cluster context, you will just run the get cluster context and you know exactly which cluster you're working with. Great, um, we, we have pre-installed all the required operators and the dots on the cluster so um you. will skip these steps like setting up the training operator, setting up task governance, and so on. What I just want to mention is that this cluster has an FSX file system attached where data, so data and the training code is stored. Uh, just for your reference for the future examples on training, and this cluster has been set up with the, so the FSX has been set up with the Data Repository Association, so it's a kind of a sync between the FSX file system and an S3 bucket so that whatever you write in S3 can gets uh um made available in the FSX file system and vice versa. It's a bi-directional sync. Great, um, now once we have seen the initial set up, let me go into, um, the, uh, steps to create hyperpod cluster using the CLI. First thing that we can do is we can run this command to uh, create, um, just a director, working directory for the create cluster operation. I can run the hype unit cluster stack command. The hypeit operation is going to create in your local directory two files, one configed of TML and the other one is a Ginger template for the cloud formation template parameters, as I mentioned. The, so the hyperpod cluster creation experience is based on cloud formation behind the scenes. So when you create a cluster, whether it's on the AWS console or through the CLI, we are deploying cloud formation templates in your account now, and this is consistent. So whatever you use as interface, either UI or. The you get the same experience. So what the CLI is doing here is downloading the values that need to be provided to the cloud formation template for deployment, and these are templatized in that Ginja template. Jinja is a framework for a templating framework essentially, and Um, we can look at these parameters here. It's very kind of complex set of parameters which go through, you know, the configuration of the availability zone if you need a NAT gate within your VPC, the distances that you want to deploy, distance groups that you want to deploy, the file system configuration, how to deploy FSX in which, um, you know, uh, availability zone through which subnet, and so on. Great. On the other side you have the config.yaml. The config.TML is a way more abstracted configuration file which gives you just almost the same knobs that you find in the console you have available in the config.yl here. So you can change, for example, the Kubernitis version that you want to use on your cluster, or maybe you can change the operators that that. That are installed on the cluster, you can decide to change the configuration of the distance groups. For example, in this example, we are just deploying a single T3 medium, so there is 11 T3 medium instance instance count is 1 and the instance type is T3 medium. So it's just one instance in one instance group in the cluster, and so forth. You can set the VPC configure and so on. The from the CLI side, you have two options. Either you edit this config.tl through your editor, so your preferred editor, or you can use a command which is called configure this one. Uh, to, uh, automatically edit that file, the config, the, the, the config file. So you, for example, here, we are updating the Cubertis version to 1.33. So you can see that that's, that has been updated. And after you do that, you can just do hype validate to validate that your configuration does not contain errors. And finally you can do hype create which will. Trigger the cluster creation, so One thing to note is that in the run directory that gets created here you will also find the evaluated cloud formation parameters, so the, the template in which the parameters have been, the placeholders have been replaced with the actual parameters for your reference. This is useful, for example, if you don't remember how you had configured the cluster, some sort of light experiment tracking solution available on your file system when working with the CLI. Great, if we go back to the cluster screen. We will see that uh we have indeed a new cluster being created. It's here, hyper pod cluster stuck here, and the creation is in progress. You can monitor the progress going to cloud formation. And you will see that there are here a few templates, a few, few, few stacks being deployed. This is the main stack, hyperpod cluster stack, which then has a number of nested stacks for the various components that need to be deployed, VPC and so on. Great, this uh concludes the uh demo for uh uh the cluster creation. We will not wait for the cluster creation because this takes, uh, this takes some time. Let me uh move on to the next topic which is about training. All right, thanks, Josephpe. Hyperpot ships with the training operator, which is a sophisticated end to end training system, which is built from the ground up for AWS. I wanted to highlight a few key features that the training operator supports. The first one being the superfast failure recovery. In the traditional training systems, when a failure happens. Usually the containers and the and the images are torn down so that all the initialization work is lost. With the hyperpot training operator, it's, it's, it's a lot more smarter in that it is not going to tear down the container when a failure happens. What this means is that when a GPU fault happens or a. Network issue happens, your training job is not. Stuck on initialization for too long, what used to take minutes will take a few seconds. Secondly, the operator supports. Something called a custom monitoring. What we mean by that is that. In addition to monitoring for the infrastructure failures, including the GPU issues and network issues, the operator can be configured to monitor the failures happening in your training job logs. For example, you may have encountered issues where the job is stuck, it's not making any progress. Or you're coming across a sudden, uh, spike in the loss curve, or you have other issues which are preventing the training job from progressing. The operator comes with the ability for you to specify what you want to monitor, and you only need to set up the config and the operator watches your back. And thirdly, The operator also integrates with the task governance system, which we will talk about a little bit later. And that'll help maximize the utilization of the precious GPU resources. OK, so with that background, let's jump into seeing how this comes alive in code. All right, so what we see here is a training data set. And we are going to train a model, not just for text completion, but also for reading the. The query from the user. And then deciding what sort of tools it needs to invoke, and then Read out loud the The thought process which the model went through. This training data set is a little bit hard to read. So I pulled up one particular example. You can see that there is a system prompt. And then we are introducing the available tools to the agent. And we have a user prompt. And the assistant decided to invoke a particular tool. And then here's a tool response, and based on this tool response, the assistant. knows to read the Various fields it asked for and construct a user facing message and then sends it to the user and then the user asks a follow up question and this and this uh multi-tone events keep going on so that's one example and for today's demo we are going to use this data set and train a Queen 3 model. Giuseppe showed you how easy it is to create a cluster at the infrastructure level with a single command. But what about the distributor training side? You do need to understand. What sort of framework you're going to use, how are you going to get that image, um, in your cluster, and then perhaps understand how do you submit or run a job with Cubanetti. The hyperpot operator simplifies all of that into a single command. I have previously uploaded the training data set to S3, and let me quickly check if, if we still have the data set. OK. So we have the training data set and validation data set. That's the same one I showed you earlier. I will also check the ECR image. So we have the ECR image set up here. Now, I'm going to submit this job. With this. Commandre job. And that's it. Once we submit this job, what the CLI is doing is that it is constructing the right cuubin is spec file and submitting it to the cluster. You can see that we asked for 2 nodes of G5 instance type and 4 tasks, 4 training processes per node. And we pointed, OK, here's how you get your data, and so on. Now let's let's check what happened to this job by listing all the jobs in this cluster. You can see that the job that we submitted is already running great. Let's get some more details about this job. You can see that this job asked for 2 replicas, 2 pods are running, and then 4 processes per node, and it was created, pods started running, and the training job itself is running. Now the next level of detail is that if you want to look at the parts themselves. We can do the list parts. Which should tell you The two parts that we asked for. Now, we can look into the pod logs, which is basically your training job logs. And see what is going on there. We can see that initialization completed. And then The checkpoint is loaded and the training is starting. So, to recap, what this operator did was that with a single command, we specified the training image and the configuration that you wanted. And The operator kicked off. The various training parts that are necessary to run on Cubinitis. And it's doing a lot of heavy lifting wherein it is asking the parts, are you doing OK, you know, with the health messages, all these health messages that you're seeing or the health checks happening from the operator side reaching out into each part asking if they're doing fine and if any of the nodes are not doing well, then the system will help repair that. Speaking of fault resilience, let's say, let's say we want to. We want to observe what happens when a fault is noticed in production. The hyperpowered nodes come with the health agents which will automatically identify if there is a fault that is happening and flag the nodes for replacement. But for this demo I'm going to manually do that. And Let's see all the parts that are running, and these two are the parts that we just now kicked off through the training job. And I'm going to mark this node as. A failing nod. I'm going to apply a Cubanitis label. Which will mark this node as. Failed and Pending reboot. To repeat, usually this is taken care of automatically by the by the system, but just for demo, I'm just labeling it. So that we can, we can observe how the operator deals with it. Now let's check what happens to the pods. Remember, we, we flagged this note, which is ending in 917. Now you can see that the same pod, it got scheduled to a different node. And it's already in running status. Let's describe the. The job itself to see. What status it recorded. When we do that, we can see that the operator. Observe the node fault that happened outside of its control. And it remediated it within 6.5 seconds. That is the key heavy lifting that the operator offers, where you don't have to worry about the faults happening in the system. And you don't have to wake up at 3 a.m. to fix some training job, which is going to fail because one random GPU decided to quit. OK. So Let's, let's quickly take a look at the other initialization, um, way for submitting the job, which Giuseppe also. Showed, but in this case, it's for the it's for the training. This is a common pattern across all of our operators, where you create a directory and then you call in it, on it, on the particular uh type that you want to work on. And The CLI creates these templated files for you. If we open up here, you can see that it has a few details, um, all blank, but you can go ahead and fill them either with the configure commands, or you can edit directly. With your editor. Now, if we quickly check this config file, the two fields that I specified, job name and image, are already filled in. And I can do validate. And then I can do Create to submit the job, which I'm not going to do right now. All right, so Up until now, we have been talking about the CLI abstraction. I also wanted to show show the SDK abstraction. Here, what you see is that in addition to the CLI interfaces, we also have the Pythonic. SDK implementation for all of our interfaces. For example, you can, you can import with the standard Python behavior. You can either use Notebook, or if you have automation that you want to build on top of this, you can write standard Python code for this. Here is the Hyperpo Py Touch job class that you can instantiate and provide all the different configurations that we talked about. And then finally, you can call create on it to simply submit a job. And you can do the list operation, the status operations, uh, pod log operations, all of the, all of the capabilities that we looked at earlier, they are also available through the SDK interface as well. OK, so that concludes the. Um, demo for, for the training operator. Now, before we jump into the inference operator, I just wanted to recap how quick and simple it was to Use the Hyperpo CLI or SDK to submit a training job and you didn't have to worry about fault recovery, and the fault recovery is really quick. And And to recall the, the layer diagram which uh Chesape shared. This is all a layer on top of Cubinitus, um, cube CTL for example, right? And if you, if you want, you can always drop into the Cubinitus level commands, for example. If you want to get the parts, that's, that's still there for you. If you're an advanced user and if you want to use Qube CTL that's all good. But if you, if you want to have an abstraction with which you can build on top, we have the CLI and SDK for you. OK, so let's move on to the inference operator. I have a slide to show you what what features it has, but. I want to kind of uh kick off 11 creation of the endpoint because it takes a few minutes hopefully, yeah, hopefully we will have time to um see that in in uh completion. So I'm gonna delete the job that we created. And make sure that that job is deleted. OK, so we, we don't have that job anymore. Now we are ready to kick off the inference deployment. This one, I'm going to use the The config way And I already have the config, um, bootstrapped, so I'm just going to say, validate. And create. So that's going to kick off the creation. OK, so that's going to take a few minutes, and we will come back to it. In a moment. All right. Now Hyperpod ships with an inference operator, which is how we are going to deploy and scale your AI models. I wanted to highlight a few key features the inference operator provides. The first is the simplified deployment. Hyperport operator not only deploys custom models that are trained by you or from the predefined, um, models from Jumpstart in Sagemaker, but also it creates all the infrastructure required for a production scale inference endpoint. For example, you need to set up the ALB to expose it to an application, and then you need to deal with the SSL certificates and termination. And then you need to deal with auto scaling policy. All of that is taken care of by the inference operator with with one single command. Secondly, Like I hinted, whether your model is in FSX or S3, which you have trained, or if you want to use the built-in models from Sagemaker, those options still exist for you to choose whatever you want to deploy. Third is the auto scaling support where the, uh, the inference operator is integrated with the metrics from CloudWatch or Prometheus, and it watches for scaling events and then it scales automatically for you. This operator works seamlessly with the Carpenter auto scaling with Hyperpod and then provisions the instances as they, as they are needed. And finally, the. The idea is that for different audiences, you can go through different flows. For example, the data scientists and AL engineers, they can use the Hyperpod CLI and SDK, which, which, which we will demo in a bit. To create the endpoints, and which in turn creates the Sagemaker endpoint, the ALB controller, CADA, and then integration with FSX S3, and metrics. All that stuff is taken care of you with a single command. And When all of this is set up and ready, the application developers can interface with the ALB directly or through Sagemaker endpoints to do actual invocations on the model themselves. With that, let's switch over to the demo. OK. I know I rushed through. The creation of the endpoint, but let's take a quick step back. What we did was that. We are, we are going to deploy a custom model, and I'm going to I'm going to show you that the model exists in the S3 bucket, which it is deploying from. You can see that this is the model that we are deploying currently. And what I did was the config way of deploying, and you can also see that the equivalent CLI way of deploying is also available for you. Where you can specify the endpoint name. Where is your model? For example, this one, we took it from S3, and, and what's the bucket name? Where is the model located, instance type, and then the image URI. Notice that this image is directly from Sagemaker, from the DJL container. And once the endpoint is ready, we will be able to. Do inference against it, but let's check what is going on with the endpoint that we created a few minutes ago. I'm going to list all the endpoints in this cluster. OK, so it says deployment complete, which means that the pods should be in a running state. We can also take a look at the internal operator logs for the inference operator itself. You can see the metrics are enabled, the inference endpoint config is all being set up. And then we can also take a look at the detailed. Logs from the. Custom endpoint CRD. OK, probably I've made a typo. OK, so what we see here is that. The deployment on the pods have successfully completed, but the front end. Endpoint, the sage maker endpoint is still under creation. While we wait for that. We can perhaps look at some of the parts. Ah OK. So we have a pod which is ready to do inference. And we can look at the pod logs, you will see that. The initialization completed, it would have loaded the model. And You see this? It's, it has loaded the model, and then it has started the worker thread and the, Endpoint is ready from the pot. We just need to wait till the Sagemaker endpoint is complete before we can do an inference. OK, there we go. The Sagemaker endpoint is also created. We can do a quick invocation on this. OK, there we go. The chat completion came along. And the inference endpoint is up. All right. So that, that concludes the demo for inference. We have another way to do. Uh, inference deployment, which probably we don't have time to do it, but the idea is that just like we did inference deployment from S3, you can do something from Sagemaker's Jumpstart as well where you only need to specify the model ID and automatically it knows to download all the model and everything and deploy this for you. So to recap, what we saw is that with a single command. The system takes care of the heavy lifting required to setting up setting up the load balancer, the auto scaling controllers, uh, the metrics, the logs, and kicking off the deployment for Cuban. All of that is taken care of you out of the box. We, we do have some examples here on. Invoking or, or forcing an auto-scaling event, but, uh, we will skip that for the time. So the next part of the demo would be, would be handled by Jessuppe again. I. One, yeah, can you delete it. Uh, Great. So as we mentioned earlier in the talk, so Hyperpod provides capabilities, built-in capabilities to optimize computer utilization on the cluster. This is thanks to Hyperpod task governance. Hyperpod task governance gives you the Possibility to define a policy at the cluster level and define multiple priorities for uh workloads in this example you can see how for example the inference priority is higher than maybe experimentation or training priority. This allows your workloads to be treated differently and you know higher priority workloads can be configured to preempt lower priority workloads. At the same time you can decide how to allocate the computer resources that you have available in the cluster to the various teams. So a team corresponds to Kuberniti's name space and what you do is you say, hey, Team A has allocated a certain number of. Instances, well, the computer resources of an instance like I don't know X GPUs, um, memory, uh, as well as CPU resources, or you can go even granular, so just a certain number of GPUs or a certain quantity of memory, certain number of VCPUs, both options, so entire resources of an instance as well as granular resources are available. And thanks to this setup, you can also configure how to borrow and lend the compute capacity. So you can see that Team A, for example, can borrow 100% of their compute capacity if there is idle compute capacity in the cluster. So this means that if Team B, for example, is not utilizing the computer resources in the. which are sitting idle, Team A could could borrow these computer resources while those are idle. When Team B comes back with guaranteed computer requests, so the, the, the quota that has been guaranteed to to Team B, then you know the the Team A job will be preempted, and that's what we're going to demonstrate right now. Let me move back to, to the code. Yeah, so. We are in the task governance section of the examples. Um, let me set up some variables. Uh. We're gonna use the same uh training job that was run earlier but we're gonna configure in a different way to assign different priorities and um running uh running the job by different teams uh if we go back to quickly go back to the cluster we can see that the cluster has been configured on the policy side you can see that um I have um the priority classes that you've seen almost the same that you have seen on the screen here configured on the cluster. And then we have the computer locations. If we go to the computer location for a specific team, we will see that Team B has just one instance that has been allocated to the team. So 1G5, 12 XL, so 4 GPUs are allocated to this team. They can borrow 100% of the compute, and Team Team A is configured in the same way, so they can. They have one instance guaranteed compute for this team, and they can borrow 100% of their computer resources. So another instance, the computer resources of another instance essentially. Great, let's go back to the code now. We can show an example. As you can see, the the the the command is almost the same. What we are setting here is we are defining the name space that we want to run this workload in which corresponds to the team that that the computer resources have been allocated to. Uh, the local queue, um, task governance works with, uh, integrates with Q, which is a popular scheduler for, for Cuber natives, just offers a layers, a layer to configure Que in the, in the appropriate way, and then, um, we, we are setting the, uh, training, the training priority for this job. Uh, OK, so let me, and, and the decencies that we are requesting are 2, OK, let me run, let me run this code. Oh sorry. Let me run this job. OK. I will check if I copied everything indeed I was wrong. OK. Uh Great The job is running now we can run this list commander to see that the job has been running. And as you might have noticed, we have run the job with 2 instances, but the team, this team, Team A, had just 1 instance allocated. So this means that we are borrowing the compute from Team B at this point. Let's check if that is the case. We can run this command on the queue side. So, first of all, we can also describe the job, the job details as needed. To see if the job is running correctly. OK, indeed it's running. And we can then run this QC command which will look into the cluster queue, the queue cluster queue, and you can see that we are borrowing 4 GPUs. So since, as I mentioned, the, the, the, this team has access to 1 more ISA and this is 4 GPUs on board, now we are borrowing 4 GPUs with, with, with this job. Great. OK, let me, let me simulate now a scenario where the Team B. wants to submit a job with just one node, OK, so they guaranteed compute this is their guaranteed quote, OK? So Let's run this task. OK, so now we should see. What, what, what would you expect to see is that the Team A job is in suspended state because it was preempted and the Team B job has been created and it's indeed running. Another thing that we can do is we can preempt a lower priority task. So within Team B, now a higher priority task kicks in, which has the experimentation priority, which is higher than the training priority. So now what we expect there is that this task will take higher priority. On the cluster And the other one will be suspended. Let's take a look OK, so we have. Team A is suspended. Team B, one job is suspended. The one which was triggered with a higher priority is is now being executed. And now what we can do is we can also, you know, delete, for example, the. Job with the higher priority. This is the one with, with the higher priority that we just submitted, and we should see that the um Team B job now gets restarted so that Team A is suspended versus Team B is 1 will be reactivated, OK, resumed. Then we can delete, for example, the job from Team B. So at this point my expectation is that we will also See, we are deleting this job and we will also see, oh sorry, what happened? Uh. Let me double check. OK, it's called. Queen Tim B, I guess, is, is that, is the name correct? Yeah, seems correct. Let me double check. OK. Maybe a typo and then now we can check the Jobs and we expect that team, Team B's jobs for sure have been all deleted and the Team A job has been resumed, so it's borrowing again the idle compute and has been resumed, OK. So now we can just delete the job to clean up the current status. Great. Um, the last thing that I would like to show you is how to manage now IDs on um. Hyperpod clusters and how to run IDs on hyperpod clusters. Great. Um, we recently launched this capability which allows you to, um, execute your IDs whether SageMaker, code editor, which is based on, based on code OSS Visual Studio code, open source, or Jupiter Lab on the, uh, Hyperpod cluster. The idea is that um once you have a cluster up and running and this cluster is being utilized for training or inference, you might also want to run your IDs, your interactive ML, let's say workloads, reusing some of the computers that is that might be available on your cluster at a certain time. You also get very fast startup latencies because you can pre-cache, you know, the container images for your IDs on the cluster, so the, the, the environments are bootstrapped very. Uh, you know, with, with a very short latency and in addition to that, you can for sure reuse the same tooling like the upper pole CLI to work with the ideas. IDs on hyperpod is integrated with task governance capabilities, the same capabilities that we have seen right now. So this means that for example, you can have your ideas running with a certain priority, but then if a higher priority workload is kicked in, you might decide to preempt those tasks because you might want to use the GPUs for some other. Task, but at the same time your users can take one GPU, for example, from one node and work on their notebook or with their VS code using this GPU, as well as even a fraction of a GPU because we have added support for a fraction of GPUs using Nvidia Amiga technology, so partitioning the GPUs into smaller fractions. Great, um, I, I'm not gonna spend too much time on the details of this capability, but you know you can create multiple templates as well for your IDs so that each template defines the image that you want to use, some configurations on the eastern side, default eastern type, the default computer resources. That are gonna be requested so that the users when they spin up the environment they can just use the template they don't have to provide all the parameters like you know I need these computer resources and so on so the administrators can set up various templates that are then utilized by the users to spin up their, their environments. OK, let me move to a demo for this as well. OK, um, let's, uh, sorry, need to switch back here we go. Um, great, let me open the last module, which is the spaces one. OK, so let's set a variable, which is, you know, the name that we want to give to the space. Uh, and then we, as you can see, we are running here this command that creates space, um, with the node selector which allows us to create this environment, this ID environment on a CPU instance. We don't want to use GPU instances for this task. We can use simple CPU instance to spin up the environment uh in this example, and we're also mounting the uh FSX file system, OK? So, uh, then we can use still the CLI, you know, to run commands like list of the spaces. I have two running actually. I've pre-created, pre-created one space on my side, and then this is the one that is being created right now. Um, yeah, we can also run a described operation to see the details on the space. Um, what I want, so this is all, all the setup about the space workspace is, is starting at this point, OK. Great. Um, what I want to show you now is how to access these spaces. There are two options here. One is to use the um remote connection via VS code, so you can set up a remote connection. So you have your pod running on the cluster, but then from your local VS code you set up, you establish a remote connection, so you use the remote compute for your workload from your local VS code, and that's one option. And in order to do that, let me, um, let me take the, the name of the space that I have already running on my side, which is called um this one is my Jupiter lab space. So space name equals my Jupiter lab space and then you can use this command which is called create hype space access. With connection types set to VS code remote, and this will give you a secure connection URL to the Remote space. Once I run this URL, let's say I run it on my. browser this will automatically open via code. I can open it And what you can see is that now via code. will establish a remote connection to the space, OK? So now I'm running on the remote space, um, just to take a look at this, I can do a new terminal. Uh, I do LS. This is the home directory on the space, but I can also do FSX. So I'm connecting to, so this is the distributed file system, the FSX file system that I, uh, I have attached to my cluster that I've mounted to the space as well, so that, you know, you can do things like, you know, look at the checkpoints. Uh, this is the, the previous example. On training that we were running, OK, and here are the checkpoints, the model artifacts, the scripts that I was, that we were executing and so on. This is one way to work with the space. The other way is um via the web UI. Um, and you can just run this command to get a web UI URL. And we will see that just clicking on this link, I can open this website and now we are connecting to the remote space via the web UI. This was Jupiter lab space and you know we're now connecting to the Jupiter lab service here. Um, yeah, that, uh, concludes the, the spaces demo and actually concludes the session for today. Um, and here you can find all the resources regarding the, uh, functionalities that we have covered today. What is important is the, uh, oh sorry. Sorry, uh, yeah, here we go. So what is important is the uh QR code that you see there from where you can download all the examples that we have, um, uh, gone through today, even more with more examples, um, and, um, yeah, um, I'll leave this for uh a minute or so and then, uh, move to the, to the next slide. And then we can stay here around in case you have any questions since I see that we are right on time, so. Thank you. Thank you.