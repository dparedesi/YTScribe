---
video_id: v--Pa6gNVQ0
video_url: https://www.youtube.com/watch?v=v--Pa6gNVQ0
is_generated: False
is_translatable: True
---

Uh, good afternoon, everyone. Welcome to NET 336. My name is Ashish Kumar. Uh, I'm a product manager in AWS Networking. And today we'll be talking about a feature in elastic load balancing that we launched about a week ago. Uh, this feature is called target optimizer. The agenda for the next 20 minutes is that we'll spend some time discussing the problem, then the solution, then we'll introduce target optimizer, describe how it works, talk about how you can set it up, and finally conclude with some ancillary information. Elastic load balancing, as we all know, is AWS's suite of uh networking, uh, load balancing products. It comprises of three main products, uh, the network load balancer, which does, uh, layer 4 load balancing, the application load balancer, which does layer 7, and gateway load balancer, which does inline traffic inspection and firewalling. Target optimizer is a capability of the application load balancer. For those of us who aren't aware of what application load balancer is, it is essentially an HTTP load balancer, so it distributes HTTP requests among application back ends, uh, which we also call targets. And a regular application setup looks somewhat like this, where on the right hand side you have the target group, which contains targets. These targets can be EC2 instances that are inside a VPC that are running your application, or these can be compute that is on premises. These can also be containers. And in front of the target group you have the load balancer which receives requests from a client. And typically the way things work is that the load balancer is configured with an algorithm, for example, round robin, and it uses this algorithm to decide which target to send that request to. So with round robin, the first request goes to the 1st target, second request goes to the 2nd target, and this process continues, and we have each target processing thousands of requests concurrently. Now, If we turn our attention towards a specialized class of applications, uh, which here I have called low concurrency applications, then the picture begins to change quite a bit. And when I say low concurrency applications, um, I mean applications that are compute intensive. So think of A large language model. Where every request requires a whole bunch of compute resources to be fulfilled, and as a result of that, each deployed instance of the target of the application or the model can only process a small number of requests at a time. And so for these low concurrency applications, the target group looks very different, where instead of having a small target group where each target is doing thousands of concurrent requests, you have a very large target group, a very large fleet size where each instance where each target can only process. A few requests at a time, and these could be as few as 100 or 10 or 5 or 2 or sometimes even 1. In my example, I've shown them to be able to process only one request at a time. And so when we are operating in this low concurrency world, that is where we begin to see traditional load balancing with its preconfigured algorithms start to fall short of delivering the outcomes that we would want from a load balancer. And we'll see how in the next slide. So let's take the example of an application that is deployed on the target group. That is doing some sort of media generation. So some application, uh, some of them could be doing, you know, video generation, some text generation, some image generation. And let's assume that each target can only process one request at a time. Let's also assume that the application is configured with a round robin. Now if at T equal to 0. All targets are busy processing requests. We can, it's reasonable to assume that at a later point in time, some of them will have finished complete, uh, will have finished processing the requests, likely the ones that were doing text generation, while the ones that were doing more complicated tasks like video generation will still be busy. Now at this point if the load balancer receives requests, we would want those requests to go to the idle targets. But because the load balancer is configured with round robin, these requests will continue to use that algorithm and might end up on targets that are already busy. And as a result of that, what happens is that the overall success rate of your application goes down. In other words, the error rate goes up. The requests that are denied have to be retried by the clients, and so the perceived latency of the application goes up. And all this while we have, we have targets that are sitting idle, that are waiting for requests that are not receiving requests. So the overall utilization of the efficiency of your, of your target group goes down. Now some of you might say that, hey, that's because the example that we've taken is that of, you know, round robin, and if we were to take something that's more advanced like least outstanding requests as the algorithm and the load balancer, then things would be much better. And you would be right. Things would be much better, but they would still fall short of the outcome that we would want. And that has got, by the way, least outstanding requests is an algorithm where the load balancer selects the target that has the least number of requests. So things would be definitely better than round robin, but it would still fall short and. That has got a lot to do with the way the load balancer is designed. So even though we refer to the load balancer as a unit, it actually comprises of multiple independent units called nodes which are decoupled by design and which make routing decisions independently. So the view of the target group as seen by one node is very different or is different from what the other node sees. So if requests on the first node get routed by. LOR or least outstanding requests and we have let's say 5 targets processing requests. This will not be the view that the second node sees, and when requests arrive on it, it will routed independently, and these requests might land on the same targets that already had requests from the first node. And as a result of that, what happens is we end up with the same three outcomes for low concurrency applications, and that is that the success rate. Of the requests goes down, error rate increases. The requests that are rejected have to be retried, so the client's perceived latency of the application increases, and we have some targets that are idle and don't receive requests, so the overall utilization of the target group goes down. So that was the problem. Now that we know the problem, let's talk about what the solution should be. Now, as we can guess, uh, the solution that we would want is that the load balancer should know which targets are idle. So essentially if target D, E, H, and J are idle, we would want the load balancer to know those targets so that it can send the next few requests to those targets. And because the load balancer is decoupled, we would want uh the nodes, we would want the list of idle targets to be divided amongst the nodes in such a way that two nodes don't end up sending requests to the same target. So, so far so good. But we would want our solution to be slightly more sophisticated than that. And that is because the example that we've been taking is where each target can only process one request at a time. So in most cases, what we'll have is each target being able to process not one, but, you know, 5 or 10 or 100 requests at a time. And so if we have the picture looking somewhat like this, where target B and C can process one more request, uh, in this picture I'm assuming each target can process two requests. So if we have the picture like this where B and C can process one more request and DNF can process 2 more requests, we would want the load balancer to know not only which target to send a request to, but also how many requests to send to that target. And again, because it is decoupled, uh, we would want those targets to be divided amongst the nodes in such a way that both nodes together don't end up sending more requests to a target than the target has the capacity to process. So now that we understand what the solution is, we can talk about how target optimizer implements the solution. The way target optimizer works is that on the target where your application is running, you install an agent that is provided by AWS. And what this agent does is that it serves as a proxy between the load balancer nodes and the application. It also establishes control channels with the load balancer nodes on which it exchanges management traffic. This management traffic is necessary for a target optimizer to work. And the moment the agent determines that the application is capable of processing one more request, it sends a signal to one of the nodes. The node registers that signal, and when a request arrives on the node, it knows that it can send that request to targeting. And the way the agent knows when to send the signal. Is by tracking the number of requests the application is processing and comparing that with a max concurrent request configuration that you've done on the agent, so you have to explicitly explicitly specify the number of concurrent requests that you want, the max number of concurrent requests that you want the application to process. So if let's say the. Setting that you've provided is 3. You've configured the target to process 3 concurrent requests at the max, and the application indeed is working on 3 requests. The agent will wait until 1 of those requests completes, and when the application is down to 2 requests, it will send a signal to one of the load balancer nodes. Let's say another request completes and the application is down to one request, the agent will send another signal to another load balancer node, and these nodes will now know that if a request arrives, they can send that request to the target because the target is capable of processing them. So in this picture I've only shown one particular target, but Things will actually look like this for the entire target group. Where every target will have the agent installed and configured on it, and this agent and each agent will be communicating with each load balancer node through these control channels, and on each agent you can configure the max number of concurrent requests that you want that target to receive from the load balancer, and that number can be a function of the underlying capacity of the target, you know, it's memory, its CPU, etc. And so we can expect two outcomes from target optimizer. One is that we can expect a very high request success rate. And that is because we know for certain that when a request arrives at the load balancer, it will go to a target which will have the capacity to process that request. It will not go to a busy target. So the overall request success rate goes up, your error rate goes down. And the second thing is that we expect a very high utilization of the target group, and that is because the moment capacity opens up on. Any one target, in other words, any one target finishes completing, uh, finishes processing a request, the load balancer will send another request to the target. So the duration in which the target is idle. Is very small and so the overall utilization of the target group increases, the, in other words, the efficiency increases, and because your target group is now more efficient, you can make do with a much smaller target group size than you had previously. Another benefit that you get from target optimizer is that if in the event where all targets are busy processing the max number of requests they are configured to process, any request that lands on the load balancer will be rejected by the load balancer itself, so you have this automatic load shedding that happens. In other words, your targets are protected from additional load, from being hotspotted, from being DOS. All right, so a recap of the results, um, with target optimizer, we can expect our request success rate to go up because lesser requests are rejected, lesser of them have to be retried, and so the perceived latency of the application decreases. Your target group as a whole, its utilization or its efficiency increases, and load shedding happens automatically if all targets are at capacity. So now let's talk about how you can set up target optimizer. Setting up is a three-step process. In step one, you install the agent on the target. Remember, the agent is a proxy between the load balancer and the application. So you need to configure the port on which it receives traffic from the load balancer. This is your HTTP HTTPSport, your AT 443, and you also need to configure the port to which it proxies that traffic too, which is the destination port, uh, and this should be the port on which your application should be listening. You also configure the control port on which the agent establishes these control channels with the load balancer. And once you've configured these 3 ports, you specify the max number of concurrent requests that you want that target to receive from the load balancer. There are 4 other optional variables that I'll not go into, but these are related to TLS. The user guide talks more about them. And so once you've installed the agent on each target, you register these targets with a new target group. I've called it the optimized target group. Um, when you create this target group, you also again need to specify the control port, which is the same as you configured on the agent. And once you've created this new target group, all that is left is to add it to an application load balancer. So if you already have an existing load balancer that is already configured with a listener, which is already sending traffic to an existing target group, you simply need to modify that listener rule and add this new target group to that listener. And once you've added it, you can simply move traffic over from the old target group to the new target group. So a recap of the setup, you install the agent on your targets, you register these targets with a new target group, and then you add this new target group to a, uh, a listener on a load balancer. Some more information. The targets that are supported for target optimizer are easy to instances. IPs, um, it also works with Amazon EKS and ECS services. For EKS and ECS you simply run the, uh, agent as a sidecar container with your application. We also have new metrics to help troubleshoot and monitor target optimizer so you can monitor the active control channels between the load balancer and the agents, any channels that are running into errors, how many requests went to a target optimized target group, how many of them were rejected, and also the control queue lengths. The control queue length is essentially the number of signals that the load balancer has received from the agents that are running on the targets. Here are some resources about target optimizer. Um, please do read the, uh, launch blog. Uh, it has a step by step process on how you can set it up. Right. Uh, yeah, that's it for this presentation. Um, thank you for attending.