---
video_id: P_C6Hv4onuU
video_url: https://www.youtube.com/watch?v=P_C6Hv4onuU
is_generated: False
is_translatable: True
---

Right. All right, welcome folks. My name is Joseph Izorri. I am a director of product management at AWS. I cover a number of our database services, uh, including Dynamo DB and Elasticash. Um, I'm really excited about this talk. Uh, Brandon is going to join us, um, from Scopey, and Brandon has been giving talks on behalf of Dynamo DB and AWS for, for, uh, almost 10 years now. I found it on YouTube. His original talk was on 20 2016 on Dynamo DB. So of all the customers that that I deal with, I love working with Scopey. I love working with Brandon. They are. One of the very earliest adopters of Dynamo DB, and they're phenomenally deep in the product. I think they make me uneasy about how well I know the product because I think they know, they know it, they know it very well, and they've been pushing the limits. They've been very early adopters of both Dynamo DB and Servius and now Valy Servius as well. Um, and for one of the most demanding workloads that we see on Dynamo TV, which is gaming, because you can go from 100,000 to a million to millions of customers in a very short period of time, um, and you know that's one of the workloads that we, uh, we're, we're very proud of and we're very thankful for Brandon to join us today and share how Scopey has been scaling on Dynamo DB and Elastic Cash, uh, Valy. Uh, with Monopoly Go. So welcome, Brandon. Thank you. All right, thank you. Um, oh, I need the clicker. Yeah, so I'm Brandon, um, and I'm here to talk about Monopoly Go. So Monopoly Go is a really big game. We launched it 2 years ago and it ended up being like 10 times bigger than we thought it would be. Uh, it was the #1 game that year by Mini Metrics, and, uh, we brought on like millions of users in a very short period of time and uh in our daily traffic peaks we would have over a million concurrent users playing the game at any given time. So this presented a pretty big technical challenge. Um, so here's a basic overview of our, of our architecture. So, uh, we have a pretty basic, um, old fashioned monolithic style architecture. Um, we have a Unity client built with C on the front and then on the server side we have a .NET application also using C, um, so there's a bit of shared code, so having C on both sides is nice. Um, also the way that we chose databases, um, was pretty important. Um, we, we chose, uh, we use the general rule of thumb, uh, where, uh, if the data scales with the number of users, we tend to put it in Amazon Dynamo DB. If, uh, the data scales with the number of employees at Scopey, then we'll put it in MySQL because it's, it's not very big. It's like configuration data and things like that. Um, and for special use cases we'll use Reddis for things like leaderboards and temporary job, stuff like that. So, uh, in this talk I'm gonna cover, you know, three main points, um, so reliability at scale, uh, I'll talk a bit about cost management, and then, um, I'll share a story on operational simplicity and why that aspect of, of, uh, doing things is really important for Scopey. Um, so first, uh, reliability, um, why Dynamo DB and uh. The main reason is because a lot of the alternatives have problems that Dynamo DB doesn't. So, um, not to pick on MySQL, but I'm gonna use it as an example of, of why we don't use MySQL, um, for most things. So, uh, MySQL has, uh, Aurora MySQL can only have up to one single host as the writer. So there's an inherent, an inherent bottleneck there on the right throughput that you could have on the whole database. Um, also we constantly run into connection limit issues. Um, so because we're a very large game, we have a million concurrent users, we have to run like something as many as 2000 or more hosts at a time in order to satisfy those load requirements, and each of those hosts has to open up, you know, 10 or more connections to MySQL. So you know when you do the math on that, uh, you easily hit sort of the upper bounds of the connection limits on the server side there. Um, we, we use, tried introducing things like MySQL proxy and scaling out reader nodes, and that helps, but it only scales to a point. Um And then also there are issues with um unconstrained queries. So you know if an engineer naively forgets to put a limit clause on a select star from users and then puts it into production, that that could easily chew up all of the performance capacity of the database and take us down for a while. Um, with Dynamo it's just really hard to to do that on purpose. Um, and then the most important one for me actually is, is that I never have to do cluster upgrades with Dynamo. Uh, with my sequel at some point I'm, they're gonna make me upgrade a major version and, uh, it almost always requires some downtime and some praying and finger crossing, um, and it's a real big pain and we'd rather not have to ever do that, um, so. That's why we don't like my sequel and we prefer Dynamo. Um, so next, um. Monopoly Go has some inherent sudden load problems that are sort of native to the game. Um, so I'll explain a bit about how the game works, so you can understand why we have this problem. The game's really simple. If you open the app, there's one button you just click the button, it rolls some dice your token moves around on the board and you collect some rewards. Um, it sounds boring but um. It, it's pretty fun when you add in all of these other time-based events that we call mini games. Um, now these mini games, they always have like a start and a stop time. So for example, like a Saturday at 9 a.m., you know, our race mini game might start and. When the race mini game starts, the game suddenly has to do a whole lot more than it used to do, um, and for example, it has to serve team recommendations because you have to form a team and you have to all get in the car together and then you have to, uh, when you go around the board, you're now collecting extra points that you can spend on to make your car go faster in the race. So, um, as soon as the event starts, we get this sudden increase in load. So you can see in this chart here we triple our baseline traffic in a short period, like 5 minutes. Um, and this doesn't actually tell the whole story because we have certain dynamo tables or other components that go from zero load to maybe 100,000 read or write units per second, uh, in that very same short time frame. Um, so because of this load pattern, uh, using Dynamo DB makes it really easy, uh, because we can, um. We can just simply put our tables in an on-demand mode and then what we can do is call a pre-warm API ahead of time so that that table has enough underlying capacity to handle sort of whatever load we think it could could need in our wildest dreams. um, and then when the load does come it's able to handle all of that load without throttling. Ah, for other components like elastic cash serverless, uh, there's a mechanism called, um, ECPUs, and what we can do is set a minimum ECPU threshold before the event starts. Ah, allow the, once the load spike starts and then the traffic stabilizes again, we'll just remove the minimum and let auto scaling take care of it after that. Um, for auto scale for EC2 and auto scaling groups who kind of do a similar thing, we'll just set a minimum on our auto scaling group. Uh, we'll add a bunch of servers, uh, when the load spike happens, then we'll remove the, the auto scaling group minimum, and then it will scale back down or stay the same. Depending on the load. Um, also for our load balancers we'll set a capacity reservation. And um that sounds like a lot of stuff and we have to do it for a lot of different events um so we got tired of doing it manually and we eventually built a system that automates all of those activities. A bit more detail on our dynamo strategy, so. When we create a new dynamo table for a new feature, we almost always launch it in on-demand mode and. We'll call the um the pre-warming API ahead of time because when you even when you create a table in on-demand mode it doesn't necessarily have like the underlying capacity to go to a million reads and writes per second right away. um, so Amazon recently introduced a pre-warming API that lets you, uh, say hey Dynamo, I wanna pay this small one-time cost to sort of, uh, make sure that you have enough partitions and underlying capacity. Uh, to, to handle additional traffic should I need it. So, um, when we make, when we do one, we'll, uh, we'll call the pre-war API, then we'll launch the, the, uh, the new feature, and we'll look at the traffic and if within a couple of weeks or a month we, we observe the traffic pattern and if it looks stable enough we may switch it into provision mode to save money, um, but if it's too spiky we won't do that. Um, note that like when we do switch it into provision mode, we also might need to switch it back in on-demand mode, uh, when we get one of those sudden load spikes again. And um you're allowed to do this about 4 times a day, switch between uh provision and on-demand. So even though we, we try really hard to avoid throttling, um, we don't always get it 100% right, uh, so we do have some extra mitigations to avoid the user impact of throttling if it does happen. Um, so our server client to Dynamo DB will, uh, retry a few times. However, we like to keep our server API's responses pretty fast, um, so we will give up pretty quickly on the server, but we will hint down to the client that, um, hey, this is an intermittent failure, and you're welcome to retry as well, and the client will retry a few times, um. And if it eventually succeeds, then the user might notice a bit of lag or something, but they won't actually get a crash or an error dialogue. All right, let me talk about a bit about cost, um. So one of the biggest uh things that we look for when we're trying to optimize for cost is um the size of our items in dynamo. So one thing that people don't always realize is that when you read or write to a dynamo or to an item in dynamo, the cost that you pay in read or write units is relative to the size of the document. So if your documents like 100 kilobytes, you're gonna pay like 100 right units to change even like one small aspect of that item. Um, and we noticed that, um, some of our biggest cost savings actually came from reducing the size of documents and, um, not actually from reducing API calls or anything like that. Um One thing that uh in this chart that I'm showing here is something that we built. um Dynamo doesn't have a way to like expose metrics on sort of your average or min or max document sizes within a table, um, so we built this ourselves we instrument our, our Dynamo clients so every time we read or write from a document we report, uh, the document size and, uh, that way we can get sort of an average or a P95 or a max size of our documents. And then we can set alarms on this so when Ah, our documents get too large, we can take a look at it and see what the issue is. Um, also note that um there's a hard limit of 400 kilobytes. So with a couple of these documents on the screen, um, we're pretty close to, to hitting a problem. So here's an example of a of a table that has a cost problem. Um, so this is a friend invitation table. So if you wanna be friends in Monopoly Go, you can send somebody an invitation and they can accept it. Um, the way that we do this in the database is, uh, or the way that we used to do this is that we have one item for each user's friend invitation. So when you send an, a friend invite to that person, you add that invite to a big JSON blob within that item. So for most users it's not a big deal because they only receive a handful of invitations, but for some users they get thousands and the documents can get quite large. So sending a new invite to that person is now really expensive. Um, so we changed this, and here's our new design. Uh, so instead of having, uh, one document per user, we have one document per invite and then when a user wants to to read their invitations, they, they query several items at once. And in this situation we also added a local secondary index on the date so that we could query the most recent ones first. Um And then here's the sort of comparison. Um, so in the old way every time we would write to a document we'd pay somewhere between 1 and 200 right units depending on the size of the document. Now in the new way we pay a fixed 2 right units. Um, in the old way we pay, you know, 1 to 50 read units, and in the new way we'd pay us. You know, 1 to 2 read units and some of the savings there comes from only having to query the first page. So when the user opens the app they don't need to look at like 1000 invites. They only need to look at the first page or the most recent ones. So that's why we got savings there. Now I'll share a story on operational simplicity and why Scoy really values this aspect. Um, so we're, we're game builders. We don't really want to spend a lot of time doing sort of mundane database maintenance or manually scaling or tweaking things to, to get the best performance possible. We'd rather have sort of a managed system that takes care of that for us. Um, So earlier I mentioned that we did have some Reddis workloads as well uh for things like leaderboards, temporary job state, large user lists, stuff like that. Um, in order to serve these workloads we had, uh, a Redd cluster that was not managed by AOBS. It was something else. Um, it was really difficult, uh, to scale without disruption. Every time, once we got to a certain size, whenever we, you know, add shards or something, we couldn't do it without like having a strong user impact. We'd see timeouts, connection drops, all sorts of problems, um, so we were kind of afraid of touching it once it got to a certain size. Also, we had to set alarms on our memory, so if we start to, if we got to like 80% of the capacity of the database, we have no choice but to add more, more capacity. And we didn't like having to do this. We didn't have to like having to get paged on a Saturday when we're about to run out of memory and we have to add nodes and then cause user disruption and explain to everyone why the game was not functioning very well that day. Um, so we're looking for alternatives and um we ultimately settled on Elastic cash server list. Uh, mainly because, you know, it auto scales behind the scenes, it does exactly what we want, you know, it, it grows and shrinks behind the scenes according to our needs, and, uh, it's supposed to do that transparently, um. Also, one thing to note about Elastic cash is it has the word cash in the name, but I think it's very misleading. Uh, it's fine, it's perfectly fine for non-cash workloads. Um, however, it doesn't have the same durability guarantees as like a, a Dynamo DB or a MySQL, um, but our old Reddest workload didn't have those guarantees either, so we were totally fine with it, um. Also we decided to use uh Valy instead of Reuss um since it's open source and 20% cheaper, so it's pretty much a no brainer there. Um, and then, uh, here are the results. So we got our auto scaling, no more getting paged on Saturday. No more responding to memory alarms, and one thing that actually was a bit surprising to us was that it was 61% cheaper after the fact. Um, so we weren't looking for cost savings, we were just looking to save our own time, but finance was always happy with us nonetheless, and, um, we realized that most of those cost savings came because we had to stay over provision constantly in our old cluster because we were too scared to, to scale it down for fear of causing sort of user disruptions. Um, And the bad, we, one of the bad things about Alaska Cash was that we, uh, we were early adopters, so we, we adopted it very early, um, and during this time period there were a few minor, um, outages of like 1 to 3 minutes that we noticed, but, uh, we worked with AWS and um they managed to fix it behind the scenes and we haven't seen this issue for months now and we ended up very happy with the result. So, um, now we're, we're in a much better place now. We're not wasting our time with sort of database tweaking and that sort of thing. Um, And that's the end of my talk. Thank you very much and uh.