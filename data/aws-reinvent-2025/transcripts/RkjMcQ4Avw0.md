---
video_id: RkjMcQ4Avw0
video_url: https://www.youtube.com/watch?v=RkjMcQ4Avw0
is_generated: False
is_translatable: True
---

Alright. Welcome to MAM 339, uh, where we'll be diving into Topalti's transformation to Windows containers on AWS. I'm joined here today by Maya More Freeman, uh, technical account manager with AWS who was part of the journey and lead DevOps architect for Topalti, uh, Danny Teller, who joins us today to give us his experience and insights on that journey. And quick question before we kick off today to make sure these headphones are working, uh, who here is actually running Windows applications, Windows workloads, and their environment? Not Windows containers, all right, perfect. How many have actually considered containerizing these applications? Alright, so this is gonna be a great session because Danny has quite the roadmap coming up here for you, where you'll be able to see all the pitfalls and successes that they've engaged over the last 12 months doing this. So our quick agenda for today, we're going to do a quick background level set on what containers are, and then we're going to dive into Tapalti, their journey, how they got where they are today, the lessons learned, and what's next, what happens after this. And Danny, if you wanna let the audience know what they'll be in for over the next 60 minutes, that'd be perfect. Definitely, thank you, Aiden. So what we're gonna do today is we're gonna walk through from ancient to modern all the, uh, technical decision making that we had to go through, the challenges we faced, and final and our performance gains, optimizations, all this fun stuff, some truly weird things and our results, and finally how it impacted our business. So stay in tuned, fellas. Thank you, Aiden. Thank you. We'll see you in a few minutes. So, like I mentioned, uh we just want a level set so everybody's familiar with what Windows containers are, how they come around, and that way if you're an advanced user or just beginning up, you'll see how easy it is to get started with them. So we're all pretty much familiar with this. Back in the day, uh, one app per server wasn't a great use of the hardware, a lot of high costs, wasn't, wasn't very, very efficient at all. Next up, as we all know, uh, hypervisors. It virtualized the operating system, the hardware, which is great, but unfortunately it still wasn't that efficient. You could only run so many hypervisors in one box, and costs again are a major component. Lastly, this is where containers really came into its own, virtualizing the operating system. This the benefit this has, you can now have a shared kernel, multiple containers using it, and really optimizing your costs on this. So big takeaway here is your cost, your ability to spin up new systems, be agile, and just to be much more efficient in the use of all these resources. So a lot of people forget that the first Docker specc was created about 10 years ago in 2015. So it's a pretty solid solution to go with today. It's not like it's brand new. I know people are familiar with Linux containers, that's what everybody thinks. But fast forward to 2017, that's when Microsoft enabled Linux support on containers that opened up hybrid strategies. Again, we won't go through every single item, but container D came up with Server 2019. 2023, Carpenter support enabled, Amazon supported at the same time, and it's continuing to evolve today. So the big takeaway here is that 10 years later, this is still an evolving process, but it is really a tried and true method for doing Windows containers. Some quick benefits, being agile is one. Being able to spin these up, shut them down, uh, rapid development, that's essential. Cost wise, uh, we do see customers save about 60% on their EC2 costs, uh, roughly about 68% on their storage costs, and reducing your management overhead. If you are something like, you know, EKS, an orchestrator, you know, patching, getting them back, getting them up, failovers, all this, it's a much, much simpler management as well. When we're choosing these, there are some considerations to be brought up. For example, this is 2025, as we go into 2026. If I have a new application today, and it's going to use cropplatform.net, I'm going to choose Linux. Let's be honest. It's just going to be modern. However, for Windows applications, they're a little bit beefier, so you're going to start off immediately with a 3 gig image, uh, which means that the host has to be larger, so cost is an issue. The size of the image is also an issue. Only certain applications can be used as well to be containerized, and we'll dive into what some of those applications can be as well. We do see across the financial industry, healthcare industry, a lot of use cases in AWS for customers taking their applications on Windows containers, and the types of applications that we do recommend to take would be your ASP.NET, WCF or Windows services applications, and finally console applications. Anything that requires a full GUI, absolutely skip that one for right now. These would be your best bet. If it runs on Windows Server today, there's a pretty good chance that you can containerize that application. Now to get started all together with this, you only need 5 items. The first item you need is your Docker file. That's your blueprint for your application, has everything in there. The second item here is your proprietary base image. Typically that's your Windows Server core image that you can just pull down from Microsoft. Third item is your registry to store your images. In this case we'll say ECR. 4th, you need your host for worker nodes and so on, which would be EKS. And finally, you need your orchestrator for just failovers, bringing these up, spinning down, all of that. Again, EKS is your frontier. So just these 5 simple items, you are actually off and running with containers. Now one thing that you can do as well is to speed up the deployment from pulling the actual image to actually starting that off as well, is using what we do is EC2 image builder cache strategy. And how this works is basically you create a custom army and within that you bake in all the components to it through the image builder. And we're going to fly through this. There is a blog that explains this much deeper, and you can Google this up online. But essentially by doing this, it does all the work for you up front. So then when you go to launch this, it actually launches about 65% faster than without a caching strategy and just so you can see this. I've got two screenshots here. The top one is using basically a vanilla .NET, ASP.NET application. Uh, it is using the image cache strategy, and the advantage of that is, as you can see, pole to poll, it's about 54 seconds. On the bottom, you see the two arrows there, we have a pole, and then finally it's um pulled and up and working and started. It takes 7 X longer. So you can see the image cache strategy does absolutely benefit and gets that launch much quicker for you. I've mentioned back in 2023, Carpenter, AWS does support Carpenter. Uh, it's highly recommended, works with the cube scheduler, so it automatically spins up nodes as is needed. They'll be cost effective, works with the compute provider. Absolutely something that should be used when you are running Windows containers. This week you're probably hearing a lot about AWS transform. We're not going to dive too much into it, but during your modernization journey, what you will be considering doing is also modernizing the code afterwards. The first step is to get it into a container. That way you can modernize without having to do any code adjustments. The follow up to this would be to use AWStransformer.net, point that at your application, and it generally will increase the conversion time above 4x is what we're seeing back from customers today. Uh, like I mentioned, if you want to learn more about Transform, pop by the Expo Center, go down by the Amazon booths. All of them have either VMware and other, uh, items that they'll discuss there, uh, code analysis, everything like that for you. And on that note, I'm going to pass you off to the guys from Talti here Maya, Danny, if you want to take it away. Thank you. Thank you, Aidan. Danny. Tell us what Tipalti is all about. Well, Tipalti is a finance automation platform powered by AI. And it's basically streamlines all your CFO operations, anything from global payouts, account payables, procurement, tax compliance, treasury, you name it, we've got it. So it actually just makes your life a lot easier. Everything, it's all financial operations just make it life a lot better. So we're, uh, we distinguish ourselves by combining. Um So, I'm sorry, uh, by combi by combining a single suite platform with the simplicity and all the complexity you can imagine, just making it simple. Amazing. Now let's talk about the technical foundation. Tell us about the application architecture that started this journey. Alright, so picture the early Tipalti days, a .NET 4.7 platform. Uh sorry, framework. Um, monolithic architecture and a payment platform and all that fun stuff was actually hosted on AC2 instances which is like most modelists are and our process was actually multiple processes running all these uh AC2 instances, you know, fast to to develop reliable, great match for a startup phase. Well, that's a good set up. Early stage priorities are all about speed and focus, where enterprise scale demands something else entirely. How did that initial success play out? It played out great. So for the initial phase it was wonderful, quick releases, stable, reliable production. It was really fun to work with, but as you expand all these things kind of hit a small bottleneck, and we quickly outgrew its capabilities. So you started with a clean, straightforward monolith. Serve me where it first. What happened when Tipati began to grow? When growth happens, it happens fast, and when it does, you need more features, more resilience, and pretty much a lot more of everything. So our single process kind of just evolved to multiple child processes from a single parent, and that's kind of becoming a little complex when all of them suddenly, well, you need to run them. They run on a schedule and sometimes you know they move out after 18 years. That growth is pretty common. I bet it will produce some serious operational headaches. Definitely headaches is just putting it lightly. Debugging was just a nightmare. Imagine just taking a single process out of these hundreds and just figuring out which one was actually the culprit. Meanwhile, we're still running on EC2 capacity. It was fixed and we had no scaling. It was truly challenging. So you had your architectural complexity and infrastructural limitations. When did it start impacting the business? The turning point was when our hundreds and thousands of transaction volume just exploded into millions. Our systems just struggled to keep up. I mean we had frequent crashes, delayed payments, slow performance, and most of all, angry engineers. That's not fun, guys. Well, that's what every scaling company dreads. The architecture that powered the initial success starts holding it back. So what the party experience isn't unique. We see in the companies that started as a monolithic a few common pain points. Well, the first is your application was built for a fixed world. You buy a server, you run your application on it, simple, predictable until it isn't. You are overprovisioned just to handle peak loads, and you are wasting resources the rest of the time. And if you have any capacity spike, there's no elasticity. Your application simply wasn't made for it. And then there's the day to day. Your deployment. That should take minutes, stretch into hours, and you are often flying blind. Troublesshooting feels more like detective work than engineering. And then there's the point when the monolith becomes the bottleneck. Release cycles slow to a crawl. Every component in the system is intertwined, and every bug is ripple through the entire system. If you try to adopt modern developed practices like, it becomes an uphill battle. The architecture is not built for it. And what really hurts is the business level. The architecture that fueled your initial success. Still simply wasn't built for it. So you are faced with two choices spend years rewriting. Or live with the constraint. So What option do you consider the party and How did you arrive to your solution? We decided that containerization was the right way to go, but then we had a choice. Was it ECS or EKS? Well, we decided on an EKS because it felt a lot more natural for us, and we are already heavily invested in it using our uh Linux workloads. So it was great for us. And then also it addressed the scalability challenge far better. And finally we needed machine access because, well, let's face it, sometimes Windows you need to understand the intricates of the stuff, so. We had that we needed that access and finally. Windows and Kubernetes is kind of the uncharted territory, so that was fun to go with. Where did the confidence come from? Absolutely. From our DevOps team, those guys are relentless. They just tore into that stuff and they just really remodeled the entire thing. So, and, and also, you know, the container ecosystem was very mature, so it helped out. So it was great and we decided on a minimal risk approach. We wanted to stay as close as we can so we could focus on the application instead of the platform. So ICAS was naturally the perfect fit for us. So basically when you are ready to containerize your Windows application, you get to a fork in the road. Do you go with Amazon Elastic Kuberne service or Amazon Elastic container? Elastic container service is yes. And let me just go over both and explain what each offers so you will know why the party made the choice. So let's start with Amazon. Think of it as AWS knows the best approach. We've taken hard-won lessons from running containers at massive scale and baked everything right into the service, and the good thing is that you don't have to to become an orchestration specialist overnight. is opinionated in the best way. And It's relevant for a team who wants to focus more on their application and less on the infrastructure. Now tells a different story. This is for a team who want the full power of Kuberne, this experience. Or maybe for teams like the party who are already running Kubernetis on other workloads, you are getting the same experience everywhere. So how do you choose? Basically it comes down to your team's DNA. And where do you want to spend your energy? Focus on the application and less on the infrastructure, while offers you that Kubernetes flexibility when you are standardizing across environment is part of your strategy. And the good thing is that both are 100% ready for Windows production workloads, so the question is not which one works better, it's which one fits your team's strategy. So it comes down to control versus simplicity and The party chose because of their extensive developed expertise and their need for Deep debugging of their environment. Now when it comes to .NET application on AWS, this flow chart is here to help make the decision how do we move. So it's kind of prescriptive and it requires some investigation and deep dive, but eventually there are 3 migration paths corresponding to those paths. The first one is rehost. It's simply moving your application as is to traditional servers like Amazon EC2 instances. The second, which is the platform, is making some changes to your application to accommodate the underlying platform compute changes like moving to run on Windows containers, and the third one is re factor, which is rearchitecting your application. This pattern helps you to use cloud native solutions such as Serverless or porting your .NET framework application to cross-platform .NET and run on Linux container. And finally, this is the path the party chose. They were already running on Amazon AC2 instances, and they decided not to spend years rewriting their code base and to move to run on Windows containers. Now, Danny, let's dive into the nuts and bolts. Walk us through what it took to get a Windows container running. So first of all, we read every single AWS doc that mentioned Windows and Kubernetes in the same breath, and we found out there are a couple of key nuances that you have to set up right from the start. Now if you don't do that, you're probably not going to have a smooth sailing in the process, so. OK, so once you had the lay of the land, what were those set of steps? Well, the first one is actually Setting up your your VPCCNI and that's the actual component that's responsible to handing out IP addresses to your pods and tying it to your VPC. So we had to tweak it with the right flag as you can see, and it was very straightforward. However, unlike Linux nodes, Windows nodes don't run the AWS node pod that actually does the tying for your IP address to your VPC. That actually happens internally, and that's a fact we had to really pay attention to going later on. Next, we had to tweak in the AI AWS IM permissions specifically for Windows nodes. It has a small addition. To Linux nodes, so we had to tweak that as well, and that actually used to live in AWS Auth config, which is now conveniently replaced with access entries. OK, so once the foundation was in place, how did you decide which Windows container or which Windows version or which container setup you want to use? We decided to stay as close to home as we could, so we decided with Windows containers Server Core 2019 running on Windows Server 2019 server server nodes, very identical, very close to home. At some point we actually tried to match. build of Windows, but that proved to be very, very tricky, so we stuck to versions and it was very, very simple for us to do that. So that's a pragmatic approach, especially for business critical workload. So compatibility can make or break a deployment. So what drove this level of caution? It was about risk reduction. So this is a payment platform, zero tolerance for downtime or any interruptions, so we wanted to stay as close and identical to something we already knew was running and by moving to the same setup on Kubernetes, we were actually able to reduce operating system overheads, so it played out well for us, but it's only part of the trick. Sometimes you have to pay attention to what Windows images you're gonna choose. You're right, so Microsoft offers 4 container-based images, which each one exposes different Windows, this is what influences on the final container image size and on this footprint. So let's start with the minimalistic dream nano server. This image exposes just enough Windows to run a cross-platform .NET or modern open source frameworks, so. It's excellent for building sidecar containers or when every megabyte matters in your deployment. It's clean. It's fast, and it gets the job done. The next is the silver core, and there's a reason this is the crowd favorite. This one struck the sweet spot between efficiency and functionality. It exposed. Those Windows sets that support the .NET framework and those bread and butter Windows Server features like And it covers the majority of enterprise application needs without going overboard. Well, the next one. Is a server. It's a little smaller than the full Windows image, but don't let that fool you. It packs the complete Windows set. It makes it perfect for applications needing to run directly its graphic capabilities. Like having a compact car with a surprisingly powerful engine under the hood. And the last one, the heavyweight champion, is the full Windows image. It exposes every single Windows you can think of. It's massive, but if you need to run a graphics intensive application or cutting edge machine learning framework, this is your go to. So when you come to choose, you need to consider everything you bring along with you those drivers, your application calls, or those specifics you need to run. If you go too small, your application won't run, and if you go too big, you're hauling around unnecessary weight. It will slow down your deployment, so. Then at this point your was configured and ready, and what were the next steps? Well, the first step is encountering a significant first blocker. Oh, OK, let's talk about those blockers. That's really fun stuff, you know. So the first challenge was about logging, but you know, launching a container is usually the easy part, and debugging what's misbehaving is actually, well, the most interesting part about containers. So picture this that you're running on. I'm sorry. Picture this that you're running on on an EC2 machine and what you usually use is an XML format which is standard reliable, and you'd install a fluid bit engine and you'd get those logs right up right into your logging system, which is very easy, very fun to do. But when you go into Kuberneti's Windows and you need JSON format, all that goes out the window. That's not gonna work that easy. So what you're actually gonna do is you're faced with a couple of choices. Do you bake the agent into the docker file? Do you run a sidecar? Do you look for a vendor to to solve it for you? Or do you rewrite the the application completely just for the logs, right? So here's where windows containers throw you a curveball that sometimes catches teams off guard. In the Linux world, logging is beautifully simple. Your application, your containerized application, dumps everything to standard out. The container runtime catches it, and boom, you have logs flying. It works every time. Now In the Windows world, pods don't generate standard out by default, and here comes a solution that's called blog monitor. Think of it as a universal translator. For your Windows login it formats everything. From ETW, from your Windows event log and your application specific logs, and it formats everything and pipes it to standard out, something that Linux does by default. It's like a bridge between the two worlds, so. How to set up it, it's pretty simple. You take a log monitor. You integrate it into your Windows during the build. You configure your log specifications as part of your pod. You build, you deploy a log forwarder like a fluent bit and configure it to send everything. Onwards to hopefully cloud watch so As Danny mentioned, I will share something from experience. So formatting is very important. Always log in a structured format like JO or C slog, because you will be very grateful if you're debugging at 2 a.m. and you have logs that you can pass. And second thing, less is more when it comes to verbosity. Keep it quiet unless you are actively debugging. Important events can get lost in the noise. Uh, so this is the technical. Um, how did it, did it actually played out? Well, we started off with lock modern just as you mentioned, and once we plugged it in, we actually saw logs. It was great, worked really well, so we actually were able to see our logs within our system and we played out pretty well, and it actually confirmed for us we were able to see a con a Windows container with a legacy application and logs flowing through our system. It was good. So seeing the pass live logic was kind of depressing, I'm guessing. So the lock monitor stay in your configuration for a long time. Initially it did until it didn't. Because we actually found out that log monitor introduced some unwanted baggage and overhead, it kind of made the application crash sometimes it crashed on its own. It wasn't very fun to hear and look, of course, and. One thing about it is when you hook one process into another it's bound to have some mysterious activity in it so it's not very uh effective. So what we actually did, we modified our application logging configuration to log directly to stand out and actually just solve the problem for us. And once we did that we had uh logging flowing into our uh centralized logging system and the first challenge just marked as completed so. That was good to go. So once this hurdle was behind you, what came next in the road to production readiness? Our next challenge was a completely different set of difficulties, something completely unexpected. So we're testing various application versions. It was great until we stumbled upon something really, really nasty. What happened was we would roll out a new version and we saw pod getting stuck in terminating state for quite a long time, I mean like 5 minutes or something, depending on your configuration, but they were just stuck there instead of just terminating as they should. So first of all, that sounds very frustrating. Can you tell me what was happening under the hood? Yes, so we actually suspected something between the cubet and container D. So in a nutshell, once you shut down a container, Cublet actually sends that signal to the container, executes any pre-stop hooks along the way, and then the container actually shuts down and reports back to Q Cubelet that it's gracefully shut down, having fun. But actually for us it wasn't really happening because you see, in Linux you have the sy term and in Windows you have something different, an equivalent of that which at that point. It wasn't really propagating properly, so we had to really investigate to understand what what was going on. So Kubernetes was doing its job, but the signal was not what the application was expecting. Yes, so our application was actually coded to handle that specific Windows event for shutdown just fine, but Kubernetes tried to send that signal and nothing was happening, so we figured along those lines that something was going on. So that stuck period where actually Kubernetti's force killing the pod using the graceful termination period so it was, it was stopping but not gracefully, very brutally in fact. So What, what were you trying to do to pinpoint the root cause? Well, we dug in. Well, you have to, so we actually found that there was a bug in the container D at the time. AWS was running two different versions for AWES Linux and uh for um for Windows, and for Windows they were running version 166, and apparently in that version a signal propagation for Windows was not supported, so that's why it wasn't working. OK, so this is where Windows diverge from Linux again, as Danny said, Kubernetes sends a signal. The Windows runtime translates to something called a control shutdown, which is the Windows native event. If your Windows application receives the event, it will just behave normally and shut down the application, but the translation is very crucial because Windows applications are built to respond only to control shutdown events and not to signal. So the container runtime acts as a bridge between the Kubernetes world and the Windows world, so. How did you work with AWS to handle the gap? Well, we reached out to AWS through GitHub, so it was a great collaboration. We opened an issue and somebody replied, very straightforward, it was very quick to do, and they confirmed that a later release fixing that issue, just upgrading a version, was coming later that year. But unfortunately for us, you know, we can't wait. We have to solve these things right here and right now. So you found a workaround, of course we did. What we did, we relied on Kuberneti's native features. We introduced the life cycle hooks we mentioned earlier, and we tuned in our graceful termination period, so we had more control over when our actual location was shutting down and how it was shutting down. So it played out well for us. So all we had to do was just sit back and wait till AWS released a newer container D version and we're set to go. OK, so, um, once that was behind you and everything was up and running, uh, how did you validate that the container diversion was behaving as expected? We decided on the ultimate head to head battle. So we compared our EC2 environment to our Kubernetes environment. So they're running side by side, same configuration, same environment, and they were both plugged into the same routing queue. And what we saw was a mysterious new client just consuming these messages, making our developers actually coming over and saying. What is this new container doing? What is this new agent that's consuming our messages? And of course it was our Windows pod just pulling its own weight. So another thing very satisfying to watch. Definitely finally we could actually see our container, Windows container, working properly on real business life logic and it was really great to see. So what we did next, we bulletproofed everything. We touched performance, we touched the baseline, we continued with testing. And then we're very eager to start the next phase. OK, so now you are ready to test the core promise of cloud native transformation, scalability. Definitely this is the moment we've been waiting for completely because we wanted to test scalability. So what we did, we of course plugged in our Windows pods and we tuned them to listen to messages on Rabbit MQ using Kita or Keta as you prefer. And because we wanted to actually scale based on amount of work to be done instead of, you know, just server work, server load, I'm sorry, uh, CPU or memory, which is classic but not enough, well, not today anyway. And finally, the other half of the battle is infrastructure. You can't scale application if your, if your infrastructure doesn't support it. And since we're, we were on Kubernetes and we're already using Carpenter, all we had to do was just extend it, right. Just another CRD. So what kind of results did you see when you tested the scalability? A great kind of results, scaling results. We were actually able to see our pods and our Windows nodes scaling up and down very seamlessly and efficiently, so that was a really great turning point for us. We enjoyed that spectacle. OK, so this is a huge shift from the old model. So beyond the scalability itself, what other improvements did you notice? We noticed that by running the EC2 and the Kuberneti's equivalent side by side, we actually got a 50% improvement boost that was completely unexpected. It wasn't just running scaling better, it was just actually more efficient in how it was working and from an operational perspective. that was a real game changer because at that point instead of just debugging hundreds of processes we had a single pod with its own process and we could see logs traces metrics were just beautiful super simple, very straightforward so. Uh Carpenter is AWS native. It understands things like spot pricing and commitments, so it can help you not just scale up, also to scale down, to consolidate workloads and turn and shut down notes, so. If we are speaking of costs, let's talk about it because I think this is something that will interest a lot of people. Going to a full blown Kumbernetis cluster, running Windows must have its financial trade-offs. Definitely. I'll be honest about it because our infrastructure and Our cost kind of spiked up a little bit but not by much, you know, on a good note, but it was actually very, very worth it. And how did you justify such additional costs? Well, the operations actually weigh out the spend. We got a 50% boost. We got scalability and ease of deployment. I mean all these things, when you factor in together, I mean they they kind of justify this extra spend, right? So while our server bill actually did spike a little bit, factoring our total cost, all the pain and suffering we've kind of alleviated from our developers and customers, was just worth it. Still, every smart company looks for optimization opportunities. What strategies are you exploring to keep the cost in check? Well, we tried spot instances, but unfortunately our application couldn't handle the shutdown event for 2 minutes with a notice for spot instances, so that was out the window. Instead, we relied on savings plans and reserved instances for right sizing. We used EC2 compute optimizer and finally we traced. Um, our resource usage using AWS container insights to right size our actual pod usage. And as you know, the cost efficiency battle and the performance battle never ends. It's an ongoing session. You're right. It never ends. Just another insight, it's very important to use Cos Explorer. There is a feature. There is an option to enable. Cost allocation split cost allocation data, also known as, and you can also leverage cloud intelligence dashboards to monitor your container costs. You can set up budgets. You can set up alerts to let you know if if you have a cost spike unexpectedly. And as Danny said, the key insight is that cost optimization is a journey. It's not a one-time activity, and You need to always have it in the back of your mind, so. What's the long term impact? Overall it's an absolute win. We were able to cut down on manual operations, on response times and debugging, and best of all, we were actually able to eliminate the infrastructure and bottleneck we had for scalability, and that kind of thing is priceless no matter how you try to go around it. OK, so now everything is stable again. What area did you focus on improving next? We decided to focus on two key areas, one of them being the node creation time and the image pull duration. And if you break it down, then node provisioning actually takes around 7 minutes from EC2 creation to joining the Kubernetes cluster, and images were actually about 4 minutes just to pull. Um OK, so let's focus on the notes 1st. 7 minutes is noticeable. Yeah, so definitely noticeable, and we dug in and we saw, of course, that EBS configuration was actually the culprit for the entire thing. So as you know, the baseline, when you create an EC2 machine, you get 125 megabytes of throughput with IOs around 3000. That's great for general workloads, normal EC2 machines running, that's fine. But when you're talking about Windows and you want to go faster, that's not enough because Windows needs more bootstrapping time and processing. So we did the natural thing. We doubled all the. All the values and actually it shaved off around a minute even more so from 7 minutes we cut down to 7 to 6 and sometimes even lower than that and we try to go different directions and trying to explore different values but that was in vain unless you're willing to go the extra spend and try using IO1 uh volumes which are, well, you know, very expensive and blazing fast. OK, so that's a fair trade-off eventually. and the second bottleneck, the container images, I imagine that around 4 gigabytes of container image is not exactly. No. So, uh, our actual image containers, they weigh around 4.5 when you take in the baseline image and the application itself. So it turned out to be to be around 4.5. And when you start pulling it out, then it kind of takes a long time. And if you compare it to Linux, then it's massive, 4.5 gigs over to what, 100 megs on a Linux machine, that's fairly massive. So what did you do? Did you change the structure, or was it something from the AWS? Actually it was a bit of both. We, we noticed that at some point AWS started preloading base images onto their EKS optimized AMIs. So after a few AMI version changes, we noticed that it was a quiet change and it dropped down from 4 minutes to around 50 seconds. It was great, yeah, pretty much, but we were not satisfied. We introduced an internal image registry. And by doing so we were actually able to cut down from 50 seconds to 20 seconds and they're just very, very fast. That's a very dramatic improvement. So what kind of overall impact the optimization had by combining both results, we were able to slash the times from 11, 11.5 minutes to around 6, sometimes even less, 6:30 was being the average. And it actually made all our deployments feel very responsive, very snappy, so it was one of those moments when you invest all this time fine tuning and actually does pay off. OK, so things were smooth by then, right? Production was stable at that point. Yeah, we're pretty stable. We're running with 4 windows nodes, scaling up and down with 35 windows pods, with being, uh, more services added every day. It was great to look at and we're actually still running with 50% improved performance boost and our deployments are around 6.5 minutes. But the best of all, the image, the. The dramatic reduction in incident response time and all the incidents was just blowing us out of the water. OK. And what about your team's operations day to day? Well, they shifted from actually firefighting all the time to, you know, innovating, writing new features, rewriting, refactoring even a bit of the of the code, but it wasn't the end, right? No, unfortunately that was not the end. We actually, well, it was a bad ending I have to say. So at this point we're after the containerity saga. It was a very difficult time and then we were finally kind of stable, but at some point we noticed that when we started rolling new versions, the pods really refused to die. You try to force delete them, they'd still be there, no termination grace period. It could help, nothing. It was just stuck there. At some point we start calling them zombies. I don't know what about you, but I don't like zombies. OK, so I'm guessing it was very frustrating. How do you begin to hunt it down? Well, we spent 6 weeks debugging with AWS support team on this. We every time get a new theory, understand, see if it can work, test it out. Now, of course not, and we're just really pulling our hair at this point, so. We're just hunting it down and nothing worked. So 6 weeks to hunt is a long time. What did the investigation finally turn up? Well, actually it was a critical regression bug at the platform level. It was one of those Windows services, the host networking service, which is actually responsible for networking that we have zero visibility into. So a black box bug. Yes, definitely a black box bug. So can you walk me through the technical details of what was actually happening? Yes, it turned out to be a nasty, really nasty race condition. When one pod was starting and one was actually shutting down. It was causing the networking component to actually create two dual endpoints. So at one point you had a living pod and a dead pod, kind of, you know, Schrodinger's pods, if you say. And that was uh kind of uh very difficult to understand how it was working, but we were able to understand uh and find it. OK, so What did the AWS do? Well, As we were working with AWS, they decided to of course uh go into the networking components. They tore them out completely and traced the bug to a recent platform change, and they fixed it at the AMI level and they of course coordinate with us. OK, so but a platform level fix is not something that is immediate. It takes time. So how did you, you know. How did you do in the meantime? What did you when you waited for the actual release? We got a little creative, so we were using a carpenter feature to introduce a time to live on our nodes. So we kind of paved over the problem by refreshing the nodes every 3 hours. It meant clearing the bad networking state entirely. And we also introduced some manual and automated procedures, you know, to exercise those demons, and finally we added alerts just to catch them just enough or even before it actually happens so we could actually tell what's gonna happen. OK. And the final resolution, did the AMI do the trick? Definitely. When AWS released that AMI fixed, we immediately implemented it and it fixed everything, so we were actually back to reliable production. Everything was stable, smooth sailing from then. And you know it was also a challenging time for us as well. We were working hard with AWS and it kind of, you know, strengthened our partnership deeper. That's always good to hear. So let's zoom out for a minute. Can you tell me, after all this work, what did this transformation actually achieve for the party? Yes, so definitely our results, we had a 50% performance boost constantly across the board when comparing to Kubernetes. Our scalability just Changed completely from 6 hours from creating a machine and installing everything to just a pod running everything. It went from 6 hours to 6 minutes. That's incredible on its own. And also just the installation and running the pod and the service itself, I'm sorry, from running on from 4 minute installation to just 30 seconds on a helmtra deployment. That's a massive improvement for us. And how did life change for the team? For the better, of course. So we went from 12 hour debugging sessions to just two because we had clear observable telemetry for logs, traces, and metrics from a single pod process instead of just hundreds of millions of multiple subprocesses. OK, and from the business side, well, we achieved something that was pretty impossible for us early on. We had no horizontal scaling and we actually were able to solve that using this transition, so the business impact was just incredible for us. And we employed of course it opened the door sorry, it opened the door for many uh modern those practices so we can just employ them and continue innovating our procedures. OK. And when you look back on all these journeys, zombie parts and all. What are the key lessons learned you will share with people thinking about a similar transformation? Well, I'll break it down into a few key areas that we actually uh learned um the hard way. So on a technical perspective you gotta start with small changes, OK? You have to test it out first, understand if you can do that, and by doing so you can preserve your existing .NET framework and. Gets modern benefits all the time. So, and kind of, you know, when you're working with Windows, just expect the some unexpected things. It's kind of a cliche to say, but you know Windows is always behaving like it does and from operations perspective engage AWS proactively. It saved us so much time if we would have done that a lot on earlier and most of all document everything we documented every single process and every single execution that we did just so we have enough information to go on. Strategically, incremental modernization works. It works 100% of the time. You don't need to rewrite your application from scratch. You can just start small and. From a crisis management point, the Schrodinger's pods actually taught us to be a lot more proactive, and by doing so we're actually able to uh overcome all these issues that we faced. So comprehensive monitoring definitely is a must, especially for these types of things. And you should also have mitigation strategies. Don't just blindly go into something. You have to think some sort of way for the unexpected issues. So There are 4 critical factors that usually contribute to a successful transformation. So the first one is executive support, and I don't just mean someone writing off on the check. Executive support means management, understanding that this is not just an IT project, OK. It's a business transformation and one of the biggest. Transformation killers is organizational silos. Solutions that work perfectly in isolation often fail in organizational reality. So it's very important to create transformation teams with developers, operations, security, business stakeholders, people who have real decision making authority. And the I'm guessing that if you want to containerize, you want to do it yesterday, but this is not a race. It's like you don't want to migrate during peak hours. You need to take it easy, to start with non-critical workloads. Learn from experience. Leave enough time for testing, for troubleshooting, and for the unexpected. And the last, container technology is always evolving. Successful teams build learning into their. Not just initial training, but an ongoing education program, feedback loops, best teams leverage, support proactively, using expertise to optimize architecture and always stay ahead of best practices, not just fix problems. And now Danny. What's next for Tipaldi? Well, the improvement game never stops. So we're gonna continue exploring how we can make everything much faster from node provisioning to image pulls because we wanna be as close to Linux speed as possible. Next we're gonna be working on more cost optimizations because you know like I mentioned this never ends and you want to save as much as you can so we're gonna be heading that direction as well. We're also gonna be tuning in our better scale for our scaling policies, so they're gonna be better scaling for us as well, more, more proactive, and all this endeavor actually created a great foundation for our microservices so we can port legacy applications onto microservices without a pressure of a complete rewrite. And what about your broader technology strategy. Get ops Get Ups is the best, so. What we're going to do is we're going to implement more GitUp's workflows. And well by doing so we can actually uh work with better and more modern DevOps applications and practices so that's not gonna end anytime soon. OK. And Yeah. Oh, so, um. Let me just say that That's a great example of what the party did that. A successful transformation can enable broader modernization effort. The containerisation is not the end goal, but it helps set the foundations on continuous modernization. And that's the real value of this approach. Um If you want to continue your containerisation journey, there are some resources we put here to help. You have the Windows container on Emmersion day, best practices for Windows on Amazon, and check out Windows container in Kubernetis. Take a picture, scan the QR, and go check it out. And in addition, beyond that, if you don't know, we have AWS SkillBuilder, which is your gateway to mastering all things AWS cloud technology. There are over 1000 free, self-pacing expert-led courses. You can follow guided journeys or just jump directly to the topics that interest you. And with that, thank you very much for being here today. Feel free to reach out and stay tuned because the blog post on this journey is in the making. Thank you. Thank you.