---
video_id: i_lLnV6NgPg
video_url: https://www.youtube.com/watch?v=i_lLnV6NgPg
is_generated: False
is_translatable: True
---

So today we're gonna talk about really optimizing your agentic AI applications with semantic caching and Amazon Elastic cash. So just a quick poll, just how many people are really happy with the time and the cost of your agentic AI applications if you've deployed any. Oh, I don't really see many hands. So if not, we're really gonna show you the tools here to really get started. So today we're gonna talk about first, the journey of agentic AI and where we are today. 2, we'll talk about why traditional caching typically fails for agentic AI applications. 3, we'll talk about how semantic caching works. And then we'll also talk about implementation and best practices in gentic AI applications. So Agentic AI continues to evolve. Early on, it was really about building a lot of those strong foundations. And in 2022, it was a lot of it was introductions to these conversational AI types of tools that we're all familiar with today. And then as we moved on in 2022 to 2024, it really moved on to quality and handling multimodal data like images, videos, and so on and so forth. And then, so and then in 2024, 2025 it started shifting to chat to action, natural language to tool invocation, all that good stuff. But now where are we today? We're really past that experimentation phase and the mindset really has changed from does it work to does it work at scale and we're thinking about security, governance, latency, cost containment, and all that and today we're gonna really focus on those latency and the cost containment piece with Samantha Casio. So as we kind of noted, there are really 3 barriers to move from demo to deployment. The first one is really scale. Scale as these applications become more and more complex, you have more and more agents, you have more and more tools, a lot of different steps, and speed as each of these individual steps through these types of applications require more LLM calls, more API calls, more tool calls generally. And then thirdly, cost, as many of these steps require LM implications, that's extremely expensive for a single transaction or single turn conversation, multi-turn conversation. So one of the biggest issues with agentic AI is compounding latency. So think about assembling all that context, retrieving memories, that structured data, that user personalization, and really planning what to do to respond, what agents to call, what order, what tools to call, and so on and so forth. And then 3, actually invoking all these tools and all those agents and then actually finally re-ranking and assembling the final response. But the biggest thing here is that in these applications they can cycle over and over and over for a single transaction and your latency and your costs can be boundless without the proper guardrails. So when you look at this orange line here, it's the cost associated to agentic AI application, and these applications are growing fast. In fact, over 79% of companies have deployed agentic AI applications in their in their production workplace. But as we add more and more agents and more and more steps to increase intelligence of these applications, we increase the complexity. So what do I mean by that? I think I mean by I mean planning and orchestration and more tools, more specialized agents, which overall it does improve the quality of your agentic AI applications. However, it does drive up costs and it drives up latency. So practically what do we need to do to really optim find those optimization techniques to really flatten this curve and really make it palpable at scale. And really maintain that intelligence. So there's a lot of different, there's a lot of different techniques you can follow. You've probably heard the concept of prompt caching. You can use cheaper models. You can do model distillation and fine tune models for specific tasks and better routing, but a tool today that we're really going to introduce to you and explain in more detail is Samantha cache. This really allows you to squeeze every bit of intelligence out of your application without ballooning costs and maintaining an efficient cost profile at scale. So let's think about these app these implications in agent. So we're really all familiar with the concept of single agent frameworks. So when we think about this in practicality, let's do something a little simple. You might have come today to Vegas or yesterday and thought a question, you know, what's there to do in Vegas? A single agent framework would basically go out and maybe fetch, you know, do a web search. Look at databases on events and you know search across the internet and then grab that context, serve that to an LM and give you a response. You know, the cost and the latency is basically the cost of the time associated to the latency or associates to LLM APIs and all that. But when you think about the complexity that has really started to be introduced with multi-agent frameworks is that you know they're including more LLN polls, more toll calls, which means more and more leniency and more and more costs. So just to give you a quick snapshot of the types of multi-agent frameworks that you see today, thinking about first supervisor models, so thinking about, you know. Primary orchestrator fanny out to all these specialized agents. Every single one of those circles is another agent call which includes a bunch of different tools, a bunch of different API calls, all more costs, more latency. You think about, you know, network models, all these agents working in collaboration, going back and forth to complete a specific task, more, more LMs again, more tools, more costs, more latency. And you think about higher, which is kind of a derivation of the supervisor models. Similarly, you can draw those same analogies. And finally, think about sequential models. Think about agents that hand off specific agents after they complete certain tasks. And today I'll walk you through kind of a quick example of a sequential multi-agent model, multi-agent framework, and how Samantha Cash could fit in there. So let's make it real. So let's think about a common question you might be asking yourself, because for me, I love Italian food. I love vegetarian food. I might be asking a question like, you know, find Italian restaurants near the Bellagio with vegetarian options at 4 p.m. 4 at 7 p.m. This is a realistic question that everyday users ask to their traveling or dining agents. It may seem true. It may seem simple on the surface, but underneath lies a really complex web of requirements thinking about the cuisine, thinking about. The distance thinking about the menus and then thinking about the availability. So when you think about you as a person and what you do to actually do this, you do all the reasoning. You think about you think you decompose a question in multiple, multiple steps. You look at all these apps and find Italian restaurants in the Bellagio, look at Google Maps and all that. You might look at all the menus on various websites. You might search all the apps to find availability, and you might make that final decision based on all that data. You have. So when you think about agentic AI applications, how does it handle this? You think about a planner agent, you think about a planner like same as what you would do, decomposes that question into individual steps, asking specific subagents for a complete certain tests, and that's similar to what I described in the scrunch. You could, you could break that down into finding all the Italian restaurants near the Bellagio. Ask a search agent that might go across all the distance calculations, might do web searches, various API calls, and then grab and find that list. Then I might hand off to a review agent that might look for the vegetarian options. and all of those menus through web searches, through rag calls, database searches, and to funnel that down into something more palpable and then finally searching maybe for that subset of restaurants and looking at all the availability they have, searching across all these different applications that you commonly look at defined availability, and then finally actually using another LLM to essentially take all that data, merge it together, and give an answer back to that user. But as you can see here, the latency and the cost really starts to compound in scale. So think about another user that made me ask something that's very semantically similar veggie items, different time, all that. In this use case, the user the question would be decomposed in a very similar fashion. A search agent would run, that review agent would run, that availability agent would run. But in practicality, you're basically doing the same thing over and over to get the same response with a slight variance of the availability. But what do you really notice here, you notice that patterns really start to emerge at scale. You think about that search agent's response, you could actually fetch that user one's response, grabbed all those Italian restaurants near the Bellagio, figured out the review agent's response, grab the veg options, and then also use that availability agent's response and grabbed what availability was associated to those restaurants. So there's a pattern here. So what we think caching is the solution here. We've seen these patterns before, so in traditional traditional workloads, you see this typically an application to offload queries from a database to cache to really save time and money. You typically first check the cache to see if the key or that query was executed before and you get that result within microseconds. If not, you run a query against your database, which might take hundreds of milliseconds. And then you add that you cache that result into cache which occupies maybe less than 1 kilobyte of data, speeding up that following user's query that might be the exact same. So we have a solution here, which is Valy. Valy is a community replacement for Reddis. It's open source. It's permissive, and it's been well adopted a lot across a lot of different cloud vendors, a lot of different organizations, and all that good stuff. And we've actually last year we launched support for Valy in Elasticash, so you get all that managed goodness around observability, security, scalability, and more. And Alas Cache has a ton of use cases. I'm not going to touch on all of them here, but you can see them. But today we're really going to focus on this one use case around machine learning and AI and one specific new capability. So Vector search for Valy is now live. So as of November 17th, vector search for Valky offers the lowest vector search at the highest levels of recall and throughput, with the best performance and price performance across popular vector databases on AWS. So let's talk about the use cases. This really supports your traditional rag use cases, which you're all familiar with, grabbing context, serving that to the LLM, generating a response, thinking about real-time semantic search for recommendation engines, thinking about agentic memory and really personalize your agents based on a user. And then finally today we're going to talk about semantic caching and how Elastic Cash can really help you reduce your cost and really improve performance for your gentic AI applications. And you know I'll leave you with the speed Elastic Cash for Vy offers really allows you to build super fast, cost effective, and accurate gentic AI applications and. With that, I'm actually gonna hand it off to Alan who's gonna walk us through all the nitty gritty details and explain how you can build this and why this high level of recall is really important for you as you navigate this world of agentic AI next year. Thank you, Sanji. So the first question is, if we're going to build a semantic cache, what is a semantic cache? So Sanjid showed us a question or showed us a picture of a classic database cache, and a semantic cache is very similar to that. The basic operations are similar. You put things in, you look for previous answers, etc. but in a classic example. What you're indexing the cache, what you're looking for is typically like a SQL query, and you're turning that SQL query into a piece of text, and you're asking the cache, have I seen this piece of text before? In the semantic world, we're gonna do all the same things with caching, but we're not gonna use exact text for the query, we're gonna use the semantics of the user's query. So In a short nut, a semantic cache is simply a cache that's indexed by semantics, not by sequel. So to go through and operate your cache, the first thing you have to do is to take the query. You have to be able to extract the semantics from that. Once you've got those semantics extracted, you're gonna have to go to the cache and search through the cache through all of the previous answers that you've stored in it and see if you can find an answer that's close enough to the query that you've gotten. And if you find one of those, then you have a cash hit. You use the answer, classic caching. However, if you can't find one, then you're gonna have to go off to your LLM, generate a new answer, and then put that in cache. The back half of this, it's a cache. You're familiar with cash's. That part operates the same way. So, semantics are a great human concept, we all know what it means, but computers don't work that way. They work on bits and bytes. And it turns out that you can turn semantics into numbers and we can represent those numbers as vectors, and that's exactly what we're gonna do now. In classic in in in your typical application, the vectors that you use to represent your semantics, they're gonna be very large, hundreds, thousands of dimensions to the vectors. In my examples here, my tiny little brain can't go past 3 dimensions, so that's what I'm gonna show you here. But where you see 3 dimensions, think hundreds or thousands. So in my little 3 dimensional world I can take some items and convert them into the semantics and then plot them in this 3 dimensional world and the thing to realize is that things that are similar, they group together in the 3 dimensional space. And what does that mean? What it means is that things that are similar semantically. Have a proximity, have a small distance in vector space, and that's something that we can compute on. So what does it mean to say that the semantic distance is the same as a vector distance? So here I have another three dimensional plot. In this case I've got some queries here and as you can see, the dining and things are kind of grouped up into the right. And let's say I get. The first piece of the query that Sanjeet was talking about here, somebody asked, where are the best Italian restaurants? Well, semantically that lands about there in this simple three dimensional world. I can look at the distance between that and the other vectors that are near it, and I can establish. A similarity threshold. Now I can say things that are within this distance, they basically mean the same thing. That distance, that similarity threshold is the key to getting high quality results uh here and we're going to return to the effect of that near the end of the presentation. So How do I actually do that? Well, it turns out, how do, how, how do I turn a question like best Italian restaurants into a vector. This is actually old technology. Uh, people that have done natural language processing for years will recognize this. Uh, we use, uh, a set of models that are called embedding models and you may have heard the term vector embeddings or embeddings or vectors. They're basically all the same thing here. I don't know why we use the word embedding. Somebody pulled that down before it stuck. So embedding models take in queries, turn them into vectors that are semantically equivalent. We can do that on Bedrock using any of the embedding models that are available on Bedrock. Amazon offers two embedding models. There's a text embedding model, or if your queries are not texturally oriented, if they're sound-based or image-based, you can use a multimodal model. The process is exactly the same after that. You can also use other models from other vendors or you can even train your own and if you have a query environment that has a domain specific nature to it, you can improve the quality of your system by training up your own embedding model. The key item for the embedding models is they are dramatically faster than large language models. Um, 750 times cheaper and faster is actually not hard to achieve, OK. That number is going to come back. So we actually have all the pieces now, we can put it together. So to extract our semantics. We convert that into a vector. Our semantic search is now a vector search. We're gonna look through vectors and find vectors that are close within our uh similarity threshold. Things that are within that threshold, those are hits. So we can reuse the answer that's associated with that vector, and if we can't find a vector within that, then we have a cache miss go off to the model, generate the answer, put that answer into the cache so that we can reuse it in the future. So now that you've put that all together and you've built it. You run into a problem, which is it doesn't scale very well. And the reason when you dig into it is you had your computer science hat on and you did your vector. Using uh you did your vector search using the exact vector search algorithm. Find me the exact right answer. There is no known algorithm for exact vector search that's better than a brute force. It's a 75 year old problem. No one's ever solved it better than that. But if it turns out. That if you're willing to give up on getting exactly the right answer, you can do a lot better. There is an entire family of algorithms called approximate nearest neighbor, and you can think of these as kind of like a bloom filter, you know, with a bloom filter you put something in and you, you know, if you look it up, you know that if it's not in the bloom filter, I've definitely never seen it before. But if I look it up and I do see it, maybe I've seen it, maybe I haven't because of collisions, etc. So that's kind of fails with a false positive. The approximate nearest neighbor algorithms sometimes won't find you the nearest vector. That's what it means by approximate. And in the caching situation, that's OK because what it means is you didn't find the best answer. But if it's within the similarity threshold, it's an acceptable answer. Or if what it found is outside the similarity threshold, then it's a mess. And instead of getting the wrong answer, you, you're simply just a bit slower than you could have been otherwise. So The question then boils down to how good is my approximate nearest neighbor algorithm. Well, we have a way of measuring that and we call that recall. And recall is exactly what you would expect it to be. What percentage of the time does it get the right answer? You know, you can walk through the math, but let's say for example I do 10 lookups and 8 of the times it gets the answer right and 2 times it gets the wrong answer. That's a recall of 80%. Just intuitively exactly what you would expect. There are lots of ANN algorithms out there. If you start digging into this space, there are dozens, may even be 100s. I don't know. I quit looking after a few dozen. What's interesting about this, you know, in standard computer science, we always talk about time and space trade-offs. It's a two dimensional space that you can trade off with different algorithms. With A&N you now have recall as a 3rd dimension. So you can trade off time and space and quality recall. And it turns out there's lots of different algorithms that trade these off in different ways that apply to a lot of different problems. But the caching problem itself, what matters number one is the recall, because that directly affects your ability to find a high quality answer. And second is time, because after all, the whole point of caching is to save time. So the best algorithm that's out there. For that corner of the trade-off space it's called hierarchical navigable small worlds, which I hope to never say again. I'm just going to call it HNSW going forward. HNSW is a graph-based algorithm. You take your vectors in and it builds a graph of them, and the graph has linkages based on the distance, so vectors that are nearby are linked together. But the people that built HNSW took a page out of the people that did skip lists, OK, and they made the observation that says, well, if my graph gets too big, if I've got 100 billion entries in it, it's gonna take me forever to search the thing. I don't know where to start. Um, searching around in it takes a really long time. But what I can do is take a subset of that data. And make a graph out of that. And now I have a much smaller problem. And in fact I can do that again, and now I have a very small problem. So what you've built is like a pyramid here, and that's exactly the way a skip list works. So let's see how this works in in action. For this example, I'm going to look for this query vector which is the yellow dot down at the bottom. I want to find the. Vector that's closest to that yellow one, so I'm gonna start at the top and randomly pick one of the spots, one of the vectors at the top here, and now what I'm gonna do is I'm gonna look around in the graph. I'm gonna search my neighborhood and find. The vector that's the nearest to the query vector, so I look all around my neighborhood and it turns out I can spot the one in the middle there is the nearest. So I go to that one and then I drop down, use that and drop down to the next layer below it and now it's rinse and repeat. I have a starting point on this layer. I look all around there, not all around, but just in that neighborhood. I find the smallest vector there and then I use that drop down again. Rinse and repeat, and in this case I'm on the bottom layer. I find the one that's closest to that, and because I'm at the bottom, we're done. This is a login algorithm. It scales. The implementation that I'll talk about here in Elastic cache, you can get answers in hundreds of microseconds or single digit milliseconds are very typical on fairly large, I think tens of millions of entries or hundreds of millions of entries in it. Very effective in a caching environment. So HNSW gives you excellent throughput with really high recall. Getting recall in the 95%, 99% range for HNSW is very easy to accomplish. Now the algorithm itself has a bunch of parameters. I'm not gonna go through them for you. It's like a PhD thesis to understand how they all interact, um, and this is only a 400 level course. But the key takeaway, and Sanjeet talked about this before. Is that high quality for your results matters at every step. So let's take the example that Sanjeet showed us. Find Italian restaurants in the Bellagio with vegetarian at 4 at 7 p.m. He broke that down into 3 agents that then gave you the overall response. Now there are some very inexpensive searching algorithms, databases that use these algorithms that will give you 90% recall on that, and that sounds pretty good until you realize that these all multiply each other and that gives you an overall 73% accuracy and I've got to tell you, I think a system that gives me the right answer only 3 out of 4 times, uh, no good. So let's repeat that at 99%, and the answer that you get now is 97% accuracy. And that's pretty acceptable. For this kind of application. So, now, I mentioned before the embedding models are 750 times faster than your than your uh large uh LLM indication. You've got your caching model which is running in um. You know, microseconds or milliseconds. What happens when you put all that together to your wallet? And here's what happens. So first of all, the numbers that I'm gonna show you here, there's a blog that goes into these numbers in detail. I'm not gonna do that uh because we only have limited time here. But if you're interested in that, by all means go to the blog and it describes the uh the basis for all of these numbers in in detail. So here I've got cost per day on the y axis and the cash hit rate on the x axis, and that's really where the win is going to come as you'll see. So the white line here is an uncashed system. So obviously the cost doesn't change regardless of the hit rate, OK. But now I'm going to plot that system with caching, and you can see if your cash hit rate is 0, this actually does cost more. You've got all the costs for your model plus the cost of the embedding models and the cost of the caching system itself. But when the embedding model is 1/750 of the cost of the regular model, it really doesn't add much. And in fact the break even point is 2.5%. So if you can achieve even a 2.5% hit rate in your model, you will break even and of course things just get better after that. In fact, the cost reduction almost exactly follows the hit rate. So, now, how do I actually build this on elastic cash? I'm gonna walk through some examples here, but first. Elastic Cash with Valy is a key value store. How do I do vector operations on that? OK, what does that mean? How do I, how do I map that? So conceptually. The search module. In elastic cash. Looks a lot like a relational table, OK, so. In a relational table, you've got rows and columns. And what the search module does is it says every key that's in the database is a row in the table. And since it's a key value store, I'm gonna use values that have multi parts in them, um, in this example, Jason, but you can use hash. Each of those fields is an element inside of the key. So for example here. You can see that the relational model maps fairly well into the key value model. So for our example problem here, I want to build a cache that's got a vector component and then it points to the answer. This is the minimal cache that you can do now in in in this particular example, I'm not actually going to store the answer. I'm going to say, you know, storing the answer, the answer could be a few kilobytes of data storing that in my cash, gets a little more expensive. I mean, price per bit of the. Cache is a bit more than some of the external storage, so your, your answer storage, you know, could be something cheaper like maybe Dynamo DB or S3 or you could stick it in a file system, you know, on an SSD or even a hard drive if you wanted to any any place. That part doesn't really matter. All that really matters is the value for each key has a vector. And it's got a reference to where the answer is. OK, that's what matters. So I wanna define an index. Now this is one command, but I'm gonna break it into pieces just to make it bite size. I'm gonna create a can. I give the index its name. You can have multiple indexes. Uh, each one has a name. I'm gonna put it on Jason. I mentioned before you could use hash. The prefix is the subset of the key space for this index. When I create an index, it is constrained to a portion of the key space. And the reason you might do that is you could host multiple different caches inside the same instance of elastic cash. It makes your management easier, cuts the overall expenses also, so you can create an index. You know, in our example here we had 3, potentially 3 caches. You could put each of those, uh, logical caches into a single elastic cash instance using different portions of the key space. So now I'm gonna define the schema for this index. Uh, since it's Jason, I gotta have the Jason path and in my query language I gotta give that a name. It's a vector field. It's got uses the HNSW algorithm. Uh, 32 bit floating point. I've got a three dimension for my example. You're going to have something that matches your embedding model. Typical embedding model sizes are 256, 512, 1536, or if you build your own, it's whatever size you choose. Elasticash doesn't care. It supports vector sizes into the tens of thousands. And I have to establish a distance metric. So how do I load data into the index? Well, the answer is you load data into Elastic cache. So when you set the data using the regular Elasticacheval key commands set, get in this case Jason set. That data is automatically indexed. It's indexed because the key matches the prefix in the index. When that gets written, it's automatically loaded. Now that right here will complete in a few 100 microseconds or a millisecond or two. And unlike a lot of other vector stores, the query that that data is in the index immediately. So you can issue a query that will include that data right after the set has completed. A lot of other vector indexes have a waiting time that can be minutes or even hours from when you insert something before it becomes queriable, we think. In a caching environment that's not really going to be effective, especially if your application is temporal or subject to like a flash event, then you're going to want those cached responses to be available immediately to help you deal with that peak spike in the load. So you could put a couple of vectors in and since it's a cache and Sanjeet's gonna come back and talk about this in more detail, you can make the data expire on a time interval. So if for example something like a uh table reservation, you might want that to expire fairly quickly, whereas say like the best restaurant that might not expire for more than a day or two and you can pick the interval of time that matters for you. So now that my index is defined, I've put some data into it, how do I query it? Well, the query command, the syntax for that is a little baroque. I'm not going to go into it in detail. The red pieces are the parts that you care about here. I'm going to search a particular index. I'm going to search the vector component of that index. The search command can actually also do hybrid searches. We can have other field types besides vector. You can have tag fields and numeric fields in Elastic cache and do what's called a hybrid search. I didn't get into the complexity of that, but it supports it. And then you have the reference vector which is that last piece and the query language um is independent of whether it's in Hah or Jason so that comes in in binary. Generally you're using a language client like uh the glide client or uh some of the Python clients or whatever your uh application language is those clients for Vallaki take care of most of this for you. So when you get an answer from that query, The things that you care about is, you know, which key. That may not be useful. Sometimes you can encode information into the key. I know the distance. That's what's gonna drive my hit or miss logic. I'm gonna compare that to my similarity threshold. And I get back the key, excuse me, the value, which in this case has the reference to the answer for me. So once I get this result, I can compare it distance wise to my similarity threshold, decide whether or not I want to go get that answer, or unfortunately it's a miss, and now I have to go to the model and generate a full answer. So now I'm going to turn it back over to Sanjeet, who's going to go over some of the implementation and best practices. Thanks Alan. So now that we've kind of gone over how to actually build your semantic cash, let's take it back to that prior use case and think about how that practically would be built here. So if we think about this complex question, this multi-step composite question. We could have actually taken the decomposed questions around the Italian restaurant near the Bellagio, the underlying restaurants that have the veg options and the availability associated to those restaurants, and actually taken those subquestion, created the embeddings and the vectors, and actually stored those embeddings and the answer within a semantic cache like Valy. As a result, the 2nd user that came through with a very semantically similar questions could have then run a simple vector search and fetched those answers from the Valky semantic cache and then returned those results back such that you never had to invoke any of those agents for the final response to user 2. So basically you're avoiding all these expensive LLM calls, these tool invocations, and you're just following the vector search path and the embedding generation path, which, as Alan noted, is significantly cheaper. And the other big piece is that as Alan noted in that one example around maintaining a high level of recall, you're still able to maintain a high degree of accuracy in that final response to the user while still saving time and saving money. So one thing to note here is that when you think about agents, you think about they all follow a very similar framework but they're tuned to very specific goals, you know they're using different tools of prompting models and have access to different underlying data when you think about semantic. To caching, you need to do the same, especially when you think about it in terms of agents. You need to think about the right embedding to model to use the right search algorithm as we discussed, and then finally you also need to think about similarity thresholds and TTLs, so, so the expiration of the keys. So one thing to note is all data is not made the same. So when you think about a lot of these 3 different agents, you think about each of these agents we're using the underlying data and the underlying tools have a different tolerance of the volatility of how quickly that data is changing. Thus you need to really set specific thresholds on expiration of that data or those agent responses to be able to reuse that data. By following those strategies you're able to actually really see a dramatic drop in those LM calls, thus dropping your cost and your latency in the sequential multi-agent framework example that we walked through. Obviously this will depend on your use case and how you deploy it, how you configure your semantic cache and. All that good stuff and you can actually apply it to a lot of those other multi-agent frameworks we discussed earlier and you know I think the big takeaway from Alan's point is the breakeven of deploying a semantic cache is extremely low, but yet you can yield significant benefits. So there's really two knobs here to really balance kind of the, you know, the freshness, the accuracy, and the performance, and those are TTLs and similarity thresholds. So I touched on this a little bit earlier. TTLs or time to live is basically how long do you maintain the keys or the agent responses or the vectors in your cache? How long? Like in terms of minutes, seconds, days, however. And then similarity thresholds, how similar does a response or a subsequent user query have to be to something that was already stored in the semantic cache for it to actually be a cache hit? So the key thing to note is a lower TTL, or how quickly you want to kick things out, means a lower hit rate, lower cash hit rate, so you're going to have more freshness, but you're going to invoke all those agents over and over and over again. You're going to invoke all those LMs. You're going to have a lot more cost and latency, but more accuracy. So you need to really balance this and how you how you want to configure it and similarly with. thresholds when you require higher similarity, you need your subsequent user's question to be very close to the question that's already stored in the cache. Obviously you're going to have another lower cash rate, more agent calls, more LLMs, more tools, more cost latency, but better accuracy. But with something like Elastic cash for Valky and the vector search capabilities, you're still able to maintain a high level of accuracy by maintaining a recall level of 99. percent and then conversely you can see higher TTLs and lower similarities means higher cash hit rates. Therefore you're gonna save a lot of time and money and you're not going to invoke those agents. And then one thing I'll also leave you with is, as is as is with the case of any traditional caching, you know, you really need to cache based on the application use cases, so you need to understand the tools and the data that your underlying agents really have access to. So for instance, if your agent has access to real-time or compliance data, you need to be cautious on actually using reusing a lot of the underlying responses for subsequent users as it might be fast changing, it might fall out you might fall out of compliance. And then when you think about slow changing data like think about your menus, think about your restaurants, those types of things are generally very safe to cache and reuse if if all the tools and the data that the agent has access to fits under that fold. And then The final thing I'll note as a key takeaway is as you deploy your agentic AI applications, optimization from a cost and latency perspective is really a continuous level of effort as you add more tools and more agents to improve the intelligence and you add more users, you're going to increase your cost and your latency, so you really need to employ. These types of techniques whether that's semantic caching with something like Valy or whether that's employing smaller LMs, model distillation, better routing to your underlying agents or your tools, and then really consider what really best fits your use case. So optimization is really a journey you need to think about as you scale up. So with that I'll leave you all with uh a few blogs. Um, the first one is a quick semantic caching blog on how you can actually get started. There's some good code samples, some good, uh, performance specs, and then our technical documentation on the actual underlying APIs and how to actually launch this, um, for yourself. So I'll kind of pause there and then I see everyone's taking a picture. So, um. Thank you everyone, um, and thanks for your time. Me and Alan will be outside, so if you have any questions feel free to come ask us, um, and fill out your survey as we really do appreciate the feedback and thank you everyone. Please do stop by our AWS database booth if you have time and thank you.