---
video_id: o92D2x2Zfw8
video_url: https://www.youtube.com/watch?v=o92D2x2Zfw8
is_generated: False
is_translatable: True
summary: "In this insightful session titled \"Create hyper-personalized voice interactions with Amazon Nova Sonic\" (AIM374), Ver Pandey, Principal Product Manager at Amazon AGI, and Ankur Gandhi, Principal Scientist, unveil Amazon Nova 2 Sonic, a revolutionary speech-to-speech foundation model engineered to enable real-time, human-like conversational AI. Pandey opens the presentation by addressing the fundamental inefficiencies of traditional conversational AI architectures, which typically rely on a \"cascade\" of disparate models: Automatic Speech Recognition (ASR) to convert speech to text, a Large Language Model (LLM) for reasoning, and Text-to-Speech (TTS) for response generation. He explains that this fragmented approach inherently introduces high latency and strips away critical non-verbal context, such as tone, emotion, and prosody, resulting in robotic and disjointed interactions. In stark contrast, Nova 2 Sonic leverages a unified, single-model architecture that processes input and generates output directly in speech audio, preserving the rich nuances of human communication and enabling ultra-low latency responses that feel instantaneous to the user. The speakers delve deep into the model's expanded feature set, announcing native support for seven major languages—English, Spanish, French, Italian, German, Hindi, and Portuguese. A standout capability demonstrated is the \"polyglot\" voice feature, which allows the AI to seamlessly switch between languages within the same session while maintaining a consistent voice persona, a crucial requirement for global enterprises serving diverse customer bases. Pandey also introduces \"asynchronous task completion,\" a significant user experience improvement that allows the AI to maintain a fluid conversation while executing complex, long-running tools in the background—such as retrieving a mortgage refinance status—effectively eliminating the awkward \"please wait\" silences that plague current voice bots. Furthermore, the new \"turn-taking controllability\" feature empowers developers to fine-tune the model's sensitivity to pauses, optimizing interactions for specific user groups like the elderly or non-native speakers who may require more time to articulate their thoughts without being interrupted. Gandhi takes the stage to present rigorous performance benchmarks, demonstrating that Nova 2 Sonic achieves state-of-the-art accuracy in ASR, specifically excelling in challenging noisy environments and showing a 50% improvement in handling complex alphanumeric data like dates and account numbers compared to its predecessor. He emphasizes that the unified architecture allows for superior reasoning capabilities and deeper context retention across long conversations. To illustrate practical application, the session welcomes Amey Paradkar, VP of Product Management at Cisco, who showcases a live demo of a \"Technology Tires\" AI receptionist built on the Cisco platform using Nova Sonic. This impressive demonstration highlights the model's ability to handle complex appointment scheduling, understand multimodal inputs (like text and DTMF tones), and robustly manage \"barge-ins\" where the user interrupts the AI mid-sentence. The presentation concludes by outlining the robust developer ecosystem, featuring seamless integrations with Amazon Connect, LiveKit, and telephony partners like Twilio and AudioCodes, solidifying Nova 2 Sonic as the premier solution for building next-generation, hyper-personalized customer service agents."
keywords: Amazon Nova Sonic, Speech-to-Speech AI, Conversational AI, Unified Foundation Model, Polyglot Voices, Asynchronous Task Completion, Customer Service Automation, Low Latency, Turn-Taking Controllability

Before we get started, let me introduce myself. I'm Ver Pandey, principal product manager for Amazon Nova Sonic as part of Amazon General Intelligence, and I'm Ankar Gandhi. I'm a uh principal scientist also working in AGI and focusing on the Noa Sonic models. Great, thank you. So today we are here to get gathered here to talk about Amazon 2, Amazon Nova 2 Sonic, our speech to speech foundation model for real-time humanlike conversational AI. Before we dive deeper, let's quickly go through the different topics that we intend to cover. First, we'll be talking about the product itself and the core features that we have now enabled key benchmarks, speech to speech architecture, and. Major use cases that our customers are using Novasonic for. Later on we'll cover developer tools which make it easy for you to integrate Amazon Nova Sonic and then we have a special guest today um who will give us a demo and speak to the product they are building with Amazon Nova Sonic. After that we'll open the floor for question answering. With that, let's get started with Amazon Nova II Sonic. At Amazon, we are at the forefront of building voice-enabled technologies for conversational AI, be it for Alexa or other services such as Amazon Connect for customer service call automation. Yesterday we heard Matt Garman speak in his keynote about Amazon Nova 2 Sonic. We launched this model which enables real-time humanlike conversational AI. It's our second generation of model which went into general availability yesterday on 12/2. It supports speech and text input and output modalities, and it's backed by a bi-directional streaming API as part of Bedrock to enable low latency conversations. It's available in 4 regions IAD, PDX, ARN, and NRT with more regions coming soon. The most important thing here is not only does this model enable smooth conversational AI, but it is also the model that offers today's industry's most. Quality and price performance offering out there. Let's look deeper into some of the conversational AI features. Accurate speech understanding. This is the foremost and most fundamental thing for any kind of conversational AI application. We are proud to say that Amazon Nova 2 Sonic delivers best in class speech understanding with robustness to different speaking styles, accents, and even noise in the background. As I mentioned, it supports bidirectional streaming API with low user perceived latency. This is critical for us to not only process information streaming audio at speed, but also generate responses which seem natural and fluid in in conversations. The model is capable of handling fluid dialogues not only when it comes back with its responses, but even when the users interrupt at will or take pauses or hesitate, nonverbal cues, which tells the model to let the users finish their thought before starting to respond. Another key aspect of this model is this ability to detect not just what the user said, but how they expressed it, the tonality, the voice-based sentiment, and adapt its own responses based on that. This unlocks the ability to empathize with the users when they express frustration or elate when they express happiness and mirror the overall sentiment of the user. But any conversational application is incomplete for enterprise use cases unless it can be actually used to make tasks and complete tasks. That's why the model also delivers tool calling knowledge grounding with responsible AI and privacy compliance. What this means is you can generate responses backed by a knowledge repository to be factually correct or invoke other tools to accomplish tasks such as making a reservation, upgrading your membership, and so on. Now, most of what I spoke up until now was also part of Novasonic One. So what's new? Let's take a look at what are the new conversational AI capabilities that we have unlocked this uh with this launch of Nova 2 Sonic. First, additional languages. We initially started off with English back in April, added a few more languages in the July August timeline, but now I'm proud to say we have 7 languages, each with its own masculine and feminine voice supported by Nova 2 Sonic. We will dive more details with each of those. Next, we have language switching, the ability to not just speak in one language, but switch languages at will within the within the same session, and the AI can come back and respond back in the user's preferred language. Another cool feature is the asynchronous task completion as we just talked about the tool calling, uh, APIs can take anywhere from 8 to 10 seconds or even longer. Instead of blocking on those tool responses, the Nova 2 Sonic model enables conversations fluidly where the user can switch topics and continue to talk or even talk uh or invoke other tools while the previous tool call is working in the background. Next, we have enabled cross model input output. This was a key ask from customers who started to adopt Nova Sonic earlier, where they would like the same model to be able to handle text and speech input and keep the conversational context together. This unlocks new features which we'll dive deeper. And then of course turn taking controllability. This refers to the ability for the developer being able to configure the duration of the pause that the user can take before the AI determines it's an end of a turn. Let's dive deeper into each one of these. As I mentioned, we have now launched 7 languages, English. With all different variants US, GB, India, and Australian, Indian and Australian English are the new additions with Nova 2 Sonic, Spanish, French, Italian, German, Hindi, and Portuguese. Now, as I mentioned, each of these languages not only depict the ability to understand these different languages and sub dialects, but also the ability to respond in expressive native sounding voices. Let's take a moment to listen to one of these examples. Let me play what we have here for INU US masculine voice in Matthew. Welcome to any bank. I'm your AI assistant and I'm excited to help you with anything you need. Wanna check your accounts, learn about loan options, or explore some investment ideas? Just let me know. I'm here to make your banking experience easier and a whole lot more enjoyable. OK, let's try some new voices here. Welcome to any bank. I'm your AI assistant, and I'm excited to help you with anything you need. Wanna check your accounts, learn about loan options, or explore some investment ideas? Just let me know. I'm here to make your banking experience easier and a whole lot more enjoyable. That was Australian English. Any bank may apka swagahe Mepka AI assistant who or Apki Madatkarneme muje Kushihogi Chahe Aapne kate dekna chahekisi loan kebareme janna chahe. That was male for Hindi and then lastly let me try one for Porvidu aweni Bavik super feliz in juano jia jia finan roost sabema sobrewing presoiscorisamento eso shamaquero de chassis. OK. Welcome to any bank. I'm your AI assistant and I'm excited to help you with anything you need. Wanna check your accounts, learn about loan options, or explore some investment ideas? Just let me know. I'm here to make your banking. Welcome to any bank. I'm your AI assistant and I'm excited to have you. OK, lesson learned. Let's take. All right. Thank you. That was quite a lot of expressive voices. Coming to language switching, so it's not only important to be able to speak expressively and with native sounding voices in a given language, but the ability to switch within the same session and speak different languages with the same voice. And that's what we have unlocked with the polyglot voices and language switching feature. Let's take a moment to see how the user was able to interact in two different languages and the voice stayed the same while responding expressively in either. Hey, how are you? Hey there, I'm doing great. Thanks for asking. How about you? What can I help you with today? I wanna buy shoes. That's an excellent idea. Good shoes make everything better. You'll want to think about the style you need and make sure they fit comfortably. Happy shopping. Ichiltunschhafen. This is Anatole idegu de Tsur mach ales Besser. Dengdaranden Richtigen steel so wen on Zigerusstein dasibekwemsitzen. Zin Tunshu Toya. As Heer and Bissen for Model on der marker up Aba skip rooted tonsur in Vscheen and Preislassen to finish the stint at Vassar Sudan buja Past. Thank you, that's great. Do you think I can find blue shoes as well? You're welcome. Absolutely, you can find blue shoes easily. Many stores offer a wide range of colors these days. Happy shopping. OK. So as you saw, the user was able to switch uh two different languages within the same session, and the voice was able to adapt to either and respond back. I mentioned earlier a new feature called asynchronous task completion. Uh, so as you will see in this demo. The user invokes a request which triggers a tool call in the background and while that tool call is happening, the user is able to switch the context and the topic and have a conversation fluidly while the AI comes back and responds back on the previous tool call. Let's take a look. While you are looking into that, can you also let me know the closest any bank Home lending branch? And maybe also let me know like what will happen with my escrow account. All right, Osmond, let me give you the info you asked for. First, the branch. Your closest any bank home lending branch is at 240 Greenwich Street, New York, New York, 1007. They're open Monday through Friday from 9:00 a.m. to 6:00 p.m. Their main phone number is 1212-232-1420. Now, regarding your escrow account after the transfer, I'll let you know that after I share the refinance status details, which I'm pulling up right now. OK, Osmond, here's the update on your refinance and your escrow account. Your refinance application for your Palm Beach property is moving along. It's a 15-year fixed at 6.125%, and the good news is it's still pending, but we do need some documents. So as you see, the user was asking about refinancing while the tool was looking up details on that, he was able to switch topics and ask about escrow and other things, and the model did not lose the context when the tool call came back, it was able to weave the response back and generate an effective response. While you are looking into that, can you also let me know the closest OK. Next feature that we wanna highlight here is the cross model speech and text input output. As you can imagine, a lot of enterprises would like to start off with a text bot and then add the voice interface to their text bot. When users would use this kind of an application from the mobile phone or the browser, they want to have the convenience of typing in their text input and with the press of a microphone button switch to voice. And that's the convenience the cross model speech and text input enables. Uh, the demo here shows how the user was able to enter the date in text format. The model picks up that date and comes back with a speech response, after which the user continues talking in in through their voice. Uh, one key aspect of this feature is it's not only unlocking the cross-modal text input output for speech and text, but it also enables new features. Customers would like to use Sonic to make outbound calls, and when they do so they encounter IVR applications where they have to navigate DTMF tones or some customers when are making some inputs, maybe they want to punch in their credit card information with their phone, which also means the model has to interpret the DTMF tones. Now with this technology, we are able to convert those into a metadata, a text metadata that the model can interpret and therefore give the convenience to either use voice or keyboard input to enter their information. Another feature that uh another application of this feature that customers have used for uh is the initial greeting. You could now give a text prompt and generate a welcome greeting at the very beginning of the session to welcome or start the conversation. So Osman, can you confirm your date of birth for me? OK, Osmond, I've got January 1st, 1990. Is that correct? Uh, no, no, it's not. Got it. Thanks for clarifying. So, to confirm, what's your correct date of birth? OK, so I've got January 1st, 1980 as your date of birth. Is that right? Yes. OK. Lastly, we talk about turn taking controllability. We have experienced that there are customers who would like very quick turn taking, for example, a gaming application, or there are applications where they are. They're non-native speakers who are learning a new language or certain cohorts of customers, the elderly, who may take a little longer to complete their thought or pause in between, and what our customers asked for was the ability to be able to configure the duration of the pause that the user can take before the AI determines the end of the turn. So with that in mind, we've launched the turn taking controllability feature which gives you three sensitivity settings. The highest sensitivity, which is the default setting, allows you to take a short pause of 1.5 seconds, and the highest can go up to 2 seconds, which allows the, the other end of it can go up to 2 seconds, which allows users to take a longer pause and be able to complete their thought. Let's take a quick look at how this plays out without actually cut cutting off the user's speech input. I'm planning to visit San Diego next month for a vacation with the whole family. We have 2 kids with us, so what places would you recommend us to visit in San Diego? San Diego in December is wonderful for family trips with mild weather and many fun activities. Let's compare the other one. I'm planning to visit San Diego next month for a vacation with the whole family. We have 2 kids with us, so what places would you recommend us to visit in San Diego? Oh, by the way, what will the weather be like there in December? For your family trip to San Diego next month, here are some top recommendations. Must visit spots include the San Diego Zoo, Legoland, California, and SeaWorld San Diego. So basically it's showing with more robustness to a longer pause. The response was more relevant to the user's query. They were really inquiring about the weather conditions and ideas to places to visit next month in December, but in the previous example they were cut off early, so the response they got was not so relevant. So this gives you more flexibility to how you can tailor our turn taking behavior of the AI to suit the different customer needs. I'm planning to visit San Diego. I'm planning to visit San Diego next month. OK. With that, let me call upon Ankur, who's gonna walk us through some of the key benchmarks. Thank you, Ver. And uh I hope you're all excited about the new features that we're launching with Sonic 2. But it's not just the new features, we're also improving the core capabilities of the model. One feedback that we got from our customers was that the ASR or speech understanding of sonic was really great, but there was still room for improvement. There was room to improve in terms of noisy conditions and especially on things like dates and numbers or alpha numerics, which our customers use often to get date of birth or account numbers and things like that. And with Sonic 2, we have improved on both. Compared to all the other models that we have evaluated. Sonic 2 is state of the art in terms of ASR recognition. And on alphanumerics, we've improved over Sonic 1 by about 50%. It's not just the ASR. We've also improved reasoning, and compared to all the other speech to speech models that we compared against, Sonic has one of the best speech to reasoning. So voice agents are not just for chit chatting or asking questions. They also need to take some action based upon whatever the user is going to ask the system to do, which is why tool calling is a critical functionality that we measure sonic against. Including both simple tasks like having a few tools to call, versus a complex task where you might want to call multiple tools to accomplish a task, and the async tool calling functionality that Ver was just mentioning. And based on the key use cases that our customers had with Sonic 1, we improved our model with more better training and better evaluations and multiple benchmarks, we showed that Sonic 2 has significantly improved over Sonic 1. Apart from this, For a great experience with Sonic, it's important to also evaluate the overall conversation quality. And for every single language that we evaluate on, we made sure that the quality of conversation was better than everybody else. And for that, we took human evaluators to compare our voice and our conversations against similar models, and Sonic won every single time. So Sonic is not just highly capable in doing speech understanding and doing uh tasks and functionality, it also sounds more expressive and natural, providing overall a great experience for anybody who is using Sonic or listening to Sonic. Now, I want to take a step back to ask a question which many people might be asking. Why did we build Sonic? Because if you guys have interacted with Alexa or with Siri, or even the more recent customer service customer service calls, you have probably already spoken to a voice agent that already exists in the world. So why did you build Sonic? So typical Uh, voice assistants are built in a cascade system. They have a speech to text model that converts speech to text. That text is then fed into a text reasoning model that will do all the reasoning, agentic workflows, and tool calling, and it generates a text response which is then sent to a text to speech model. In fact, I spent most of my career working on Alexa trying to improve this entire pipeline. And these systems work great. In the past decade, they have made major improvements in terms of architecture and training to improve the reasoning abilities, to improve the accuracy of these models into the high 90s. So why build Sonic? We realized that these traditional voice AI systems have a constraint, that they are fragmented. Each model is designed to do a different task. And what that fragmentation does is that because each component is working on its own, it creates a bottleneck. The systems cannot talk to each other without this text. And without this common brain, you are unable to carry over context beyond what's feasible by the limitations of the system. But it's not that we didn't try. We have tried this. We have tried to build Orchestration around this for something like error recovery. We've tried to build error recovery systems that will detect when the ASR was wrong and then ask for clarification questions. We've tried to build personalization that will feed into each and every single component of this pipeline, so that we get better ASR, better pronunciations, and better understanding out of it. And we have also tried to add more contextualization by carrying context through these different models. But again, the problem is that this requires a lot of orchestration, designed by engineering and by scientists, and a lot of handholding and building feature by feature. And it's not scalable, and we also realized that even though we're building all these things, there are a lot of edge cases which could not be solved just due to the limitations of this architecture. So when Foundation models came out, we saw something that could change this. We could get rid of the constraints that were in this cascade system and truly build a conversational model that was like a human assistant. And that's what Noah Sonic is. It's a foundational speech to speech model that understands and generates speech at the same time, preserving tone, prosody, and conversational flow in a single dialogue, resulting in a more natural interaction. And we did this by training over millions of hours of data, learning from human-human interactions and human-agent interactions. So that the model learns on its own how to act in different conditions, different scenarios, and different acoustic conditions. And because our training data is tailored towards specific use cases, the model just learns naturally how to do things like context carryover, how to do things like error handling, and all these improvements would not have been possible without this architectural change. And the numbers that are showing earlier on ASI accuracy, on tool calling, and on reasoning, these have been a result of this unified architecture. Now customers don't need to build this complex orchestration trying to do everything. So with this unified rhythm as its backbone, Noah Sonic is able to naturally carry context of the entire conversation, and things like personalization through rag or through prompting or even through some document upload can happen easily, and it's available to all the components of the model, not just a single one. The other one of the other feedback that we always get with voice assistants is that high latency is very annoying to somebody who's used to talking to human assistants. With this unified model, it minimizes the time that it takes to understand what the user said, reason about it, and then respond. And this significantly reduces the user's perceived latency. The Foundation model, like I was mentioning before, has been trained on millions of hours of data, so it has learned naturally how to understand our responses to the nuances of speech, something that would not be possible to encode in just text or some other binary functions. So when you talk, it's not just about how you're, what you're talking, but also how you're talking and the tone and the intonation of your voice. With Noa Sonic, it learns to adapt its responses based on the prosody and the emotion of the user. We also build this moral responsibility so that the entire conversation in both voice and in text, stays consistent. And with every generation of the speech to speech model, all the abilities I was talking about will continue to improve, which is why now is the time to build with these models and give your customers a much better experience. But you know what, I think I've spoken enough about the benefits. I'll head back to V, so he can talk about real customer use cases and how customers are actually using Sonic. Thank you. Thank you. Thank you, Anchor. So as you can imagine, There are a lot of different use cases that have emerged, but the three most primary use cases that have emerged are the self-service voice first customer service automation on interactions. Then we have voice-enabled personal assistants and education and language learning apps which lend themselves very well for the application of this kind of speech to speech technology. We have key customers who are building applications and productionizing workloads with this. Those include Crescendo, iFrame, Rujum, Cisco, Ring, and Amazon Connect. Here's a testimonial, uh, from the CTO of Rujum highlighting the accuracy of understanding speech, uh, the ability to handle dialogues fluently with low latency, and, uh, overall deliver a better humanlike customer experience. One thing that we learned very quickly after the launch of Sonic 1.0 was it's not just enough to deploy a model in Bedrock and provide bidirectional streaming APIs. There's more nuance to building these kind of real-time applications, and that required developers in particular to handle all the input output signals, manage. The session life cycle and so on, this turned out to be a last mile problem where really deploying this and scaling to their applications took much effort on their behalf. Therefore, we have partnered with a couple of different teams here which I want to walk through who've made this integration seamless. Let's start with Life Kit and PipeCat. As I mentioned, these are frameworks available for conversational AI which handle all of the input output signaling. They abstract these so that it's easier for developers to directly integrate their mobile apps and browser applications with Sonic on Bedrock while also managing the session life cycle. Typically the Bedrock supports 8 minutes of a session life cycle, session lifetime, but there the developers would like to extend this using these frameworks and carry forward the conversational context to extend the overall cycle. Next we have Amazon Connect. More recently, as of yesterday, in fact, Amazon Connect has now gone into GA with Nova Sonic to enable customer service call automation. This not only brings the expressivity, the fluid dialogue handling that we just talked about of Sonic, but it also unlocks the agentic capabilities of Connect AI agents and different AI tools that they bring to for customer service call automation. This integration is now available in general avail for general availability in IAD and PDX regions supporting at least 3 languages English, Spanish, and Portuguese with more coming in Q1. And lastly, not all enterprises have a customer service contact support center. Maybe there are small business owners who would just like to enable an outbound call mechanism to maybe reach out to their customers or while their shop is closed, they may want to receive phone calls on their behalf and take the orders, which is where telephony integration is key. Therefore, we have partnered with the likes of Audiocode, Oneage, and Twillo which make it seamless and easy for customers to not only integrate through the telephony channel but also extend through other ways such as SIP telephony, which allows you to connect internet telephones through your browser or mobile app to talk to Sonic and get the same sort of conversational expressivity and dialogue that you get directly integrating to Bedrock. With that, it's time to see a partner demo. And it, it's my pleasure to welcome Mr. Amma Pandekar, who is the VP of product management at Cisco Calling, who would like to speak about what he's doing with Nova Sonic. Hey, thanks for, uh, thank you for having me. You folks having a good show so far? Yeah, it's, it's amazing to be here and uh we have a great partnership with AWS over the years. Uh, uh, I'm, I'm a Peranakker. I work for Cisco, uh, a lead product for our voice platform that powers our calling, our contact center, and so on. Uh, just by a quick show of hands, how many of you, like, you know, deal with voice workloads or manage them, right? Uh, quite a sizable. But how many of you work with like you know, customer service enablement type roles, right? Uh, so what's happening, I mean, we're here to talk about like how AI is changing the overall like voice enabled workload ecosystem, right? So on our side, like, you know, Cisco is a big networking powerhouse, uh, but we have networking, security devices and collaboration, right, all together. And uh by that I mean like we serve the largest financial service houses, for example, all of the Wall Street banks we look at like the largest development bank in the world is our customer we have large retailers including Amazon in our platform, all of the pharmacies, most of the governments across the world, including the US government, uh, and healthcare, uh, which is like huge for us too. Uh, and we have over 50 million users on our platform, um, and then we enable about a like 1.5 billion minutes, uh, in the AI assistant that we have built, uh, on the Amazon platform, right? On the other side, you also have Apex Contact Center, which essentially is a contact center like Amazon Connect that powers uh customer care scenarios for the likes of United Airlines, uh, Maersk, and large customers, right? And is the, the key is that, you know, this is available globally in all markets on the AWS platform like we have 1 195 markets today across the world, and then, and the bar is like 59 SLA, right? Uh, so, so it has to be super, super critical for large customers, particularly when it comes to voice. So, but today I'm really excited here to, uh, show you what we have built, uh, building with, uh, partnership with the Nomasonic team uh for an AI receptionist use case, right? So previously, like all of these workloads were like. Hard to deploy, manage, and like, you know, for example, uh, if you have a, your bank and like, you know, after 5 o'clock there's nobody there to answer the phone, uh, so what are you going to do, right? So you don't have staff in those locations. So if you want to call a pharmacy and then I want to update my prescription or do a refill after 5:00 p.m. there's nobody. So that's one case where you're seeing a lot where customers want to do less with more and so they're using tools like AIS receptionist for building that. Uh, the other scenario that we're seeing is that cyclic, for example, you have large demand during a low, there is a short period of time, like tax season is a typical scenario, right? So you want people to be able to answer questions, uh, answer the customer calls, book appointments, uh, using the tool handling that we talked about, uh, even transfer calls, uh, to people who, uh, might be able to assist with all the information. How many of you have started on a call and had to repeat your information like 3 times over when you call customer service, right? So like, for me, it starts with my last name, and it's complex and so I have to go with P for Pennsylvania and A for blood, and by the end of it, I'm so tired, right? So, but like things like the multimodal input kind of gives you the ability to, you know, text it, right, or put it on a website, so you don't have to specify or call things out. And lastly but not the least, what this, what these tools are doing is. Helping customers prioritize their employee time for the high-value customer interactions, uh, so, so that you can provide really good customer service. Uh, so let me show you, um, A demo of how this works, uh, but before that, very quickly, so if you like, you know, if you call any business, what we do is that, like, you know, we have a receptionist that answers the phone and it is powered using the languages and the voices that the platform has. And then we train it with, like, you know, a knowledge base. It, it could, it could be, for example, if you're a pharmacy, you can take all of your, your pharmacy information and input that and create a knowledge base. So it can answer questions about, like, you know, medicines, products, hours, like name it, right? And then thirdly, it can take actions. Like for example, you can book an appointment or you can you can schedule an appointment, you can look up your data from a CRM system and, and like, you know, know your account information, right? And then when you need help, you can say, hey, I need to book an appointment with the doctor, for example, right? And then If that's a complex task, then it transfers to a live person, so it's human assisted, right? And that person already has the knowledge that got told to the AI receptionist, so then you don't have to repeat yourself. So I'm going to show this to you in action, so to kind of make it real. So here, I mean this is a demo we built, uh, and we're actually building it for a customer, uh, I cannot name them, but they're a large tire chain in the US and uh they do a lot of tile stores across the West Coast and the Midwest too. Uh, so they, I mean, they have like thousands of tire chain stores and, but they don't have staff to manage all of those like answer those calls, right? So what they want to do is like, hey, somebody has a problem, call in. Try to automate 50% of the, the questions, like, you know, and answer them and even do the appointment handling on the in an automated way so that the technicians can focus on actually fixing like tires and car problems, right? So that is their goal and so we built this for them in mind and then let me just explain that uh here. Hello. Welcome to Technology Tires Emergency Service. I can help you in Spanish, English, French, or Hindi for your tire emergency. Speak in your language of preference. How can I assist you today? Hey, I have a tire blew out and I need a complete replacement. I'm sorry to hear about your tire blowout. I can help you get that sorted right away. To get started, could you please share your phone number with me? This will let me look up your vehicle details and account information. Great, thank you for sharing that. I found your account, Ame. You have a 2024 BMW X4. Since you mentioned a tire blowout and need a complete replacement, let me present the tire options available for your vehicle. Just a moment while I pull up the options for you. Perfect. I have the tire options for your 2024 BMW X4. Here are your three options for a complete tire replacement. Option one, Michelin Pilot Sport AS4, premium performance tire with excellent grip and handling. Option 2, Pirelli Centerado. I'll take the Michelin pilot, please. Excellent choice. The Michelin Pilot Sport AS4 is a great option. To schedule your tire replacement, I'll need two more details. One, your current location where you'd like the service performed. Two, your preferred date and time for the appointment. Could you share those details with me? Um, I'm at 67 Broadway, Las Vegas, Nevada, and I'd like the technician to be here somewhere between 11:00 a.m. and 12 noon. Just a moment. I'm getting that scheduled for you now. And uh while you're doing that, do you have any towing services in case you're not able to fix it here? Yes, we do offer towing services if tire repair is not successful, or your vehicle needs to be moved immediately to avoid a potential hazard. Your appointment is now confirmed. Your appointment number is tire 260715. A technician will arrive at 67 Broadway, Las Vegas on November 26th between 10:00 a.m. and 10 p.m. to replace the tire on your 2024 BMW X4 with the Michelin Pilot Sport AS4. You'll receive a confirmation call 30 minutes before arrival. Please make sure your vehicle is accessible at that location. Is there anything else I can help you with today? No, that's all. You've been great. Thank you. You're very welcome, Ame. I'm glad I could help. Your technician will arrive shortly and we'll get that tire replaced quickly. If you need any further assistance, don't hesitate to call us. Have a safe day. What do you think of that? You know, the best thing I like about that demo is that it got my name pronunciation right. Because my name is complex and I mean all that's simple, but most of the time it gets pronounced as Amy. So, so you can even train, like, you know, these models to, for the correct pronunciations and the phonetics to kind of power a great customer. Experience, uh, a few things I'll call out that, you know, we talked about that we used here. Uh, one was that we use a multi-modal dialogue so they can enter a phone number, like you can specify an address, uh, I mean, credit card number, all of that. That's very easy to provide, right? The second thing is that, you know, like bargin, which is like I don't want to wait for all the information before I can talk back and then the model will recognize that and respond appropriately. Uh, that's the second one, right? And, uh, so this is actually real, uh, and then we are planning to, uh, roll this out, like, you know, sometime early next calendar year to, to our customers. Um, so you see the power of this tool set and what it can do, uh, for customers, uh, and then essentially revolutionize the voice interactions, uh, that you may have with your employees or your customers on a data uh data database. I wanna thank, uh, Ver and uh Aur team for Uh, being, letting me be part of this, and then thank you for being at Reinvent. Um, uh, I think it's on the next questions next, right? Thank you, man. Thank you. All right. Uh, that brings to the end of our session, but I want to point out a few links and open up the floor for questions. Here's a link to the user guide, our technical report, which has all the benchmarks that we talked about. The user guide has code samples and other details. On how you can prompt the model to make it work for different use cases and I also want to point out two other sessions that are related to Sonic that the team is doing to really give you a hands-on experience in building applications. Thank you so much, everyone. Thanks for coming in.