---
video_id: vMM138DHa7s
video_url: https://www.youtube.com/watch?v=vMM138DHa7s
is_generated: False
is_translatable: True
---

OK, cool. Um, let's get started. So H2AI, um, HOAI basically is a global leader in the enterprise AI realm. H2AI does both generative and and predictive AI. And the Have an entire platform which we are able to install in on-prem environments, in clouds, and in air gapped environments. H2AI is a leader in um the Gaia benchmark, and we deliver an entire platform. Um, So our technology stack, basically our entire platform runs entirely on Kubernetes. Everything runs in EKS in the cloud, and we rely very heavily on EBS storage. The reason we rely on EBS storage is when we train data and we we train our models, we need very fast and um good storage for our AI engines. The problem that we had with H2AI with EBS storage is that we had a lot of unutilized storage. Basically we were storing over 2 petabytes of storage and it was growing very quickly and the issue is that we couldn't scale very good. We couldn't scale down, we couldn't be more efficient with our cloud storage, and we ended up wasting a lot of storage in the cloud. We looked at this issue and we tried to find several solutions. Most of the solutions that we found out there were solutions that required us to migrate data from old EBS storages to new EBS volumes, so basically it was a very hard and painful um process and um that was the issue that we had previously before we started working with Dattafi. Thanks Ophira. So when we met H2O initially, we were very pleasantly surprised because the problems that Ophira has described are exactly what we set out to solve with Datafi large EBS capacity, overprovisioned and underutilized. Datafi is an autonomous storage solution which manages cloud storage automatically, autonomously for AWS customers. You can deploy it on the fly and it will adjust your volume capacity in EBS automatically, auto scaling it based on your needs. If you fill it up, it will grow automatically, and if you delete files, it will shrink it automatically. It has no impact to performance. You can deploy it in real time. With Datai you get dynamic auto scaling, so the solution is completely autonomous and it overcomes those EBS limitations that Air mentioned that prevented him from being efficient in his consumption of EBS. It allows to basically endlessly scale your EBS capacity both up and down as customers fill up data and delete and enter these cycles of growth and deletion. Whenever you use data file, there's no downtime at all. We've always thought that customers will not tolerate introducing downtime to their applications when they want to improve their storage utilization. You can install it or uninstall it without any impact to applications, to your file system, or to any other thing running in your stack. Furthermore, there's no changes needed to your stack. It integrates seamlessly with Kubertis, with cloud formation, with Terraform, and it supports any Linux operating system and any tech stack you're running on top of it. So you don't need to inform your customers, oh, we're going to take down time because we want to do something with the storage. The whole thing is done automatically and seamlessly without any intervention required by you or your application owners. So how does Data fight work? It is based on a low level agent that you install on your EC2 servers or you recupernate these clusters. This agent changes the destination volumes of EBS automatically and dynamically without impacting your applications. In addition to that, we have a SAS control plane or. A back end that runs in RVPC, it monitors the agents and it gives them commands like growing volumes or shrinking volumes. It also provides analytics and shows you what exactly is going on with your EBS deployments, how much storage you're consuming, how efficient are you, how much efficiency Datafi brought to the table. Finally, we've spent a lot of effort into integrating with infrastructurist code environments, both Kubernetes and other infrastructurist code environments. This allows you to use the product without making any modifications and it completely uh uh integrates into the life cycle of CICD. So along the way when we started working with Dattaify we had a couple of issues and we had a couple of challenges that we needed to solve in order to streamline the entire process and make sure everything works with our existing platform. The first thing that we had a challenge with was bottle Rocket. So together with Dattaify we worked together and we made sure that the Dataify agent basically runs on our existing bottle rocket infrastructure that we have in EKS. The second thing that we had, the second challenge that we had was security. Basically as an AI platform we host our customers' data and we wanted to make sure that working with Dataify everything is just as secure as it was previously. And the main part here was with Dataify that no data leaves the actual cluster, no data leaves the EKS, and the task thativan talked about is only management, so we keep. The same level of security and the same level of reliability for our customers' data. The last challenge that we had with um with working with Datafi is basically keeping our existing backup implementation so we have a solution that we work with with Vallevo to make backups send to backup PVs and together with Datafi we made sure that we're able to to continue using the existing solution with Vallejo. And seamlessly continue to back up and restore dated volumes without any, any issues. So we thought it would be nice to show you an example of the kind of optimization that Data I brought to H2O. On the left side chart, you see the capacity utilization. Customers typically have low capacity utilization, especially on EBS. They tend to overprovision because they're afraid they're going to run out of space, and they're afraid they're not going to be able to grow in time. So on the left side. The beginning of the chart, you see that the capacity utilization is just 25%, which means that they're overpaying 4x compared to if they could have paid based on utilization rather than capacity reservation as EBS charges them today. As they deploy data on more and more environments, the chart for utilization keeps growing and improving to the point where it stabilizes around 80%, which is our natural place where we want to hold the buffer. We don't want to make it hit 100% because that means we're running out of space. So it's quite typical to stabilize around 80%, which is what we consider success criteria. On the right chart you can see what's happening in terms of the capacity itself. When we started out, their total capacity footprint was about 2 petabytes, but in terms of how much data they've actually written to it, the green line shows you only 0.5 petabyte written. That's why we say the capacity utilization is 25%. As time progressed with the utilization of Dafi, what you're seeing is really interesting. Even though the green line grows a bit because they wrote more data, the blue line, which is how much capacity they're paying for an EBS, keeps dropping because of the data file or scale capacity. A long time, the blue line almost reaches the green line, which is the 80% utilization that we were targeting. The savings for H2O are not paying for 2 petabytes, but actually paying for less than 1 petabyte, and that's significant savings, and this is applicable for just about any EBS customer out there. So basically the result of our work with Dattaify was that we were able to deploy the Dataify solution across all our customers. The Dataify solution is deployed with our existing tools and solutions, meaning that we still use Terrafone to deploy, um, DataF. It's, it's integrated into our GitOps process, and we didn't have to make any substantial changes to our infrastructure to get Datafi to work. Of course we have a very big um cost saving which Devan just showed um we were able to. Take up our utilization and reduce our cost. While we're we're doing that, we were also able to give better performance to our customers. So basically the, the data 5 solution was able to reduce our cost for EBS and also increase our performance for our EBS volumes. And I think that the most important part of this solution, the data file solution, is that there was zero downtime. So once the data file agent was deployed and was in all the clusters in a read-only mode, we just had to flip a switch and basically from that point on it started doing its magic. Basically reducing our storage without any manual intervention that we had to do. So just, again, on the flip of a switch, we started seeing reduced um storage costs. Thank you, thank you very much.