---
video_id: LeFO82ra_PE
video_url: https://www.youtube.com/watch?v=LeFO82ra_PE
is_generated: False
is_translatable: True
---

Hi, everyone. A very good afternoon to all of you. Hope you all have been having a good day today. Uh, a lot of sessions and, uh, today, uh, we are here to discuss about how at CCC Intelligence Solutions, uh, we were able to achieve and improve our operational efficiency with database modernization from Oracle to AWS Aurora Postgrace. We have a very nice agenda over here for today. Uh, the first one, I will, uh, try to explain, uh, why data is, uh, critical for CCC. Uh, I'll give you a brief background on what is CCC, and then I will explain or take you all through our cloud journey. Uh, but to understand all of this, we need to understand, uh, why we did it, how we did it, and what did we get, uh, after achieving, uh, or completing this journey. So, to, to start with, uh, CCC is a leader, uh, as a SAS provider, uh, to provide solutions for uh auto claims uh and collision repair industry. Uh, we help all the auto insurers, uh, we help repair, uh, shops, automakers, uh, parts suppliers, and other, um, vendors to automate and streamline their claim processing workflow. Uh, we, uh, build solutions for the repair shops, uh, for, uh, to streamline their repair process, and we provide, uh, as part of our SAS platform, uh, AI enabled advanced solutions, uh, so that they can maintain and manage their claim cycle for their customers. When we talk about our customers, maybe the customers are auto insurers or repair shops, but at the end of the day their customers are our customers. So what we try to do or how we help our customers is to turn all these data points into actions and all their key moments into intelligent experiences. It's never a great experience to be in an auto accident and go through that journey. So we, we try our best to provide simplified solutions to make that journey through that claim cycle better and smoother. Uh, what, uh, we, we kind of what set CCC apart is we, we are an industry leader in collision estimating solution provider and also what we do is as part of helping our customers customers, we also help our customers to fuel their growth. Now, if I talk about data, uh, what we do or what we process into our SAS platform is we process around 18 million claims, um. Over a year, which is close to 50,000 to 55,000 per day. Uh, I may be talking about claims, but think, but think about this from a human, um, point of view, like 50,000 accidents or 50,000 claims every day, that means we are touching at least 50,000 human lives, uh, at any point of time in a particular day. So who, who do we work with? Uh, we have, uh, top 300 auto insurers. Uh, we, our solutions are implemented by 35,000 repair shops across, uh, United States. Uh, we have our integration with 5000 suppliers, uh, through which we do parts procurement, and then we integrate with top 12 of top 15 automakers to do our integration with OEM providers. Now all through this, CCC's primary focus is when An auto collision estimating processing, the claim is going through all its processes. It generates a huge amount of data. Now how we differentiate from others is we take these data points, we create intelligent visions out of it, we create prediction out of it, and then all this data. Turns into a predictive solution, and we have AI enabled analytics on top of it to help our customers. As I mentioned, we have around 5000 parts suppliers who do direct integration with them. We have 12 OEM providers who we are directly integrated with. Now this is all I said about what was CCC Intelligence Solution and its SAS platform, but we started this journey back in 2022 to support our growth, to scale ourselves, to provide our customers the platform for their growth also. Now, what, what we did, how did we achieve our cloud modernization journey? Initially, what we did is we broke that apart into 3 cycles. The first cycle we did was we focused on the infrastructure. The second, we looked into the application tier, and then the third and the last one was the database. Uh, you can ask why database was the last one, because our. Core business sits inside that data. As I mentioned, we rely on data very heavily, so we kept that towards the end. First we looked into the infrastructure, we modernized the infrastructure layer from migrating all our workload from on-prem to AWS cloud so that we can have more freedom and we can have a better and faster scalable options. Then what we did is we went through a complete application modernization. Earlier we were pretty uh kind of uh tied up with uh vendor solutions. So what we did is we went for a more open source approach where we migrated our Oracle Weblogic workloads into microservices and a JBs. Then we migrated our service bus components from Oracle to MuleSoft, and then we leveraged more of Kafka architecture to get into even driven architecture design. Then came last as the database part, which was the heavy lift for us. As I already mentioned that we process close to 50,000 claims per day, which turns into around 18 million. And then on top of that, if you look at every transaction or every action which happens on a particular claim, it turns out that our databases process almost 50,000 transactions per second. So you can imagine how busy that database is. There are tons of applications, small applications, talking to each other, tightly integrated. So that it can manage each touching point smoothly without any failure. So there are like hundreds of applications we need to think that how we will simplify them. So we took this approach. Infrastructure was done. Then as a second step, we finished the application, um, uh, modernization, and then we started the journey of database modernization. Earlier, earlier, like when we were on on-prem and even after we moved to AWS Cloud initially with our infrastructure. Our applications were running on Oracle, a database. So what we said is that We looked into our database modernization as a project or as a full uh challenge and then said we said that let's go through what are the challenges we have to accomplish this journey. So the first one was that we had to look into all our databases, and I think it will, it will be applicable for all of you if you're going through that journey. You need to identify all the integration points into that database. You need to identify all the sources which are going to put data into it. You need to identify all the downstream applications which are going to either read or come back and update those data data points. We, we went through that analysis and we identified that what are all the dependencies between each of them so that one, I can do a divide and rule approach. I say that all the components or applications which are independent, I will first put them into a phase one deliverable and then I will look into all the dependency heavy applications and see how I can segregate them further. When I'm talking about hundreds of applications, I'm talking about. Close to more than 50,000 SQLs which were kind of implemented through those applications which were running when all these applications generate workload into the databases. So what we did is we went through analyzing all those Oracle Native SQLs and we. Try to analyze them, how many of them are easily transportable or can be migrated into a post SQL structure. And then also we looked into a lot of sequels which we could kind of tune or modify so that it can fit easily into the postgraous architecture. The next challenge was the data migration, where you can work through your application, you can work through your SQLs, but our data migration was the next step. And on an average, the amount of data we have in all our databases was more than 150 terabytes, and one of them was even close to 100 terabytes. So what we had to do is we need to, needed to find a solution where I can take in all these 100 terabytes and fit it into an Oracle Pores engine architecture, which was a tough challenge for all of us. But while we were going through all this modernization, we are trying to find solutions for all these challenges. There was one constant factor in all of those. There was no compromise on our SLOs or SLAs because business has to run as is. So that was our primary goal, number one, that any simplification, any modernization, any improvement, any tuning we are trying to come up with has to meet those SLA or even better. So what we did now with all this journey, there always has to be a risk, correct, as I mentioned, there are tons of data, there are tons of different data types, there are different applications accessing those data more or less at the same point in time concurrently. So what we had to do. Now I cannot come and go to business and say that, OK, all these products applications will not be available for 6 months because we are going through this journey. So how I can mitigate that risk. So what we did is we created an isolated, separated test environment. Where we were able to take the production like data, we were able to generate production like a load volume so that I can not only test my compatibility of my applications into the post architecture, but also ensure that the performance efficiency hasn't been compromised. So what we did is we built a separate test environment for all of us to try out all these code changes, all the SQL changes, test our compatibility, test our performance, and then the next challenge, as I mentioned earlier, was how to fit into 100 terabytes or more than 100 terabytes of data worth into an Aurora Postgrace. As we all know, Aurora has a postgrace has a limitation of maximum amount of storage it can hold. So what we did was we went back to our Oracle database and we started to analyze different types of data which we were storing in that database, which I think is always going to be a critical part if you're going through that database modernization journey. We went through, we first identified that if there are any binary data which were stored on the Oracle database, if those can be migrated out of a database. Again, these databases were used by a lot of legacy applications, so we identified that a lot of storage is used by this binary data such as blob and club where we used to store our documents, images related to associated to those claims. So we came up with an approach. We built automation to migrate close to 30 to 40 gig of 40 terabytes of data, all this binary data out of this Oracle database and Migrate it over to S3 first before even we migrate the rest of the data into Postgrace. So if you try to understand the breakdown of this database migration journey, first we analyze the database types like all the data types. We analyze the data, we analyze what is binary and what is non-binary. We identified and marked down and came up with an approach to migrate all those binary data from our Oracle database to S3. Now, what I have been left with is all the non-binary data which are sitting on the oracle. There comes how I will take transfer all those non-binary data from Oracle to Postgrace. So the challenge is, Postgrace doesn't support all the data types as is uh in comparison to Oracle. Thus the team came up with an approach. There were different tools available to us to run some analysis and generate a report, and we did a comparison between a data type, let's say a number data type in Oracle database to a numeric data type in postres, how that conversion can happen. You will always end up with challenges when there are precisions kind of included in your data. If you have decimal values, if you have. Uh, like non full numeric values, that is always a risk when you migrate it over to your post-race engine. So with all these challenges, once we were done with all this risk mitigation approach, we went to our solution uh details. So all those database workloads, which were independent of each other, what we did is we simplified them out of these databases from the central core database, Oracle database, and we created individual dedicated postgrace databases for those application workload. That help us to trim the fat out of it. Now comes to the core meat. Correct? That still has close to 60 terabytes of data for me with all the complex data type columns. Not to worry, the next set of challenges was there were a lot of tables which were used by ETLs, staging tables, no primary key. So good luck with your data transfer when any tool doesn't understand that what is the reliability of the transfer of these datasets which do not have any primary key, whether it has been successfully migrated to the destination or not. So to achieve this data transfer from Oracle to Postgrace, we leveraged AWSS data DMS solution. The tool comes in kind of a lot of out of the box configuration, but for us we had to do a lot of customization, particularly on a busy transactional databases such as which was supporting 50,000 transactions per second. It was always a battle with how to configure the DMSS up to. Kind of maximum and correct point so that it supports that transaction rate because one, I cannot put a load on my source database because I cannot impact my business. It has to run. Uh, uh, as usual, but at the same time I need to maintain my DMS transfer rate so that I'm not lagging behind a lot to try and move that data into, uh, the post grace, because if your lag is too much, then we all know that the DMS will break because it will, it will find it difficult to keep track of those sinking points and at 1.1 point in time it will just, just break apart it will say, I'm way behind in the circum. Please come back and start me again. So there were certain challenges on that data transfer exercise. And I already mentioned there were not a 1 to 1 mapping when we were talking about converting a table data type column from Oracle to Posttrace. There were certain data types which were not supported on Posttrace. There were certain data types which were created or designed in a little bit different way in posters compared to Oracle. So how can I know when I have close to 50 schemas across 15 databases, and if I take all my life cycle of that particular environment, if I duplicate it across 5 different environments, I'm talking about close to 100 database instances with 200 to 300 schemas. So what we did is we again went back to AWS. We used their SCT tool. To do that initial schema conversion. Now what this tool will do, it will look into your source schemas. It will go through and analyze and translate all the individual database objects which are on the Oracle side. You can take tables, views, your procedures, your Oracle Oracle functions, and then what it will do is it will run an assessment. And it will generate a report at the end of the execution. And what it will do is it will basically provide you three information 1, How much out of the box conversion can be done. For example, it can generate a script for you, and it will tell that out of these 100 database objects, 70 of them are straightaway 1 to 1 conversion. I will generate a script for you. And then for stored procedures or for some other complex data types, it will give you an option, but you have to manually create a script or customize your configuration to support that transformation. And then for the rest 3, which is the most challenging group, is it will say that it cannot be converted by this SCT tool. So what you have to do, you have to go, you have to review them manually, and then. Uh, rewrote all those uh database objects, procedures as function or all those logic in, um, a post, uh, post-res compatible SQL. So, again, what we did is we did not do everything at once. We did all this in an iterative manner, and then we did all these releases in phases, and then slowly moved one pace at a time. Which is very critical if your database is complex, if your database size is larger. Now this, this journey took us close to 3 years, um, and I'm sure everyone would be interested to know that 3 years of investment of working through this modernization, working through this cloud transformation, at the end of the day what it gives back to us, correct? That that's where the buck stops, it's the dollar value, but sometimes the dollar value is not direct, correct? Uh, it can be direct, it can be indirect, uh, particularly in the operation world. Most of the time it is indirect. So how we can justify that return value out of this journey? So what are all the benefits we received as part of this journey? What we have observed so far, after working through this modernization and completing this journey that we have achieved. And improved operational efficiency. It helped us to sustain the same performance throughput, in some cases even better. It helped us to have an optimized cost strategy as we started using a lot of managed database solutions provided by AWS, it was easy for our FIOs team to come up with a better prediction and forecast so that we can manage our cost in a better way. Um, we, we were able to engage our site reliability team and DevOps team to focus on more automation, more scripting, uh, so that we can leverage those and optimize our cloud management. Uh, those procedures are pretty proven procedures. Some we were able to leverage, uh, provided by AWS. Some were built and developed by our own teams, and that helped us to enforce and expedite our automation track. The next one, what I'm going to highlight over here is With all this, we were not only able to achieve an improved RTO and RPO capabilities, but we were also able to have a better sustainable strategy. How we were able to achieve it, we were able to create a plan in some cases where we can use multi AZ across the same region. And then we created a kind of east-waste regions strategy where we can use waste as our disaster recovery solution or sustainable recovery solution and then use east region as our primary production. And then by leveraging AWSS managed solution, what we were able to achieve is an improved security footprint. We did not have to worry a lot the way we used to think about our security compliance when we were running an on-prem with Oracle. Scaling opportunity again, it enabled us to grow faster. If there are any faster go to market requirements there, we are able to scale faster there. And then we were able to support our business growth. There was no pushback from technology to business saying that if there is a new requirement or new workload, oh, it is going to take us months. No, it's it's days. You give us the requirement, we run through the new workload pattern in our performance test environment. Once it gets certified, you are ready to go to move it to production, as long as you are hitting all the checklist and checkmarks for your production readiness. And then the next best thing which happened, as I mentioned that every time it is not direct dollar impact with operation is a better and proactive incident management with a lot of automation and auto remediation. So with. When everything was running on managed solutions, we were able to focus on our incident management, on our runbook. We we were able to automate our SOPs. In a lot of cases we were able to eliminate those SOPs with a better modernization on the application and database layer. Now, I would like to highlight what kind of infrastructure we were running before we were running on Aurora post Race and why I I am trying to say that it is cost effective, it is operational efficiency effective, why it gives us a better sleep at night than those sleepless days in on-prem. So before Aurora post trace, we were running our database workload on an Oracle rack cluster. Earlier we were running on the extra data. Now after we moved to AWS we are running on the custom Oracle rack on AWS. We were running on a rack cluster with 9 nodes, and all those nodes were at that point in time, were running on the largest instance type whatever AWS was providing at that time. All of them were 32 extra large. This is just to manage the workload distribution so that one, I'm not creating a single point of failure, and second is the load is well distributed. We all know with Oracle you can create services, distribute your application workload, but this is a huge server. Not only your monthly billing meter goes high with this, but also it's an operational headache. You think about patching, you think about. Managing all these nine-node rack clusters day in and day out, it's a nightmare for support engineers. I mentioned about application modernization. To support all those hundreds of applications, we are running 100s of web logic VMs there. The Oracle Rules engine cluster, the Oracle service bus, those were complex and high maintenance. It required a lot of customization to support our application requirements, and support engineers were having a lot of challenges to manage and support them. Um, With moving to AWS cloud, there were kind of technical limitation around how many nodes you can add, um, include or add in a new Oracle rack cluster on AWS. Um, let's say tomorrow we got requirement for a new application, new workload, or existing workload is going to increase because of new customer rollout. It was always a challenge and hustle with AWS to understand how I can manage that new workload requirement. Now the life after Aurora post Chris. What we did is we designed these Aurora postrace instances with a writer and reader instance concept. We not only were able to reduce the number of nodes, or in this case number of instances after we moved to Aurora postrace, we were able to. Reduce the number of nodes of 9 nodes oracle rack cluster to a 2 instance Aurora postrace database instance. So we had 1 writer instance where all the writer insert, update, delete workload was going. Uh, from all the applications and then we created a reader instance to support any cash refresh, any reporting type of requirements, or any, any application workload requirements where there are selects or read operations were required. Um, again, tomorrow if a workload comes in, more reporting requirement, more businesses need more analytical data, we can easily horizontally scale this cluster by adding more reader instances. It is not going to take us 5 different discussions with AWS, 10 different discussions for weeks. It's just only going to need a click of a button or run a pipeline to add 1 more reader instance there. So horizontal scaling was never an issue after we moved to Aurora posters. Headless DR that's a very nice feature provided by Aurora Postgrace. Now, what we did is, these Aurora Postgrace database instances were huge to support this workload. Again, when we moved to Aurora Postgrass, we were running on the largest instance provided by AWS on this. So creating another uh compute and the storage on DR just to support the DR environment was a cost um. Uh, business for us. So what we did is we created or designed our DR solutions wherever possible and applicable with minimizing our availability risk to use the headless DR. So what is headless DR? Headless DR is, as we all know, like Aurora, you can replicate or clone the storage onto your DR recovery region. Uh, in our case, it was, let's say on the west side, but you don't need to add any compute over there, so you save a huge cost. Whenever you need your compute for a failover reason or for any other reason, you can add a compute to those instances in a click of button and your DI database instances will be ready and available in minutes. So that's what we did to manage the cost. Wherever DR instance compute is not needed, we used all headless DR for those. And with Aurora Postgrace what we achieved was subsequent data sync lag. It's it's in milliseconds. So if, if you're talking about doing an insert operation in your writer instance and uh doing a read operation on reader, it's less than 1 2nd in it's in milliseconds. So unless you have very time critical operations, you can very easily isolate your write and read and move them into two different instances or as many instances as you want. Same goes for DR. If you need it, you can run a lot of reporting from your DR engine. You add a computer over there. Even the cross region replication is pretty quick there. So that that reduced a lot of headache. When we were on Oracle rack, I had to maintain a primary instance. I had to maintain a local standby, and then I had to maintain a remote standby for my DR. So we reduced a lot of redundancy over that with. The advantage of AWSS automation. Now, before Aurora Postgrace, when we were running on Oracle, I had mentioned earlier, patching was a nightmare, even on the XA data era or even when we were running on AWS custom Oracle rack. It used to be a weekend business. It used to run for hours, and you don't know until that whole patching ends whether you are successful or not successful. It was a huge risk for our business. Run books to manage operational. Risk and operational task. So we were having very limited option to create run books when we were running on our Oracle rack clusters. It it was giving us very limited options over there. License management, we all know Oracle is not cheap in the market. You have to do annual license renewal. You need to do annual support renewal, so it has its own cost implication. And you get locked down with vendor software. You don't have any freedoms. You, you wait for their patches. You have their uh uh uh lockdown with Oracle. So Oracle basically manages and drives what you have to do with your application workload. Uh, we, uh, do to meet our complaints, uh, we used to do annual DR test and with all these, uh, like remote standby failover bringing up that data, um, it used to take hours, as I mentioned, so we, we, we were having like larger RTO and RPOs. Now after Aurora post grace what we were able to do, we all know with AWS managed solution. We can enable our automated patching. In a lot of cases, all the minor patchings are auto enabled, which means if you use those flags, AWS how you have defined your maintenance window, AWS is going to run automated patching on your databases, depending on how your business runs. Now all our DBAs, data architects, they can really focus on application designing, application database designing, rather than focusing on how to manage those rack clusters that gave them a good platform to learn new technologies, to bring development in their professional growth also. And what we were able to do is as we were able to implement site reliability engineering practices on our application stack, we were able to also roll out our database reliability engineering practice. So DBAs were not considered as those orthodox DBAs anymore. They were working as reliability engineers to ensure reliability. and stability of the database rather than thinking about patching and all those regular operational tasks which we used to do on Oracle rack clusters. Easy to scale, as I mentioned. Horizontal scaling is a button away. You click a button, you run your automation pipeline, you can scale it out horizontally as per your application need. Significant reduction in RPO and RTO. As I said, with Aurora posttrace, your application sync lag is very minimum. You have a better cross region replication solution available by AWS itself, so it reduced our RPO and RTO factors. When we try to compare how much effort we were putting on Oracle patching and managing Oracle versus Aurora Posgrace, we have observed more than 30% reduction in patching effort, which is huge, not only from a business point of view, but also for a database team, uh, how much challenges it reduces, how much sleepless nights it can reduce out of your calendar, and then. As I mentioned, for security compliance and for other compliances, regulatory compliances, we used to test our DR capability every year. What we have achieved is we have observed that even similar more than 30% reduction in those DR exercises across the teams, not only database team, be it site reliability team, be it DevOps, be it application team. We saw a huge improvement over those. Now, we, we completed this journey. Now the platform is there, the foundation migration was done, foundation is running stable and well on AWS cloud. We are using most of the AWS managed solution to enable us to move faster to achieve a better go to market strategy. Now, how are we scaling with AWS? So I mentioned earlier, we are still able to manage the same or even in some cases as we are growing more than or close to 50,000 transactions per second. This database in total handles close to 5 billion database transactions per day. So you can imagine how busy is this. Um, so we needed a hyperscaler enabler for us to support this workload, and we found it with AWS. Uh, we We started this journey with a very defined strategy. There were no opponents. There are no loose end points. We knew exactly what was the challenge. We knew exactly what was the risk. We knew exactly what risk mitigation path we have, and what are the solutions we're going to use for each of them. Uh, we, we were able to get freed out from this vendor locking. We were more open source, uh, technology adapter right now. Uh, we were able to modernize all our application stack. We were able to leverage, uh, event driven Kafka architecture. Which basically opened up a lot of other options for us. And then what at the end of the day it helped us to achieve is our core strength is data management because we get so many data points from so many claims, so many different types of data, because every claim is different by its nature. And with the complexity slowly increasing with all the automobiles, I think it is going to grow even more. So we, we, we focused on our database modernization. We identified each layer. We created a solution and strategy for each of them, and we did them in phases. And after the initial binary data migration done, we took the non-binary data and in the second phase, we migrated the databases from our AWS Oracle custom rack cluster to AWS Aurora Postgrace cluster. As I said, it not only enable us to achieve operational efficiency, better cost mitigation strategy, it also helps us to simplify both our application architecture and database architecture. So this journey was for years, more than 3 years. There were lots of partners with us who kind of helped us to enable this journey. We were never being able to do this by ourselves, so we leveraged. AWSS expertise whenever we were trying to implement their solutions, then we also leveraged Percona's postgrace expertise when we were towards the end of this migration and we needed help on their SQL expertise and postgrace expertise, but nonetheless, there was another critical partner for us. Which was uh cognizant, uh, they, they were our strategic partner from day one. I have Chandana with me from cognizant, so she will be able to give more details around it, but nonetheless it was a joint effort. We all worked together. We all worked with a concrete plan and at the end of the day we were able to achieve all the accomplishment whatever we aimed for. Thank you gentle. Thank you so much. That was tremendous actually. So I'm Chandana Paul, and as you said, I'm from Cognizant. I manage the relationship between Cognizant and CCC and um. Based out of Chicago, I almost did not make it here, you know, the Midwest is slammed, right? We had 14 inches of snow. But here I am So as a strategic partner of CCC, um, we were involved end to end in this whole journey, um, across the phases, you know, um, starting from planning all the way to production cut over, and this continuity basically helped with minimizing risk with quick issue resolution. We were, and we are intimately aware of the landscape and the applications, etc. So, it's really been a, a wonderful journey, and I'd like to give, uh, maybe, you know, just a brief about uh how we were engaged. Uh, this slide actually gives a sampling of, uh, the various areas that, uh, that we were involved in. Um, some highlights that I would like to, uh, talk about is one Surat mentioned close teamwork, right? So there were, there was AWS, um, the CCC, uh, involved, you know, and Cognizant was involved together as a single team. There wasn't any difference, so it was a one team effort. Cognizant was able to bring in mind share from, uh, similar engagements that we have, uh, accomplished with other customers. And so, um. Together, um, we were able to, you know, minimize risk, risk, um, resolve issues quickly. Secondly, on the, um, because we were so intimately, um, you know, we had so much in depth knowledge of the applications and the various, uh, objects, uh, we were able to, uh, put together a, a strategy, I mean, you know, do the analysis, put together a systematic strategy for remediation. Um, and, um. That remediation of the applications, the queries, um, uh, the stored procedures, etc. helped in achieving this, you know, with minimal risk, minimal challenges, um, and we were able to resolve issues as they came up along the way. On the DevOps front, um, the DevOps strategy was key to, to accelerating, uh, migration timelines and reducing risk, and this was achieved through automation. Um, using parallel EKS environment, bulk deployment across 30 clusters, and all of this helps to enable seamless deployment of over 300 applications, um. Using, uh, you know, using these, uh, applications as well as portals actually across EKS environments with zero downtime. So this was, uh, you know, that, that really helped along the way, uh, and then Cognizant, of course, it collaborated on the actual migration efforts. Uh, Sura talked about, um, using the SCT, you know, the, the schema conversion tool wherever possible, but there was a large number of, um, uh, metadata, uh, objects that needed manual intervention, you know, manual remediation, customization, etc. So that was achieved, and then, uh, uh, working collaborate, you know, collaboratively on the actual data migration using the DMS tool. Um, on application modernization, uh, more than 200 applications were modernized to JBOS, EKS, uh, over 500 SQL queries were remediated, validated, etc. So again, you know, large volumes of stored procedures were updated to post res SQL. Um, load testing, performance testing throughout that whole, you know, intense process, um, bringing in quick fixes, um, helping through deployment. And then, of course, once, uh, once moved into production, um, on the SRE standpoint, the cognizant team supported all the necessary, uh, SRE functions through 24/7 support and that included cloud watch deployment, dashboard set up, um, you know, all to ensure, uh, the reliable platform that uh CCC is known to provide for their end customers. So, again, a great journey together, um, a lot of learnings, a lot of best practices, which. After, uh, completion of this, we were actually able to take to our other customers as well. And uh we're really happy to have you as an audience here. I'm sure you've either embarked on or apply or are thinking about this uh similar transformation journey. We'd be happy to um share, you know, some of the, the best practices, lessons learned, etc. uh, what, what to watch out for, and so on and so forth. So thank you very much. So before I conclude this session today. If you are planning for a database modernization. If you are in the path of migrating from a native database solution to any cloud managed database solutions, if you're working on modifying your application workload from one database engine type to another one. Don't be overwhelmed by it. There are solutions. There are tools available as long as you have identified all your challenges, you have a better plan, and you execute through it, your journey will be successful. It has happened to us and I'm sure it will happen to all of you. Thank you for your time today.