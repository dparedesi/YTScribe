---
video_id: -pD5eY_d27k
video_url: https://www.youtube.com/watch?v=-pD5eY_d27k
is_generated: False
is_translatable: True
summary: |
  Prashant Awale and Rahul Sunnani show how autonomous agents powered by real-time streaming data and retrieval-augmented generation (RAG) can reduce alert fatigue and speed incident response in IT/network operations (and beyond). Problem: analysts get flooded with millions of events, stacked alerts, and fragmented logs/metrics/traces; context is missing, and runbooks/tribal knowledge are buried in wikis/PDFs. Downtime or missed anomalies can cost millions. Pain is felt from CISOs (trust/visibility) to ops managers (MTTR) to developers (innovation blocked) and end users (outages).
  
  Solution approach: pair streaming + processing + vectorized knowledge with agents that reason and suggest actions. Key technologies: ingestion via Amazon MSK (Kafka) or Kinesis for durable, ordered, high-velocity streams; processing with Apache Flink to aggregate metrics, detect anomalies, maintain state/context, and generate prompts (“summarize top anomalies in last 10 minutes”). Storage/search: Amazon OpenSearch Service as hot/warm/cold storage and vector DB (single-digit ms latency, cost tiering), integrated with Amazon Bedrock knowledge bases. Bedrock offers model choice/customization, serverless deployment, and RAG; OpenSearch can serve as Bedrock’s knowledge base. Agents can answer “what/why/impact/what next” using real-time signals plus historical SOPs/runbooks.
  
  Architecture (demo): EventBridge triggers simulated fragmented DDoS traffic against an EC2 instance every 2 minutes. Traffic logs flow to MSK Serverless, Flink extracts key attributes, a Lambda bridges to a Bedrock agent. Knowledge base (Titan embeddings, 1,024 dimensions) stores AWS Network Firewall docs and NIST protocol guidance; stored in OpenSearch Serverless. A Bedrock agent (Claude 3.7) with system prompt categorizes severity (1–3), references the knowledge base, and, if sev 2/3, generates email notifications with recommended actions and time-to-act. A “security tool” Lambda can also be invoked. Emails show start/end time, IPs, traffic %, risk assessment from KB, recommended mitigation (block IP/move traffic), and projected urgency (e.g., 4 hours vs 24 hours). Alternative implementation: Strands agent running on AgentCore runtime/gateway with MCP tools; Python code defines system prompt, tool descriptions, authentication, and tool discovery. Tool descriptions are critical for LLM tool selection; gateway handles auth (client ID/secret) and MCP communications.
  
  Broader use cases: manufacturing (sensor streams, predictive maintenance), automotive (routing to gas/charge stations, auto-scheduling service), healthcare (wearables triggering clinician/EMS actions), travel/hospitality (trip planning with agent memory for preferences; Swami’s keynote highlighted agentic short/long-term memory). Takeaways: real-time value lies in context, not just speed; agents + streaming + RAG shift orgs from reactive firefighting to proactive/fully autonomous decisioning. Start small (single loop: one agent, one DB), then scale. Resources include Bedrock agents/AgentCore, Strands, anomaly detection workshop, and AWS Skill Builder courses.
keywords: streaming, Bedrock agents, RAG, OpenSearch, Flink
---

All right. Good morning, everyone. Welcome to this session. My name is Prashantawal. I'm Principal Open Search Spaceless Solution Architect, and I'm joined by Rahul Sunnani, who is principal Solution architect focusing on the GEI and ML. We are super excited to be here and talk about how you can use the intelligent autonomous agents which could use the real-time streaming data and retrieval augment generation to enable dynamic context-aware decision making which can help you deliver real-time, real-time and real world values in various use cases such as IT ops, FinOops, net ops, and the health. Care systems. So to set the expectation of this session, as you know, this is the silent session, so we won't be having any live Q&A, but we would be available after the session. So if you have any questions, we would be happy to answer those things. To get started, here is a quick agenda. So we start with a customer challenge which is faced by the IT ops and the network ops, and That is kind of just an example to get started with. Then we will discuss about some of the key technologies to use for building this kind of solutions. And then Rahul is going to cover the overall architecture which we have used for making this kind of system, followed by the live demo. And I hope demogo works well because we are planning to do the live one. If not, we might switch to the manual one. And then we will wrap it up with some of the key takeaways and the call to action. So let's jump into the challenges like which you all might be having. So let's say you are an analyst on call and suddenly at night, midnight, 2 a.m., you start getting the pager alerts where you are having an unusual unbound traffic coming from, say, one of the US East one region. And the system is reporting a sudden spike of 600% spike in the traffic with the anomaly detected at very high severity. So basically you pause for a second and think of like why I'm getting these alerts at this time. It's really midnight, right? And from where I'm getting those 600% of traffic. Spike coming from here. You basically get up, grab your laptop. Within a few seconds, you start looking through those monitors, but the thing is here your monitors and the charts are filled up with the data and you are seeing the spikes on those charts. But the thing over here is there are many alerts which are stacked one over another. So the thing is log entries are streaming from various sources which you cannot really manually read it out. Each line looks like the same, but out of those lines of errors there are like one certain line which is actually the anomaly in that particular data. So here the thing is your system is telling like what the error is, but it's not really giving you the context because you are scrolling through those 30,000 different lines to find that one line which is having the issue. So if you look at those data sets, there is one hidden line which could be causing that kind of a spike for the anomaly detection. And if you look at the previous data sets and the various studies, then one missed anomaly and those kind of an issue can cost millions of dollars in the downtime. So here it could also be a false alarm which you want to isolate from those alerts. And if I'm not wrong, out of in this room there could be many people over here which could be getting those kind of issues. So raise your hands if you have seen any such kind of incident in the past where you got a lot of arrests but not able to find any such issues. OK, so a lot of you are raising hand means you all have had these kind of situations, and definitely we all have had that kind of night, and that's how the reality is with respect to the networking and the IT ops because our networks are not static. They are distributed FML devices connected with various devices like you have laptops, camera systems, printers, monitors, mobile phones, etc. which is completely connected. And all these events events are generated from various devices and the various sources. This could be coming from your IOT devices. It could be coming from the different API systems. And over a period of the last few years, the data volume has really exploded. And earlier it used to be like a few thousands of lines per minute, but now it's like millions of events coming at a very high scale, like in a few minutes and a few seconds, and our network is not really quiet anymore. It is kind of moving the data and they speak in millions of events per second. But the problem over here is the threat what you are getting that evolves over a very fast way which human eyes or the dashboard cannot read. And if I look at some of the Gartner studies and the other studies, we have found like 70% of such anomalies go undetected until after the damage is done. So now we see how the network has evolved over a period of time which is no longer a static map, and It is kind of causing the causing the problem into the real-time system. So we talked about the networks. Let's now zoom into the human side of the aspect where like for each such dashboard there is basically an analyst who is looking through the dashboards and the alerts which are like in dozens of dashboards or hundreds of alerts, and most of them could be like meaningless and some could be like even redundant because you have the data explosion coming at a very high scale. So here the real problem is not really on the, uh, not on the volume, but it's more on the context. So you have log data which is scattered across multiple devices and multiple systems which I mentioned in the previous slide. Which is missing the historical and the contextual linkage. So as you might have locks in one system, you have matrix on the other system, and then traces on a different system. So you need a system which can kind of correlate all of such incidents and events which could be helpful to kind of find what might have happened in the past so that you can make some judgment and get the responses, what might be happening today. So if we talk about the manually looking at those errors or manual triaging, it could take like hours or days of the analyst times, but the irony is like every organization has some sort of data. You might be having the confluence pages or the wiki pages where you have the SOPs or the run books. Where you have seen those kinds of issues in the past, but the thing is those answers exist in those PDFs which could be like say a 500 page manual or a playbook. But when the real anomaly happens, you cannot really go back and look into those PDFs to find the exact issue, what might be happening. So in the middle of the incident, the knowledge might become very critical and important, which could be as good as like you have locked it in a vault which you want to find in the middle of the night. So it's like you have the answer, but you're not able to find that, and that's where we are thinking of building a system where you can have the context for your real-time data which is linked to. Your knowledge base which is stored into one of the knowledge base which can find the anomalies what might have happened in the past and then what's coming next. And if we talk about like who feels the pain, so I talked about the analyst who is feeling the pain, but if we look at the broader ecosystem, then analyst on call is not only the person who feels the pain. There is a full hierarchy. It flows from CISO to CTO, but they have a different level of like viewing the system. So if we talk about the CISO who is sitting at the top and he feels the pain like on the visibility and the trust, he might be getting into the board meeting and they would be asking the question like what could have been done to avoid that kind of situation. So that's. Pain what CSO is failing. Then top to bottom we have the security analyst. So as I was talking about earlier, like analyst screen is flooded with the alerts and the dashboards, and he really, he might be like getting into the alert overload or the alert foottigue which could be kind of getting into the risk of oversight where he could miss one line which could be having an issue. Then we have the network engineer which would be kind of trying to find out the data across different sources like your logs are buried in one system, matrix on another, or maybe traces on another. So it's trying to find those correlations between those data to find the reason for the anomalies. Then the third one is the ops manager, and their view is also not very different, but it's he's trying to reduce the mean time to resolution for any such errors, and most of the times ops managers are reactive, not proactive. So we want to really build a system which is proactive in terms of those monitoring so that it can tell you what the error is and what could be the resolution of that. Now if we get into the next layer, we have the CTO, so who is kind of getting to the innovation, building product, and working with the development team. So there is a developer who wants to kind of sift the code faster, but he's kind of buried into investigating the issue which is happening at early midnight or the next morning. So in that way the developer productivity gets impacted. Then we have the developer manager who kind of keeps on missing the deadlines for delivering the feature. So instead of working on the innovation and building new features, they spend a lot of time troubleshooting the issues into their system. And we talk about the organization things where you have the pains, but even before these people see those pains, we have the end user or the customer who experience those pains even when it comes to the system, because the customer might be seeing the. Latency being high or or maybe like they're not able to access the system whenever any such downtime happens, so they would be having the outages or the degraded performance for their application which would be impacting like millions of dollars for these customers as well. So now think of like if you have a network which could really think that's what we are heading into the autonomous agents. So instead of looking at the dashboard for your attention, your system could actually region just like your best analysis would. So instead of looking at the playbooks, let's say if you have the incident history and then you have the tribal knowledge from the past experiences, then it could come alive and respond in minutes. So now picture the same scenario we had at the beginning, like it's 2 a.m. in the morning and the spike happens, but this time what happens is instead of Showing you the hundreds of alerts, your system gets you a single message like outbound spike is coming from the US East 1, which is resembling the DDoS pattern we might have had in the previous quarter, and the resolution for that is to isolate that particular network, move the traffic to the US West too, so that your system starts working again. So that could be the suggested action based on the agents and how does this really work under the hooded. So it's not really a magic. It's kind of marrying your data which you are getting at the real time from various devices. Then you get the enrichment of those data, identify the key matrix or the pattern from that logs which could be causing that causing that anomaly. Then you have the autonomous agents which would be looking at your streaming data, and then on the other side you have the knowledge base which is having the history from the past incident, and then using those knowledge base you will be getting the suggestion with respect to say predicted predicted maintenance or like what could be the alerts which you need to solve at the right moment. So that's how the autonomous agents has come into the picture. And now if I correlate this with the network analyst, what I had in the beginning is looking at different charts for the monitors to find the issue that has been converted into the agents which are looking for the suspicious traffic detected. But the thing is, I think you all have been here and you all have been hearing about the agentic AI agents almost in all the sessions, keynotes and everything. And you might have a question like, can the agent replace the human? So I would just like to highlight the Matt's keynote yesterday if you, if you were there. So he was talking about like agents are not really going to replace the human, but it's going to multiply them. So. The action or the task which you were doing earlier in like say 24 hours or a week, that could get reduced to 4 hours or 6 hours or even a day so that you get more time working on the product or building building the innovation rather than working on fixing the problem. So this is not really the sci-fi dream, I would say, and you can think of like the agent which never sleeps, but it's all time attentive. Whenever any issue happens, it will just respond with its best judgment and the knowledge it has into the database. So there will be kind of another human in the loop which would be speaking like human. So now if we look at the dashboard, so instead of analyst asking the question, your agent can ask the question like, show me the top 3 anomalies which have happened in the last 10 minutes, and the agents would be responding with an intelligence. Summary of those data along after looking through like tens of thousands of similar events and these events would be kind of complementing you by reducing the noise from the system and doing the heavy lifting of correlation across various systems. So in that case humans can really focus on the decision making. So let's look at how does the architecture look like. So at the top you have the different devices which are streaming the data. So at the streaming side you have the streaming services which would receive the data, and then you will send it for the analytical processing. So in this particular demo of the architecture, we are going, we are not really using the streaming data for the analytical processing, we're only forwarding that data for the next layer to find the anomalies, and then you have the knowledge base where you would be storing your previous experiences like notebook or the runbook, SOP, etc. And then you would be running the large language models which would be queried by the agents to find the anomalies by looking at the knowledge base. It's a high level architecture, but Rahul is going to cover more detailed architecture once we go forward along with the demo. So let's see, like we talked about the high level architecture and the issue what Kashmir have. So what are key technologies we have used to build this kind of solution? So the first one is the streamings where we saw like Devices are generating the events every second. Then you, you really need a service which can capture, store and and data reliably to the next layer. This is where a streaming service comes into the picture. So we have two options. One is the Amazon MSK, which is the manage streaming for Kafka, and the Amazon can say data stream. So if you're looking for the managed service, then you can go for either of them. And with the Kafka you can also have the self-managed Kafka which could be used for receiving the data at the streaming layer. So both of these services provides a mechanism to stream data durably and to scale as per the demand. It can handle the high velocity data as per the agent's capacity or as per the capacity of what you're planning to send. And then if needed, it can also hold the data into the buffer until the next layer is ready to receive that data. And lastly, the data which is being sent through those streaming services is ordered so that it guaranteed the ordered delivery of the event, which is really crucial for deterministic agents' reasonings by the agents. Once you have the data being flowing from the streaming services, then you need a processing layer which can help enrich or transform the data. So here we have the Fli which could process continuous stream of data in motion by aggregating the metrics, correlating the anomalies, and producing the real-time features which AI agents can use for the reasoning. And they can then decide, like, think of like each log line being sent to the agents. It creates the rolling average of, uh, for example, the errors what you're getting like the network latency or the response code, etc. and maybe you might want to capture the frequency of failed login attempt if any of the DDoS attack is happening into your network. It can also chain together the event from various sources. So like as I was talking about MSK and Chemistry, the streaming agent, in the flink you can receive the data from both of these streaming solutions and then you can use, then agent can use those asynchronously. And lastly, it also has. The built-in state management, so it kind of resembles the recent activity by maintaining the context and then whatever data it gets, it can generate the prompts to an agent which could be like summarize the top 3 anomalies in a particular region in a particular time frame so that you will get the overall context about your data, what you're getting in a certain time window. So now we have the streaming, we have the processing, then we need the storage layer. So that's where open search service comes into the picture. So open search service, it's the managed service which simplify AI powered search, durability and vector database operation and gives you really a secure and cost effective managed service. So in terms of the operational side, it's managed service you can deploy into the AWS Cloud. We have both options like the managed service as well as the surveillance option over there. And if we talk about the performance with respect to the open search, we have seen a tremendous improvement into the open search from its previous version, which was there around like 1 to 2 years back. We have seen 6/6 performance improvement. And when we talk about the agent's knowledge base and the vector database, you can use the open search to get the single digit millisecond level latencies. Then we have the different tiering strategy in the open search, so that can help you reduce the cost. So like if you have any data which you're not really actively querying, you can push it into the warm tier or cold tier, and the recent data you can keep it into the hot tier. And lastly we have the integration with other AWS services and even open source tooling as well with the open search. So one of the integrations is with the Bedrock. So that's where we have the open search as the knowledge base for the Bedrock, and Bedrock has a set of capabilities which could be used for building and scaling GI application with the highest level of privacy and security. So if you look at the top left side, like one of the reasons why. A lot of customers and people are using Bread Rocket because you have an ability to select a different foundation model using a few simple APIs. And once you have your own, and the thing is you cannot really, you're not really bound to use the foundation model. You can also train those models with your own data sets so you can customize the foundation model with your own data so that it, it gives you the relevant relevant output and not really hallucinating the data. And all of this you can get at the best price performance because bedrock is serverless, so you don't really have to manage and maintain any infrastructure for that one. Once you have that data and once you have finalized on the models, then it gives you the capability to use the retrieval augment generation where bedrock acts as the knowledge base so that you can use it for different rag use cases. That's the way you're not really hallucinating on the output and apart from the foundation model, Bedrock also has other capabilities which anyway Rahul is going to talk about which is more on the agent core and the bedrock agents. And lastly, like it happens by following the strict compliance and the security so that your data is not impacted with any security breaches. And as I was mentioning earlier, like OpenSearch is the recommended vector database whenever you are building for G AI application and using with the Bedrock. And the reason why people are using the AI OpenSearch is because you have the one-click semantic embedding for your text data, and even it supports the multi-model for the image or the video data as well. And then you can scale the open search for the billions, and not even billions. We recently announced yesterday like you can scale the open search for trillion scale vector embeddings with a very high dimension of like 16,000. And it supports different algorithms like HNSW, IVF, and then it has different quantization options if you're looking to build the cost optimized vector database. And lastly, it's not only the vector database, you can do the hybrid search by combining the lexical and the vector search into the same search solutions. So with that, I have covered the challenges and the key technologies, what we have used to build this kind of streaming application. Next, I will hand it over to Rahul, who is going to talk about how AI agents act as the bridge between the streaming data and the knowledge base. Perfect. Um, thank you, Prashan for that. Uh, so before I start diving deeper into the autonomous side of it, uh, mainly into the agent side of it, can I see a raise of a hand where some of you might have already tried writing some agents by yourself, maybe in the POC, maybe in development environment. I'd say quite a few of the hands. Thank you for that. Now, can you raise your hand again if you integrate that agent into a real-time streaming data to find the analysis around that? OK, very few. Thank you for that. So this is this session we'll be focusing on how we can integrate the agent into a real-time data set and then analyze the particular event and try to find out corresponding context awareness to understand what action is to be taken care of. So before I start, I'm going to quickly put the definition of an agent. Again, at the end of it, agent is just a piece of a software. Which is capable of working with artificial intelligence systems, predominantly making use of large language models to understand the problem statement, create a corresponding plan to execute it, and at the end of it you have the option to execute those steps on behalf of a human. So that's where the main power of large language model with respect to agent is set in. Now to make that happen, imagine like once you get a problem statement, what you do? First of all, you try to understand, let's say I received a log file. I need to understand what is present in the log file. Then I do understand like, hey, what is corresponding particular IP addresses, whether it belongs to one of my EC2 machines or belongs to some of my other applications which might be running on surveillance applications as well. Once I get that information, I try to find out what is corresponding impact, a blast radius associated with that. Now let's put that brain thought inside an agent. The core of the agent is understanding the goal, the problem statement, with understanding of which different tools are available. In our case, maybe calling an API to get more contextual information or go to my knowledge base to understand. More detailed information about my run books, my security standards, combine it together and then make action besides the corresponding actions, what it can take to either eliminate it or at least notify what is the right action needs to take next. That's the power of the agent in a nutshell. That's how multiple components came in the picture to work together as a brain of the system. Now when I talk about brain of the system, that's where the agent comes with a very change in the paradigm which is called a react, which is nothing but reasoning and action. As you can guess, reasoning is nothing but using large language models power to understand the problem statement, get corresponding context, and decide which different tools are available, which one to use at that point in time, and action is the part where it actually invokes the tool to get more information. Now this again in the real world, this cannot be a one step process. As a human, I go through log files, then I go through run books, then I go through security logs, multiple different places. That means I'm looping myself multiple times. Getting the inputs from my previous steps and deciding and orchestrating my next steps, that is also repeatable. So combined together, the agentic loop is not just capable of doing reasoning and deciding the action, plus addition to that, it is capable of orchestrating that action repeated multiple times to come up with a common conclusion or a final conclusion of the result. Now let's see that in the diagram process flow wise. So first of all, your prompt query, which is nothing but in this case, maybe a security log, maybe a problem statement, maybe some information which you're giving it, and with that you're giving the system prompt, what you're expecting agent to do. That's what it mainly is. Agent takes that information, combining it together, and does the LLM call, which is nothing but calling a large language model. Now large language model behaves as a brain of the system. Which takes all this information together to decide which different tools to invoke because at the same time it understands which different tools are available because the agent is providing all that information to large language model. Once it decides what tools to invoke, it sends that information back to the agent. The agent combines that information together and invokes that particular tool. That's where the execution step comes in the picture. The execution step can be debugging your logs, going through a knowledge base to understand corresponding context, get some more information, multiple different steps involved, and this loop generates a result. That result may not be the final result yet because this might be like, hey, this is just a step, the first step before I reach the final conclusion. This all information or this can continuously repeat. At the end of it, when the model gives a green signal that yes, this is a good final result, agent combines it together and sends the result out. So that's what the agent loop in a nutshell is. Now let's dive further deeper because especially those talking about bedrock. I'm going to look at one of the important features of a bedrock. Importantly, when we start talking about writing the agents, we have multiple options. In AWS, we like to give you multiple options. I'm going to talk about a couple of them. So the first one is related to bedrock agents. This is one of the key features inside bedrock, which is completely provide fully managed agent creation. It is capable of doing its own orchestration as well. You have an easy option to do host it as well. So that's the key feature that we'll be exploring today. And a couple of them which we will dive further deeper into agent. First thing is nothing, but I talked about I'm going to call some APIs. That's where the action group comes in the picture. That means the tools which my agent can invoke, making my agent aware of those tools. That's where the. Group comes in the picture. Prasan talked about the runbooks, talked about the previous SOPs. All that information combined together creates a knowledge basis. So these are the two important key features we will be talking about today, but there is a lot of variety of different features available. Now let's, let's change the gears a bit. So I've talked about the bedrock agents, which is, in my mind, it's more towards lesser coding environment. But now, as a developer, we're like, hey, I, I know my Python coding, just give me the framework to work with. That's where strand agents come in pictures. So these are option number 2 if you decide to write the agent by yourself. Strand is open SDK. Still yesterday it was Python. Today morning you must have heard the TypeScript starts supporting it as well, where you are as a developer behavior, your persona is more of a developer persona where you're writing the few lines of code to write the complete agent functionality. So we have two options. We're going to explore the first one in details. Second one, I'll show the snippet snippets of that as well. Now let's dive further deeper. Let's talk about how the agent is calling a tool, because when you talk about tools, the tool is capable of doing a lot of variety of actions. It may be capable of going to a database, get some information, capable of going to my knowledge base, get some information around that, maybe calling an API to do some action steps around that. Each of these steps has its own nature of execution. So there should be a common mechanism where agents should be able to understand and hand over the responsibilities to the tool to execute it. That's where the model context protocol MC protocol came in the picture. Again, it's generated by anthropic and it is widely used by many of the agents, many of the tools around the world now. So let's look at, let's dive further deeper into what we've done the agentic loop and now where the MCB plays a role. Let's let's look at that part. So as usual, first of all, the query came where agent absorbed the query. It got the corresponding information as well as which different tools are available. It passed along all that information to large language model. Large language model, as you've seen previously, is capable of doing the reasoning. It understood the problem statement. It understood the context, and now creating a plan to execute different agents, different tools. That's where the invocation of tool comes in the picture. Now when the invocation of tool comes in the picture, that's where the MCP protocol come in the picture. So your MCP client is making a call to MCP server, which is going to invoke a tool. That tool can be different. API calls tool can be accessing the knowledge base for getting further information. That knowledge base can be based on your runbooks, but the whole idea here is giving the option for the for the large long email to discover the tools. And once discovery happens, the action part of it is taken over by a common protocol. That's where the comes in the picture. Now this loop, as you can see, results come back to the large language model where large language can decide whether it's a final good result or not. If it is not, it can repeat this particular step again and again. That's where the MC protocol comes in the picture for coordinating or communicating between agent and different tools. At the end of it, when the LLM says yeah this looks good, all the results looks fine, that time the results come back out. Now you see that fragment attack, that's what the scenario we have mimic it today, OK. Let's combine the things together. So till now we talked about RAG, where it is nothing but a good open search mechanism where we can save all the information. We talked about agentic side of it, which is nothing but capable of gathering the information, creation of plans, and execute it together. Now let's move one step forward. What if we combine them together now? That means dynamically whenever the problem comes together, first thing LLM is doing, it is nothing but breaking the problem into smaller chunkable steps. For each step it's complementing it with it's understandable knowledge, which is nothing but a rag. Based on the rag and the problem statement can combine it together to create more context awareness, making more personalized or more. Specific results to come up with the tool selections. Once the tool execution happens using orchestration by large language models, it can generate the results which are more effective and dynamic in nature as well. That's where the agentic rack comes in the picture, and we will dive deeper into the agentic rack as a part of a demo today. Alternatively, because as I said for a developer things things a bit, change a bit. That's where the agent core comes in the picture. Whenever I wrote an agent using strands, now I have to deploy that. So there are different, different paradigms available as a part of agent core which gives me surveillance environment to host my agents quickly. So runtime is an environment where I can host the agent. Identity is another paradigm where I can connect back to my identity provider to make sure if I'm calling the agent, it should not step over my responsibilities. It should follow the same responsibility with authentication as well as it is capable of integrating with different tools using gateways. To discover the tools, call the tools. That's where the MC protocol plays a quite vital role in there as well. Additionally, it is capable of understanding what is the current context of the issue or context of the current problem, as well as what are the previous contexts of the similar problems that might have arrived. That's where the memory comes in the picture. There are a couple of additional tools which is browser and code interpreter. If you're interested, we have a bunch of these sessions going around for the agent core. I will encourage you to attend some of them, which we will dive further deeper into this. So that being said, I'm going to start diving deeper into the demo architecture, what we are generating today. So the first part of it, what we are doing is we are mimicking a scenario where we have multiple different systems or machines sitting either inside cloud or inside on premises. All the logs are coming together. Those logs are getting transferred through streaming solution. MSK is the one which we are using. Getting analyzed initially to extract the key attribute information using Apache link application. Once that information is available, we are passing along to another MSK to integrate that with the lambda. What that lambda is capable of is it is integrated with a bedrock agent. So whenever the record comes in the picture, it will invoke the agent. The agent already has a couple of tools already mentioned through lambda. As well as has integration with large language model, we are using anthropic 3.7 model for our purposes, but at the same time, for context awareness, we have created a knowledge base inside Bedrock. For that knowledge base we are using Titan V2 model, and the knowledge base is saved inside OpenSearch as a vector dimensions. At the end of it, as an action, we are just sending a notification at this moment, but absolutely you can further improvise this by adding some more tools to actually take the action on behalf of the user as well. OK, so that being said, uh, one small change which we have done because I cannot just create a system which can keep attacking the system, that's why we have simulated it. So I simulated an attack using the event bridge, which will generate an attack every 2 minutes. So every 2 minutes it's going to attack one of my AC2 machines and it will send the logs accordingly. So my demo, I'm going to focus on this particular architecture and let me switch off to a live demo now. Again, please be patient with me. It's a live demo, so things might not go in my favor, but let's try. All right, so first of all, I'm gonna go a little bit backwards. So we, we call it the working backwards. So I'm going to go from the right to left this time talking first of all about what we have set up in bedrock. OK, so we're starting from top, first I'm gonna start explaining about bedrock knowledge base, how we have set up that first, OK. So this is my Bedrock service where I have created a knowledge base. Again, Knowledge base is one of the options available in Bedrock. I intentionally kept the tabs open to make sure that it's quickly available. So first of all, in knowledge bases, again, it's a managed part of it, so where as a user you can pick and choose the model. So I use the Titan embedding V2 model as a target. I'm saving the data into open source server list, and my number of vector dimension it is 1,024. So that's my vector dimensionality. Now what information I'm saving there is, is my data sources. Now here the sources I have is information which is previously available with me. So I have two of them available. One of them is nothing but my network firewall developer's guide came from AWS, and I'm using a NIST standard network protocol coming from publicly available information. You can easily enhance this by adding your own specific security standards for your company or your protocols or your knowledge base off your network as well. So two documents which I'm embedded are, again, if you're interested, these are the two documents. It's a combination of multimodality, so I have a text, images, multiple tables in there. All of that information is available as a part of it. Similarly, I have a NIS document which has all this information. So these two documents are for my additional context awareness. Coming back into agent, so which is nothing but in the diagram, I'm focusing on the heart or the brain of the system right now. So I'm using bedrock agent. I do have a strand agent which I'll show towards the end. So in bedrock agent creation, I created a couple of versions of it, so I'm going to focus on version two. So the first part of when you're creating an agent is which model which we'll be using. So I'm using anthropic 3.7 model. And then for us important information is associated to the instruction which I'm giving to my agent, which is called a system prompt. So in this case I'm telling my system prompt is like, hey, you are a security analyst. What are the analysis or key information you are receiving? Analyze it, whether it's going to create an attack. If it is creating an attack, categorize those attacks, whether it's a severity 1 attack, severity 2 attack, or severity 3 attack. If it is severity 2 or 3, generate an email and send out a notification as soon as possible. At the same time, provide the information by which somebody can overcome that attack as well. If possible, give the information related to how much time they have to act on it as well. So that's the complete, if you see it's an actual natural language prompt I have written, no coding required in this case. OK, now another part which I talked about is the tools availability. So one tool which I have, again, I will talk about tool in a minute, but I will talk about knowledge-based integration first. So the knowledge base which we have created, that is also available for my agent to work with, which behaves like a tool for me. OK. Now let's look at the tool which is called a security tool. I've written that security in a lambda. The security tool is very simple lambda invocation, which is again going through a simple prompt and getting using the bedrock information, creating a prompt. Again, prompt is related to very simple analyzing the analyzing the logs, and at the end of it generate an email as well if it is severity 2, so that all part of invocation as well as generating the email as well and to send a notification. So those are the two parts I've created in there. So once these parts are available, let's look at the other part of the system. Again, it's nothing but at the end of it sending the email too. So I'm going to dive deeper into the second lambda, which I'm calling as an invocation lambda, which behaves as a bridge between my internal data set, which is coming through the streaming, as well as integrated with my bedrock agent. So whenever the data comes in, it is capable of calling the bedrock agent now. So in that lambda, I'm just gonna close this. So this lambda is again a very simple one. It has agent ID and corresponding alias names, which both came from my previous Bedrock agent. So if you go to the agent details, you'll get corresponding information related to my alias names and the agent ID as well. So ID and my version 2 I'm using, so the alias name of version 2. Those are the two information it is passing along. While it's passing, it's formulating a message and sending that information to the agent to act upon it. That's the whole purpose of it. It's very simple lambda which is capable of getting and sending the wrong one, this one, very simple lambda which is capable of taking the agent corresponding aliases and pass on the information which is extracted by the previous FL application. Now let's go one step back. Let's look at the ingestion part of it, where the MSK and the FL application came in the picture. I So in this part of it, I have a simple MSK serverless again created. I'm constantly sending some messages to attack, create the system attack, and that is getting captured in here. So simple streaming data ingestion using MSK and with a streaming Apache link for analyzing and extracting the key information. So this is one of the applications I hosted, and from the last few minutes I think I sent out almost 9 million records, around 1.84 GB of a data set. If you want to dive further deeper, you can click on it, get corresponding detailed information just like any other Fling application. So I'm generating attack constantly. Now to generate that attack, I'm going to go back in here and talk about the attack. So again, as I said, I'm going to create an event bridge system which will go ahead and every 2 minutes send an attack to my one of my EC2 machines. So let's look at that part. So while generating an attack, I created an event bridge which will go ahead and every 2 minutes generate a fragmented attack which is behaving as of a DS attack. So every 2 minutes some system is trying to send a lot of traffic on my EC2 machine and try to get the fragmented attack for me. So at the end of it, assuming that I'm assuming that it's going to generate a DS attack for me. That's what the thing I'm trying to do. Here the EC2 corresponding EC2 machine. This is the machine which I'm attacking at this moment. For our analysis, I went ahead and displayed that attack as well. So every few seconds, every 2 minutes it's sending an attack, and I'm just exploring that attack as well. So now overall what system which we have done is our event bridge is sending attack every 2 minutes to EC2 machine. EC2 machine is sending that information through MSK. And integrated with Apache Fling to extract the key attributes. The key attributes now pass on to my bedrock agent, which is capable of doing the reasoning as well as using knowledge base to find out the attack mechanism. So as I started the attack already and at the end of it you're going to send me the email. So again, these emails are coming constantly, but I have intentionally identified a couple of those emails for us to dive deeper into it. So that's the outcome of the attack now. So when my large language model is extracted at the start time of the attack, end time of the attack, different IP addresses involved, corresponding size and percentage of that as well, some of the key attributes. Now the second part, if you see risk assessment, that came from my knowledge base. That assessment made use of the previous AWS firewall guide, developer guide, as well as NI document to come up with corresponding information and mix it together with the metrics available. Towards the end, it came up with the recommended actions that needs to take care of. The actions it's suggesting is, yes, there is an attack coming from one of the IP addresses ending with 1.7, and it's suggesting me to potentially threat, block the traffic from that particular IP address, and the other IP address I'm hitting only one IP address, so destination IP address is also provided in that case. Now combine it together, it identified that this is a critical attack, so it came up with the timeline, projected timeline that within 4 hours you have to act on it because this is one of the critical attacks. Mainly the driver is the attack duration and the percentage as well. So I'll show you another one which is more of the medium sale attack. Now if you see the difference, this is another which is my LLM realized it is like a, it's not like a high critical attack, but it is like medium sensitivity attack. Now it generated the similar information but provided a time window of 24 hours. So overall what's happening here is my agent is making use of my runbooks and plus integrating that with the current log file, combining it together to come up with the recommendations as well as the time which will take for the attack to be more critical than this. So that's why those time windows again, those differ based on some of the attacks. Sometimes it will not provide the time window as soon as possible, or sometimes it may provide the time window as well. So that being said, so now I talked about the option number 1. Now let me change the gears. I'm going to talk about the option 2. This is more for if you're more from the development background side of it, and that's where the strands come in the picture. So the whole agent which we saw a few minutes ago where we use the bedrock agent to write the corresponding system prompts, corresponding tool integration, I rewrote that everything using strands now. So mainly I'm using the strand as my agentic framework, combining it together with Agent Core, where I'm using Agent Core runtime to host my agent and Agent Core gateway to make the tools available for it. Again, it's using MCP protocol for tool communication as well. So if you see the code, first of all, I'm integrating with the strands libraries, combining it together, created again, this is what the model which I'm using, cloud 3.7 model, and then if you look at the complete system prompt, which is giving the information which different tools are available, how to identify the sensitivity, generate the sensitivity correspond to the prompt based on the prompt. At the end of it, it will talk about the gateway, which is nothing but the integration of tools available. So agent core gateway is making sure different tools which I have defined already are available for my agent to work with. When you talk about gateway, the important part is authentication as well. So that's where my client ID, client secret comes. A picture to make sure agent is communicating, it's a secure communication and it's the right communication where the agent has the authority to call the particular tools as well. Further going down the line, we talked about URL. That's where the server is hosted, which is giving the ability for agent to communicate with the tool using protocol. And the tool discovery, these are the support of the tools. Another important key aspect which we will observe here is there is a tool description because LLM will use tool description to understand what the tools functionality is. That's why when you are defining a tool, it's quite important that that description is clear enough so that whenever an agent makes a decision to call one tool versus another, it has clear information associated with that as well. Further down these tools are available. The next part is just invocation of the agent. That's what the part is. So overall this is more towards developer friendly environment where I return using Python. If you want to use alternative, which is bedrock agent, where I have not written a lot of code, more towards configuration of it. So that was a part of our demo. Again, these demos are available for you. We have already created a workshop and Prashant will talk about further details, how can you absorb those as well. Thank you, and I'll hand it over back to you. All right, thanks Rahul for showing the excellent demo and you saw like how you are getting the real-time attacks being simulated and then you have the agents and the knowledge base which is giving you the recommendation on the knowledge base what you already have that's kind of based on the contextual. And also this is kind of safer, faster, and less stressful from guessing from the pop-up what you get from the DDoS attack. So here we have moved from the reactive fire fighting to the proactive defense where you get the faster responses and the recommendation. So your system is no longer waiting for something to break. It gives you the recommendation even before you get any sort of such anomalies. And here our analyst analysts are like spending more time making the strategic calls, not really digging through those kind of logs. And the best part is like network gets smarter with the continuous learning, and each event it processes over a period of time. So with that, what I want to say is like the demo that we had for the IT ops or the net tops, that is not only the one domain, but it could be applied for any domains or the system where you get real-time events which need the contextual intelligence. So you can think of that as the blueprint for the autonomous reasoning which is powered by the streaming and retrieval augment generation. So another potential in the cross-industry side is the manufacturing where you have a lot of machines which are sending a lot of data from the devices. It could be the sensor data and you want to detect the failure even before it could happen and machines are generating terabytes worth of data. On a daily basis, so agents would be continuously monitoring those streams for anomalies coming from those devices. And then when a deviation occurs, then the agent would be referencing the manufacturing SOPs, run books, or the operating guides, and then the outcome would be like you will have reduced downtime and faster recovery with the safer operation. The other potential is on the automobile industry. So let's say you are driving at a highway and in the middle of the night your car breaks down like if you're running out of fuel. So then your agent can help you route or schedule your routing to the nearest gas station or the charging station. And also if you have any service coming up for your car maintenance, then the agent can call the. Automobile industries and book the appointment for your service so that way you don't really have to spend a lot of time calling to the agencies, book the call for your service rather than it will automatically book it for you. And the third one is the healthcare. So we know like if you have the wearable devices which are sending anonymous sending the large amount of data, it could be say heart rate data or your 02 data, etc. and then the agents would be referencing the clinical guide. And the data about that particular devices and it would be maybe scheduling the appointment to your nearest hospital or to your favorite doctor if any such condition happens or maybe like if you fell down, then it could call the ambulance so that you can get to the hospital faster. And the last one which I'm going to talk about is the travel and hospitality. So let's say you are coming for the reinvent, you want to book a trip for 5 days for the Vegas, and you want to book the hotel, you want to book the flight. I know like you are coming on the work trip, but you also want to experience something else as well, so you want to book some experiences in and around. Vegas. So in that case, your agents can look at your past history, like what kind of flight you prefer, whether you prefer the morning departure or the late evening departure, what kind of hotel you're staying, if you have any membership with such hotels. It can kind of store that information into the memory and then give you the recommendation and even like it can book the flight. So here one of the important pieces is the agentic memory as well, which would be maintaining the context, and I'm not sure if you all have been in the Swami's keynote this morning. He also announced the agnostic memory memory as well, so that way you can also have the short term memory as well as the long term memory with your agents, so that can give you the better recommendation for your planning. So with that, some of the key takeaways which I would like to call out is like we often think of real-time data is just based on the speed where events are flowing through the pipeline, but the real power is not really on that speed, but it's in the context, so real-time system understands what's happening now in relation to what has been happening. In the past, so that way this context is what makes an alert, a metric or behavior fully meaningful. Without the context, I would say you just have the noise, and this is where the intelligent agents come into the picture where you pair the streaming data with the retrieval augment retrieval augment generation which converts your output into the reasoning. And also it's not like you have to start very big. It's not really a moonshot project. You can just start small. So as Rahul was talking about the single loop system, you start with one agent, one database, and then you scale based on, and you can scale based on your requirement. You can keep on adding the agents and more systems onto your framework. And lastly, you have the decision which would be made by the Agents so they can tell like why did this happen, what's the likely impact of that scenarios, and even like what should I do next. So it's kind of a shift from the dashboard to decisions and that way you can see how your autonomous agents can help you power your businesses. So with that, if I have to summarize our session, I would say like the next generation of AI agents is not only going to process the data, they're going to understand it in real time. And which itself is a journey from reactive firefighting to the proactive monitoring to fully autonomous system that can act on your behalf. And here it's not like one industry you can combine all the 3 or 4 different use cases of the industry which can give you the 360 degree view of what's happening in your system. And some of the call to Excel. So here we have some resources where you can learn about like the Bedrock agents, Amazon Bedrock, and also at the last we have the anomaly detection workshop. So the demo that we had, you can also look through that and see how you can implement into your own system as well. And then a few other resources we have with respect to the bedrock agent core and the stance agents which Rahul was talking about, so you can look through those, implement it. And last but not the least, I would say like we have a lot of skill builder course in AWS where you will be having like thousands of free learning resources, hands-on lab, and you can also prepare for the AWS certification as we have a wide variety of certificates and then you can validate those as well. So with that, I would thank everyone for attending this session and staying through the end. But last but not the least, I would say if you can spend a few minutes to complete the survey as well, because we do listen to you, we read your feedbacks what you submit after the session so that we can make our next events and the next session even more fruitful for you. Thank you everyone. Thank you.
