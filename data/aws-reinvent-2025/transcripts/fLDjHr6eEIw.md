---
video_id: fLDjHr6eEIw
video_url: https://www.youtube.com/watch?v=fLDjHr6eEIw
is_generated: False
is_translatable: True
---

Hello everyone. I have a question Who here has been woken up at 3 a.m. to look at a log for a production outage? Can I see a show of hands? I said everybody I see some people with hands down. What's your secret? Maybe you wanna come up and do the talk. Um, I know I have. Uh, and so what we're talking about today is how we can use some of our new agentic tools to help avoid that situation. My name's Kevin. I have Andrea with me here today. And we're here to talk to you about, this is a code talk, by the way, so we're gonna talk about what tools do we have available and how can we build something that helps us improve our observability posture. So I started working with observability at AWS, I think 7 years ago, and The space has changed a lot. We used to see a problem where customers did not have the metrics, logs, and traces that they needed to solve their problems. Uh, maybe a key log method was missing, maybe distributed tracing wasn't there. But that situation has changed. With tools like open telemetry and cloudwaatch being integrated into everything. Uh, it's easier than ever to produce usable observability data. Uh, sometimes just a few clicks of a button, sometimes, you know, one terraform module, and all of a sudden you have all the observability you need. So the problem has changed. Uh, what we see is that customers have too much data now to quickly solve the problems they need. You can produce all of the data and traces and logging that you need, but if you can't quickly solve a problem. You're not really getting your value for that. Uh, and it's very exciting because. In our opinion, AI is a really good use case for us. And I know you hear it's a good use case for everything, but I think for observability, we really do make a case for AI and it works really good for us. And So when we think about an agent, I'm sure you all know most of this, but, uh, we're essentially having, uh, you know, some LLM do some processing, uh, we're giving it a lot of context. Uh, we Giving it a query and then we want it to perform the programmatic work, right? We want it to do SQL queries, it to write metric insights queries, and it to process all those values and we wanna see just a natural language response. Um, I know that sometimes like I would be deploying something a long time ago and you look and you just see a Java stack trace or something and you're like, what is this, right? And this really helps that problem a lot, right? To take that and make it natural language that is saying we think this is the problem, we recommend you do this, that's a big leap. Um, but the agents don't really work great for this unless we give them access to the right data. Um, show of hands who here uses MCP tools today. I think probably a lot of you, great, yeah, most of you, um. So at AWS we of course make a lot of them, but the, the, this talk is not really about that, right? We want to think about how we use the tools. And how we can gain extra value from them. And. Uh, what we're doing is thinking about how our MCPs can access these tools like CloudWatch or Prometheus or some type of postmortem tool. And the MCP's job is to, one, Um, tell the agent what it can do, what APIs are available, and then maybe more importantly, provide like a security boundary where you're saying my MCP tool can query Prometheus, but I don't want 1000 agents on everybody's computer directly with permissions to query. Um, and so the MCP is very important for that. Um, and so how are we gonna use this? Uh, has anybody seen this screen before? Has anybody done our one observability demo? Just OK. You guys should all check that out, but, uh, essentially what we're gonna do today and what we're gonna build and Andrea is gonna show you is, uh, we have this pet adoption website with some very cute pets, uh, and these are actually real pictures from our team, most of them, and the, the, the, the pet adoption clinic, uh, is what it's called, and you could, um, it's a fictional web application where you can go and you could adopt a pet, you could, uh, bring your pet to the vets and. Uh, buy food for your pets and all different types of things. Um, so when we look at this. Let's think about. The data, right? So, uh, as applications have changed, they've become more and more distributed and so each of these services is going to produce observability data, right? Metrics, logs, and traces. So maybe, you know. You used to have a monolith where you have your logs and your data all in one spot. Now you have this explosion of data because you've took one thing and abstracted it to maybe 50 things, right? I have 3 up here, but it could be way more for you guys in a more complicated deployment. And When we see that happening, uh, we need a way to easily analyze this. So in our case, we're sending it to CloudWatch, uh, but this could be even other observability tools or other data stores, the same thing still applies. Um, and so what's our strategy here? When we build an agent. And I think this is the most important thing. We need to provide it with as much context as possible. Uh, we want to define SLOs. That's a service level objective where you're essentially telling the agent what is important to you and your organization. You're saying, uh, over the last 30 days, I expect my application. To have a latency of 30 MS for 99% of requests, uh, that's an example of an SLO. And when you provide this rich context to the agent, you're going to get a lot more, uh, data that is specific to you and your team. And then we need to take that data, we need to analyze the trends, we need to say to our agent, uh, don't just look at the current state, look at, look around the corners, right? What can, what could potentially be an issue later. And then finally, we want to make our agent better over time. So let's take an example of our pet adoption website and we. Run this agent that creates, let's say, an HTML report of observability findings, and we want to feed those findings back into the agent on each successful invocation. That way it can get better over time. And so what are we gonna actually show today or what are we gonna build? And I promise we're getting to code, right? I'm not just gonna talk the whole time. Um, but essentially we are starting with a natural language query. Uh, it could be manual or it could be triggered by an alarm, right? That would be a little bit better to have something lying in wait and an alarm is triggered and your agent gets to work for you. We have our LLM and what we do and what I think is really cool is using the strands agent. Uh, this is an SDK. It's open source. You can build an agent in, I don't know, 10 lines of code. We're gonna, we're gonna show you a lot more and not just 10 lines, but you, you can get started that quickly. It's really amazing how quickly you can build something interesting for you. And then we wanna provide it with all the context that it needs. So, uh, we wanna provide it with our operational tenants with our SLOs. We want to tell it exactly its purpose and its job and how it should do things, and we also need to provide it with the MCP access so it can properly query things and get data. And finally, we want some type of usable output. So, uh, it could be an HTML report that you send out daily to a stakeholder. It could be, um, it finds a gap and then it creates an alarm for you. It's very useful and we're gonna show that in a little bit. Uh, but the, the goal is to have some type of, uh, human understandable output from your programmatic queries of structured data. And I'm gonna pass it over to Andrea now. He's going to show you the code and show you what we built and how we think about this and. Thank you Kevin. Alright, so before I, um, swap the screen, so one thing I wanna mention is, uh, you're gonna see a QR code at the end of this presentation. So all the code you're gonna see is open source, so you can go off, download it. But before, uh, um, going into that, I wanna do a premise, uh, so the code will run, will do stuff. So make sure that if you try, you know, modify, uh, you feel stuff like you do with your pre-production environment because it's gonna create artifacts as we're gonna see. OK, having said that, uh, I wanna start this by looking at what the agent is gonna, um, is gonna create after each run, right, so what you see on screen is, uh, is the observability report from, uh, one run of the agent where what we. You're, uh, asking the agent to do is like go off into my account and find, uh, interesting stuff for me and we can see like what interesting stuff means because, uh, uh, you need to steer, uh the agent in a very specific way if you want it to be valuable so I wanna analyze that report with you so that you can actually see. What's the outcome and so we can relate that with the code that we're gonna see in a second, OK, so, uh, the report is split into a different section. The first one, is the executive summary. And so the agent is telling us that, uh, you know, we have, uh, AWS infrastructure, uh, with mainly an ECAS based, uh, pet site adoption, uh, uh, system that Kevin, uh, mentioned before. We have, uh, lambda, uh, functions and, you know, comprehensive logging. And one interesting thing that he's already highlighting is that our pet pet side deployment service, uh, there are, you know, there is active traffic and there are like some security scan happening in my system. And then the agent took the decision to create uh critical monitoring alarms for us, OK, so that's kind of like a TLDR of uh what the agent has done really, uh, so now we can actually go down and look a little bit more in detail. So we can see we have uh an alarm section here when it says like, OK, so this system didn't have any alarm. And so, uh, when I did when, when the agent ran it created 4 critical alarms apes site high latency, high fault error rate, and then, uh, a lambda duration alarm. If you think about those like it's quite, you know, impressive that the, you know, the, the agent is. You know, interpreting what we have into the account, understand what are the alarms that needs to be created and creating them, and we're gonna see how we, we did this, uh, uh, in a very, uh, in a few minutes, all right? And then, uh, definitely, you know, there is security scan, uh, again is highlighting that. Then next one, we have some key metrics that, uh, the agent is telling us that we should be looking at, and you can see here we have, uh, pet site error rate, pet site rate, and pet site latency. So now. Interestingly, if we go to the fault rate first, you're gonna see we have a low level amount of fault is 0.07%, nothing concerning, but something definitely you wanna be aware of. But most interestingly is the, the error rate flat. Time series here now there's one thing that in the in this new AI world uh people used to say which is like oh I'm gonna find something that is anomalous, but most of the time looking at something that is not anomalous it's it's as equally important as what is going wrong in this case. Specifically, the agenties has found out that we have faults, but there are no error related to to those faults. All right, so you know you can see that, you know, probably we have a, you know, a level of 500 errors like but not 400, you know, and you can relate that with your code because in general like they may be related, OK. And at the same time, uh, agent found out that we have a latency of 400 milliseconds, uh, on average, uh, roughly is also not very good, and we're gonna see is gonna, is gonna talk about that in a few, uh, sections here. Then next time we have, uh, um, an operational, uh, audit review where the agent is telling us we have, uh, an EKS application. Uh, where we have this, uh, this latency 402 milliseconds on average, uh, and, uh, you know, yeah, with the last day average, and we have a link to the console to actually go off and check it ourselves, and then the filtrate and error rate that we just saw, uh, above, you know, and they saying to us like, OK, it's 45 500 server errors, uh, are still acceptable acceptable levels because it's 0. 0.06% not, not bad, uh, but you know, definitely something to, uh, to notice and definitely there is active traffic. Then, uh, we have a lambda functional analysis where we, we have like few, uh, status updated functions we have step step functions and we have synthetic monitoring. How many of you know what synthetic is, uh, anyone familiar with it? Raise your hand. OK, for everybody that doesn't know what synthetic is, uh, is an A, it's a college service, Abr service that what it does is, uh, you can write your code and run canary testing, OK, so it's a way to do like full end to end testing to your service. You call your API, you expect the result is 200, 500, you know, depending on what you're testing there, uh, and it's based on lambda, um, so it's actually saying like, OK, so we have 4 end to end tests. That's the way to read it. Um, then sure, we have an EKS infrastructure with a cluster, you know, linked to the cluster if you want to go there. There are 2 gig of storage which is like, you know, probably something interesting. In fact, uh, um, you know, the agent also, uh, thought that it's worth to think about the college logs analysis because of those 2 gig of storage we have 1.36 that are just in my application logs, right? So some optimization costs that we can actually, uh, look at. Now operational finding, um, you know this is where the security active uh security scan has been, uh, uh, described a little bit more so the agent found out that, you know, there were like significant scan across all those 100 operations that were happening and, you know, to different common paths, right? So PHP exploitation, uh, uh, admin panel probing, there is war price v vulnerability and so on. And And now, uh, the agent tells us like what should we do, all right, so now, uh, this, this section and we're gonna see in the code is split into, into like three tiers like what, what you should right now, which, which, what you should do right now and, uh, medium term and, uh, long term and so immediate actions you say like, OK, so you should be enabling, uh, web, you know, AWS web application firewall on pet site to block all these parts, you know, the, the scanning that it's happening and review cloud trail. And then as part of monitoring uh there are like you know there weren't alarms remember right? so the agent say like OK I took an action and I created the alarms for you and we're gonna see how so this is the first step where we make the agent to do uh uh you know to operate into our observability uh artifacts and. There should be also SLOs uh uh into application signals. Now the agent haven't done this because we haven't created the tools. So, uh, that's an exercise for home, right? So I, I'll show you how we did for alarms, and you can use the same approach to create, uh, application signal SLOs. And then, uh, moving forward, uh, we have short term improvements, uh, that is our 400 millisecond latency, uh, you know, definitely it's something to look into. And long term, uh, you know, that we have a 2, almost 3%, uh, service quarter utilization so we can improve this by using, uh, uh, auto scanning policies and, and, and things like that. Now one interesting part I wanna highlight here is like a relation with industry create incident. So one of the things our the engine we're gonna build today is gonna do is, uh, it's not only going into your account, find stuff and maybe create some artifacts, but we want like what. What I want when I wake up at 3 a.m. in the morning and something is happening is like is what I'm saying something that it's well known in the industry and that's what the agent is doing and if you look at here, you know, the, the scan, the security scan that we're seeing into the system, they're related to uh WordPress, uh security vulnerability of 2022 and the agent is telling us like what was that? It's multiple requests to WordPress or WordPress log in, um. You know what's the industry context this was like a problem in 2022 and uh what is the applied solution AWS web application firewall. So effectively the framework the agent is telling us like hey this is a well known thing and you know the industry has fixed this in this way. Go off and do it and it's also telling us like how to do it like deploy web application firewall, use the top 10 rule sets, and then enable crack trail for API level monitoring and optionally we can also uh set up automated incident response uh for for those requests, right? It's pretty cool. So we, you know, we just this is just a single run of the agent that is creating this report and he found out like a few things, uh, for us. So now, uh, I think what I wanna do is, uh, I wanna show you how the agent is doing that. So it's like, are you ready for some coding here? Raise your hand. OK, so your laptop out. I'm kidding, right, I'm gonna walk you through the code and, uh, we're gonna see, uh, unfortunately we only have like, uh, you know, an hour for this session, uh, so I'll try my best like to walk through the, uh, most interesting code. And then, uh, you know, definitely, as, as I mentioned, like this is this is, uh, uh, open source so you can go off and, um, you know, download, change and try, OK, so as I talk, I'm actually starting the agent in the terminal that's what you're gonna see there running and, uh, we're gonna, we're gonna look at what is is doing, uh, bit by bit but don't get distracted too much, OK, OK, so. When you're gonna download this repository, uh, you're gonna see a folder called observability agent, OK? And in this folder, uh, effectively it's a, it's a Python agent, um, and you know there's a main file, so in this main file, uh, there is, um, you know, effectively some up, you know, some settings that we're using arguments. So effectively when we run the agent we wanna specify option the regions that we're gonna be using unless we use the default. You know whether we want this reporting markdown because effectively we wanna feed a different agent for example or we want HTML for consuming from from an animal or maybe both, right? And then uh definitely uh if you want some verbosity there you know in case we we're you're debugging and you wanna see what's going on. Uh, but effectively the most important lines of code here is, uh, is here line 52 and 53 where what we're doing is we're creating these natural language observability agent and we're actually asking the agent to run the analysis. OK, so now let's go into this, uh, observability agent and let's see what we're talking about here, OK? So, uh, a premise here Kevin mentioned before, we're gonna be using, uh, strands agent across, uh, all the code here, and we're gonna do that to create our, uh, uh, tools to use MCP, uh, uh, servers and tools and to create the actual agent. So, and you can see like how easy this is, all right. So the natural language observability agent, what it does here is, uh, as an elite, uh, um, method that it's loading the application settings, so things like regions and, and stuff like that it's not super interesting, but it's also, uh, initializing the MCP client manager which we're gonna see in a second that's. Where we're gonna use existing tools out there and connect that to our agent to announce its capabilities and we have a report generator, the, uh, the, um, the cluster is gonna be in charge of creating HTML or um markdown for us and definitely we initialize the components. So let me, uh, go one second into the MCP client manager because I wanna show you what this is really so. Few of you were not familiar with the MCP, so, uh, let me, uh, let me do a step back here. I wanna explain a little bit like what it is. So, uh, um, so we all on the same page, OK? So MCP, uh, servers, those are, uh, tools. So MCP stands for Mother Context Protocol, right? And those are servers that are, you know, in general like vended by, uh, companies like AWS, and they contain tools. To do work as simple as that, imagine this like so a way to imagine MCP tools is like the APIs for LLMs for, you know, for, for LLMs, OK, uh, so APIs are not like that LLM friendly because you know you have a bunch of parameters, but the LLM doesn't really know like. It can do that trial and error, right, uh, but MCP tools are meant to be like use cases that the agent and an LLM can actually use and to, to fulfill a, a full job, OK? And we're gonna see this again when we're gonna build our own, uh, custom tool. So there are different tools. So the idea here is like we don't want to, like, for example, like we wanna talk about, uh, you know, we wanna connect to Cloudwitch or cloud trail application signals like all those things, things like we don't want to have like a client and you know make the server, you know, the. Make, make the agent to try this API or this other API without really having the context about uh what it's really doing. That's what MCPs are for. And, uh, today in this agent we're gonna use 3 MCP tools, uh, the cloud-ish ones that contains like metrics, logs, alarms, and so on, uh, application signals for monitoring your, uh, uh, applications, and cloud trail to see logs into, you know, the cloud trail logs into your account, so. To create MCP client, uh, uh, with strengths, uh, agents is as simple as creating an MCP client and passing in the, uh, as an argument the handle to the GitHub, uh, public repository, uh, handle, right, which is Ebras labs called the MMCP server. AWS Labs called the Shop Signal MCP server. AWS Labs called the CHAMCP server, right? And you're gonna find like a lot of them into, into the AWS Labs repository. In this case we're just gonna use those three, all right, for our use case. Um, and then, um, you know, these MCP clients, so what it does is it creates the, uh, the, the MCP, sorry, this MCP manager, uh, it creates those MCP clients for, for the clients that we want, and then expose them with a get all tool functions where we return the tool that is gonna be used within the agent. So if we go back to the um. To the agent again we're gonna see um uh we're gonna see here like that we initialize the the the the components so effectively we ask the MCP manager set up the clients, the code that we just saw, OK, so use the AWS Labs repository to create the clients for us and then create agent. OK, so now before I go and show you the literally two lines of code on how you can create an agent, um. I'm exaggerating a little bit. I, I mean there are technically two lines, but there is a little bit around that. I wanna explain to you like one thing that is very important that we're doing here, right? So one assumption we're making. So you saw before in the, in the slides that Kevin showed us that, um, you know, to create an agent. We're gonna, we're gonna need to provide them some context and we're gonna be interactive with them, all right? It's a two step process really and this is exactly what we're gonna do, uh, in the code, right? So the first step is we're gonna create an agent and set up like make it sticking to some context and then we're gonna prepare a, uh, a good, you know, well written prompt of what we're doing to actually invoke it, um. And it's like one thing like in this experience of building this agent uh uh we also learned is uh writing a good quality prompt for the agent is probably as important as the code itself because if you think about this like LLMs are a general knowledge so you wanna like you know the last thing that you want is uh you run this agent it goes off, you know, maybe finds something for you, maybe doesn't, but you know every time you know it's telling you something completely different you wanna really, you know. Almost create a tunnel vision for it like you know you know about everything in the world we're talking about like monitoring here so I want to restrict your focus to monitoring and do exactly what I'm asking you to do, all right? and you're gonna see a lot of that into into the code starting from now. OK, so the first step is to create an agent as I said um. We start with a system prompt and we're gonna go back into this prompt in a second. I wanna show you how easy it is to create an agent itself. So to create an agent, all you need to do is, uh, instantiate an agent class. You're gonna pass the system prompt to it. This is gonna be the initial context who the agent is, right? Is the persona like who, who are you impersonating. And then you're gonna send a list of tools to it. So those are the, uh, tools that an agent can use and you can see here HTTP requests, which, you know, basically means you can go off the web and search stuff, uh, and that's gonna be used for postmortem search and create cloudish alarm, which is our custom tools we're gonna see, uh, in a second, and then there are MCP tools which are the tools that are coming from the MCP manager, so Cloudish, cloud trail and application signal. So now, uh, let's spend some time just looking into these prompts because I think it's, uh, it's, it's really worth it. So this is the context where, uh, that, that we set the agent on and we say as the first thing you are an observability expert so we're really remember the tunnel visioning thing like, you know, we're saying to the agent who you are, what's the persona. And we describe what are the capabilities, so discovering analyzing resources, uh, quitting cage metrics, cultural event, correlating data, and so on, uh, you know, create cado alarm using the, using the tool that we created for the agent itself. And generate uh reports. So now one thing we're gonna need to do is like if you're generic enough and say like, you know, go off in your prompt, right? So you say like go off and, you know, find the stuff for me like what does that mean for the LLM like, you know, you, you probably we go off and you know and do definitely something, but we want to be precise, remember and so like we wanna tell the we wanna tell the agent how. Is supposed to analyze the AWS infrastructure and so we say like discover the resources that exist in the region, get infrastructure and application metrics, that's one way you can discover resources like don't go off and call all the APIs for 250 services, that's gonna be mental. It's gonna take like days to run. So 11 way you can avoid that, go off into your observability artifacts, you're gonna see metrics and logs, and that's how you can reverse engineer the resources that you have, right? Uh, it's a short, shortcut for, for the agent, but you see like how important that is, and then check for existing alarms and look for patterns anomalies and then, uh, generate actionable recommendation. Now one important thing here is like we're saying to the agent context be conservative with alarm creation. Remember the agent will do what we tell him to do so if you don't, if you're not really like precise to the intent like every time this agent will run will create alarms for us because we're saying like go off identify alarms that you should create and create them, but we don't want that because you're gonna. End up creating like alarms that are like very similar like you know maybe different configurations slightly you know data point or threshold to alarm but effectively the same semantic all right we don't want that we want alarms only on genuine gaps and that's exactly what we're saying here so be conservative we want those alarms for genuine gaps to not create duplicate or near duplicate alarms, OK. Um, then, um, you know, we, we ask him like, you know, please, uh, take it, you know, uh, put some attention into the resource type, the key metrics, deep links, because we want that 11 step click to the, uh, AWS console, and, uh, we say like what. It should be focusing on performance monitoring, customer experience, customer impact, and optimization, cost, security. That's how we found out the security scam that we saw before and operational excellence improvement, you know, all the thing about like, hey, you have 2 gigs of logs, you know, there is something you can do about it. Um, So now, uh, one interesting thing here, uh, uh, as we say is like, hey, we're also providing a, a create cloudish alarm tool here so when you need to create those alarm, that's the tool you're gonna need to use. Don't go off and try to use a, a, um, a bottle client or any API by yourself. I'm gonna provide you a tool to do that. Last but not the least, be thorough but concise. So, OK, so Ryan, you know, at this stage what we've done is, uh, you know, I go back here and this is what our natural observability agent is and at this stage is first step we have the agent primed with the context. He has the tools that he can use the MCP uh tools as well, and now it's the next step where we can actually go off and run the analysis, all right. OK, cool, so let's see, uh, let's see how do we run this analysis, OK, so in this method here you're gonna see, um, this, uh, that there are two steps. So one that says like, OK, discover and analyze resources and then generate the report, OK? So. Let's go and look at how we discover these resources. So once again, here, here is the second step where we are going to be interacting with the agent and. I'm gonna skip the query for a second because we're gonna see what it looks like, but, uh, one thing that it's worth to, uh, to look at here is like how do we invoke invoke the agent. So invoking the agent is as simple as use agent as a as a method and pass on the query. That we the prompt that we wrote above above, uh, that we're gonna see in a second, OK, so what that will do is uh. We'll start the agent. We'll send in the prompt. I I will start working. We'll go find out, you know, the things, you know, using the tools and everything like that, and then we're gonna have our results, another language result from, uh, from the agent, and then we're gonna parse it. We're gonna extract and do some modification that we're gonna see in a second. Now there are two things here that are, that are important to notice, right? So. The first is, uh, when we say MCP manager execute with client. So if you go back here, uh, you know, we have this function that is, uh, maybe can be, uh, seen as cryptic at the beginning but effectively we have a context manager here that is, um. Connecting the clients to the MCP server. So remember when we say like we have agent creations step 1 and agent invocation step #2. So the idea here is like we don't want the clients to be connected for all the lifetime of the agent to the agent itself. We want to connect the clients just and only when we invoke him, and that's the reason why. When we create the agent, we, we, we invoke at all tools so we prime the agent with the client initialization, but there is no connection to the MCP server and the client itself. But then when we're actually invoking the agent here, we do agent query. That's where we actually need to execute with clients. So effectively what we're doing there is like we're saying to the initialized client to connect to the MCP servers now because, uh, I'm gonna, I'm gonna need to use, uh, the LLM, right? OK, cool. So now. Effectively this line here is what you're seeing like in my terminal here like everything that is happening it's because the agent that is being invoked and after this what we do is uh we extract the metric from the response and then we return the markdown and the metric data so now before we look into this, uh, I wanna, I wanna go back into our prompt here because you can see this prompt is quite verbose, right? and uh there's a reason for that. So this is, uh, so the prompt is what we're asking the agent to do this is the core of the thing here, right? So what should it do, um, and so the query is like analyze my infrastructure and create the report that's what we want, uh, but before we go down like that is that is something that, uh, uh, it's worth to mention and I'm stressing this a lot because it's important which is like. Agent is stateless unless you're gonna be fitting with the previous run, right, which is something that we can do here, uh, but like every time you're gonna run it it's gonna go off and do stuff we say this already and so to create a report that makes sense for humans you don't want them to be random, all right? And you're gonna find very quickly if you try to run your the agent like to to create an agent yourself without looking. Into this code that the you know the the the LLM what it's gonna do is, uh, it is gonna make like a few mistakes and a few hallucinations, for example, it's gonna go off, look at stuff and then perhaps generate metrics with with what it thinks is the right thing to show you or. You know, creating sections, you know, that are probably random, right, it's not the one that you expect, so it's not structured in a way for a human to, to read or for you to parse and send to a, for example, to an agent pipeline, all right, and we wanna fix all of that with this prompt, OK, so that's why it's very important so. What we say here is like this is a step, you know, multiple steps job. So please, first thing is you're gonna need to discover resources, find all the resources, and they are relevant metrics. There are compute resources, database, serverless load balancers, storage services, and so on. Then you're gonna need to analyze the health for each resource, go look into their metrics, values, trends, health status, active alarms, recent incidents, performance, uh, and so on, and then we need to generate deep links because, uh, the report is gonna, uh, is gonna allow us to, to go into, um, into, into the councille. And then step 4 is where we link to the postmortem, right, so the public incidents and so for any issue. Get from postmortem or uh for looking into postmortem up for similar incidents and don't do that just for the love. Go check if what you're seeing makes sense with the publicly available incidents. Don't force it, OK, because otherwise, you know, it would just say like, you know, OK, so there is this incident that maybe it's related, but there is really nothing to do with it. So we're just saying like, OK, so go look into that but only show up like if it's really required no. Then there is the alarm management section where, so this is kind of interesting because. What we do here is like, so think about this, so, so you, you know, the agent is gonna go into your account and we need to get some recommendation and perhaps creating some alarms, OK, and we need to tell the agent like how do you create alarms? Like how do you, how can you come up with an alarm that makes sense, you know, alarms are complicated, right? You need to choose the right metrics or maybe set the metrics or, you know, a query to. Uh, to, to, to alarm on, you need to find the right threshold, uh, data point, uh, missing. How do you treat that? Should the alarm be breaching or not? And so solve all these problems, what we do is, uh, we actually use MCP tools, uh, at the College MCP tools. Uh, they have a tool that is called get recommended metrics alarms. So this tool, it's vendored by AWS and it's in the MCP tools, uh, servers, and what it does is, uh, it, it provides the LLM a kind of best practice recommendation for alarming where you're gonna see like, you know, how different AWS services metrics relate to a good alarms recommendation thresholds and things like that and that's to prime the LLM and say like go check that first, right? This is our like the you know best practices alarm from AWS itself and after you do that, uh, create, you know, check existing alarms you do comparison with best practices and then uh only create new alarm if there are gaps as we say do not create duplicate alarms and then at some point, uh, we say, um, you know that you can actually use the tool that we created to, to create those alarms. And then we ask, uh, next, give me a recommendation, include actionable insights for performance, cost monitoring, uh, and security. So far so good. So this is what we're asking the agent to do, but we can't stop there, as we say, because the agent will output something that is completely different every time. So we need to parse this because I remember we want to create a report and we want that report to be as much stable as possible and that's why we say to the agent critical we're gonna need to structure this report exactly this way we want an executive summary. 2-3 sentences of uh uh of what you found active alarms and issues, operational audit uh overview, operational findings, actional recommendations and relation to industry create incidents. Remember that like in our report that's exactly the sections that we've been through, right? Uh, so, and then another critical requirement we're saying is like don't come up with some time series on your own. Use the time series that I have in college. Use the MCP tool get metric data, and then return them back so that I can actually print my. Time series like as we saw in the report and that's also very important because uh you know what the LNM will do you know in general will do is like it will analyze uh your environment and stuff like that perhaps like you know looking into some slice window and then just make up values for you which is not what we want. And to do that we're also asking uh um the agent to provide adjacent into the into the output where we have like the different, you know, the list of the metric names with the time stamps, the values, the unit and the brief description so we can uh craft that into uh into our report. So perfect. So we're back to here where uh we run the agent with the with this prompt and and so like from the markdown results that you're gonna see like in the run here um you see like uh in the in the report that is um effectively like all the section we talked about but what we need to do here is uh we need to parse the Jason. All right, for, uh, for the metrics and, and all of that, so effectively we don't want to show the, uh, Jason into, into the, into the, um, HTML report and so that's why I like what we do is like we struct this Jason parset so that we can provide to our JavaScript library and we remove it from the markdown itself and that's, you know, that's effectively what we're doing here and then we return the markdown with all the content that we found and the metric data. So at this stage, uh, if we go back copier. Um, you know, that's, that's where we analyze, uh, our results and then at this stage we're gonna generate the report, so. We generate this report with the analysis result and uh and the metric data. Now we can check uh um this sorry this uh method very quickly but in reality um. What we're doing here is, uh, we're getting the, the markdown and the chasing, and then we're using Ginger templates, so there is a mark, uh, there is a in the template folder there is an HTML, uh, which is like, you know, CSS and everything else, uh, and, uh, you know, effectively this is a Ginger template where we're iterating over the markdown and print it over and then use the JSON for the metrics and, uh, generate the graphs that you've seen before, OK, so nothing special there. Um, so one thing I wanna, uh, I wanna talk about though is, uh. We saw how we invoked this agent, but uh we want someone to see like how we're instructing the agent to create those alarms right? and that's where we actually creating our tools so if you go into the tool folder there is an alarm, an alarm tool file so this is the. Tool that is used for the agent to create an alarm. There are a few things that are important here to, uh, uh, to keep in mind. So the first thing is, uh, we need to make our tool, our our methods, our Python code to be exposed as a tool for, for MCP, and that, uh, that is as easy as annotator with a tool. A tool is a strand, uh, uh, is a strands, uh, you know, annotation that you can just use on any method that you want. And that's about it. You have your tool, right? So at that stage you can actually, you know, take it and like, you know, fit it into the agent, however. If you do that you're gonna have a problem pretty soon, right, because if you look into this create alarms, um, you know when you create an alarm as we said before like it's not as easy as like you know just create a name, you know, maybe a metric and that's about it like there is a lot, there is a lot of. Configuration like there's alarm name metric name name space and so on but there are like most importantly there are like some configuration here like for example comparison operator greater than threshold or like statistic coverage that if we don't instruct the agent on how to use them it would just go off and try like average AVG. Or something else or like you know maybe it's like all over letters and you're gonna have like errors from from the client every time and then at some point the agent will surrender and say I'm gonna try a different way, right? We don't want that and that's why although creating a tool is as simple as adding this annotation again given the context. To the to the LLM is uh probably one of the most critical thing you're gonna need to do here and that's what exactly what we do next so we say what this tool is about this is for the LLM. To know what this code is about to do and you can see like you know we we say this is for creating alarms that are common use case and then we talk about the arguments so we have alarm name metric name name space and so on and see here comparison operator don't come up with some parameters that probably won't work. This comparator operator can be greater than threshold, less than threshold, greater than or equals to threshold. Same with statistics you can only use average sum, minimum and maximum, right? And this is important. This is how the agent can go off, use the tool, one shot, it works, OK? And then we say like what we returns back and some examples of how to use the tool. And then the tool itself is, uh, uh, you know, it's a, it's a mirror of, uh, a college alarms API. So a college, uh, college alarms provides an API that is called P Metric alarm where you send all those parameters and you, uh, uh, you just create an alarm. Now this tool has been done on purpose just for simplicity. It's a mirror of the API, but in reality think about tools as like, you know, you pass only the parameters that you really need for your use case and then, you know, the, the code will do the rest, right? So you make it simple for the LLM to use it. And so here all we're doing is uh we're using a budgetary client for cottage and then we do some validation and all of that and then we create a uh a dictionary with the alarm configuration after, you know, validating parameters and then um. We call put metric alarm with that configuration, right? And this is exactly the same strategy that is used in the MCP tools. If you go off and open the AWS Labs code, you're gonna see that they're kind of like very similar to this. Some are like a little bit more complex, like, you know, for example, there is a metrics Insights with, uh, you know, metrics Insights tool which is uh. Um, you know, metrics Society is like the SQL, uh, language to query metrics, uh, for college, so that's a little bit more complex, so obviously you're gonna need to build the, the SQL statements and stuff, but like, you know, most of the common tools they're kind of like, you know, a, a, a mirror of the APIs or like, you know, with, with some, uh, flavor of it. So that's, uh, that's our agent. Now, one thing, um, one thing, uh, I wanna show you quickly is, um, um, you know, what our terminal has output to us, right? So you can see like I can go up above here. When the terminal was running and you can see like all the metrics that he that he found out, so now I actually run out of the of the space here but um you know you can see like that is uh you know application signal as MCP server which is doing like execute Oit API right? So there are like multiple invocation of the of the MCP servers here like uh or if you, if we scroll down you're gonna see um. So this is still auditing, still auditing. That's good. So I'm gonna show you something there. Yeah, so for example, like Cloudish metrics tools, this is another MCP, uh, tools where we say like, OK, so building metrics inside SQD for AWSEC2 CPU utilization. So effectively this line here. You see, like we're using the MCP tools that contains, uh, build metrics inside query that I just talked about, which is very complex if you have to do all of that, just out of the box. So I'm actually doing, you know, taking, taking all the CPU authorization across all your fleet, you know, no matter how big it is, and then get. In the metrics out like one single query you get a lot of metrics back and the agent can be instructed is uh is, is there any easy to instance that is like, you know, not performing well or like a layer into my system that's exactly how you're instructing the agent to, uh, to behave, right? and um. You know, similar log group describe blog group. I'm gonna see like, oh my log group here, you know, or, um, you know, uh, I sell those tools and all and and all of that and then at some point all it does is, uh, as you can see here is, uh. This is of my analysis. This is my, uh, report, and then. You know, uh, uh, infrastructure consists of 66 monitoring services across ECS, EKS, lambda, and so on, and is severity, uh, latency issues identified in these other services, not critical outage detected, so effectively the scan has been fixed, right? So you know this is a new run of the, of the system. Um I just wanna show you very, very quickly how you can actually uh deploy this uh in in uh your environment so one of the idea here is uh you're gonna find a uh um. An agent set up a Python file that what it's doing here is, uh, effectively what we want is this agent not running manually but, uh, perhaps like run it with, uh, you know, automatically based on an event or like, you know, even, uh, um, you know, uh, uh, periodically, right? And so, uh, there are really many ways to do this and there are like a lot of talks here reinvented talk about deployment so I'm not gonna go there but like you know you're gonna CDK uh. Um, uh, bedrock, a core, like, you know, really you name it, but one idea here is, uh, from an observability standpoint is, uh, we, we would like from the agent to be woken up before us, so it'd be cool if all the alarms that we have, we have, uh, a duplicate of them with more aggressive thresholds in a way that can actually trigger your agent to run. So the idea is like the moment I get paged at 3. in the morning the agent was already there doing the work for me so I can actually go off, look at the report and see what's going on, right? And this is exactly what these agents that Pi is doing. So again we're using the MCB clients and all of that. There is a uh uh uh a set up, uh, context prompt, but then what we do here is, uh, create cloudish alarms with human alarms for operator and trigger alarms for preemptive assessment, all right. And, uh, and you know, we, we create, we created event bridge rules to actually, uh, start up the agent itself. So this is uh one way clearly that, uh, uh, that you can run, uh, the agent, um, you know, and deploy it. We said that, uh, I think, uh, I can pass it over to you so we can actually, uh, show the QR code if you wanna look at the repository once again, like please try into pre-prod, right, as you can see, like, you know, the agent will do, uh, uh, go off and do stuff, right. Yeah, and we have time for a question too. Uh, a few questions if anybody wants to ask or talk about anything. Let me come give you a mic, so I can hear you. Go ahead. Thanks. MCP servers are notoriously inefficient with token usage. I'm just kind of curious how many tokens this report took to run. So I don't think we have the, the statistics, um, but we did make sure to carefully scope down, uh, and be very specific with the context because we ran into the same problem. Uh, so I think it comes down to, uh, being very specific, uh, in terms of what you tell it to do. Um, yeah, I don't know a token count, um, you know, for this, the runs, but we had the same problem, so you're right, um, so yeah, I think with an observability tool you really want to narrow down what its scope is, right? For us it was producing a report it wasn't, um, you know, doing everything it was saying give us an overview. Uh, and I think when you keep the scope down you can also keep the token usage down, yeah, and, and 111 thing you've seen in the code there is, uh, we took some shortcuts to make sure that we consume less token, right? So for example, the metrics inside SQD to understand if there, there is something wrong with your EC2 instances, that's one. Execution of one MCP tool only as opposed to go into EC2 MCP tools and find out all your EC2 instances and then for each one of those go into metrics. So there are some shortcuts like that that can help you, uh, you know, reduce the number of tokens quite significantly. Yeah. Yeah, no, they're Uh, sorry, how do you, uh, execute the analysis? Uh, do you execute periodically or, uh, or each time we have an alert we uh. Execute the whole workflow, yeah, so you could do either, um, so with our specific tool we, uh, you can see on GitHub you can just execute it, you know, from the terminal on a ground schedule if you wanted, or, um, you probably may have seen we came out with something called Bedrock Agent Core where you can essentially put this into, uh, a docker container image and you can just deploy it to AWS very easily. Um, so you, it could be listening for like a, an event bridge event or something like that, um, since we essentially were just making a demonstration, we kind of just we're running it manually on a schedule, uh, but there's a lot of options that are new and, uh, allow you to deploy it and trigger it, uh, easily, yeah, yeah. Yeah, OK, you, uh, test this workflow at scale, for example, if we have, uh, a platform with, uh, uh, 100, uh, hundreds of thousands of alerts per day, uh, if we, uh, execute, uh, with the web hook, for example, how, how have you, uh, to imagine the, the platform behind to support the toll, uh, all this, uh, uh, workflow. Yeah, well, definitely, um, so we did test this with a, you know, not a 100K scale, uh, that's definitely not, you know, take the repository as a demonstration of what you can do, right? But like if you have like a high scale system like 100K, that's exactly where like what we're just. Saying before, like all the simplification that we have done, like, you know, don't go off and discover resources like you like there are some tricks, uh, that you can do in AWS, especially, uh, when you, when you talk about observability like how you know use metrics insights, right? So metrics insights is like a 10K, uh, um. Metric query. So like for example if you have to analyze CPU authorization for 10K metric, it just takes like one MCP invocation right for for one specific tool. So like definitely under case is a big scale. So you, you're gonna expect to perhaps like write tools yourself, you know, to optimize for, for that scale, but like. The trick there is like try to, you know, if you use those, uh, uh, you know, like bulk MCP tools or like bulk APIs behind the scenes that's gonna make your life easier when you go at that big scale. Yeah. Yeah. So this model has a limitation through it works only through Bedrock or if I have uh my own uh open a uh keys, can I use this one with this repository? It's a Python cube like there is nothing to do with Pedrock core. It's just Python, pure Python. The strands SDK you can put in, uh, an open AI, uh, API key or, or any of them, all models, whatever you're using, yeah, OK, thank you. Yeah. Yes. It's on, thanks for the talk. Uh, really, really cool stuff. I just cloned the, the code. I was curious about the, uh, if you ran into any issues with structured outputs on any of this, since it sounds since like the rendering, it looks like it's, it's taking a lot of data that was generated as JSO from the model and, and basically putting that into the HTML output. Um, I've run into that before and I was just curious how you kind of dealt with that. Maybe it was prompting and that kind of thing, yeah, and I know a lot just started supporting that. Yeah, so, uh, we, we had to rely on templating to do it, right? So you're right, like if you, uh, essentially say like just make an output, right, it's not there yet, uh, at least in our experience, so we actually made our template and, uh, we just fill it in. It's just a standard Python Ginger template where we can go and, um, fill in the sections that it extracted from the fields, uh, from the log and metric data because we, we had similar. Um, things if you just kind of tell it to do things like the graphs would look all strange or would use some strange CSS or something like that. So yeah, we had to be more a manual in that part, uh, to get it to look proper, yeah. Yeah. We're about out of time here and we, we'll be standing outside if you guys wanna ask us more questions, but, um, please take the survey. Thanks for hanging out with us and please take the survey. It helps us a lot. You guys were awesome. Bap for yourselves. Thank you. um, thanks for joining us today. Yeah