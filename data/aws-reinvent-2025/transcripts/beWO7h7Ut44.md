---
video_id: beWO7h7Ut44
video_url: https://www.youtube.com/watch?v=beWO7h7Ut44
title: AWS re:Invent 2025 - AWS storage beyond data boundaries: Building the data foundation (INV215)
author: AWS Events
published_date: 2025-12-03
length_minutes: 54.68
views: 1054
description: "AWS storage services have evolved into the data foundation powering virtually any modern workload from massive data lakes to cutting-edge generative AI applications. This session unveils breakthrough innovations like S3 Tables for analytics optimization, S3 Vectors for AI/ML acceleration, and seamless SAN migration pathways that eliminate traditional infrastructure constraints. Discover how customers are building the world's largest data repositories while simultaneously running real-time analyt..."
keywords: AWS, Amazon Web Services, AWS Cloud, Amazon Cloud, AWS re:Invent, AWS Summit, AWS re:Inforce, AWS reInforce, AWS reInvent, AWS Events
is_generated: False
is_translatable: True
---

Please welcome to the stage vice president and distinguished engineer at AWS Andy Warfield. Hi everyone. Thanks for coming. It's really wonderful to be up here. This is, uh, this is one of the highlights of the year for me because I get to stand up and, uh, spend a bit of time, uh, telling you about all the things that have happened over the last 12 months and putting the talk together is really fun because I get to go and reflect on all the stuff we did and all the things we've learned. Um, I'm Andy. I'm an engineer across our storage and our analytics teams, um, and I'm gonna, I'm gonna tell you about some of the things that have happened this year. Um, so thanks for coming. Um. When we talk about the stuff that we build at AWS we use this term a lot, building blocks, and, um, as I was starting to put things together today and thinking about the work that our storage teams do, I found myself thinking about building blocks a lot. And when we say building blocks at AWS we mean a fairly precise thing and it's a thing that I think our engineering teams really internalize. I think it's like useful to to talk about it for a sec. So when we talk about building blocks. The services, the primitives that we build are rarely used directly by end consumers. They're often used by builders who build products on top of AWS and then deliver their own services out to consumers. And so our building blocks are polished for you as builders. And building blocks have kind of a specific shape, right? A good building block, if we are getting these right. Is something that you don't have to think about very much. It's something that removes kind of a thankless, effortful bit of work and allows you to move on and work higher up the stack from that level. And so when I work with the engineering teams across storage and data at AWS, it's remarkable to see how they approach things thinking about building, building blocks. And at the start of this talk, this is my, my 3rd year doing the storage and data talk, and it's been really, really enjoyable to go through it. I, I try to take a little bit of time to, to talk through the lens of the engineers and the, the folks that work building these building blocks. And building building blocks is kind of a neat thing because the measure of success is actually a lack of friction. Right, if we're doing things right, you're not struggling with what we have. And that's really a mindset that you see in the folks that build these systems. They would like to be invisible, right? They would like you to not think about the stuff that we do. And if you think about what that means, building those building blocks for a service like S3 or a lot of our other storage services, S3 this year is 19 years old, it's almost 2 decades old. And we've been working on this building block. For almost 20 years. And a lot of that kind of work is, it's finishing work, it's sanding, it's refining, it's identifying those places where we're still not perfect. And when I talk to the engineers on the team, they're so acutely aware of those imperfections, and it bothers us all, and we spend a lot of time working on those things. Now, the problems and the work that we have to do, year on year changes every year because the scale just continues to change. At this point. S3 stores 500 trillion, over 500 trillion objects and serves over 200 million requests per second worldwide. We process over a quadrillion requests every single year. It's just a phenomenal scale, and I've been working on these systems for 8 years at AWS and I was blown away by the scale on the first day that I started. I spent like the first few months getting in the car. Quietly reflecting on the size of the systems that I got to work with, and it's just continued to grow over that entire time. When we think about that through the perspective of building blocks and working at this scale, there are these things that are kind of in variance in how we engineer, and the teams refer to these as fundamentals, security, durability, availability, performance, and elasticity. If we are getting these things right. You will find the tools that we build easy to use, and all of these things are incredibly difficult to build, and we often have to rewrite whole sections of our systems as we hit sort of like the next levels of scale. And so at any given point we're constantly rewriting things. One example is in the time that I've worked on S3, we've rewritten almost the entire data path in Rust. Almost all of the code that touches requests from start to finish. Different than it was 8 years ago when I started. It's just continuous reinvention and improvement of the services we built. And so about, I would say 80% of what we do falls into this category of like quiet, innovative work. And within that 80%, we do announce some stuff, right? In fact, we announce quite a lot of stuff, but they, they either surface not at all because they're sort of scaling issues, or they surface as small what's new posts or little updates on refinements to the system. Since 2020 we've had over 1000 launches. It's really cool to reflect at the rate of change and all of the work that's been done. And I have to say that storage more than anything else is something that has very little tolerance. For not working, and so the teams that build storage tend to be really, really conservative, and there's this tension in the teams between going fast and being careful. And we're always working on that sort of tension and trying to deliver with velocity while maintaining the level of quality that you expect from our services. Now, I'm gonna call out one example of the type of thing that I mean with the scale of these smaller updates. In August 2024, so a little over a year ago, we launched a conditional put operation for S3. So this is like kind of a, a nerdy little feature, right? What conditional put does is you're all familiar with the fact that you can put data into S3. The conditional put adds a header on the put to S3 that says only let this put succeed if the thing that I am overwriting is the thing that I expect to be overwriting. Right, and it allows me to specify an invariant atput time that I'm moving the system from a known state to a new known state. And the reason that that's important is if I'm building a distributed application and I've got a whole bunch of clients writing data into S3 and two of them write to the same object, and those puts race, there's a risk that I silently lose one of those updates, which depending on the semantics of my application, may be quite a bad thing to have happen. And so what customers tend to do is without having a facility like this, they have to add complexity to their systems to track sort of the potentially racing puts externally with something that has that level of uh of consistency and tracking, possibly a lock or something like that. Now, by launching conditional put. We start to see it pick up, and in fact, today conditional puts are actually used for about 2% of all puts to S3, which may not sound like a lot, but it's actually like a remarkable property to see at the scale of S3, 2% of foot traffic using this flag. It's tens of millions of requests every minute. Now the team was really excited when we launched this because there was a bunch of blog coverage and we're all technologists and we really love it when people express excitement about the stuff that we build. And so we got picked up in Architecture Weekly, for example, and there were a bunch of really cool examples of people building distributed systems constructs using just S3 conditionals, doing write ahead logs and leader election and things like that. It was just kind of cool. The bit that I think we all found to be more rewarding than that, the bit that was really, really satisfying, was that after the launch of conditional put in customer meetings, we would occasionally have someone say, thank you for doing that conditional put feature. It let us delete a whole bunch of code. And it's so awesome when you sit down and you hear about an improvement that we made at the data and storage level, simplifying applications and being worth the effort to go and tear out a bunch of complexity because now we take care of that. And this is the kind of improvement that the teams are just relentlessly curious and focused on. Here's 4 other examples out of, out of the huge number of things that have happened over the past year. Um, as you heard from Matt this morning, we increased the size of the largest object that you can store in S3 to 50 terabytes. Um, we didn't stop at conditional put, we've added conditional copy and delete operations. Um, we've added an atomic rename operation in S3 Express. S3 Express is a. Performance zonal S3 and it's used a lot for file style workloads and so having a rename operation turned out to be a really valuable thing down there in terms of like having sort of the ability to close out transactional interactions at the file system level. And with S3 batch operations we've added the ability to instead of specifying a manifest of the objects that you want to work with, you can now just specify a prefix, which is a small thing. But when you have a manifest of millions of objects that you want to apply batch operations to, it's actually like a really meaningful change. Now, I've talked about the work that happens with scale and some of these small features that the teams are just continuously focused on. I, I wanted to take an opportunity to go way down to the bottom of the stack and tell you about some of the innovation that we don't get to talk about as much, but I absolutely love. And so I wanted to talk a little bit about some of the hardware and software work that happens at the very bottom of our storage services. So as you know, um, S3 is built on top of hard drives. And, uh, I don't know if any of you have heard me talk before, especially about hard drives, but I can stop right now, we can talk for three hours about hard drives. I find hard drives really, really exciting. I'm gonna spare you that. But it's, it's, they're engineering marvels, and I love talking about hard drives, um. We put the hard drives into shelves, and we put the shelves inside servers, and this was how S3 was built for the 1st 10 plus years of its life. I mean we still build in effectively this form factor. This model is called a JBOD and so there is a server sitting underneath a sled of hard drives, and we drive to get more and more efficiency in our data centers year on year. And so hard drives get larger in terms of their capacity and so. We put larger hard drives into our shelves. We try and drive the density of hard drives per server to higher levels. We're really, really trying to optimize costs and optimize efficiency. It is a remarkable thing to me as we scale out even further, putting this JBOT in a rack, putting that rack in a row, building out into a data center, that in these situations, we actually have this like metric that I'd never heard before working on storage at Amazon, which is bytes per square foot. Which is just a thing that, like, think about that later today because it's a, it's a bizarre way to think about systems like a, like a sort of aerial metric of storage capacity. Um, this thing of us building denser and denser JBots went on for years, and it got to a point where we actually decided to back off of it a little bit. Dave talked about this, Dave Brown in his talk last year. The sort of like peak JBT for us was a system design called Barge that we actually stepped back from. Barge at the time was using 20 terabyte drives. We were packing as many of those things as we could into a single server, and we were building single racks with 6 petabytes of capacity. It's a lot. We really liked. The cost basis of barge, the density, right, and the economics that it presented. But as we were moving to these like densely packed servers, if the server failed or if we needed to move things around, we were dealing with a really coarse chunk of data. And we were starting to get to the point where the server scale of physical bytes in S3 was becoming unwieldy for us to manage and to really provide on top of. And so we wanted to see if we could move to something that had a bit more flexibility. And as we talked about it, we ended up pulling in folks from the EBS team, from EC2, talking to folks that build EC2 nitro, and we decided to completely reinvent the storage construct that we use. What we ended up building was something called metal volumes. Uh, this is a picture of a metal volumes rack, and what you see is the, uh, the teal sort of outline thing is a nitro compute node. Its job is to securely virtualize the physical discs in the shelf. And the rest of what you see there is discs. And so nitro manages a bunch of discs, it securely virtualizes them and allows us to connect the physical storage fleet in S3. To our shard store host, the file system that we build that maintains our disks running on diskless EC2 instances. And so rather now than running those servers on physical servers connected to the JBods, we're moving and we're able to use whatever compute we want from EC-2. Now this is really neat. It gives us a greater degree of availability because when those servers do fail, we can simply. Remap the disks to an alternate server, and we can decouple the servicing task of working with that initial server that needed attention. And so we're able to move things around that way. Similarly, when drives fail, we can map an additional drive. So we're decoupling physical service from like the software job of maintaining availability. A second benefit that came out of the design was the ability to change form factor for the compute host that we were using to manage the drives. And so as we realize software efficiencies, we can increase the rads of drives to servers and move to smaller servers, and vice versa. If we need extra compute for some reason to perform some job or to work with the data that's on the disks, we can provision that up as well. And so the construct has really, really been a cool thing. Now it's not without technical challenges. And one of the challenges was that we added this network link between the disks and the servers. And like a lot of storage systems, we write in chunks of data on the disk. This is especially true with zonal media like SMR. And not all data has the same lifetime. And so as data is deleted, you end up with these holes in the data, and what you really, really want is large physical extents that you can write new data into. And so we have to do background maintenance in our system, defragmentation, right? This was even a thing on Windows 95, remember. Um, and so we're continuously. Rereading and scrubbing and cleaning the data on the drives to free up space. Except that when we were doing this with um with metal volumes, we were having to move all that data over the network. And so the team worked with nitro, and we built a facility that we called nitro offloads. And what nitro offloads let us do is basically extend the NVME interface and push some of these low level operations to be performed by nitro at the drive. And as a result, we get about a 60% reduction in this case in the amount of traffic that goes over the wire. And this is one example of the kinds of innovative stuff that the team's doing with this disaggregated structure of hard disks. Now we're early on, and this is like a super enjoyable thing and it's like a good illustration of the scale that we work with. We're very early on in deploying. This construct and when we deploy anything interesting in our storage services, we do it very carefully and we take advantage of a erasure coating to deploy it to a small subset of any given object at a time. So we roll it out carefully and very, very methodically limit the exposure that it has, and we're very early in that process. And when I say early, we're only 53 exabytes in to deploying this thing. Which is just like a a remarkable thing to me. Now, another really cool outcome of the metal volumes work was we found that in right sizing that virtualization compute, the network, and the flexibility of host compute at the end, we were actually able to realize over a 10% power savings end to end in our storage fleet, which was a really, really great result. Um, the last bit that I'm gonna talk about is I talk about kind of the, the core aspects of our services is S3 Express. Um, S3 Express I already mentioned briefly, but this is a, a high performance zonal version of S3. Um, I think we launched it 3 years ago now, and Express, that team just continues to innovate with Express and so we've, um, gotten efficiencies in how Express is built. Resulting in up to 85% price reductions for the system. Um, and that's been really, really popular. Um, we've improved the performance of express buckets up to 2 million requests per second. As I mentioned, we added object rename, and we've also added access point support. One customer who the Express team has been working with closely is Meta, and Meta is doing large scale training on top of AWS using Express as a direct zonal backend, and they're like an incredible storage workload for Express. They have over 60 petabytes of storage. They perform over a million transactions per second and are driving 140 terabits per second of traffic into Express, and the experience has been great, and the team has loved working with them. Now, as I mentioned at the start, a lot of the work that we do is that refining, sanding, completion work. And even after 20 years of working on some of our earliest building blocks, we're still improving and the team still feels like we're refining them. But occasionally. We get to carve entirely new faces. We get to build and introduce new primitives in storage. Two of these primitives that we've talked about in the past year are S3 tables and S3 vectors. Tables offer managed Apache iceberg tables as a structured way of storing data on top of S3, and S3 vectors are a vector database, a vector index API that allows you to store vectors and perform similarity search within S3. All of these things are built with the same sort of fundamentals that S3 has. Now, last year, I was standing about. 18 minutes ago, just back there, about to come out and announce tables, and I was wearing the anxiety of the entire team. Of here and it was like a really, really like remarkable moment because S3 is, is a service that we all are so, you know, invested in and we like have a lot of pride in in the way we build it and we run it and we were extending its identity in some senses, right? We were like. Changing a little bit what the surface was for S3, and we were worried that while we'd talked to a lot of customers about tables and we'd had the feedback on Iceberg that this was like exactly what the largest Iceberg customers on S3 wanted, that changing S3 from being an object service to being a more general storage service that also offered other data types was a thing that that maybe wouldn't like land as well as we hoped it would. And so I remember sitting back there and like sweating about how the delivery of that slide was going to go. And in fact what happened was the response was really remarkable. In fact, there was sort of a tone of of impatience from some folks around the fact that like what took so long to get to this, and people immediately. Had this reaction to the launch of tables that I think the team really learned a lot from, which was that Your definition of what it was to be a table on S3 was much closer to that list of fundamentals than it was to a specific API like objects. And that's really where we've kind of come from as we've approached building vectors, which is what I'd like to talk to you about now. Now This is a really neat, um, sort of story. Um, when we launched vectors in July, we launched it as a preview. Uh, it's probably been the most popular preview that we've had, uh, in our storage services. In the 5 months that we've been operating the preview of vectors, uh, over 250,000 vector indices have been created. Um, over 40 billion vectors have been ingested into the system, and we've served over a billion, uh, queries in total. And so it's really, really taken a lot of, um, a lot of sort of action. Um, now why? Why is a vector API in an S3 form factor interesting? Why would the S3 team think about building a vector database? What's the story here? I think a useful way to think about the value and the, the sort of like interest that we're seeing with vectors is that the way that we work is changing, and I, I, I hope this like kind of resonates with folks. Um, I'm a, a huge music enthusiast. Um, I'm planning a summer barbecue. I had to come up with like a little bit of a contrived example for this, so you're gonna have to bear with me. I've got a CD collection and I am building an index of my CDs in, um, parquet in a table. I'm gonna write some code for this thing. And so I sit down. And I'm going to write this code, and if we imagine what it was like to write this code, right, to go and scan this list and pick the songs to play and maybe even play the songs, um, even 2 or 3 years ago, you know what this experience is like, right? I sit down and I write the code. I write the software that tells the computer how to do exactly what I want. And I tell it exactly what data it should use. And I'm sure we've all had the experience, especially over the last year, moving into these more agentic-driven workflows and using Gener AI for development, that the way that we interact is changing and is upleveled a little bit. I asked for a much more ambiguous thing, right? I asked to build the mix of 80s songs for my summer barbecue, and as we're all familiar with, this is going to go through some kind of like chain of thought reasoning process, and it's going to produce the code. And at this point. Producing that code is a reasonably unsurprising result. Right, at first it was shocking, and now it's just like, wow, it's like there's the code. The bit that is really interesting here from a storage perspective is not that part. It's this part. It's that the agent hopefully sees an awful lot of my data. And the agent needs to decide that to answer that question, it's gonna have to go and use that table of metadata for my CDs and the bucket of music that I have. And it's worth reflecting on the fact that this thing that I did as a developer of like knowing that I had those two data sources, of plumbing them into the software is a thing that I really take for granted that like my brain's pretty good at and most data structures and algorithms in computer science actually have a pretty hard time with. And this idea of finding the right data without pre-indexing, without knowing exactly what has to be done, is where vectors step in, right? That ambiguous problem of choosing the right data to answer this question. And so when I go back to my CD collection, I've given it this metadata. I've given it the columnar structure of artists, track, albums, release dates, things like that. But there are so many other things that are properties of this music that are important in thinking about it. There's like the tonality of the music or whether it's upbeat or whether it was in a commercial or things like that, and. I can't conceivably go through and label all of these things upfront. These are just semantic concepts about the data. And so this is the thing that's really changed and been remarkable recently, that I can take these things. I can turn them, the media, into a vector representation, which is really just a list of floating point numbers. And I can embed that vector in a space. This is a 3 dimensional space. When you ask a graphic designer, even a very, very excellent graphic designer to draw you a 1500 dimensional space, they get really upset at you. And so we decided to stick with 3 for the talk. The concepts that you see called out kind of map to clusters inside the vector space, neighborhoods. And now what I do is I use an embedding model, the same embedding model that I used to place those vectors in there in the first place. I take my query, I generate a vector that is a point inside the space. I do an approximate nearest neighbor search, and I find the results that are similar to that thing. And this is how a similarity search works. It's the basis for retrieval augmented generation. It's also used in a bunch of other emerging fields outside of AI like identifying interesting images in radiology. Doing fraud detection and all sorts of other really interesting things. Vectors are very much emergent as a neat sort of tool. Now, A tricky thing and part of what really drove the S3 team with the work that they did on vectors was as we looked at the size of vectors relative to the size of data, we found that there were kind of some interesting properties. If you were working with photos or video, and these are really hand wavy kind of numbers that I'm using here, there's all sorts of assumptions that you can kind of cook. To how you build your vectors, but ballpark, 0.1%, the size of the data is the size of the vectors that you have to store. Vector databases typically have to be in DRAM or SSD, and I'll come back to that in a sec. When you start generating vectors on text, you find that text is a very dense source of information relative to its storage size. And so you often find that vector indices for text are actually larger to the tune of 2 or 3x the size of the text that you're storing. And since that vector index has to be stored in more expensive storage, it's quite an explosion over that type of data. And this text data that people are indexing is everything from PDF documents to call transcripts from call centers to code to things like that. And so it's a really meaningful thing. And this was a reason not to use vectors. And so a lot of what drove us into the work with vectors in S3 was wanting to get to an S3 cost structure for vectors that made this very useful indexing structure available to any data that anyone wants to bring it to, at any scale. Now today we're launching vectors. Matt talked about it this morning. The 5 months of preview was incredibly helpful for the product and the team. We worked so closely with the customers who adopted vectors on day one. We work to improve the latency of queries down to a design of 100 milliseconds, um, which is a big improvement over where we launched. That was one big bit of feedback. We've increased the maximum index size to store up to 2 billion vectors with 10,000 indices per bucket, and you could upload at a rate of 1000 vectors per second. Um, now, I'm gonna tell you a little bit more about vectors, but I thought it would be good to take a second and let you hear about, um, the experience of using our storage services from a customer. And so I'm just gonna show you a little clip here. Please help me welcome Cedric Fernandez, um, the VP of engineering at GoPro, um, to the stage. Thank you, Andy, there you go. I love that video, um, just the pure joy on their faces coming down the waterslide, etc. is just so uplifting. Love it. Good afternoon everybody. If I were to ask you about GoPro's line of business, You are more than likely to say, you guys are a consumer electronics company, you make action cameras. And you'd be right, we've sold 50 million plus of these devices till date globally. But we're so much more than that. Our mission is to help people live and experience life's moments and then share that with their friends and family. And so in order to achieve that mission, we've got to deliver on a GoPro experience that goes well beyond capture. That helps you offload that content, helps you manage that content, helps you discover that content effectively. Tell beautiful stories in an immersive and evocative way. And share that with your friends and family. And in order to do that, since 2015, we've been working on this platform. It's cloud-backed, runs across multiple devices, etc. but it helps you do this in a painless and seamless way. That's the GoPro experience we want for our customers. Now, if I were to ask you similarly, What do you believe people capture with their GoPro cameras? You're more than likely to say action sports because that's what we're known for. But interestingly, people also use these devices to capture all sorts of content beyond action sports. Whether it's a backyard birthday party, religious ceremony, bar mitzvah, recently, in my own case, my daughter just graduated from high school, capturing her graduation party. So we have a lot of diverse content in our systems. It's not just divorce, it's actually also long form. You all use your cellphones, mirrorless cameras, etc. to capture content and your life's moments these days. The GoPro experience is very different. What we found is people follow what we call this fire and forget model, right? You go out, you turn on your camera. You let it run, you live your life, and then You know, hopefully you've got some beautiful moments that you can then share with the rest of the world. So in that scenario, unlike your cell phones, where you're actually framing the shot, you're composing the shot, you let your shutter go, it's very different. So we end up with all of this tremendous long form content in our systems as well. Take for example A surfing session, right? So you go out, you've turned your camera on, you're paddling, you're waiting for a wave, if you're lucky, you catch a really good wave. Maybe 1015 minutes out there, you're capturing content, what you really need is those 5 seconds or the 7 seconds of you riding that wave. Right, so That can be a very painful experience. I've gone through this. I'm sure many of you have as well, where you've got to go through reams and reams of video to just go try and find. Those special moments And that's the experience we want to solve for at GoPro. So, why is all of this relevant to a conversation on S3 and, and S3 vectors? Well, we have a lot of content. 450 + petabytes of media today sitting in S3. a billion plus media assets, we're growing at about a million assets or so per day. Now, as an aside, we've been able to scale out the way we have over these years because S3 interestingly also follows this fire and forget model that we experienced with GoPro. We don't have to worry about SSDs. Andy was talking about that earlier. I am so thankful that's his problem and not mine. Uh, um, but yeah, we've been able to scale out, I mean, honestly, without even blinking an eyelid. Uh, so it's super helpful for us. Now, our challenge, given that we've got all of this content, like I said, is how do we make sense. Of all this immense content. In a cost-effective and scalable way for our customers. That's the challenge, that's what we need to solve for at GoPro. Now, in the past, we've done this through numerous mechanisms, computer vision models. Machine learning algorithms, we have a number of sensors on our camera that's collecting a lot of information that we can leverage to identify certain key moments in, in videos. We've used those things and. These are all very complicated pipelines. They kind of do the job, but they're very complicated pipelines, and they're missing a very key component for good storytelling. Now, you all know, those of you who capture video and want to tell stories. Everybody tells stories differently. The way I tell a story of a particular moment in my life might be different from how one of you tells a story, even if it's a very similar moment. And in order to achieve that level, Of experience, context is super important, and the key ingredient we've missed for a number of years while we've tried to achieve our mission. It was really hard to bring context to that experience. Well, fast forward to today, that's the promise of AI, right? Um, LLMs. Agent Core from memory, vectors, it's all about vectors, baby. These days it's all about vectors. All of these things put together allow us to achieve that. So we're really excited about what these technologies can do for us and, and for our customers, and we're really looking forward to going and making this happen in the future. Interestingly, it's a mouthful. You look at LLMs, vectors, multimodal embeddings. But when I look at the architectures we're deploying today with these capabilities, They're much more simplified than the architectures we've actually employed in the past. So, lots of opportunity here and very excited about doing this going forward. Thank you. Andy, back to you. It's so cool to see that, um. I've, uh, I have 3 kids that ski with GoPro cameras and I'm very familiar with scrolling through GoPro film to find the air in them and so this democratization of um data structures that let us go and search in really complex media like video I think is, is like a really, really interesting thing and I think that we're gonna see a lot more out of it, um, as I sort of hinted at before. This is not just an AIML thing. Um, the, the use of, um, similarity search in these like high dimensional spaces is something that we are seeing, um, customers, especially in healthcare and sciences, um, using to do like structural search on molecules, uh, as I mentioned, looking for differences with radiology images. It's something that feels very young. In terms of the computer science that's behind it and it's something that I think is like super exciting to see that that both the algorithms and the applications moving forward so quickly and I, I think that we're probably gonna be talking a lot more about the kinds of innovative stuff that you are doing with vectors in the coming years. Now I wanted to tell you a little bit about the implementation and how we do this in S3 because it's kind of cool, um. As I told you, a vector database is usually an in-memory construct, and it's because to index these high dimensional spaces, you typically build a graph, right, you typically take these vectors and you add metadata in the structure. If you can imagine using a tree to search lower dimensional data, you build a graph that points to other vectors that are sort of nearby in terms of a distance metric. And as a result, finding nearby vectors means hopping back and forth to memory a whole bunch. Now that works great when your data's in memory or on fast SSD. It does not work nearly as well when your data is entirely in S3 and you have higher hard drive style latencies to access that data. Your vector searches end up taking like many seconds or even minutes because of all the round trips to storage. And so to drive the cost structure down and to get the elasticity that we wanted, we had to approach the design of vectors as a sort of S3 problem. And what we ended up doing was observing that a lot of the vector data structures use a clustering or a a sort of neighborhood style top level encapsulation of the vectors, right? They group them into nearby groups and vectors may exist in one or more of these. And when you do a query, you go find the relevant neighborhoods and then you search inside it. And so rather than using S3 as a latency sensitive store, we shifted to use it the way that it works really well, which is a widely parallel throughput sensitive store. We took these neighborhoods. We encoded the neighborhoods as objects, and when you do a query to S3 vectors, the query will actually promote a number of those neighborhoods into faster memory for the duration of your query. It'll search locally and in parallel across those objects, and it'll leave that data resident long enough for you to handle any subsequent queries that come by in the next little while, and then eventually it'll go back to using the storage basis of vectors. And so the system is kind of a co-design between a traditional vector database and a really, really throughput optimized backend. Um, we borrowed ideas from some of the implementation under S3 tables. It's structured as a log structured merge tree underneath. We do a lot of aggregation of data in the background and continuous indexing. And so the system's really cool. Um Another example in addition to what we heard from Cedric about GoPro's use is BMW, and there's, there's a pile of, I had trouble choosing the sort of other customer story to tell you about with vectors, but BMW has built a vector integrated data lake for understanding their own product quality. And this is a really cool system in that the BMW team doing analysis against their data is doing natural language search, and so they've built a natural language search interface that sits above both Athena for querying structured data and S3 vectors for querying less structured data, and then fuses those things together and is able to let the analyst team quickly work through enormous amounts of performance and historical data. OK, So now, we'll go back to last year's launch of Tables and talk about what's happened on that since then. Um, Tables was also a really, really, um, wonderfully popular, uh, launch. We were really happy with the feedback that we got on it. At this point, there are over 400,000 S3 tables, um, being hosted on the system. Um. An example customer for SV tables is Indeed, uh, who already had a large iceberg-based data lake. Um, uh, Indeed helps people find jobs, and, um, in order to do that, needs data to be very, very recent, um, and up to date. And so every day they ingest over 0.5 petabyte of update data into the system. They maintain an 85 petabyte data lake, um, and are continuously serving queries, uh, to help their customers. Uh, Indeed is in the middle of a big migration into SV tables and is, um, is, is being driven to do it largely because of both the performance acceleration and the reduced both cost and operational effort of running on top of tables. Now I need to rewind back. I guess I'm using CDs, maybe I can't say rewind, uh, skip back a few tracks and go back to my example of um of my CD collection. So this is not a bad example of uh tables-based database, right? Like when we're talking about tables in Iceberg, we're talking about structured columnar data. Um, and for years and years and years people have been hosting these types of data lakes on top of S3. MapReduce jobs ran against Parquet and S3. The thing that has kind of changed over the past bunch of years and what led us to build tables was that the more powerful the analytics frameworks became running on top of this data, the more people wanted this data to be mutable and act like a database back end. And that meant being able to do inserts of new data, for instance, adding rows to the table, doing upserts, deletions, all the changes that you'd expect to be able to do by your table store at the back end of your database. Now, when we look at what's happening in Iceberg under the covers of that workload, it's important to realize what it takes to build a construct like that on top of S3. Remember S3 is immutable, and so this mutable construct that's built on top of S3 is effectively a file system being built on top of the object store. And so my catalog is a bunch of layers of metadata, pointing ultimately to data stored in parquet. And as I make changes to my table, those changes, And I've kind of simplified this diagram a whole bunch, but those changes are gonna result in new metadata files being written, pointing to both the new parquet files and the old data inside the table. And as that proceeds, um, I will accumulate more metadata, snapshots of the database, and more parquet files. And so when we were first sitting down to design tables for S3, we talked to founders Iceberg, the folks at Netflix that that were very passionate about that work. They shaped our design enormously. We worked with some of the largest Iceberg customers that were using Iceberg in production at the time, and the thing that we heard from a lot of those. Customers was they loved the flexibility, the expressiveness, and the mutability that they could get on top of their tables. They did not love running background tasks and the maintenance that was involved in doing garbage collection, cleaning up these small parquet files into larger parquet files, which is required to maintain performance. And so customers were asking us. To make tables behave more like they were used to S3 behaving in terms of being a first-class storage system for tables. As I said, the expectation at launch was even ahead of what we'd managed to deliver at the time, and the team has been trying to fill the shoes that you set out for us, or maybe that we set out for ourselves at launch, and the number of launches through this year has been absolutely incredible. One launch I think the team was really proud of, that was earlier in the year, was we were able to achieve a 90% compaction price reduction for maintaining those tables. And so when we first built tables, we were running compaction with our own isolated full scale spark clusters, and the team has worked ruthlessly to just continuously optimize this. We moved our compaction to run on top of a lighterweight BM container in the form of a firecracker, and we've continuously worked to optimize and basically own that compaction code to make it as efficient as we can. So that's been one improvement. There are a few new improvements this week that I'm really happy to share. One of them is cross-reg replication. Um, with cross-region replication, customers have asked to be able to replicate their tables across regions. Pretty simple thing conceptually, um, it's something that everyone's accustomed to doing with objects in S3. Now. This is something where, and we find this with a lot of stuff with tables, the feature is easy to explain, but subtle and a little bit complex when you think about these data structures under the cover. And the reason that replicating snapshots or views of your table across regions is tricky, is that when we replicate objects, um, we kind of have the freedom to do it in any order. Right, we, we replicate objects with RTC we provide a guarantee of how quickly we'll replicate, but as the objects arrive, we move them in the background as a massive batch operation. With tables, if you do that, you can cause trouble, because the table internally has data dependencies down that snapshot. And if I replicate that top level of metadata, and I don't replicate the metadata nodes that it points to, my query will actually fail because the engine that I'm using, Spark or Pi Spark or whatever, can't find the files or the objects that other objects point to. And so the team had to innovate to clean this up. And we built something that I'm gonna refer to as DLWKC consistency. Um, this is not a super technical term. It stands for don't leave without the kids in the car, consistency. And what this really is doing is, is pretty much what you'd imagine, right? We, we wait with publishing that top level node until we know that everything is there. Until we know that everything that sits below it is there, and then we make the top level of the snapshot visible, and now your snapshots atomically show up as groups on the remote end and you can trust your workloads to run against them. And so, you know, everybody's happy. Um, a second aspect of what we've done is intelligent hearing. Um, intelligenteering launched 8 years ago for S3. Um, I'm sure you're familiar with S3 storage classes. Um, when we launched intelligent hearing, we had heard the customers loved the gradiation of storage classes that we have for data in S3, um, but it's a bit of work to manage which objects should live in which one. And so intelligent cheering tracks the behavior of your objects, and as your objects remain cold for over 30 days, we will move them, um, into an encoding that's more efficient and allow you to get a cost savings, and similarly at 90 days. And, If you go and access the objects, they're promoted back up to standard, effectively. When you run um these OTFs like Iceberg, those background tasks act as a bit of interference for that. Because if the background cleaning tasks need to go through and touch those old versions of your tables, they end up pulling them back up to front. Right? And they end up not playing well with intelligent tiering. And so the team sat down and worked out how to make those background maintenance tasks something that we could schedule in the background, like genuinely get out of the way. And we were able to structure it in a way that let us keep the cost of that out of the way. And so when you turn on intelligent tiering for SV tables, background access to the tables to do garbage collection and compaction does not change the tier of storage that your data is stored in, only primary access, which has been a, a really cool innovation on the team side. Um, finally, we've also rolled out, uh, support, um, as we start to fill in features for Iceberg V3. So, um, roll lineage, deletion vectors are into SV tables. There's more to come and, um, a bunch of really great engine support for this. And so that's the update on SV tables. Now, SV tables, objects, vectors, and tables as these like core data constructs. Have had this really neat knock-on effect in terms of how people are using storage, and one thing that I hope resonates with all of you as you think about your own systems is one property that we're seeing a bit of work that almost every customer, especially with meaningful amounts of data on S3, you see it in in Cedric's talk, um, what, what so many folks are realizing is there's enormous value in their data. And there's enormous opportunities to grow into new analysis, new applications on top of it, and do new stuff, but they have to cure it, right? They end up building effectively metadata layers over the data that they have and in probably, I don't know, like 60 or 70% of the S3 customer conversations that I have um. Somebody is working on this kind of thing. They're building this type of curated layer for metadata. And so last year, and a lot of like the thinking that drove us to launch tables in the first place was we wanted to remove the undifferentiated heavy lifting of being able to build those metadata layers and also get you to a metadata structure in Iceberg and in vector APIs that you could trust other tools would be built to use, right, to further amplify the effort of collecting your own metadata. And so at Reinvent last year we launched S3 metadata. At Reinvent Last year Metadata was a captured journal of mutation events inside a bucket. Over the course of this year, we've continued to mature metadata, and now it presents a full both journal and inventory view of your bucket. And so you can turn on metadata, and this is the uh the the new data notebook that that we've launched over the past few weeks, and you can pull up that table, you can. Interrogate properties of all of your objects using SQL queries. And so you can interact with the contents of your bucket or prefix in S3 using queries, which is really, really accelerative. You can also use natural language queries converted to SQL to access information about your bucket, and that's the thing that we're seeing folks do all sorts of cool stuff with in terms of understanding cost, understanding where they're growing, and things like that. This primitive of a managed table, which is what we use to build estuary metadata. It's a read-only table where the service manages it and fills it in with the with the content, and so in S3 metadata, the content is high integrity, right? It actually reflects what's in the bucket. You can't edit it, which allows you to have confidence that it is what you think it's going to be. That pattern actually was really, really popular across customers and across services. And so we were almost immediately asked to add it to other things. So what you see with the announcement this morning of Amazon Cloudwatch logs being now available inside a managed SUV table with uh SV storage lens, Sagemaker Unified Studio, and of course the SV metadata data in there is a set of effectively AWS service or system tables that have state about your use of services. And the really neat thing about this is now you can do joints across all of that data. And so I'm not gonna call this a simple SQL query, um, but it's not bad. Um, but you see here, like an example query where I'm actually joining across, um, access logs to S3, uh, VPC flow logs, and the objects in S3. And I am triangulating on 403s on access denied errors, not 403s, um, and mapping those through to the VPC and the account that ran into the access error. And so it's like a really powerful tool to be able to do this kind of debugging directly with SQL against logs that you already have access to. Um, OK, the last thing that I would like to talk to you about today, um, is enterprise migrations. Enterprise migrations are something that continues to be an absolutely fascinatingly complex challenge, and it's something that we still, you know, now 20 years in, often find ourselves supporting customers on, moving between enterprise data centers and the cloud, and from a storage perspective, storage often is the kind of like critical component for those migrations. An enterprise that has, you know, admins managing loads of servers, often in VMs, often finds that it is not trivial, but it is workable to move those VMs and application workloads into cloud compute constructs. But the storage admin has a very complicated job, and they have lots of bindings and mappings of that storage data across the enterprise, and they're familiar with the constructs that they have in terms of filers. And so we launched AWS FSX as a set years ago of managed storage offerings, managed filers that you could use to maintain the same experience of running the enterprise filer in the cloud. We have FSX for um ZFS, FSX for NetApp ONTAP, um, FSX for Windows storage Server, and it's a very, very popular way to initially land data in the cloud and start to build out new workloads. Now as FSX continues to grow in adoption, a pattern that we have heard in a lot of customer conversations is they love the ability to replicate that data into the cloud, take advantage and burst often workloads into the cloud, but they'd like to move into cloud-first development. You'd like to be able to take all of that data that you've now replicated. And light it up under other applications that are written in the cloud, often written against object APIs. And so things like Amazon Athena, Bedrock, Bedrock Knowledgebases, Glue, and Sagemaker, and bringing those tools to bear against your existing enterprise data. And so earlier this year, we launched um S3 access points for FSX and what this lets you do is stand up an S3 endpoint in front of that existing FSX data and effectively attach to that data as if it was S3. And today we're extending that initial launch of access points for S3 on Open ZFS with support for NetApp ONTA, which is obviously very, very broadly deployed and an incredible filer for enterprise data centers. And so with that, I'm gonna wrap things up. Um, I hope that, um, I hope that I'm able to tell you a story about the quiet velocity of these teams. Um, it's really, really wonderful to learn about the ways that you work with data and build these building blocks that help you move faster. And I hope the sense that I'm able to convey to you today is the fact that 80% of what the teams do is like literally listening and looking for those sharp edges and continuing to polish, and I'm sure we'll still be doing that polishing and sanding 10 years from now. But the work that we've been able to do in this moment of really, really enormous change that we're seeing over how we build over the past few years has given us this opening to grow and offer new primitives that are even more meaningful, I think, for the applications and the way that you build and so I really hope that you go and play with them and I would love to hear what you learned. So thank you very much for coming today and enjoy the rest of the week.