---
video_id: wZ8yX8OkJbQ
video_url: https://www.youtube.com/watch?v=wZ8yX8OkJbQ
title: AWS re:Invent 2025 - Building Customer Trust with AI Agents: Transform Your MSP Operations (PEX318)
author: AWS Events
published_date: 2025-12-03
length_minutes: 57.58
views: 572
description: "As Managed Service Providers (MSPs) evolve to meet the growing demand for proactive cloud management, AI agents are becoming essential collaborators in service delivery. Through live demonstrations, you'll learn how to build and deploy incident response and infrastructure monitoring agents using AWS's agentic AI capabilities. Follow along as we will walk through the code required to create AI agents with Strands and integrate them with AWS services and third-party solutions through MCP servers t..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

So thank you everyone for joining here. We are really excited to have you all here uh as we dive deep into the exciting world of Agene AI and its transformative potential for MSP. My name is Bapaditya. I go by Bapa. I'm the global tech lead for AWS MSP partner program, and with me I have Jing. Jing, we're going to introduce herself. Yeah, hi everybody. Can you hear me OK? OK, good afternoon. My name is Jing Jing Ning. I am an AWS, um, I'm an AWS partner solution architect Arg. I lead a team where we're focusing on AI powered automation and partner AI transformation. This is one of the use cases that we're partnering up with Bapa on. Excited to hear your feedback on this, um. So hopefully we get a good use case or two for you to. So, throughout the next 15 minutes, we are going to dive deep into the exciting world of Gen AI and show you how you can harness the power of AI to not only streamline your operations, but how you evolve your services that you offer to your customer. So we'll be covering a range of use cases and examples that will spark your imagination and get you really excited about the possibilities that lie ahead. So, before I dive further into the content, how many of you are here AWS MSP partner? Can you please raise your hand? OK. How many of you are partner but not into the WSMSP program? OK. And the last question. How many of you here have used agentic AI or genetic AI to build the MSP use case? OK. Very few. OK, we, we should talk, OK? We need to learn what you are doing so that we can see if there is any joint uh GTA motion can be developed or not. So, thank you. So, before we dive deep into the uh uh real session code talk of the session, I want to give some context about the evolution of MSP over the last 30 years. So, if you see this slide, there's a two S curve. So, the NS services is not a new thing, OK? But it's really a kind of changing dramatically as with AI and Agenic AI. If you see that we started, the MSP started with the standardization in the 90s and then moved to Linux and DevOps in 2000 and 2010. And that was more of a people-driven operations, OK. Labor RPGI shadow. But now, we are at a very pivotal point. There's a new curve uh getting developed, and it started around 2015 with RPA, process mining, and AIML. But the real game changer for this evolution of the new S curve is a genetic AI and a genetic AI, which you can see at the very top. And it is going to transform how MSP operate in the market today. So the idea here is to not kind of, you know, replace humans doing the managed services. The idea is to build a cohesive strategy where human and AI can work together. Next, there can be any number of use case that MSP can build with Agentic AI and AI, uh, based on what you do and what the offerings you have for your customer. And here are a few examples. The examples that we have selected here are pretty much production and really delivering some significant impact uh based on what our partners are telling us. So you can see the AI ops, dynamic runbook generations, root cause analysis. These are very standard and kind of, you know, should be automated if it is possible. And this is kind of transform how MSP handle the operations, the day to day operations. On the bottom layer, we have knowledge knowledge-based management, health-based automation, and smart companion. So this is knowledge base is all about building, uh, breaking the silos because each team has different process, tools, knowledge base, how AI can bring all the knowledge together, and then the help desk and smart companion is going to show you that not every ticket needs to go to level 2. The level 1 guy can solve the ticket if they are really equipped with their AI and generative AI solutions. And in today's court talk, we are going to cover two of the use cases from this particular slide. Here are some really kind of very compelling data, the early data that our partners are sharing with us. Number 1, 45% of the, there's a 45% reduction in average ticket, which required manual intervention. So these all data are coming from our MSP who have implemented AI and entic AI into the day to day operations. But we are still early in the process or early stage, but these are impressive results that our partners are telling us. We also see, number 2, the 30% average cost reduction in the customer was reported by customer. And this is because the AI is really smart in kind of resource management and automated cost management. Last but not the least, 60% improvement in the MT tier, that means MSPs can solve tickets much faster, which is, which in turn helping customers to retain them for the years and improving their customer satisfaction. And when you are achieving all these details, you are not kind of, you know, putting all your people doing some basic work because now the agent can take care of all those things and your people can do much higher complex work for your end customer. So the first use case we are going to show you today is the AI ops. All about doing from incident management, event management, and the problem management. How you find the incident and solve it. So think of DNA life of MSP support organizations. So the red line in the in the middle, just consider that as a service desk and your end customers are calling, Hey, my user ID is not working. My database is not responding, my UI is not opening, my API response time is slow. Today, what this team is doing, taking all the information and creating the ticket for the level 2 and sending the ticket to the to the downstream or level 2 or level 3. This is what we want to change. We are going to empower the level 1 and level 2 team with the agent AI layer so that they can be capable of solving the ticket by themselves, and not all tickets should go to level 2 or level 3. They should fix themselves. And this is what our demo will look like today. We are, we are going to dive deep into the solution. So, here's the call flow of the solution that I am referring to. For example, let's say as an MSP you are managing a uh web applications. And suddenly something broke and your service desk or level one got a call from the end customer that that something is not working. So, in a demo that we are going to show you, so there's a UI applications where your agent can log in and ask the question, Hey, what is wrong? My customer is complaining that my application is not working. So then the agent will go and talk with her. There's a supervisor agent who is kind of orchestrating all the subspecialized agent doing multiple work. The supervisor agent is going to understand the intent of the query. That is coming from the end users, that there's a problem or something is not working, and the supervisor agent is going to talk with a different agent to see what needs to be done. And the supervisor agent is responsible for routing, orchestration, as well as remediation, and it is powered by Bedrock Knowledge Base and all the LLMs. Here is a set of specialized agents responsible to do multiple shots. For example, some agent will go and do the trouble shooting. Some agent will go and check the cloud watch alarm if there is something alarm. Some agent will go and open a ticket in Jira platform. Some agent will go start the trouble shooting and the remediation. Once it is done, the other agent agent will go and close the ticket in the Jira platform. So this is a kind of high-level call flow, but I will ask uh my colleague Jing to dive deep into the architecture and show you the real code that how it is possible. Awesome thank you Papa. Uh, before we switch laptop to show you the code, I wanted to set up the context over here so we will see a cute puppy picture that's actually hosted on static website hosting. We got an API endpoint and that we're pulling random images out of Bedrock Nova so that's a simple set up and then what we're. Gonna do is by looking at how multiple agents are working together to solve a problem when we say now I'm gonna add an APID policy where the users will not be able to see this cute puppy picture anymore. So instead of human intervention to all that whole thing, how we can automate it with the coordinated system. So before I switch. laptop. I just want to show. I'm gonna talk through every single all this function over here, but a high level, I want everybody to think through how an AI, AI agent would work. There are 4 aspects to it, right? So you got to be understanding what the problem is. So we start with how AI understanding layer is. And then with that. We've got to say, OK, I understand the problem. I understand how to route this, what specialist agent to pull. Then the next thing is you got to give enough context or knowledge to say this is what you equip the brain knowledge to go ahead and fix this. So we're gonna go from AI understanding layer to knowledge. Integration layer. So once they understand what the problem is, got enough to know how to solve it, then the execution plan execution layer, we're gonna, that's when AI agent is gonna transform all of that into actual actions. So those are the three things we're gonna do. And putting all this together is a workflow coordination. Um, so I'm gonna go ahead and switch laptop to show you the setup, uh, but 3 things I'm gonna do. Number one, I'm gonna show you the cute puppet picture, show you the API deny policy, and show you how that's added, and it'll be trigger a, um, alarm in my environment. So that's the setup. So with that going on, then I'm gonna go into the code to show you how the supervisor agent, all the agents work together, step through the understanding, knowledge integration, planning, execution. Then we're gonna come back and using a we actually build all this in the streamlet um UI. You can see in the UI with one user query and one click, how everything was fully automated and solving the alarm, the API Dnife policy, update the API gateway, redeploying and then coming back and close all the tickets, the whole thing you will see it and we will see how that was done live after we do the code, uh, deep dive. OK, everybody sees my laptop, right? So this is that stacked website right now I'm gonna pull. I just refreshed this show you a random image it's working. And then I'm gonna show you here right now I don't have a resource policy to deny anything and then I don't have any alarms so now let's make some um real world infrastructure problem. What I'm gonna do is I'm gonna come over here. And then manually adding a deny policy. OK. So I'm gonna add a deny policy. It's just a quick script. And then it's gonna populate and deploying that when that's done. I'll come back over here. You should be able to see that that's already happened. In the API policy over here if I refresh. OK, so I added deny policy. What's gonna happen with this cute puppy pictures if I refresh them again? I should be getting access denied there's caching happening, so let's come back and try it in the terminal. Um I'm gonna, I have the command over here. OK, there you go. I'm starting getting 43 error. Over here I should be getting. Access denied. OK, so that happened. Um, it's gonna take a while, take a minute or two to have that alarm happen because the threshold is denied within a minute. So I'm gonna accelerate that by creating, manually creating, sending a lot of traffic. Uh, to the API endpoint, so you see what I'm doing. I'm sending. 10 traffic going there they're just gonna generate we're getting a lot of 43 errors, right? API deny error. So the whole point of me doing is accelerate the alarm happening so we simulate what's happening right now. Uh, let's see, it said it did say wait a minute or two, but I'm gonna refresh to see how fast we can get it. While I talk while we wait for this, think about if you, you do have this problem happen either for your customer or even your any website you're hosting. The traditional way to do this is what you say, if I got a cloud watch alarm for the APF 400 error, I will deploy automation script. And then create a Jira ticket and try to solve that, right? The problem with that is, it's static. If either of us change our API that won't work. It's brutal. And if you, it can only solve that one specific error you have in there, and it's not, it's it's siloed. The system don't talk to each other. It doesn't scale well, and it's not safe because it's a hard coded way to solve for that. So today we're going to show you instead of that traditional way of doing that, how the automated AI ops would be able to solve that in a much more intelligent and automated way. So hopefully with that there you go. I got alarm happening great. So we have our full set up over here. The last thing I'm gonna show you is one UI before I switch to the notebook to show you some code. Um, this is actually the streamlet UI that we build. I'm not gonna show all of this in action yet. I'm gonna go through the code, then we're gonna show everything in action. So over here you see how I turned on full automation mode after we walk through the code we're gonna come in, ask, do you have an alarm? And then with that happening, it should direct me, do you want to create all this workflows end to end automated and it should work if we do things right, OK. So with that I'm switching gears to the code over here. Uh, Jane, quick comment on the UI app. So that is the kind of tool you can get your support desk access so that they are not required to call level 2 or level 3 for every issue they get. So they can start the troubleshooting right from the UI. Great, thank you, Papa. Is everybody still with me now we have our set up. We're gonna go into our cold deep dive. The first thing, so this is a almost production level code that we're gonna publish to AWS uh GitHub AWS samples. So you will see that before the end of the year if you wanna be piloting this with us, feel free to talk to me and Bapa this week. We can get you early access, uh, for the code itself. But there are 7900 lines of code. I'm only gonna show some key functions over here. Um, so the overall is I'm gonna call out the agent related stuff and why do we think, why do we do things this way and how did it help solving that traditional problem in a much better way. OK, so to begin with, um, supervisor agent, this is where a lot of the core stuff happening that the agent thinks, analyze, and coordinate different agents to work together to solve the problem we just saw. If you have used strengths before, by show of hands, anybody use strength agents before? OK, only a small section of the of the team over here. Good. Yeah, OK, great, um, so this is, if you haven't used that, this is a very easy way to deploy, uh, agents, and then you can build it with a couple lines of code open source, um, um, open source SDK framework AWA has for deploying agents, and it has built. Tools and you can have the native integration with MCP as well. Um, if you have look at the AWS sample code GitHub, the strengths, you can easily see the 4 line or six line of code. You got a weather forecasting agents ready, right? So that's the context. So we're using strengths over here. Um, quick thing I want to point out over here with the supervisor agent is, of course, we would have a supervisor prompt for the interest of time. I wouldn't show you that, but that's really instruction or how would you want the supervisor agent to guide and coordinate different agents to work together. Um, so the model over here we're using an anthropic, um, the lazy and the sonic model. OK, so now think about the different layers we mentioned earlier on the slide. Number 1 is AI understanding layer, right? So orchestrating everything entry point. This is number 1. This is whenever an alarm got triggered or your customers saying, do I have an alarm? What's my cost for this account? or do I have a security issue? Any of that user query, number 1, the, the entry point for the AI orchestrator to start working. This is where everything is. So I do want to highlight two things over here. So we're doing two things. Number 1, we're analyzing the intent of the query. What is the query is for. That's the first step. Could this help us determine whether we need to coordinate among different specialist agents? Specialist agent, I do mean Cos Explorer, security hub, uh, uh, cloud watch, a lot of. The stuff that if we do need coordinate between different agents this is what we need to do if you look at that if secondary services are true what we're doing with the results is first we get the response from the primary agent, primary service, and then with the second one we are pending the response together. And the results are synthesized through the multi-agent to do that collective response for the next step. So this is very important because we're not just we're saying the manual orchestration by coordinate manual, sorry, the, the uh query in. Intent analyzing how many specialists you need gave you the fine-grained control. Think about if the customer wanted to reduce costs with the security funding or improvement, then you're looking at both cost-related consideration and security funding related situation. You will need to coordinate between those two. All right, so this is how the entry point is. Next function I'm going in is now AI can understand the query, right? So we call this analyze query intent. I'm gonna actually pause for a few seconds for you to look at this prompt. The reason I want to highlight over here is it takes a Unstructured text from the query. Determine what AWS service you're gonna use, what are the actions requested, what are the parameter we need to extract. What are the intent? We talk about monitoring security optimization, troubleshooting, and what are the priority level, right? So these are setting in the context of manage service provider. You're managing a lot of customer accounts, you got different security level, you got different problems. So it's intended for that large scale operations. But what's most important here is we are forcing. A format in the JSON format. So this is less error prone. So it forces the AI to take the unstructured text, not only understanding what it is, but print out a machine actable steps over here. So all this stored are stored in the JSON format. Same thing with how you're trying to understand all of this query intent. This is #1. I wanted to emphasize. Number 2 is, remember I mentioned to everybody this is we wanted to push this to new production level. So a lot of fallback or resilience were built. If this Jason format we couldn't parse it, then we default back to the keyword analysis. If customer says I want to do customization, you go there. OK, AI understanding layer. I'm gonna continue. So now AI got customer or user query, understand, analyze what they are, then the multi-agent orchestration gotta happen so that the supervisor can coordinate the different action to say you go here, understand cost, you go here to figure out the cloud watch alarm happening, you go here, create a Jira ticket, right? That's where the action needs to happen. So this is multi-agent coordination with context enhancement. When I say that, I'm gonna show you in the code, but two things to look out for here. Number one is the supervisor agent isn't just doing simple routing, saying, OK, this user ask for cost, go for the cost of explorer agent. It isn't just for simple routing. While it routed to that specialist agent is actually enhancing it with more context. So I'll show you in the code what I mean with um enhanced context. OK, two things I want everybody to pay, pay attention to. We did, I keep using Cos Explorer as an example because that's the use case to back up in a demo in a little bit. Um, so in any, anytime you do a, a, a cost vending um for any of your uh or your account, then time is a very important parameter, right? So this is a if the agent is cost explorer, I got to making sure all the time parameters were extracted and the query, both on the time parameter and the query intender passed to that agent together with the enhanced enhanced context. Same thing this is very, very important. Um, I'm gonna show this right away is we talk about specialized agent we talk about, you know, the agent is able to make intelligent decision or sometimes even better than human because we wanted to have those. Agents have fresh live data. So in this case, all the primary services we're getting from this analysis when we were, uh, when we were going routing to specific APA operation actions, that's super important. We're actually using AWS documentation to augment it so that it is precise, it is up to date, and it is accurate. So I'm gonna go show you exactly how that piece is done, but this is where we use MTP server. Um, so this is just to recap. When we do multi-agent corestration, not just simple routing, when we routed to specific agent, we gave the right context over so that agent can get informed response back by considering live AWS, uh, documentation and any other contact that are critical for that action. All right. Um, before I show you how the AWS documentation MCP servers are being used, uh, this is important for the context of manage service provider. I'll do a quick intro here because not many of you guys are managed service provider, but if you are. Again, the context you're managing a lot of customer accounts. There's a lot of complexity things at large scale. It is super important to have the credential management in the multi-tenant environment set right. So this is just to highlight again. When you do get a specialist agent, you can take a quick look over here. But the idea is, if you're doing security hub or Cos explorer, those are the customer credentials the session we need to pass along to the agent. But if you're doing Jira like in our case we're acting as MSP management service provider we own all this Jira ticket to create troubleshooting and solve issues for so the credentials should be MSP credentials um so this. is just to show that how the the code level are able to do the credential credential isolation so that you got the right customerization with the right credentials passed to the right uh agents. It's not universally the same. It's other customers or your MSPs. OK, so we had AI understanding we said for um routing execution, we're giving enough context. This is where the knowledge integration layer is happening. I'm moving on to next next function. OK, um. Maybe I'll do a quick pause, let you, let you read this for like a quick 1 or 2 seconds. As you read it. I'm gonna highlight why are we doing this. This is a tiered document, document access strategy where the primary path is MCP client will, will understand those AWS documentation APIs because you wanted the data to be fresh. You want to every time we push API update your agent understand that executed without any issue. Instead of hard coded in that way. But again, this is a production level system. So in addition to the AWS knowledge MCP server to search for documentation, we also have a local cache documentation helper. So if you know your customer or this set of customer environment, always have this similar set of issue, you can always have those service specific documentation. In S3 bucket where you or locally you cash it, right? So this is a tier strategic primary is MCP server fallback is the documentation helper. Again, you always are making sure your your agent have the right knowledge integration instead of going rogue by itself. Um, quick plug-in for the AWS Knowledge MCP server again, if you haven't used it or explored it, if you go to GitHubsh AWSlab/ MCP, you'll see all the available MCP servers. If I'm using it, you should give it a try, take a look, explore, see what's there. In this case, we're using the documentation MCP servers. All right, so, um, AI understand customer query, understand routing, how many services do I need, what specialist agent I pool, and then we routed with the right knowledge integration now. We gotta plan this and execute. So the next function I'm going in is um. Remediation plan generation. This is a super long prompt over here. I'm gonna get, I'm gonna let you read it again, then I'm gonna go do my highlight over here. So this function is generating a remediation plan from the knowledge-based results with the supervisor validation. So that I really mean it. I'll, I'll come back and pointing out why we need those two focus over here, but high level, you see what we're doing here, right? So we take that alarm context, we take that knowledge based results, and then we say based on all you know. Generate an APA operation that AI agent can execute in this context to fix the problem. Sounds straightforward, but two things I really want to highlight over here is the um safety guard rails over here. So, number one, you see this exactly in this case, we say extract only the operations mentioned in the knowledge base. This is super critical. Think about it. Again, you don't want your AI to start hallucinations, do a bunch of things you don't want them to. You're going to have the right boundary. In this case, is a knowledge-based constrained AI automation. In our demo we are allowing, you know, of course API gateway updated the actions and the service names are related are API gateway EC2 RDS in your actual environment you can customize this, of course. Again you can customize it for your customer, right? So this this set of customers, these are the resources, these are issues you have then proven. Procedure you want to automate have that pass in the knowledge database um what we wanted to happen over here is when agent generate this remediation plan it only does the ones that are allowed in the knowledge base. You may wanna ask what if it doesn't? So in this case our set up is if it doesn't, you'll just still create that ticket and route it for human intervention. It doesn't do any automation on your behalf. Uh, the question is, we have a lot of specific details. How would a person when we build it needed to? They don't need to. You can start with literally with one line. So this one I'm showing as in how much safety you can put it there in terms of having AI push the boundary or not, right? So you don't have to start there. You can start there, literally extract only the API operation, uh, AWS operation mentioned in the, in the knowledge base. You can start with the resources. You don't need to. Be this specific, but I'm highlighting here for the sake of the demo and for the sake of making the point of how when people say hallucination or issues or accuracy, a lot of the time is still because we have to guide it the right way. So this is one way of guiding it. I'm gonna make my second point. I know there's probably more questions. Believe me, we'll have questions because when I do the demo we gotta wait for something, so we'll, we'll have questions. Let me finish the second point. It's important to think about this is the same thing, right? You want it to be safe, reversible operations. If you could, you could just say never delete anything in my customer environment, that's probably obvious, um, but again, in this world, depending on how you set it up, you may or may not be able to honor that. So I wanted to highlight the way when you. You wanted the AI agent to generate mediation plan, you can customize it per customer workload are common issues. At the same time, you really want to make sure what is the boundary like do, do not ever do anything irreversible reversible, irreversible. Uh, so those are very important. So I want to highlight over here, um, this is. The additional details in the actual code I'm not showing here is again there's another layer we put in. So we said search knowledge-based results, right? There's a confidence score, relevant score. There's a threshold you can put in there. So in this case is anything more than 0.5. So if you're not confident about what you're finding this issue is. The relevance with your knowledge base then don't execute again as an organization or as you deploy your your application your working customers you can dial that back and down um I always start with being super strictive with the AI agent to begin with and then as you as you scale or you deploy more and have run more workflows with your customer then you can. Allow more flexibility once you got the AI accuracy and the trust with customers. Um, so that's just a nuanced way to say there's multiple levers we put into the code where you can when AI agents generate that, uh, that remediation plan, there's a lot of layers you can put in, um, in terms of guard rails, in addition to the building guard rails you can use in bedrock and all the other stuff. OK, so we got the knowledge integration. We generated the uh remediation plan last step, execution, right? We are at the last step, execution. Here, the only thing I want to emphasize again is Safety mechanism built in every single step we gotta go through validation step function. Every action is locked, every result is captured, and if something goes wrong, we stop immediately. Again, you can dial back and down on the spectrum of how you wanted to do this, but. I'm showing you our recommendation why we do this way. You don't have to read all this code here, but this is literally to show you if you want to complete audit trail, you know, in addition to whatever building in AWS environment for the agent itself for agent action, every single step is locked here. And if anything goes wrong, we stop. Only thing I'm going to highlight two words over here, um. There you go, validation step. That's another function before agent execute the action, the remediation plan from the last function. We ask a supervisor, we ask the supervisor agent to validate it. So at this point, if the supervisor agent says no, reject, then you still won't execute. Again, that's another layer of check on, are you doing something potentially dangerous in the customer environment? Do I want you to do that or not? A lot of this consideration because we're trying to look at in the product level, production level, how this would look like. OK, so production grade execution with multi-layer safety validation. The question was when this last validation step got rejected by the supervisor, what do we do? Do you bring a human in the loop to do that, or do you further push it to send another loop to further automate? Um, the answer is depends. The answer is depends. In this code ripple we have, this is when we still create the Jira ticket and route it to SME review. So that's how human in the. We brought in this code, but in some other production level agent that we've done for other use cases we actually done the secondary the branching logic, right? So this is the initial set of automation I trust you to do and then I branch off when this get rejected and to further analyze have finer control to see where, why it got rejected, whether we allow more flexibility to further advance on your automation journey. No. OK, um, the demo, yes, let's see. One last thing I wanted to show you before I show you the demo, uh, is think about what we just did, right? So we said, you know, the supervisor agent is awesome. It's use strands and takes prom. It does a lot of nice fancy things, but think about our workflow. What are we doing? We needed a cloud watch agent to detect the lar. We need to create a Jira ticket. We needed to, you know, route it to the right agent. We need to search the knowledge base. We need to generate remation plan. We need to execute it and we need to come back and close the Jira ticket with all the comments you've done so that we have logging. So all of this requires a persistent state sharing between all the agents. Instead of simple routing. So that's the last function I'm going to show you if I can get my password. We actually use workflow graph that's also in strengths to coordinate different agents with this multi-step operational processes by building nodes and stuff so I'm not gonna show you the exact code, but this is, these are exactly what's happening. We're actually adding the Jira, the knowledge-based remediation and closure all this together so that collectively this work. Flow would have shared a memory and state between the agents so you don't forget about which ticket am I solving for which customer, which step am I, which remediation did I do again complete that large scale managing multiple part customer account in the environment that you needed both tenant uh credential isolation but also clear understanding on what states of that ticket is at. OK, so that is all for the code and then I'm ready for some live demo with you guys. OK. Um What are we gonna do is. I'm actually gonna send a couple of more alarms. Just because you know we talked 20 minutes on the code and maybe the alarm is not very active anymore so I'm gonna trigger some alarm. And wait a little bit we should be able to see that. And then I'm gonna set up one. I'm gonna set up the UI, right, that streamly UI. I wanna see how it looks like when you have an operator on that literally how fully automated it is. And then 2, I'm gonna show you my terminal of this code. Then you can see in the back end how the different steps of agents are walking through again every single function, pretty much all the function we just talked about. OK, so I'm gonna come up here let me set this one up. I'll show you that. Yeah, so I'm gonna get the two windows open. And then I needed this one. OK, so if I don't get throttled, I should have this demo. And What I want everybody to do is to watch both window again this is what Iran earlier right? so you would see in the back end how agent is doing what the logging all of that and this is the front end or how again an operator could easily do all of that for your customers. So hopefully we got our alarm. Yeah, and then let's see, do I have. Active alarms. Thinking. OK, I do have an active alarm. This is the APID9 policy that we created in the beginning, so it says full automation ready click to execute. OK, let's give it a try. Exit full automation. Pay attention to the steps on the left and also steps on the right. OK, I'm gonna making sure you can see this window. There we go. Creating geoticket, geotic. She has the. Including the issues in there. And then, um, finding my knowledge base results, I think we found a relevant score of 0.85. That's considered good. I'm going ahead right now. I'm executing it already. Two steps approved by supervisor. Step one, I think it's update my API, and second is redeployment. Uh, did that go through? Yeah, it did go through. And the last thing we're gonna come back closer to your ticket. And then paste all the comments over there. So you actually, oh there you go wula our live demo works so. Let's take a quick look. What would you look like will be a actual this is a Jira ticket, of course I have it earlier there for you, but this is what you would see. I opened it and automatically closed it for you. With the right comments saying the alarm, what was used, was it successful? What did I do? Complete audit trail for you. OK, great. So that is our use case number 1. OK, let's good use case 2. Yes. OK, but it's not. Yeah, this is good. No, but, oh, OK, yeah, go ahead. OK, use case two Bapa gonna show a cold deep dive and um a demo as well, but use case 2, think about it. We just showed the alarm watch how it automatically do all that. Another common thing our customer, when if you work with a lot of customers at MSP is cost optimization, like figure out what my cost in my environment last month and then go figure out a ways to write sizing and optimize all of that going forward, right? So smart companion, uh, I'll do this super quick. So really the idea. Is we wanted the AI agent to serve you, serve your customers. The way you build it would be, in addition to do a simple query, answer a question, right? That's like a last year chat that's ancient history. Instead of doing that, it's able to again have the right knowledge knowing this is a customer account that I'm, I'm doing. I pass right cre credentials. I'm able to make informed and accurate decision for the customer or recommendation. So this way you can free up your human expert for more complex challenge having AI agent to help you do a lot of that, um, the, the call flow, I'm not gonna belabor the point it's the same guys now if you look at this call flow, it should be a little familiar from we just walked through, right? User come in, you know, you have that UI, but user come in, there's routing orchestration stuff you needed to do and you know, in this case instead of sending into different agents instead of the Jira and. And cloud watch is what is your cost, uh, explorer and then how is using the again live documentation API to find right API to use and do all of that, uh, stuff with the recommendation and and all of that it's very similar and then if that's the case, Papa, you wanted to do a quick cold snippet can can you just yeah. Yeah, so one thing I just want to highlight here, so this is one of the examples that we are presenting on a specialized agent, the cost explorer agent. Similarly, you can do the same thing for security agent, trusted advisor agent, anything. The logic will be the same. The code that we are going to show you will be the pretty much the same with some little kind of adjustment, and that is what I am going to show you. All right, ready for demo? Yes. So again, let's understand the call flow first. So, as an MSP, a Level one team or help desk, your customer mentioned, oh, my cost is really high. So you are not required to go and open a ticket and work with the Finns team, right? So, you come to the UI agent, uh, like a UI what Jing was showing earlier, and then you put a con that query, what exactly you need to know. Then the request will come to the supervisor agent to understand the intent of the query, what exactly the MSP Level one team want to know from that, from the transactions. Once the supervisor agent know that this is a cost-specific request, then it will go and talk with the cost explorer agent. It will select the cost explorer agent to do all the analysis, to get, get all the APIs and then do the API call, get all the information and everything. Then it will come back to the supervisor agent. The response from the cost agent will come to the supervisor agent, and then they will formatting the response to the LLM, make it beautiful so that the end user understand all the kind of costs and so on and so forth. So this is the overall flow, how it looks like. So let me cover some of the core components of this flow. For example, core dependencies import. I'm not going to cover that. This is very standard. Like a prompt configuration. This is the prompt configuration what exactly you want the cost agent to do, right? So use business language, use emojis, whether it is your cost going up or going down. So a lot of, a lot of things that that agent can do, and it is all part of the code that we can share with you. Next, uh, class definition initialization. This is also a very standard process. I'm not going to dive deep it to here. Jing already covered that. And then we do a quick connection testing to make sure that supervisor agent can talk with the uh cost agent, cost management, and cost management agent can call the AWS API. So the connection is working. So we do a quick test with a very minimal single API transaction. Now, here is the main entry point as uh similar to what Jing Waxing sharing earlier, the query. This is where your, the users are calling this, this cost agent, uh, uh, cost management agent to that UI interface that what is my cost for the last 12 months. So this is the entry point and it is lending into these functions, the query method. So here what is happening here. So here the supervisor agent is trying to understand the intent, what exactly the users want to know, whether it is a recommendation kind of thing, whether they want to understand the cost anomalies, or whether they want some kind of recommendation to optimize the cost, or this is a very standard user request on the cost management side. Then If you see the code here, so basically, recommendation optimize safe, they are looking for that keyword in the query. If the keyword matches, the recommendation matches, then the recommendation, they have a different subsection like, OK, should I recommend my cost? Should I get a SI or the RI or savings plan, or should I go and change my instance size? So those kind of recommendations happens here. And then there's a there's an error handling process as well. Now what this query is doing now, the query understands, the supervisor agent understands what is the intent of the query. Now it will go and call the the report where we have all the API information. So once I share the code with you, you will see there's a Python file which is. Like what Jean was saying earlier, is a repository of all the AWS API documentations so that it is not hardcoded in the, in the, in the call, in the, in the code. So, now the supervisor agent will go and read those kind of API information from that uh doc helper, and now they will understand that these are the available APIs. Now it will come to the next step. Now for the cost management, what is the right selection of the API? Which API they need to select from that particular report. It is happening in the section 6, API selection logic. Service-specific API selection. So you can see here, the first thing it is doing to understand that what kind of action the agent should do, whether it is a recommendation, whether it is a cost anomaly, or it is just a cost breakdown per service, per geo per regions based on the intent of the query. So it will do the selection of the best API that matches the intent of the query. Now, you can see here for the, as I said earlier, for the recommendation, we are further going into the details. If it is the right sizing kind of requirement, then it is selecting the get right sizing recommendation API. If you can see here. If it is a cost savings kind of request, it is selecting the cost savings purchase recommendation API or it is a reserve instance kind of recommendation, it is selecting the reserve reservation purchase recommendation, whether RP or SI. Now, similarly, if it is required, if the request is based on anomaly detection, it is get calling the get anomalies APIs. So based on the intent of the query, this particular batch, this particular function is calling the right API from the report. So, here's the real-world examples, like, how can I save cost of my EC2? Let's say this is a query that came from the, from the end customer. So basically, it is detecting what is the keyword here, Save EC2. So that means it is matching with the right recommendation API. So it is calling that particular API because it read the keyword from the user query. In example 2, show me cost anomalies. So that means it understands the customer wants to know the cost anomalies. Now it is calling that particular API which will help to get all the anomalies information. Third example, what are the top spending surveys? So basically it is asking, OK, show me all the services from the cost perspective for that last month. Now, it is a get cost and usage report, kind of very standard API, default API, which is also acting like a default API. Number 7 is a customization. Here, we need to understand the user query that is coming through the UI interface is a natural language. So, but here the what the agent is doing, it is putting all the right parameters so that it matched the AWB's API format. Because the natural query that is coming from the UI is not matching the API format. This is exactly what this part is doing. It is taking all the natural query that is coming as part of the UI interface, and it is converting into the format that AWS API matches so that AWS API can go and put the right information to the, to the end users. So you can see that for example, line 9 to 10, it is showing the date. For example, the customer might say, OK, show me the data for the last 30 months, 30 days. What the 30 days means? So AWCK will not understand what the 30 days means. We have to convert into a date-time format. So that is what exactly this is happening in the line number 9 to 10. It's setting the time period. It is setting setting the granularity, whether you want on part day basis or part month basis, depending on the time frame that you are giving. For example, if they want to know, OK, for the last one month, it will give part day January. If you say per last one year, it will give per month January because it will be too much data if we do per day. So this is what the example looked like. So all the input that came through the UI interface, this is what the customization is done. I mean, the transformation has been done. So it exactly matched what the AWBS API can understand. Because it converted all the natural language into the AWS API format. This is the section on time extraction. So basically, as I said earlier, any query can come, OK, show me the cost for the last 30 months, 30 days. So what the 30 days means? So AWS need to understand, so it will do the extraction that how the calculation will be done. 30 days convert into a time date format so that AWS can understand. Next, this is a real API call happening here in the section 9. So what it is doing, so there are two files that when you get access to the code, you will see. So one is executor. So now this function, this base doc agent. PI, this is a kind of Python uh agent Python code, which is doing the actual API call. So the query is calling this function because this function has access to all the API information from the doc's helper. So there's another file which is listing all the API information. And this file is using that, the report to select the right API. Now, this is the one calling the API direct API call to the, to understand what, what information they need to pick up from there. And this is what AWB has done something very similar like this based on the query. So this is the format that the user will get once the query get executed and the API call get executed. And how it is executed, Jing already covered that with all the secret man secret manager is there storing all the credentials, uh, because it is assuming a cross cross account role so that it can log into the customer account, fetch all the informations, and storing in the circuit manager whenever the agent wants to call the AWBS account, it is taking the data from the circuit manager and then making a cross-count, uh, access. Now, the last thing, number 10, is formatting the response with LLM. So, if you go back to the previous diagram, you saw that under the supervisor agent is powered by the Bedrock and all the LLMs. Now it is formatting the response. Now, whatever the AWS is responding back, this is particularly converting to a night response which the partner customer can understand or any non-technical people can understand, OK? So this is how it looks like. Beautiful formatted response that are coming from AWS uh through AWS APIs. Now, this function is converting all the API response into a nice template that go to the end users. So, to recap, this is the kind of flow that I just talked about. Let me go through a live demo, OK? Again, the same EI what Jing was showing earlier, and now what we can do. We can go here and this is something the cost explorer kind of examples, OK. Now, let's say, let's say that this is a user, user query that what they want to know. Show me the cost trend for the past 6 months. So this is the kind of information is landing on your help desk team, and your help desk can get the information to the end users. It doesn't need to go to the level 1 or level 2, enough to get the information. They can do it by themselves. OK, perfect. So, I'm not going to the, the, the code part because Jing already covered how that happens. So this is the response coming back from the AWS. So based on the inquiry and this is how it looked like. So all the functions that I just described 1 to 7. It is running in the background and this is how the response will look like in the UI interface, how this code. This is very similar, like this is an example of cost management, but if there is a similar requirement for security, trusted advisor, everything can be done through a similar kind of template and the code that we will be sharing, it has access to all the defined agency code and so that you don't need to build anything from scratch. You can get a very solid foundation and then you can customize based on the need that that you have. One of the reasons why we are having this session, I think we have only 3 minutes left. One of the reasons why we are having this session, because the early feedback from the partner after getting all the responses from the customer and the partner was very impressive. So now the partner, the customer came to us with two different options. One set of customers, partner, they said, I want to build everything natively with my in-house resources using data-based services. I don't want to buy ISV platform. So this is the kind of solution for the partner or the customer who want to build everything in-house. There are other set of partner customers that said, OK, I want to get AWB's guidance on a vetted ISV solution that can do the work for me. I don't need to build all those things. I need a solution from the market that can do the things. So we also have a program called Benefit where we are telling the partner these are the solutions that you can buy. With the agentic workflow embedded there, you don't need to build anything like what we have presented right now, and there's AWS funding available for them to go and offset the license cost for the, for the partner in the MSP program. So, if anybody here like want to part of the MSP program but not yet. And you want to try these things, you have two options. Get in touch with us so that we can share the code with you, and you can customize the code the way that you feel, uh, fit for your environment. Or if you believe that I don't want to deal with the code, I want to buy an ISV solution, you need to be a, you want, you should become an AWS MSP partner first, and then you get access to the AWS funding so that with the funding you go and buy the tool that you want. And the tool that you want should be part from our recommended list because we are doing a selection. We are vetting the solution that this tool actually do the work that we believe the MSP should be doing, and then from that list, you can buy the tool and get the funding, use the funding from AWS. Any questions from anyone? We have one minute left. gonna get there. We'll put, we'll give you early access. So as Jing mentioned, yeah, Github AWS sample code before the end of the year, by the end of the year. But before that, if you want an early access and be, be a pilot, talk to any one of us. I'm gonna show you that page right now where it has our contact info. But before we close, let's, I just refresh that we saved the puppies. The cute puppies showed up. That's we were getting access denied. AI solved it for us, and then the API policy, the denied policy we add. Were removed and things were deployed for closure that's what it is and now for contact info where you can you can reach out to BPA and me um those are our LinkedIn barcode. Feel free to connect, um, find any one of us for the next couple days while we're here and most importantly, please do a session survey for us if you like how this talk is please give us feedback over there and you're doing the mobile app now I think those are the linking barcode for us.