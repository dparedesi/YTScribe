---
video_id: 2_Ev5YCO2Ik
video_url: https://www.youtube.com/watch?v=2_Ev5YCO2Ik
is_generated: False
is_translatable: True
---

Please welcome to the stage, vice president of Amazon EC2 core platform at AWS Willem Viser. Hello everyone. Good to see all of you here. Um, What an exciting time to be in our industry. I, uh, I have the privilege of leading EC2. And uh I spend a lot of time with customers every day. And a lot of those customer conversations are actually about what features they're looking for. And I then actually also get to experience my team looking at those features and deciding how we develop that. So we're living and competing in a world today where technology is advancing faster than strategy can keep up. Artificial intelligence, automation, real-time analytics, these are all no longer future concepts. They are driving measurable results today. And what's key here is that organizations that move first, adapt faster. And make data-driven decisions, those are the ones that's gonna win markets and those are the ones that are gonna rewrite industries. And this is more than just a technological shift. It's a business revolution. And success belongs to those who can turn innovation into execution at speed and scale. And today I want to share how AWSS Compute continues to transform what's possible in cloud computing. How we develop and apply technology to solve what were previously considered impossible problems. As I've said, I have the privilege of leading Amazon EC2, and I get to see this every day. And every innovation that I'll share today stems from similar customer journeys. Real challenges that demanded delivering new solutions. So, I wanna start by talking about scale. Scale creates possibilities. Our customers every day launch more than 271 million instances on EC2. And so kind of like if you do the math that gets you to just over 1 billion instances every week. Um, and since 2006, we've launched 287 billion instances on EC2, which is a staggering number. And so with 38 regions globally and over 1000 instance types to pick from for every kind of workload, we're delivering compute capabilities from cloud to edge. And what does this mean for you? You can launch global applications within minutes instead of months. Our managed services automatically pick the right resource for you while our global presence lets you deploy as close as possible to your customers. And you can run anything from microservices up to massive AI models. We love them all. Operating at this scale gives us unique insights into your challenges, all while continually optimizing our own infrastructure across compute and networking and storage. So every layer of our stack represents countless conversations with customers who've pushed us to think differently about computes. Innovation requires partnership and collaboration at every layer from silicon to software, and we're gonna go into each of those in this presentation. But that brings me perhaps to the most important lesson we've learned of them all. It's not just about building a future. It's about building building blocks. Building things that millions of customers can actually build on top of. So as promised earlier, I wanna share remarkable customer stories today. And if you can't tell from that image already, the first customer story is very close to my heart. It's Zou's. Now these cars, you might have seen them zooming around the strip already. You can download the app and you can take a ride in them. And so in October, I actually had the opportunity to go and ride in one of these. And so the car really,
really is pretty cool. There's,
there's, there's a lot of innovation in the design of the car itself. The way that you sit is different. You actually face each other. It's like little train compartments, lots of space, and the ride is just absolutely incredible. It's just different than anything else in the market, and I loved it. But what's fascinating is not just the physical design. What's fascinating, of course, for us, is the massive data operation behind these cars. Zoos operates on a unique rhythm. During the week, their instruments and vehicles. Gather lots and lots of information, ta petabytes of information in actual fact. And then what happens over weekends, they take the data set of the week and they run this through advanced ML models in massive simulation runs, the whole week of data. And so this creates an interesting challenge. How do you efficiently manage your compute resources when your needs dramatically swing from that week of gathering information and then this intense weekend run that you have. And so Xos manages shared infrastructure where application teams deploy and run the services on EC2. And then when the weekend simulation run comes around and they require thousands of GPU instances, they leverage AWS ML capacity blocks, on-demand capacity reservations, and finally, Sage make a hyperpod with flexible training plan. So Just a few weeks. I met with a new bank in Seattle. This is a company that in 2013 Set out to take on a really big challenge in Brazil. So this was a market at the time in 2013 that was dominated by essentially five banks, new bank obviously not being one of them. And so their mission seemed impossible. Build a digital bank first in this market dominated by these 5 banks and specifically in this market where financial literacy was low and millions were unbanked. Today, massive success story. New Bank serves more than 130 million customers across Brazil and Mexico and Colombia. 90% of their work complete workloads run on AWS. 30% of that compute workload runs on Graviton. So the fact that they're running Graviton had them achieve a 14% reduction in cloud costs and a 21% reduction in cloud emissions, the carbon emissions, apologies. New Bank also uses Amazon EKS with Carpenter to run 60% of their EC2 capacity. With an Amazon itself, we're also tackling seemingly impossible challenges. Amazon Leo aims to deliver worldwide internet access through over 3000 satellites. So before launching a single satellite, their simulations include entire constellations using easy to graviton instances and using FPGA enabled F2 instances. These simulations model satellites moving at over 17,000 MPH while dodging space debris. And at the same time, Requiring complex communications all the while ensuring reliable internet service delivery. And consider Intercom's journey into revolutionary customer service. Intercom served over 25,000 global customers, and they've developed Fin. Their AI agent which has transformed the customer service landscape. What once seemed impossible, having an AI handle complex customer queries across multiple channels with a human-like understanding is now a reality. With Finn successfully resolving 67% of customer queries and doing over a million a week. Their technical architecture is equally impressive. They run on AWS Graviton. You see a little bit of a pattern here, and they process 50,000 asynchronous jobs per second. Their migration to Graviton 4 for Elastic Search achieved a 56% latency reduction in search queries while cutting their EC2 costs by 22%. Partnerships are an important component to AWS success because they allow us to accelerate innovation and quickly deliver the latest computer technologies to our customers. Today, I want to take you through the journey of our strategic partnerships that have fundamentally transformed cloud computing. And I wanna start with our oldest. Intel has been there since the beginning of AWS and we still work closely today to solve challenging customer problems. And let me share a very powerful example of one such interaction. Mercedes-Benz recently completed an extraordinary migration of their one ERP system. Basically, they moved us to SAP Rise on AWS using our U7 INH platform. And this platform we developed with Intel specifically for memory-intensive workloads like SAP Hanna. And this wasn't a simple migration. We're talking about a 32 terabyte supporting system supporting over 40,000 users across 17 different systems. And so what would have seemed impossible before and what it would have taken multiple years was accomplished in just 9 months. So together with Intel, we've redefined what's possible in cloud computing through innovation, but also very importantly through custom processors. When we say custom processors at AWS, we truly mean custom designed specifically for your cloud workloads. We introduced our latest 8th generation of Amazon EC2 instances powered by custom Intel Xeon processors, which delivers the highest Intel 6 performance, not just in the cloud, but anywhere. What does this mean for you? If you're using the latest Intel Xeon sex-based instances, you'll see a 15% better price performance, first of all, and you can process data 2.5 times faster with our enhanced memory throughput. And for those demanding workloads, you're gonna look at 20% higher performance, directly adding to your bottom line. These improvements come to life across our new R8I, C8I, and M8I instance families. CHI instance families deliver up to 60% faster engine X performance. R8I instance deliver industry-leading SAP performance workloads, and MHI instance deliver accelerated post-stress performance by up to 30%. Each of these families offer our standard. 13 different additional sizes, including bare metal, and 96 X large options for your most demanding applications. We're also expanding our flex instance options. Now,
as a reminder, these are our instances designed to specifically help customers save money when their applications do not consistently need all the available CPU resources. Building on the success of our C7 iFlex and M7 iFlex instances, we've now added R8iFlex to join the C8iFlex and M8iFlex family. These instances offer the most common sizes from large to 16X large. This means whether you're running compute optimized workloads like web servers and caches and general purpose applications like microservices and virtual desktops. Now, memory intensive workloads are added and you can save an additional 5% on your compute cost. Our customers continually ask for more options, and we're now launching a preview of X8I instances powered by custom Intel Xion 6 processors. These instances deliver up to 6 terabytes of memory, 1.5 more capacity, and 3.4 more memory bandwidth compared to our previous generation. Making them ideal for in-memory databases and mission-critical SAP workloads, delivering 46% higher SAPS performance. That's not our only new Intel instance this week. We're also introducing new C8 INE also powered by Intel Xeon 6 processors and our latest Nitro V6 card. C8 INE instances deliver up to 2.5x higher packet performance per VCPU and 2 times higher network bandwidth versus C6 IN instances, making them ideal for security appliances and firewalls. Our partnership with AMD represents another breakthrough in cloud computing. AWS was the first major cloud provider to launch AMD Epic instances, and since that introduction in 2018, we've launched every generation of their processors. Earlier this year, we introduced the fifth-gen AMD Epic processors into our portfolio and we're now launching the latest instance C8A. C8A instances deliver up to 30% higher performance compared to C7A instances with memory bandwidth and networking capabilities that transform what's possible for your compute intensive workloads. These instances deliver up to 384 gigabytes of memory backed by up to 75 gigs of network bandwidth and 60 gigs of EBS bandwidth. CHA instances are perfect for everything from batch processing, distributed analytics, to high performance computing and highly scalable multiplayer gaming. Next, we're launching a tongue twister. The AMD X-based instance. X 8 AEDZ, right?
That's a lot of letters. I apologize for the letters, but that's important. Um,
they all, they all tell you exactly what, uh,
what these are. Um,
so these are powered by fifth-gen AMD Epic processors with a maximum frequency of 5 gigahertz, the highest CPU frequency in the cloud, and one of the reasons for the many letters. They deliver up to 2 times higher compute performance and 31% better price performance compared to previous generation X2 iEZNs with up to 8 terabytes of local AirBnB SSD storage. These instances are ideal for EDA workloads. And so for chip designers, this means faster processing. In memory intensive tasks like floor planning or routing and power integrity analysis. And to round out today's AMD CPU based announcements, I have two final instance types to share with you. First, we're announcing HPC 8A instances powered by 5th-gen AMD Epic available in 2026. These instances are designed for compute incentive, high-performance computing workloads, including computational fluid dynamics, weather forecasting, finite element analytics, and drug discovery applications. And then available in preview, M8AZN. These general purpose instances are powered again by 5th-gen AMD Epic processors and deliver industry leading that 5 gigahertz of clock speed, the highest maximum CPU frequency in the cloud. M88 AZN instances deliver up to 2 times higher compute performance and up to 2 times network throughput compared to prior generation M5 AZN instances. So the high computes and network performance makes them ideal for latency sensitive and compute insensitive, sensitive workloads such as gaming. Again, financial services, high frequency trading, and simulation modeling for the automotive, aerospace, energy, and telco industries. Let me put all of these AMD innovations into perspective because there's been quite a lot of them that I, that I went through now. So for EDA customers, the X8 AEDZ instances will accelerate chip design and verification. For e-commerce platforms, using our M8AZN instances, this means supporting more concurrent customers during peak shopping periods with the highest single threaded performance available. Our new R8A, M8A, and C8A instances deliver up to 30% higher performance and up to 19% better price performance compared to their predecessors. With R8A also offering 45% more memory bandwidth for databases, and M8A showing off that 39% faster Cassandra workloads, and C8A enabling thousands of additional players to concurrently join the same game. Without experiencing latency issues. And these are the ones that my sons are the most interested in because they'd like the games to be less laggy. Now shifting gears, AWS was the first and still is the only major cloud provider to offer Apple Mac-based instances. And today I'm happy to announce the next generation of our Mac instances available on AWS. So first, EC2 M4 Max Mac instances powered by Apple M4 Mac Mac Studio are now in preview, and EC 2 M3 Ultra Mac instances powered by the Apple M3 Ultra Mac Studio are coming soon. Now these join our recently launched Mac 4 and Mac 4 Pro instances, completing our portfolio of the latest Apple Silicon options. And these new instances will further help eliminate the need for physical lab environments, reduce capital expenses, and simplify CICD pipelines, enabling developers to build and test and also sign your Apple apps using the very latest Apple hardware. We've been collaborating with Nvidia for 15 years. That's a long time. And we were the first to launch Nvidia GPUs to the cloud. And since then, we've never stopped innovating. And we've spent all of this time learning about operating GPUs at scale. And I think we excel at doing exactly that. Through our first elastic fabric adapter, EFA we enabled HPC grade networking with lower latency and higher throughput than traditional TCP transport. And we think this differentiates us. It allows customers to achieve on-premise cluster performance with that cloud flexibility. Earlier this year, we released our P6 generation, easy to instance with Nvidia Blackwall. The P6 GB 200 instance deliver up to 2 times faster performance for AI training and inference compared to P5s. Three weeks ago, for those of you that might have seen it, we launched P6EB 300 as well. And these instances are ideal for larger scale AI training and delivered 2 times more networking bandwidth and 1.5 times more GPU memory than previous generations. Yesterday, for those of you that watched it, Matt announced the launch of EC2 P6E GB 300 Ultra Server, which is the first AWS instance to be powered by the Nvidia's latest GB 300 NVL 72 system. GB 300 offers 1.5X, FP4 flops, and GPU memory to improve performance for the biggest models and the most demanding AI workloads. A couple of weeks ago, we also launched P6B 300, which delivered two times networking bandwidth and 1.5X GPU memory size and 1.5X GPU teraflops compared to P6B 200 instances, making them well suited. For those of you that um is in this detail, makes them well suited to train and deploy large trillion parameter uh foundational models and large language models. The recent launch of B300 and GB 300 instances demonstrate our commitment to stay at the forefront of GPU computing. These are the latest and greatest generations of GPUs. And so as you've heard today, AWS continues to deliver breakthrough innovations at every layer of the stack. And from a deep collaboration with Intel and AMD and Apple and Nvidia, to the incredible customer stories that we've already shared, we're helping customers unlock entirely new possibilities in the cloud. These innovations aren't just about raw performance. They're about enabling better customer experience and delivering more value at lower cost. But innovation at AWS isn't limited just to these large technological leaps. We're equally passionate about the smaller things, the countless of smaller things that help our customers achieve operational excellence in their day to day work. And to tell you a little bit more about how we've turned this focus into real innovation for our customers, I'm excited to announce or to welcome Jeremy Connolly to the stage. Jeremy. Hi everyone. Good afternoon. I'm Jeremy Connolly, the principal technical customer success manager in Amazon EC2. You know, in today's cloud environment, every millisecond of downtime and every minute of operational overhead directly affects our customers. That's why I'm particularly excited to share some breakthrough improvements we've made in EC2 operational excellence. Improvements that are transforming how our customers launch, run. And manage their compute workloads. At AWS we believe that every incremental improvement unlocks new possibilities for our customers. Today, I'll show you how these targeted innovations are creating transformative outcomes across our compute platform. Let's start with launch time improvements. With our latest Amazon Linux OI release, we've achieved a 12% improvement in instance launch to SSH ready time. Across our most commonly used in instance types. This comes from extensive optimization work, including streamlined random number generation services and boot process improvements. On our C6G medium instances, for example, Excuse me. We've reduced launch times from 9.1 seconds to 8.05 seconds with similar improvements across other compute optimized and general purpose instance families. And speaking of ready time, time is critical in cloud operations, which is why I'm particularly excited about our new Amazon EBS time-based snapshot copy capability. Customers can now. Copy EBS snapshots within or between AWS regions and accounts with a predictable completion duration. Ranging from 15 minutes to 48 hours, and some customers are seeing up to 350% improvements when adding a 1 terabyte node. Let me walk you through how this works. This feature guarantees consistent and predictable snapshot copy performance by reserving dedicated throughput for our copy operations based on the completion time you specify. When you initiate a time-based copy, AWS allocates the necessary bandwidth, giving you the guaranteed completion times needed for your most time-sensitive workloads. You can monitor progress using the console, or API. And even Amazon event Bridge events that notify you when a copy completes. With guaranteed completion duration, you can address strict RPO requirements, improved disaster recovery runbooks, development workflows, and testing cycles with confidence. Knowing exactly when your snapshot will be available in the destination region or account. Customers such as MongoDB are using this feature to accelerate their cross-region node provisioning for their Atlas multi-region clusters. And speaking of data assurance. I'm excited to share a major feature in how we help customers maintain instance health. Our EBS status health check capability represents a fundamental shift in how we detect and prevent storage-related issues. Previously, customers might only discover a rare EBS uh related problem after their application had experienced impact. Today, We can detect potential issues and deliver that in the form of a health check. Removing the operational overhead of troubleshooting any data IO availability concerns. In building on our storage innovations, I'm thrilled to announce Amazon EBS provision rate for volume initialization. This feature allows customers to create fully performing EBS volumes within a predictable time frame by specifying initialization rates between 100 megabytes per 2nd and 300 megabytes. We've seen customers use this feature to accelerate their easy to instance launch to build more resilient disaster recovery posture, to achieve faster workload readiness for applications such as gaming and virtual desktops. On average, customers are realizing 40 to 60% improvement in their rate of volume initialization. Also, understanding instance placement and proximity is another area where we've made advances. Our instance typology feature provides unprecedented visibility into the relative proximity of your EC2 instances, delivering up to 50% faster latency improvements. I'm also excited to announce a new capacity reservation typology API. This capability allows you to determine the placement of your reserved capacity before launching instances, a critical advancement in workloads for every minute. Customers such as Anthropic used to spend up to 60 minutes launching and validating their instance placement and can now optimize on infrastructure placement strategies before deployment, reducing the operational overhead. All these improvements work together to create a more reliable, efficient, and manageable compute environment. But perhaps more importantly, It represents our commitment to continuous operational excellence. We're not just fixing problems, but we're preventing them, and we're not just improving performance. We're making it more predictable. And we're not just reducing operational overhead, we're innovating on how operations work. These improvements reflect our commitment to operational excellence, but as always, we're just getting started. As Andy Jasse says, there's not a compression algorithm for experience. And it's this, excuse me, accumulated experience that enables us to continuously improve every aspect of our service. Just as we've seen with these operational innovations. This is the same drive and excellence leads us to dive deeper. All the way to the silicon level. And now, I'd like to welcome Willem Vissor back to the stage. He'll share on our focus of these operational details that has led us to develop our own silicon innovations, bringing even more benefits to our customers. Thank you. Thanks,
Jeremy. I,
uh, managed to make the stage. I'm told yesterday somebody took a stumble. So at least that's 1 point better than the person that took a stumble. So Jeremy is my right-hand person, uh, when it comes to spending lots of time with customer on operational things. And, uh, he certainly is the right person to speak to the improvements that, uh,
that we've made. So Next, I wanna show you how we're redefining what's possible at the chip level. You see,
silicon innovation isn't just about better performance. It's about where performance meets possibility. And every advancement we make at the silicon level creates a ripple effect across thousands of applications worldwide, as well as those, those partners that we've just discussed. Our journey into custom silicon innovation began with a seemingly impossible challenge, reimagining cloud infrastructure from the ground up. We started very small, working with KVM on the off-shelf hardware for our nitro system. The initial step led to something transformative, the AWS Nitro System, the foundation today for all of EC2. Traditionally, hypervisors were responsible for everything. Protecting physical hardware and BIOS and virtualization, CPU, storage and networking while providing management capabilities. And that,
of course, requires resources. The Nitro system completely reimagined this approach by breaking apart these functions and then offloading them onto dedicated hardware and software. And we can now deliver practically all of the service resources to our customers. And it's fundamentally changed how we deliver cloud computing. Enabling us to innovate faster and reducing costs for our customers, as well of course as providing enhanced security. The AWS Nitro System is a comprehensive security and performance foundation for EC2. It provides zero operator access. Something that we're so confident in that we had it independently verified. The AWS Nitroy delivers hardware-based protection and enables encryption for your VPC traffic and EBS volumes. Those of you that listened to Rob about networking a little bit before. Through the AWS Nitro Hypervisor, customers get bare metal performance, and for demanding workloads, Nitro supports elastic fabric adapter, what I mentioned before, for ultra low latency RDMA communications. The secure Foundation extends to our networking infrastructure. We've built the most secure, reliable, and scalable network connecting all of EC2. And unlike traditional architectures, we've created one large, fast network that unifies all of our traffic flows. And the scale here is unprecedented. We connect over 1 million accelerators as an example in the single flat network. But it's not just about scale. It's about reliability. Our network automatically routes around link or switch failures without performance impact, completely transparently. We've built one common network that connects everything from GPUs to raniums to AMDs to Intels to gravitons to, um, you know, the Apple processes that we talked about before. Our graviton journey Which began in 2018, kind of like rapidly moved through the generations of here that you see in front of you, starting from 2018, 2018 to 2019 up to 2023 when we released Graviton 4. And each generation has marked a significant leap forward. Graviton 4, released in 2024 remains one of the most exciting chips that we've ever released. It features 96 cores and 73 billion transistors. Like I can't even compre comprehend that number. Um, and so if you, if you take that 73 billion for graviton 4, graviton 1 had 5. So it's just amazing, right?
So these, these numbers just become incomprehensible really to, to wrap your mind around how many, how much we pack into these chips. So the benefits are compelling. Up to 40% better price performance while assuming up to 60% less energy. One of the things for Graviton that we're really, really proud of, like uses less energy. So net potential soft soft cost savings um of up to 20%. The performance gain often means using fewer instances for you, for your workloads and potentially decreasing your total cost of ownership even further. And we spoke about that carbon footprint earlier in the deck as well. So, here is my challenge for you today. Take our, it only takes one initiative. I promise you'll be amazed by this and what you're gonna accomplish in just one week. With one developer and one workload. I've had so many examples from customers that took up this challenge and took a, took one developer, put them for one week on this. Absolutely incredible. Just think about it, right? Less time that it takes you to plan your next sprint, you can take one engineer and put them on this and see what you get. And the potential upside for you is incredible over here. So I really encourage you to go and spend that one week of one engineer and see what actually, um,
what you can gain here. So building on our security first approach, we've now made EC2 instances also at the station generally available. And this is a game changer for a big subset of our customers who needs to validate that only trusted software is running on the EC2 instances, including those with AI chips and GPUs. So with a combination of Nitro TPM and a testable Amazon machine images, customers can now cryptographically verify their instances are actually running trusted configurations on the software. And by integrating with AWS KMS, customers can restrict key operations to instances that passes this attestation test. And let's talk about Amazon EC2 ranium 3 Ultra Servers, powered by our 4th generation ranium 3 chips. Now again, there's gonna be some numbers here. It's very hard really to put these numbers into, into uh context, but I'm gonna try and kind of like, you know, give them to you so that you can interpret them. So these instances delivers up to 4.4% higher performance, 3.9% higher memory bandwidth, and over 4 times better performance for what compared to ranium 2 ultra servers. And what's truly groundbreaking is the scale that we can achieve. Terranium 3 Ultra servers represent significant leap forward in performance, um, scale up to an impressive 144 chips that we packed together in the Ultra servers, which is essentially 4 of these hosts packed together. Um, So truly groundbreaking performance for uranium 3. But numbers and specifications only tell part of the story. The true power of these innovations emerges when you see how they enable entirely new ways of building applications, which is the more interesting part. You've heard me talk extensively about our innovations in silicon, our broad portfolio of EC2 instances, and hopefully you interpreted some of those numbers, and the incredible performance that we're delivering at the chip level. Many of our customers love this level of control. The ability to fine tune their infrastructure down to the instance type and processor architecture. They need that control. But we also hear from customers who want to abstract away infrastructure decisions. They want to focus purely on the application code and business logic, letting AWS handle everything else. And so I like to think of my team as, uh,
you know, if you can visualize this, I like to think of my team as the team that creates little Lego blocks, right? And we decide on the size and we decide on the color, and we decide where we're gonna actually go and produce them and, you know,
which regions we're gonna put them down, where you can go and buy them. And our next speaker over here, Barry Cooks, um, VP of Computer abstractions, he's the, he and his team is that kid who walks in. Looks at all the Lego blocks that we've built, and he snaps them all together, and he's the one that wins the, that, that wins the best spaceship award. So let me show you how it is. Barry Cooks, please come onto the stage. Thank you. Right. How's everybody doing? I'm gonna take us in a slightly new direction here, and that is, I,
I'm gonna talk about compute abstractions, but I also kinda wanna do this in the frame of a little inside baseball. So let's talk a little bit about our strategy internally for our compute abstractions. What are our strategic pillars? What are we trying to do with all of these abstractions? I think there's a, a great quote, it's attributed to Mark Twain, although I hear there's some French mathematician that in the 1600s also claims the same quote. And that is, I didn't have time to write you a short letter, so I wrote you a long one instead. One of our key goals with our compute abstractions is that we want to be able to give you exactly what you need in a concise, clean way so that you can use that abstraction to meet your goals. And there looks like a lot of different logo madness going on on here, and there is. Because the other thing that's also true around compute abstractions, is that we have this inside joke slash criticism, and that is, if there's a good idea, there's at least 12 ways to do it in AWS. And the reality of that quote and that joke. I, yeah, there is. But realistically, there's just one way, it's the way you chose. All of those different 12 ways, we do that on purpose. We do that because customers tell us, here's how I wanna do it, and here's how I wanna do it. And we try to meet customers where they are, we don't try to force function them into other things. So that's been a key attribute of how we are trying to build these compute abstractions over time. There's a lot of detail hidden behind our compute abstractions. So there's 3 things to take away with respect to our strategy, our strategic pillars. Number 1, we do wanna make it easy. We wanna make it easy for your newer employees to use compute abstractions. We wanna make it easy for AI agents to leverage these compute abstractions. We don't want you to feel like you've stepped into a cul-de-sac. Right?
The worst thing we can do for a customer is they make a critical decision to leverage one of our abstractions. And then a year or two later as their workload's growing and changing, they feel like we pushed them into a cul-de-sac and they have to back out and re-implement or rearchitect in a different way. So we want to avoid those cul-de-sacs, we wanna be able to have you come in, pick an abstraction, and grow on that as needed. Then the third one. Is we want our expertise, we've got 20 years of experience running workloads at global scale, all over the place. We want that expertise built into the abstraction so that you can take advantage of the hard lessons we've learned over those 20 years. So that's a super important piece for us. Um, and I wanna say like, for you, what does it really boil down to? So our goal for our customers, we want anyone. To be able to use these abstractions. We want you to use them for any application, whatever language you've chosen. Whether you're doing a big fat old school monolith, whether you're gonna use containers, if you wanna just have functions, we want that to work across the board. We want you to be able to run it anywhere. Willem early in his talk, he showed you the global reach that AWS has. We want you to be able to run your abstraction and your code as close to your customers as you choose. And lastly, we want you to be able to do it at any scale. And I think this is an important one because, you know,
we talked a lot of startups and I have yet to meet a startup where they think they have a terrible idea and they're gonna be out of business in 9 months. Most of them want to grow in scale, and we want to be able to take it from small to global scale, and we want to support customers with that. So I asked my teams, what should we cover off as, as interesting launches, and they gave me 50 things. And I don't have 50 minutes, so I couldn't even do any kind of justice on launches today. Uh,
so a huge shout out though to the org because it has been a very busy year, we've done a ton of things. I'm gonna walk you through a few items that I think are super interesting. A great place to start is lambda. Lambda is kind of the ultimate of abstractions, right?
It's a developer-friendly system where you don't know anything about what we're doing under the covers. You just run your code. We take care of everything to do with infrastructure, so it's a super high level abstraction, it's super friendly for developers. But Lambda doesn't know everything about what you're doing. So one of the challenges that we identified this year. Was, hey, we are running at a scale and we have adversarial tenants. And we want to continue to use lambda, but we need to protect tenant A from tenant B. So you're running a multi-tenant workload. How can I do this? And the answer is you can, you can do it today, it's great. You just create a copy of every function that you have a tenant for. And if you've got 10,000 tenants, guess what you gotta do? 10,000 copies of the same function. Right?
And then you have to operate that, and you've gotta go manage that. And that is, um, frankly not very fun. So, what do we wanna do? We wanna make this much simpler, so we're pretty excited. We're launching lambda tenant isolation. With this, your developer stays where they wanna live, they're in the code. All they have to do is pass us a tenant ID. Tell us you wanna be in the tenant isolation mode, pass the tenant ID we will take care of the rest under the cover. So lambda will make sure that you get the hard security boundary around your tenants that you want. We will log and provide information about the tenants and where they've run. So this way you stay in the abstraction that you wanna be in. You're still working in the world of lambda, you're just telling us, hey,
this is multi-tenant, and here's the ID for this tenant. And then we will manage the infrastructures so that you don't have to worry about it, and we'll give you the logging and reporting on how that's working. Another area in lambda that we're pretty excited about. These days, agents are everywhere, so you have these emerging new workload patterns, one of which is, I need to call out to an agent, but then I gotta wait for something. Um, maybe I'm doing payment processing, I need to go do some fraud checks before I do the next step in my process. And you can create these multiple lambdas, and you're waiting, and you're building your own retry logic, right? You're having to deal with the fact that now I've gotta go replay some portion of a transaction when it comes back. You've got long waits that you've got to deal with, you're creating boilerplate code for all of these things. What you really wanna do is have a way to orchestrate this and have lambda help you go down that path. So we're super excited to launch durable functions. With durable functions, you basically can get in there, focus on the code, tell us it's gonna be done with durable functions, set your steps up. I even have a cute little weight in there while you hang out for 5 seconds, just to point out that we're not gonna charge you while you're hanging out waiting, right,
you're only charged for the CPU you use. So, with this. You're not writing the boilerplate, we're gonna make sure it's item potent. We're gonna store off the checkpoint between steps in your process. We're gonna restore all of that for you so you don't have to deal with those scenarios. We're gonna deal with retry logic. So we're gonna create a system where you focus on your problem and your problem was payment processing. Or maybe your problem is that you've got a manual process and like your VP of IT needs to go approve something in email, and so welcome to the world of like waiting 4 weeks for them to get back from their golf trip, right? Not a problem, we'll wait 1 year, if that's how long you need. So you can wait up to 1 year with these and then continue execution. So it creates a really nice environment for you to be able to go step through and build a different type of application than you've traditionally done with lambda in the past. Another area, and this one's gonna be fun. In the world of lambda, I talked about no cul-de-sacs, right,
we really don't want that. So something to keep in mind, dirty little secret, and it's not actually a secret. Guess where all your serverless functions run? On a server somewhere, right?
There are servers under the cover in lambda. As what one of the things we've discovered in talking with customers. They have loved the lambda model, they've grown massively in scale and lambda. And they've gotten to the point where they want to be able to take advantage of individual tweaks for performance reasons, right,
for cost reasons, a number of different things. And lambda doesn't let you do that, right?
It's a very high level abstraction. We're doing everything behind the scenes, you have no visibility into it and you don't get to touch anything. One of the things we realized was that's not gonna work for everyone. And I don't want you in a cul-de-sac. I don't want you to back out of lambda and try to implement on some other service only because your workload has changed over time and grown to the point where you have these economies of scale. So we're really excited now to roll out lambda managed instances. Some of you would have seen this about a year ago this week, we launched EKS Auto mode. EKS Auto mode takes advantage of the same concept, managed instances. We're gonna take care of the instance, we're gonna patch, so nobody has to patch, right? So now with lambda, you have that same capability. Your developer, unchanged experience. They're still just writing lambda functions, they're still focused on the same problem that they've been focused on, which is their business logic. But what this allows you to do at scale is to give us the decisions that you want us to implement with respect to the actual underlying hardware that we use to run those functions. And there's a number of really good benefits for this, that's an example I've got on the slide. Dylan was just talking about it. Maybe you wanna go use graviton and take advantage of the things you can do with graviton. Now you can tell us, go give me a graviton instances for these, right? Maybe you have an IO heavy workload. In the past, you couldn't tell us that. There was nothing you could do to get lambda to recognize your IO patterns. Now you can pick out IO intensive instances, and we will use those, and we'll execute on those. Another thing that's super useful here is that this is multi-concurrent. So if you are going to go write thread-sa code, we will run your functions multi-concurrently. It's a huge benefit. We're seeing customers who've been playing with this, getting 40+% improvements. Uh,
it can really both save you a lot of money and dramatically improve your performance. So we think this one is, is gonna be super useful because it doesn't disrupt your developer experience, but it does allow large scale customers with large scale workloads to take full advantage of the EC2 concepts and fleet and the different kinds of patterns that we have there. Now, as we've looked at the simplicity and we've said how do we keep things simple, how do we move things forward, we didn't just do things in lambda this year, we've done a number of things in other spaces. Not surprisingly, ECS is now also gonna roll into managed instances. So traditionally in ECS you had kind of these two spectrums. I could sit on one side, and on that side I had Fargate. Fully abstracting, right,
I'm telling you about CPU memory. And on the other side I had ECS EC2. I get to go on and drive and patch and deal with a bunch of issues. Now with managed instances, we're gonna meet you in the middle, maybe it's the Goldilocks zone. We're gonna let you go and take, uh, full control and run those instances. We'll still do all the patching, all the other components. The next space I want to touch on, still in the world of ECS. Deployments tend to be hard. People write lots of code to handle their deployments. They're trying to get a sort of scenario laid out where they're testing and safe, right? We wanna do things, step it into production. I talked earlier about how we're trying to make things easier. We're also trying to leverage our own expertise. We do a lot of deployments at AWS as you can imagine with our scale. And what we wanna be able to do is put some of that expertise into ECS. We wanna allow you to take full advantage of the things that we've learned over the years. Um, so we're super excited today to roll out ECS native deployments. With native deployments, you can do blue-green. You can do uh canaries. Our best practices are built into these. We will auto roll back, right?
So you get these capabilities, the same sorts of things that we do day in and day out in our regions as we roll out updates to our services. You can now do built-in as part of ECS. Another space that we've talked about quite a bit. Containers as a concept aren't terribly complicated, they're fairly straightforward, lots of developers uh have wrapped their heads around them. Container orchestration is a complicated set of tooling, right,
like we just talked about deployments, for example. So it's not always easy for people to get started who are new to this space. One of the things that we've recognized is it's, it frankly takes a lot of time in terms of learning curve for folks, and we wanna see if we can shorten that. Um, and so today we're rolling out a new change in ECS, it's called ECS express mode. And the idea with ECS express mode was, can I get it down to one create call to go create my application? And the answer is yes, cause we just did it. Can I do this in a way that is easier for me to consume? Yes. So, when you would do this in ECS in the past, you had about 300 parameters you would be setting and telling us about in order to roll a new app out. We've got that down to 30. We dropped it by 10, factor of 10. So, now you can go set a relatively small set of parameters. Um,
I like to call this the George Foreman grill, but I'm dating myself slightly with that. You wanna set it and forget it. You want us to manage as much of that complexity as possible. And that's what ECS express mode is all about. And I think, uh,
it's been such a busy year for ECS that I wanted to kind of walk you through how we've approached this pattern. What does it look like that we've done? And,
and so if you look at the bottom of this slide, you can see what we've typically managed in the ECS world when you're on ECS EC2. And you can see what you've owned and what you've had to manage and what you've had to control and think about and wrap your head around to run things in ECS. As we've rolled forward over the course of this year, when we add in ECS managed instances, we take a lot of burden off of folks' plate. All of a sudden you're letting us take care of a slew of things that have been your problem in the past. When we rolled in deployments, now we're handling that aspect of things for you. Right? As we roll in and we say, hey,
we're gonna do ECS Express, we've rolled another set of things that we're gonna go and manage on your behalf. And what this leaves you with is a few security permissions that you're gonna own and set, and your actual container app. The thing you actually care about, the thing you're trying to deliver to your customers. And we can take care of these other pieces. Now,
I mentioned before, we don't want cul-de-sacs. And we definitely do not want to create cul-de-sacs with ECS. So, do I have to revert to ECS EC2 if I find that some decision we're making in here, let's say on the load balancer is making you unhappy? Cause we are making decisions on your behalf if we're managing it. Um,
we want you to be able to peel the cover back. We want you to be able to take on management responsibility for these components. Things like that load balancer, maybe you wanna change those settings and you wanna own the load balancer piece, but you don't wanna revert everything. So we're gonna support letting you peel that cover back, letting you go in and take over management capabilities for some of the things that we're managing. It gives you more flexibility, uh,
lets you kind of dive in and do things in your own way. Now, ECS is not our only container orchestrator. It is not the only place that we've been trying to focus on building things out that will help our customers and manage more of their components. We have a lot of folks with open source first policies, a lot of folks who love Kubernetes. And,
in the Kuberneti space, we have iterated over time, we've been adding more and more management capabilities. Um, that's what our customers have asked us to do. As you go through and you try to scale up and manage dozens or hundreds of clusters across regions, it starts to get pretty complicated. People have lots of tooling that they want to use for these things, and now you're managing tooling and the clusters. And so it can create a pretty challenging environment in terms of your headspace. So what we've rolled out is Amazon EKS capabilities. Similar to what I showed in the ECS model, we're taking on more management responsibility for you. We're gonna try to make things a little easier, especially when operating at scale. So in this case, the first thing that we're gonna roll out, Argo CD. Super popular open source framework, most of our customers were using. I didn't meet a single one, including people who were maintainers of Argo CD who enjoyed managing the thing. Right?
So we will manage Argo CD on your behalf, make it significantly easier for you to step in and get your deployments done the way you want without all of the management overhead. Uh, a couple of other things that are rolling out immediately in this is also crow. The Cube Resource Orchestrator that we rolled out as an open source component, uh, delivered by our team here at AWS, ACK, another, uh, controller for, uh,
Kubernetes from Amazon. These will all be managed components inside the stack, so you don't have to go and take on the management responsibility for those. If you remember last year, Carpenter in auto mode became a managed component in the ECS, uh,
EKS stack. So we continue to kind of build out that framework and get more and more of those capabilities built in. Have feedback, have favorite ones, want something else, let us know. We're super excited about this framework. This is not the last of our capabilities. We want to continue to add them as customers ask us for new and additional things. Now as we talk about running clusters at scale, one of the focus areas that we had this year was uh around scaling for AI workloads. So 1.6 million trinium clusters in a single EKS cluster. That's a lot of trainium power. Uh,
this is one of the ones that's being used by Anthropic. We worked directly with the team in Anthropic, worked with the community in Kubernetes, worked on our own backend components, and we're able to support the scale needs for cloud and future training on cloud. We did the same with the Nova team here at Amazon. And now we're working with customers who are not in the AI space, who also have clusters of, of huge scale, would like to have uniform single clusters for various reasons, and we're working to unlock those capabilities next. Now as you look at the clusters and you start thinking, man, this is 100,000 nodes, you run into a really interesting problem when you hit these kinds of scales. And that problem is math and statistics. Right, when you start running things at this scale, something's gonna break, I guarantee it. Every single day something's gonna break. And that is a huge time sink for customers. So in Sagemaker. We have observed this, we've worked closely with a lot of customers who are doing training, and we wanted to get your good put numbers up. We're super excited to say that we are now going to do checkpointless training. So, we started this year, we focused on checkpoints, we've put them in memory, we tried to make them faster to load, faster to save. And at some point we said, why do we need checkpoints in the first place? Like why can't we recover your cluster with the right mechanisms built into the core of how Sagemaker operates? And that's what you will get with restart list checkpoint, or sorry, restart list training. Checkpoint list. Um. The great thing about this, you don't have to do anything other than turn it on. We will manage this for you. We will automatically handle these things. We're seeing good put numbers north of 95% from customers who've been using this in, in anger at this point. So it can dramatically change in a completely hands-off way how you recover from faults. The faults are still happening. But you don't have to worry about them. Your data science team doesn't have to restart a giant cluster, reorganize things, load that snapshot back, right?
None of that stuff has to happen. And so it's a huge move forward for us, uh,
with respect to training on Sagemaker. Another place that we've struggled with Sagemaker has been. People build the cluster out. They have multiple things they want to run on that cluster. Invariably, you have conflict, usually business priorities is your conflict, um,
and starting and stopping training is super painful. It takes too much time, it's hard for people to do. So, what are we gonna do about it? We decided to introduce elastic training. So with elastic training on hyperpod, based on your priorities, we can allocate. Additional CPU and additional GPU to the nodes that are, that are needed for the higher priority task, automatically reallocate them back for the lower prior ones in the future. So it allows you to very quickly and flexibly move forward with your training, again, without having to deal with all of the pain points on your side of the, of the responsibility line. So the next challenge we've seen with customers is customizing models. Right?
Models are fun, but they're not really that great unless you can get them to do what you need them to do in your shop. And using your data is requiring a lot of skill, a lot of time, a lot of experimentation. And with Sagemaker now, we're gonna change the game again. With AI model customization, you will get clean, simple workflows, so that non-experts can actually come in and do those training runs, get the customization in place. And this model, uh, includes an AI backend that will help you select, like, if you're not super familiar. Which model should I even customize in the first place? What if you don't have your own data, you just want some additional data in a partic of a particular type. We can now start to help you with each of these core problems. We can generate data sets, we can evaluate your model for success, we can look for bias introduced by the new data that you just rolled into an existing model. And we can help you iterate to get to a successful outcome for your customers. We thought about that for a while. And kind of like the one on Checkpoint, we said, well, so we're gonna let you customize models, but one of the challenges with AI models is that you train them and they go, they're kind of like an old dog, right?
They get old fast. They don't learn new tricks really well. And the biggest challenge we've seen with training models has been. Once you get into going and customize a model that's already gone through its full pre-training. You,
you're not gonna get the kind of results you might want. What you really wanted was your own model. And you couldn't figure out how to build one. It's too hard. So now, what we've decided to do is NovaForge. So you can use Sagemaker. Open the hood, grab an intermediate checkpoint for our training of a Nova model, and then modify it and continue the pre-training. At that stage in pre-training is the best time to introduce new data to a model. So this will allow you to go in. Change the data mix, we'll give you full capability to look at exactly what data set we were using and how we were doing it, you can change the mix. You want more scientific data and less data on other aspects of language? Great, change it around. You've got custom physics modeling and stuff that you want to put in the model, you can add that data. It
allows you to come out the other side with your very own model, fully customized. It's your way to get to a model that is your proprietary capabilities built into a frontier model. So we're super excited about that, we've got that working and a number of customers have seen amazing results with this, including Reddit. So We talked about a whole bunch of things here with respect to how we're kind of trying to move the ball forward. And I think that one of the things to keep in mind as we go forward is you can get anyone, any app, anywhere at any scale. And before I bring Villain back out, cause he's gonna come back up here for a second, I wanted to highlight, he had that really cool Zooks customer experience demo. I'm super competitive, so I called a friend. We've been working really closely with Aurora. Aurora does self-driving trucks, they're much bigger than the Zook's cars. I was going for size. These are off and running. They're, they've been working across our systems. They're doing runs in Texas today on the interstates. You'll see them, uh,
give them a wave. They won't wave back. They're expanding to the US. They're gonna run coast to coast, California to Florida in the near future. Uh,
and with that villain, why don't you come on back up here and tell folks about it. That's right, and you? All right. Um, like I said, the cool kid that snaps, uh, all of my Lego blocks together and then walks away with a spaceship and the prize. Um, so to put this into perspective, every single day, Aurora processes more than 5 million transactions and about 17 petabytes of information that it needs to process every single day. So this is another example of rapid innovations complementing our longer-term engineering investments. But the real power of these investments come alive in how our customers are using this. So from Intercom, pushing the boundaries of AI agents to Amazon Leo building those cloud architecture and space to Zos processing real-time autonomous vehicle data to new bank transforming financial services for millions, each of these customers represents how AWS innovates and enables transformation at scale. And so I want to emphasize that everything that we've discussed today began with a customer conversation, a challenge that needed to solve or a possibility waiting to be unlocked. And whether you're optimizing current workloads like Mercedes-Benz that we spoke about that SAP workload, or scaling globally with your expanded regional presence and enhanced capacity management. Or transforming your business with the latest GPU and custom silicon innovations, AWS Compute provides the foundation you need to succeed. And today we've seen that transformation happens, how it happens, not just through technology, but through that partnerships that we spoke about, the innovation and the relentless focus on your needs. And I'm excited to see what you'll build with these capabilities. Thank you for your time today, and now let's continue to build what others think is impossible together. Thank you.