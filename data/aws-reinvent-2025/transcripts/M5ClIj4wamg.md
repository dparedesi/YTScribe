---
video_id: M5ClIj4wamg
video_url: https://www.youtube.com/watch?v=M5ClIj4wamg
is_generated: False
is_translatable: True
---

Thanks for joining on the end of the day. Um, I imagine I'm holding you back from your, um, you know, your dinner plans and your happy hours, but thanks for joining us and meeting us all the way out here at the MGM as well. Um, I think we have a great talk today we're I'm pretty excited for this one. what we wanna look at today is looking at what the end to end model life cycle is and how we can optimize every part of that and we're gonna be looking at how we do that on trainium. And so today, um, I have a couple of guest presenters with us to bring this topic to you Matt McLean, who leads our customer engineering team at Annapurna, and we also have Randeep, uh, Bhatia that is the CTO of Splash, and he's gonna bring a lot of color and excitement, uh, to the end of this presentation, um, so with that kind of jumping in, kind of starting from the beginning, like, you know, the, the circle of the AI model life cycle. Um, and, uh, we generally start looking at applications from, you know, like, um, you know, looking at a, uh, gen AI models or agentic workflows, and that's a big deal right now, especially at AWS. How are we going to optimize for these agentic workflows when the compute demands and the cost demands are getting, uh, so high. So what I wanted to look at uh with you guys today is really looking at well how do we optimize every level of this and what is this kind of AI uh model life cycle that we should be looking at and what are the different parts of this puzzle that we can optimize for so this probably looks very familiar to all the builders here in the room today, right? Um, you wanna typically start with an idea, yeah, a business problem, an application that you want to build a model for to serve those needs, right? So you're gonna start with your use case discovery prioritization, some type of, um, uh, qualification of what the use case you're driving towards. Next you're gonna kind of look at the data you have. You're gonna look at open source data sets. You're gonna look at your proprietary data, what is your value add to this as well. And then you're gonna move on to model selection. You're gonna think about all the different types of models that could solve this problem and what the best case, um, what the model is gonna be best for that use case. You're gonna adapt that model, make it very specific to your users, um, uh, uh, you know. Uh, care about and their applications, um, and then you're gonna start evaluating, you're gonna do some offline evaluation, then you're gonna optimize it for deployment. You're gonna then reevaluate it under your production metrics, right? Or is you serving it fast enough? Is the latency meeting the customer's your customer's expectations, um, and then you're gonna deploy for scale. You're gonna scale it up, you're gonna roll it out, you're gonna test it with that audience, um, hopefully you're gonna start making some money on this one as well, right? And then ultimately you're gonna rinse and repeat, you're gonna build on to the next application and and the cycle continues and it never really ends, right? But one of the, there's key parts of this kind of circle of life that we wanna talk about today and that's really looking at all of these pink levels from model selection down to um uh optimizing for your production environment and really when we look at the cost of building and deploying models this is where you're gonna spend the most money. And the reason for that is the decisions you make along these points on this axis on on this part of the circle are really gonna dictate the business value metrics you're gonna see at scale, right? So if we can optimize the circle of life at this level first, um, choosing the right model, choosing the right hardware to deploy that model to choosing the right libraries and techniques to adapt that model in the most efficient way. Um, we're gonna be able to kind of optimize all the way through to our deployment life cycle and then scale it up, right? So when we take those are kind of AI bottlenecks out of this, right, like we wanna be able to iterate quickly on the front part of our model life cycle which is. You know choosing the right model uh choosing uh typically being able to leverage a lot of the um uh resources out in the uh open source community to do this we wanna find, um, we wanna overcome slow iteration processes slow, uh, fine tuning processes and and accelerate that we wanna manage our rising compute costs right because the more times we go around that circle, uh, because we weren't. Um, optimizing at all of these different, uh, um, um, steps, it adds to our cost and it adds to the cost of our application that then we need to recoup in the market so we wanna. Um, manage our compute costs, um, we wanna be able to deploy on hardware and to think about, um, the kind of compute scarcity when we're getting closer to our deployment environment and starting to scale up. But before we get to our scale and deployment, we wanna make those, um, considerations early in this, uh, um, um, model life cycle as well. And then even from selecting our model. And how we're kind of adapting that model and all the way through optimizing it for deployment we're gonna be considerate of you know is this gonna meet my customers um careouts right can I serve this fast? can I serve it cheaply? Can I serve it at scale so all of those things that we wanna talk about today of how to overcome those, um, we wanna think about our you know we're here to to kind of consider um and look at overcoming those bottlenecks with training. Um, and so we're gonna be focusing on how do we create faster iterations. We're gonna be looking at how do we select from open source models so that we don't have to pre-train models, um, so that we can adapt those models more quickly, choosing the right hardware to not only reduce our cost to deploy and build these models with utilizing trainium, uh, but also being able to serve them and, um, um, serve them both, uh, from a deployment perspective and easier access to compute as well. Um, and then of course optimizing for um lower inference times and total latencies or the objectives that you're looking for that could be high throughput, low latency, or, or total end and, uh, throughput as well. So we're gonna be touching on all of these, uh, throughout, uh, the, uh, this presentation. And I wanted to start with what do we have at AWS. So at AWS, I'm sure with the other sessions you're visiting today and all of the other keynote messaging, we have a full stack for you. So if you're developing at a higher level of the stack where you want to just, um, take models, um, from our agentic workflows, um, through a, um, uh, Amazon Bedrock or utilize the SK SDK for agents using Agent Core and other, um. Strands libraries, it makes it really easy to start developing those more complex gen AI and agentic work flows, um, rather than starting from scratch and as you kind of go down the stack and you wanna own more of this stack yourself and develop it in your, um, environment because maybe you have, uh, data residency issues that you wanna be cognizant of maybe you are worried about, um, you know, you wanna. Optimize for the end to end life cycle of our um model life cycle as well to optimize at all levels and own all of the data and you wanna make sure the data lineage is is gonna be clean throughout then you'll wanna kind of take um an infrastructure first approach where we're um you know you're um doing this on your self managed instances right and so today we'll focus on training and infer as part of that and. This is probably a good segue to training in influentia. How many of you have heard of training in inferentia? Oh, that's fantastic. Um, that's more hands than I'm used to. Um, how many of you tried inferential and training for your applications today? OK, so, well, we got a couple. That's wonderful. Um, so for those of you that are not super familiar, we're part of Entreprene Labs. We're the hardware division of AWS and we build purpose built silicon. That serves our customers' most challenging problems, right? So the first product that in, uh, that Annapurna brought to market was nitro for network acceleration and, um, um, hypervisor solutions. Uh, the next product was Graviton, um, uh, bringing general purpose compute with lower cost and higher performance, and of course inferential ranium. And we've been doing this for a very long time right now over 10 years, um, in, in that 10 years we've produced and, and got to market many chips across our three product families and now with Tranium 3 launching this morning in the keynote, this is our 4th generation ML chip, right? And we've been building these solutions and focusing on inferential and training for a few reasons, right? Uh, fundamentally when we were working with our customers here at AWS we noticed that the types of bottlenecks our customers were facing. Um, first performance right as the models get more complex as we expect more out of our models as the now agentic workflows that are expecting 10x or 100x more token generation requirements or even training these models, uh, more efficiently, um, we wanted to make it, uh, we wanted to provide high performance options for both training and inference. We also wanted to make it lower cost for our customers and so investing in our own silicon, uh, made it a more um achievable to deliver those cost savings to, uh, customers as well. And lastly we wanna make it more accessible. We wanna democratize AI. What that means is it shouldn't just be the ones with the deepest pockets get access to the compute they need. And so we wanna give customers choice, um, and we want to, uh, invest in these technologies, um, to build our own AI chips for these reasons, right? And so over the last 6 years we've now introduced our 4th generation chip, um, now Trinium 3, and we're super excited about bringing these features to you and what it could mean. To inspire the next generation of agentic models or MOE models or video generation models, but our other generations like Trinium 2 or Inferential 2 are great accelerators for small models for, um, you know, um, uh, embedding models as well. So there's still a lot of opportunity to use the entire product portfolio. But with our time today I'm just gonna be looking at, um, or wanting to introduce Trinium 3. Um, ranium 3, we've doubled our compute on chip, we've, uh, increased our memory capacity on chip by, um, 50%, and we've increased the memory bandwidth by an additional 1.7 times. And what this really means is when we put it all together into our second generation ultra server we're giving you 4.4 times more compute. Um, FP8 compute then we, um, made available in our last generation. We also are giving 3.9x higher memory bandwidth and 3.4x more memory capacity at the full ultra server level, and we've innovated at every level of this, um, um. Uh, server design as well. So starting from the chip, we also redesigned our, our, uh, compute sleds and the servers themselves, bringing in better switching matrix as well with our, uh, introduction of neuron. Uh, switch and what this allows is that all 144 chips to work, um, in very low latency, high bandwidth communication with each other and this is especially important as we're moving into these more complex, um, AI workloads we're using multiple models together, uh, to make your final end and output and so it allows you to scale your let's say for example large MOE models very efficiently across all of these chips in an extremely low latency. Um, and so with this now, you know, all the specs on the slide are always, you know, kind of, uh, maybe a little dry. What does this really mean from a, a performance perspective, right? So there's kind of two ways to look at what this performance really means. The bottom line on blue here, that's our, uh, training 2, our previous architecture, and the light blue line above it, um, is our, uh, training 3 ultra server, and we're looking at GPT OSS and serving this at scale. And so the dimensions of this chart are interesting on the um bottom on the on the horizontal axis we have um. Uh, our interactivity or tokens per second, how many tokens can an individual user get? And on the y axis we're looking at how many users can I serve or my concurrency of that model. So with Tranium 2 you're able to kind of deploy your workloads today and. Get, you know, really responsive, uh, models, um, and you're able to do this with, uh, you know, kind of I would say medium concurrency, right? So we can serve these large models we can do it, uh, we can serve multiple users at a time and get good performance. So if you have workloads today that are utilizing models like this, you can deploy it directly on Tranium 3 and immediately now serve more, um, you know, 5.4 times more users with that same exact interactivity, right? Each user will get the same experience. And now you can serve more of these uh customers bringing down your total cost and being able to uh expand to more workloads. The other way to think about this is, well, you know, I want to increase the number of features and performance I'm giving each of my users in each of my customers, right? I wanna give them deeper reasoning. I wanna mix, uh, provide them content for more models as I'm building this out so. Um, the other way to think about it is now we go horizontally, right, with the same exact, um, concurrency I serve, now I can generate more tokens per user, uh, per chip. And what this allows me to do is do deeper reasoning, more complex models, more turns inside of my agentic workflows, um, you know, uh, serve and generate more tokens, um, without increasing the cost, without increasing or reducing the user experience to my end customers, and we can deliver over 6x the number of tokens with ranium 3 ultra servers than we could with ranium 2 ultra servers and so. We're really excited about that, uh, clearly I'm really excited about that as well, right? Um, and so we're also excited to see Anthropic and our other lead customers kind of leaning in. And sharing that excitement as well as you may have already heard, um, um, through other parts of reinvent this year or in in previous news, um, you know, Anthropic's been ramping up their trainium to usage over this last year and are now with our, um, at Reinvent last year we launched Project Rainier. It was an ambitious project at the time where we wanted to launch the largest compute cluster for anthropic to take advantage of. And we're pleased to say that that is now in full production. We had a goal, a stated goal at the time of deploying hundreds of thousands of chips for, uh, Anthropicus Project Rainier. We ended that with 500,000 chips deployed, uh, in Project Rainier and a million chips already deployed, um, uh, serving all of our customers this year, and we're on track to exceed that already with Trainium 3. The other really exciting part of this is not just for LLMs um and I think that LLMs have a lot of excitement going on right now um but there's also a lot of innovation happening in the computer vision space or in the video to video space one of our lead partners. In this um uh area is Descartes. Descartes is a startup focused on building more visually exciting experiences with their models and so they have a lot of innovation there as well. They're actually doing a session tomorrow which I'll share the session idea at the end and I highly encourage everyone to take a look at them. We worked with them, um, uh, you know, ahead of our Tranium 3 launch today, uh, to see what they could do with Tranium. Um, they were, you know, very extensive and experts of using, um, GPUs for a very long time, um, and they built a lot of kernels to accelerate their models and innovate on top of a GPU platform and so when we introducedranium to them they were really excited and really met the kind of, um. Architectural criteria that they had in mind as well and in under 2 months they're able to optimize their workflows and models and bring it toranium 3 and experience kind of higher performance lower latencies than they were able to achieve with their previous architectures. So we have a session tomorrow where the, uh, one of the co-founders, um, actually it's a, it's a brother pair. So Dean is the CEO and the CTO is Oren. And he's gonna be presenting tomorrow. He's gonna go to a lot of depth. I highly encourage that. We also have the Descarte demo running in our expo, uh, um, booth. So definitely, uh, uh, talk about that as well. And of course Splash Music. So we're also have the, um, uh, privilege of having, uh, uh, Splash Music's, uh, CTO, uh, run deep here as well. So he's gonna be going into more details on their use case, and I'm super excited for everyone to hear about that. So with that I wanted to pass it over to my colleague Matt to talk a little bit more on the libraries. Thanks, Cameron. So, what I'm gonna do is kind of, he, Cameron, uh, gave you an overview of the AI model life cycle. So I'm kind of gonna double click on a few of these stages to give you a little bit more details on how you should approach this, especially with AWS training and and friendship. So the first stage is model selection. That's when you want to select the model that you want to use. Um, so there are, you know, many different models to choose from. Uh, here is a benchmark from a website called Artificial analysis. AI, and you can see the, uh, most high intelligence. So intelligence is essentially a key metric for selecting your model. Uh, the, um, models in black, uh, are the proprietary models. So these are models typically only accessible through an API. These are the likes of Claude, uh, from Anthropic, uh, Gemini from, uh, Google, and, uh, GBT5 uh from OpenAI. And these are, you know, awesome models, right, uh, really high on the intelligence scale. Um, but what you may not know is that openweight models, so these are models, for example, that you can download from hugging face, are actually quite comparable. So, uh, this is a fairly recent benchmark, uh, just from last week, and the Kimmy K2 model is actually very, very close to the top-performing Gemini 3 model. So the key message here is don't exclude openweight models. They're actually very, very competitive and have very high intelligence, um, uh, compared to these proprietary models. So, intelligence is one criteria. Another key criteria is cost, right, because especially if you're deploying, you're gonna be making a lot of inference calls to these models, right? One thing is to do a small proof of concept. Another thing is to deploy at scale, and here's really cost is another key criteria. And so this is showing uh on the vertical axis, the intelligence scale and on the horizontal axis, it's showing you the actual cost, cost to do inference for these models. So again, proprietary model is great on the intelligence axis, but they tend to be uh on the right-hand side of the cost to run. So sort of in that top right quadrant. And really what you should be thinking about and the ideal models is essentially the top left corner. So that is high on the intelligence scale, but lower on the cost. An interesting thing here is that the only models that are present in this top left green quadrant are actually the openweight models. So models such as GPTOSS, uh, there's Min Max, uh, M2, and also Deep Seek, and these are only openweight models. And the good thing is, is that all of these models uh can actually be uh fine-tuned and deployed on AWS training and inferential. So we've selected our model. Now typically the next stage is the adapting the model. Uh, so in technical terms we often refer this to post-training. So where you're doing things such as supervised fine-tuning or using reinforcement learning to really adapt the model to your specific use case. So one of the most popular open-source libraries for doing this post-training is the Hugging Face Transformers library. So any users of hugging Face Transformers? Alright, so we've got a few users. So, we've actually collaborated with Hugging Face into an optimized version of the Transformers library for AWS training in Frenchia called Optimum Neuron. So essentially it provides you the same APIs you're familiar with in the Transformers library. But under the hood, we've done a lot of optimization. We've integrated very closely to our software stack and we've integrated a whole bunch of kernels to make sure that the performance is really good when you're fine-tuning or you're deploying these models. And this is just showing a snapshot of the landing page, the documentation page where you can go and find more information on this library. So the steps in terms of when you're doing your post-training using optimum neurons are essentially these 4 steps. You're going to load and prepare your data sets. You're going to fine tune your model using Laura. Laura is a very efficient way to fine tune a model. It uses a lot less memory than standard full model fine tuning. Step 3 is you want to consolidate your Laura adapters into the open weights and then finally, optionally you can push up into the hugging face hub. So if we go into each of one of these, so loading and preparing the dataset, this is nothing specific to AWS training. This is what you would have to do on any sort of fine-tuning project. Um, you have a choice of over 400,000 data sets. These are all up in Hugging Face Hub, you can choose from, or, uh, typically in a business will actually take their proprietary data and you want to use that in order to generate. Uh, you know, uh, have your model generate things that are specific for your business. So once you've selected the data set, the next thing you need to do is kind of format it depending on the kind of application. So for example, if you're building a chatbot kind of application, you want to format the data set into a kind of an instruction, um, format, so you can kind of telling the model how it needs to the types of data that will be sent to the model and the way it has to respond. So now we get into, once we've prepared the dataset, we're ready to fine tune, to launch our training jobs. And here you can actually take existing, say, re scripts you've used on other accelerators and basically bring it across because the code changes are essentially minimal. And here I'm highlighting essentially the only kind of classes you'd need to replace. This is sort of replacing the standard Transformers classes with optimum neuron classes, uh, and then things should just work. So we've trained our model, uh, we've got our Laura adapters. So now we want to kind of consolidate everything together. Uh, and here's a couple of different options. Uh, the first option is to use the Optimum neuron CLI. So this will essentially consolidate the Laura adaptors into the open weights, uh, downloaded from hugging face and bring them all together. So that's one option. And the other option shown below is just standard Python code where you have a little bit more control over how that consolidation will work. So now we've consolidated all into one package. Now you can either deploy or if you want to actually share this newly finely fine-tuned model to, for example, other users, then you have the option of then pushing it back up into hugging face. So here's just a code snippet showing you how you can actually do that. Alright, so we've adapted our model, we've uh done the fine-tuning. Uh now often what you can do is you want to optimize the performance of your model, uh, and this is what we'll, we'll look into now. So before we kind of show how you can do this, just to kind of level set on a few kind of base practices and principles. Essentially performance optimization is all about trying to maximize the utilization of our AI chips. And when you're talking about um AI chips that typically means you want to ensure that it's doing, it's what we call compute bound. So it's basically using all of the available flops in the accelerator to the maximum extent possible. And there's a few different ways you can do this. Uh, one way is you can pipeline operations. So what this means is essentially while you're doing, um, for example, doing a matrix multiplication for one operation, you can at the same time load the data for the next operation in parallel. And you can also, um, save the data for the subsequent operation at the same time as well. So you're ensuring that your, um, your compute engines are always being utilized. The second principle is you want to minimize the data movement, um, because, you know, it is, while the bandwidth between the different, um, hierarchies of memory are fast, you still want to typically keep, um, memory on the chip as much as possible. So this is what we talk about minimizing data movement. So for example, for our activation tensors, we want to keep them in the chips. SRAM has a small amount of very high bandwidth, but small capacity SRAM, typically in the sort of tens of megabytes, and we want to keep our activation tensors there, in order to save you doing lots of reads and writes back to our high bandwidth memory. Um, as well as minimizing data movement, when we do have to move data, we want to maximize the throughput. So this is typically we want to make sure that our read and write operations are using large chunks of data rather than having small amounts. So that will just ensure that our bandwidth is maximized. And finally, when we're uh doing running inference on our models, we're typically running large models, right? It has uh tens of hundreds of billions of parameters, uh, so it means it can't fit on a single accelerator. Uh, the model has to be sharded across multiple accelerators and those accelerators have to communicate between each other. And that communication is what we call the collective's operations. So, one key kind of core principle is we want to ensure our collective's time is less than the time it takes to perform those computations such as our matrix multiplications and the like. So how can you actually know if your model is optimum, if it's really performance optimized for AWS training and influentia? So we've launched this week a new tool called Neuron Explorer, and essentially this is our new profiler, um, and this provides you a nice different levels of hierarchy, so you can view your model from a high-level sort of each layer by layer module or operation, right down to the low-level instructions running on the hardware. We provide it via a web application or an integration like a plug-in to VS code. And very soon we'll have integrations to do system profiling. And what system profiling means is you can also view how your hosts CPU and memory are also performing as well as your device. And here is a kind of a screenshot and, in a second, I will actually go through a full demo of this new neuron explorer tool. So let's say we've found a bottleneck in our code. Well, what can you do about it? How can you actually improve the utilization of your AI chip? And this is where our kernel interface comes in. It's called neuron kernel Interface, or NICI for short. And essentially this provides you full control and programmability of our AI chips. Um, we provide it as a Python DSL, um, and you can basically write low-level ISA commands, which essentially are the machine instructions that your AI chip will execute. And this gives you control of all such of things, such as the way your memory layout, uh, how you do tiling, how you do scheduling and allocation of your, of your model. And so for example, Cameron mentioned earlier Descartes. So Descartes used Nicky to essentially optimize their video to video model to get the best performance possible. OK, so now we're going to have a demo of the Neuron Explorer, which again is a new tool just launched this week. So here is a profile of a large language model. It's a decoder. It's been not optimized. Um, and here we can see a summary page. So on the top left, we can see sort of just some overall, um, some flops utilization, how compute efficient our model is. We've got these uh bars that we're going over, which is essentially the utilization of the different engines within the chip. So we have a tensor engine, we have a scalar engine, a vector engine, and a general-purpose simmed engine. We've got some recommendations that it will actually tell you things that it's picked up and give you some recommendation next steps for how you can alleviate some of these bottlenecks. We also have some info on the collective operations around the size. You can spot outliers that you can dive into. And also we have a memory bandwidth utilization, so typically we want to keep this as high as possible. It's quite low in this particular case, so that's another indication there's a bit of work here we can do to optimize this particular model. So that's one view. Another view is a bit more detailed view. This is of the same models, um, and here we have our hierarchical view. So we can essentially go from the each layer in our model, uh, we can dive into it and then within each layer we can see, for example, the major components. Typically these things like an attention component and an MLP component and within each of those, we can break them down into the specific operations down to, for example, a matrix multiplication, and add or say a value. On the right-hand side, top right, you'll see a similar sort of view, right down to the operation level, and we can see how efficient each of those operations is on our hardware. And then on the bottom left, we can actually see how this all maps down into the specific engines running on our AI chip. So for example, we can see the tensor engine utilization, we can see vector scalar engine, all the engine utilization. We can also see the Collective operations when we are having to communicate between chips, we can see how long each of those operations takes. We can also see our memory bandwidth utilization, so how the memory is being moved around on the chip and how efficient that is through the sort of low level device view. So what we can do is, um, also look at a specific module. So we can, for example, let's take the attention module, and we can actually look and see how long the latency is for that particular operation. So here we're taking sort of two markers and we're seeing that it takes around 100, uh, it's pretty hard to see here, but it's about 125 microseconds to run the attention part of our model. So that's one sort of data point, so we can use that as our baseline. And then what we're doing now is we're actually looking, I've opened a new profile and this is an optimized version of that same model, uh, but this time we're using a Niki kernel. So again, we have that hierarchical view that I mentioned before. We can go down and see down to the low-level operations. We can go again and mark, for example, the time, the latency of the particular component. Uh, and here you can see that we've actually sped it up. So we've gone from 125 microseconds down to around 79, uh microseconds. So I have, have about a 50% speed up of, on this particular model. We also have a new feature, uh, which is a code view. So we can actually map the specific instructions running on the hardware back to our NII code. This is our kernel code, which is showing you on the top right. So on the top right is our NII code written in Python, and we can highlight an area in our device profile and actually see which lines of code. And our Niki kernel actually are responsible for that instruction. So what you can do is point back to a line of code and then you can go back to your code, optimize it, try something new and see the effect on the operation or the speed up of that particular component and how it maps to the low-level device instructions. Right, so that's the uh demo of the new explorer tool. So once we've kind of optimized the model, now we're ready to actually deploy a model into production. So this is the deploy and scale phase. So one of the most popular libraries used for deploying, especially foundation models, is. So any users of here? OK, we've got a couple of folk. So it's a really popular uh open-source library. It's really designed for high throughput, low latency LLM serving. And it does this through various different mechanisms. It has a really efficient KiwiCash, um, uh, management, um, mechanism. So it's doing things such as page detention, uh, and also really efficient batching. So it's using concepts such as continuous batching. Has a very open, vibrant, open-source community. There's, uh, folks from Red Hat and many other companies contributing to this, uh, and has a lot of different model support. So your popular models like Lama, GPTOSS, DeepSeek, you know, the most popular open weight models, uh, supported by VLLM. And now it's part of the Linux Foundation. So a lot of collaboration happening. So here's just an example of a code snippet of how you would actually use VLLM. Really simple to use. You set up things such as your sampling parameters like your temperature. Um, you would configure things such as how you want to shard your model, so how many, you know, accelerant chips you want to shard your model weights over. In this case, we're using tensor parallel to shard our, um, model over two, tensor cores. Um, and then we can basically call generate to generate some outputs. So VLLM is fully integrated with AWBS Tranium and influentia, and this is done via the VLLM neuron plug-in. So we have support for some of the most popular openweight models, LA, Quinn, GPTOSS, and Mistral. We've integrated a lot of kernels, so our team has been busy writing kernels, so you don't need to write them for the most popular open-source models. There's things such as flash attention, uh, which is a popular approach to accelerate the attention component of a model. We have things that other kernels such as FUE QKV to get better support, better performance. And also other features such as speculative decoding, which is another way for you to speed up your model to generate tokens a lot faster during the decoding phase. All right, so now I've kind of uh explained the end to end life cycle in a kind of hypothetical way. I'd actually like to invite Randeep who are actually going to show you a concrete use case of how they've actually managed the end to end model life cycle using AWS Trainer and and Friendship. Thanks, bud. Hello, everyone. I'm Pradeep Bhatia, CTO for Splash Music. And today with AWS we are launching something new. Not a product, not a platform, but a new way of making music, a completely new format, and that is going to be interactive, and we call it Remix. Wait before I go and explain, I'm getting a call. This person usually doesn't call me unless there's an emergency. I, I hope you guys don't mind. I'm gonna take this. And FaceTime it. Hello. Hey, Randy, I know you're talking at the AWS reinvent conference today. Oh, Whoa, you're on stage right now. Hey everyone. Sorry for interrupting. I just wanted to give you a call and let you know that I sent you a good luck mix. I just texted it to you. Check it out. All right, we'll talk later. Bye. Wow, that's a really awesome. Good luck to receive from a coworker right in the middle of a presentation. Perfect timing as well. So what we are also going to do, I'm gonna respond back with a message that she has sent me over. I am on this stage. I'm gonna show how to co-create. I am on the stage. Done. All I did was just sang my melody with the lyrics, with vocals. And combined it into a single musical composition that we both created together. It's interactive, and it's fun. It's music, it's a vibe, and it is magical. So what makes us interactive? Music is one of the ways that people love communicating with. It's the melody that makes us feel connected. Gen Z and Gen Alpha have been communicating with each other through text messages, through videos, through images. But we wanted to change that. We wanted to bring in a new format of communication through music. And that's where we made We Mix. Now, in under 15 seconds, you can have infinite variations of compositions that you can add on top of each other. And this really changes the way where music is actually headed today. And it is another way of supporting artists. So how do we do this? Let's take a look behind the scenes. When we started this, we asked ourselves, why music making is so hard. People don't really express themselves in prompts. How do you express feelings in prompts? It's impossible to do it. They hum in the shower. They make random noises. Sometimes they tap on the desk and sitting. And so our goal was very simple. We wanted to take these everyday moments and turn that into a mode of communication. And we wanted to take technology out of it, but we also wanted to do it ethically and at scale. So to solve this, we built the first ever LLM model called Humming LM. It is the model that takes your hum, listens to it. If you're off-key, totally fine. You're offbeat, that's even better. Because in every single imperfection, we can understand the intent that is behind your expression. And, you know, Humming LM is built to understand that intent and make it sound great. Of course, converting random hums into music is not easy, because it is a challenging problem. We built a massive data sets of people singing, people recording their vocals, getting the right key, the BPM, the melody, and then we build the entire model end to end. We first trained it on GPUs, which is very expensive for any startup. But then we started working on AWS. With the help of GE AIC Innovation Center team, we experimented on AWS trainium chips. That cut our training costs by 54%. It increased our throughput by 8.3%. And it gave us 159% better efficiency on the metrics that represent what people can actually hear, which means you have cheaper, faster, better music to your humps. How do you measure the better? Ah, we basically have the metrics that define the quality of the music, so that's how we know it better. Thank you. And, of course, everybody needs a little inspiration because it's, you have an idea, you want to take it, but what we have curated is these sounds from different artists that represent the different intent. Have a heartbreak, we have a sound for that. Are you happy? We have a sound for that. You have a good vibe, we have a sound for that as well. And that gives direct attribution back to the artist, and it benefits all the creators. Now, you might wonder why we didn't use an open-source model to begin with, because open-source models aren't built for melodies. They don't understand the intentions behind the melodies that are created with. People usually hum off-key. They start off off the note, they drift, and then they express emotions that are not mathematically clean. And in music, those imperfections defines the intent. These are the clues that make them magical. So to create the new formation of music, we wanted a way for us to understand your hum, your emotion, your intent. And that's when we built the humming LM that encapsulates all of these intents and music together. So when people ask, what do we do at Splash, we empower anyone to make music with their favorite artist, with friends, family, and even people from the crowd. So we are not an AI music company. In fact, we are not a music company. We started off with a simple goal on how people want to express themselves through music. But to build that, we built the entire system around understanding melody, timbre, structure, emotion that represents music. And for our complete journey, CPUs were too slow to run the algorithms. GPUs get too expensive. So we needed something that can define our availability and scale at the same time. That's when ranium came into the picture. So with ranium and SageMaker Hyperpod, We started off our journey with just 4 ranium nodes. We were able to scale it all the way up to 100 cranium nodes, get our model trained and ready to be served across millions of endpoints through internship. Now, we did use other AWS services such as FSX Luster for storage and had the orchestration around it. Now, you saw that we can take a very simple hum and convert it into a song with your favorite artist in just 15 seconds. You hum, we capture your intent, and that's what you hear. But that's only half of the story. Since music is a form of communication, we want people to add their own voice to these songs. Like how you saw my co-worker sent me a good luck. And I was able to relate with that and send my response back into that message. And the capabilities that we have built in our player is around that you can do it right from that experience. You don't have to navigate or anything. It is as simple as typing a message to somebody. Now since we have launched in less than a year. Across 30,000 creations on our platform. We have streamed over 750 million times. That's almost 10,000 years of listening. In less than a year. The only question is, what are you all going to create today? And I truly appreciate Edler Willis for having me on the stage here and sharing our story. Thank you so much Randeep. Um, it's, uh, really inspirational to see what, how creative his team has been and able to kind of leverage, uh, both the services from AWS but also training them to achieve their goals, you know, to lower their costs, of course, but to create something new, right? Um, you know, even in my own world right now we kind of focus on LLMs as the source of AI. We're like, oh yeah, gente workflows, they're gonna take over and to be honest they will, but. It's also really interesting to see um how AI is going to be um applied and utilized in so many different kind of applications and markets today right? we're just scratching the surface of this really. Um, you know, from creative endeavors like with music with what Splash is doing today where the impacts that these new models generative AI and, and what's coming after generative AI is really gonna be impacting like health care, medicine, therapeutics, um. Video is another great example of this as well so I think that there's gonna be a lot of growth still left to be discovered and a lot of inspirational moments are still ahead of us, right? And so at the entrepreneur labs and AWS team what we're thinking about is, well, how do we make our developer stack, um, more accessible to more people? How do we make training and inferentia more valuable to this, uh, discovery of different ideas, right? And so we have our developer stack called Neuron. Um, and we've been really focused on how do we make it better and more accessible to more developers at all the levels, right? Whether you're an ML developer looking to utilize AI, uh, building blocks, models, um, from the some of the resources that our presenters here talked about today, making it easier for you to integrate inside of your applications, extend your applications with AI, um, and you want off the shelf great resources to do that in a very easy way to achieve the performance goals you want without having to kind of. Really, you know, get in there and, and optimize these models whether you're researchers like the designers at Splash were like, you know what, there's not an open source resource, um, or open source model that makes the, uh, uh, you know, best use here. I really need to design something from the ground up, right? And for that we're really investing in resources at the framework level so native Pytorch support we wanna make it really robust. Have the full ecosystem around PyTorch where you can leverage that. So starting this week we're introducing our new PyTorch native um uh support for ranium. This will allow you to basically take any of your PyTorch code you're running today or the library is running on top of PyTorch, be able to change that device, um, description from CPU or GPU and now to neuron and be able to deploy directly on, um, uh. and inferentia take advantage of the performance and cost savings that we can offer you and to do it very easily and take advantage of the Pytorch ecosystem with FSDP Pytorch eager mode, um, dot compile, and many of the other features there and then kind of plugging into the ecosystem of discoverability. So, um, you know, Pytorch profiler and weights and biases and many others, right. And then for performance engineers, right, and performance engineers we think about them at the bottom level of the stack. It's and, and Matt kind of touched on a lot of the features we're building here with new. Um, uh, you know, our new profiler capabilities coming through our, uh, neuron explorer, um, really gives you unparalleled access into what's happening, how your models are performing. We're gonna keep investing in that space as well, um, more access with our neuron kernel interface, expanding our access into our ISA. Of our devices as well and then of course our optimized kernel library um which is gonna be launching this week as well with neuron kernel library and so these are pre-optimized kernels our team is developing you know some of the numbers that we're sharing earlier um to achieve the GPT OSS those are kernels we built and we're gonna be open sourcing and sharing all of those as well, right? And so our entire developer stack that's available for all the different types of developers we want to engage with what underpins all of that is open source, right? We wanna make it easy and we wanna make it accessible. We want you, um, to build with you, um, in this room and kind of everyone out there as well so we're committing to open sourcing our entire software stack, right. Um, so today that includes our, our, uh, neuron kernel interface kernels, uh, our library of kernels, our Niki compiler is also gonna be open source. All our plug-ins for Pytorch and VLM, hugging Face and others, and over time our entire software stack that includes our core graph compiler as well, so. Um, with that, and we'll just, we're, we're, we have some time for questions, but I wanted to also share we're, we're sitting at the end of Tuesday, and I thank you so much for sitting here. Um, what time is it? It's almost, you know, past 5 on a Tuesday, and I, and I expect you guys to, um. Have a lot of evening plans as well, but thank you for joining us all throughout today. We still have two more exciting full days, um, at Reinvent. We have a lot of sessions if you're inspired from this one, we have a workshop tomorrow, um, I believe it's aimed for 4:14, um. No, I apologize, it's AIM 309. So if you wanted to get hands on with infer in training and kind of take your knowledge to the next level, it's a great opportunity. We have all our neuron experts in the room. You can ask them a ton of questions as well. And then, of course, join us on Thursday for more deep dives and innovation talks. Join us for AIM 201, where we're gonna be. Looking at all of the more uh um uh kind of going down peeling the onion a little bit talking about the innovation that went into our software stack that went into our chips and server design um this one will be featured with two partners of ours as well that will be talking about innovation. And how they've uncovered that and utilized in French and training along the way, um, and then of course lastly, um, and probably most importantly come build with us right? so we're super excited to build this, uh, with you guys, um, we want to, um, um leverage we want everyone to leverage in French and training to build. The next big thing we want them to kind of build unique new experiences for their customers um and and do it at lower cost and easier accessibility we don't want this to be compute limited anymore so with that I'd like to thank everyone for joining us today. Yeah.