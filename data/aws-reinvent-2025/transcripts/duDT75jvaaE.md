---
video_id: duDT75jvaaE
video_url: https://www.youtube.com/watch?v=duDT75jvaaE
is_generated: False
is_translatable: True
---

Good afternoon everyone. Thank you so much for joining us today. My name is Matt Kosbla, and I'm so excited to be on the stage today because today we're not only going to be talking about any digital commerce at scale, but I will show you how it's done by T-Mobile, one of the largest carriers. Now, I'm not gonna be doing that alone. I have a privilege to be joined by 3 absolute rock stars, the engineers who designed, built, and operate this platform. Please help me welcome Glenn. Nick and Vin from T-Mobile. So today we are telling you a story. Now the exciting part is Some of you already know pieces of that story. Some of you Well, you are the story. We will share the challenges that we faced, but more importantly, how we solve them. This is the journey of innovation at scale. What happens when AWS meets T-Mobile digital commerce platform serving more than 130 million subscribers? Now my hope. When you walk out of here, You actually learn something that you can use and implement yourself. Whether that's Architectural pattern. Scaling strategy, or just one thing, one idea that changes how you think about, think and deploy your own code. So before we go to the details, let me explain to you what the plan is. So I will kick us off telling you the T-Mobile challenge that started it all. Essentially the why behind everything you're going to hear today. Then Glenn will walk us through the T-Mobile digital transformation journeys. The pivots, the iterations, and the lessons learned along the way. Nick will reveal their battle-hardened architecture, essentially what's powering their success. Then Vin will talk about how cloud native architecture has completely revolutionized how they think about their code and how they approach their deployments. No Find me Glenn will take us all home, talking about the exciting future that is ahead of T-Mobile and their platform. Now, fair warning, he's tying it to something absolutely extraordinary that literally just happened. So you really want to stick around and hear this one. So let me start with that. I'm going to ask you a couple of questions. So how many of you are like me? Cracked screen all the time. Dying battery, or simply, you just couldn't resist to get a new device upgrade. Could you please raise your hands, who purchased a new phone this year? There you go, yes. So we have new iPhones, we have new Pixels, maybe even those foldable Samsungs. You can always count on technology people, right? Show us something new, and we're all in. So with that said, let me ask you a second question. How many of you are actually T-Mobile customers? Beautiful again, thank you so much for joining us today. Now think back to that moment in the past. Whether that was in February when Samsung had their launch, maybe August, when Google dropped their Pixel, or just a few months ago in September, during the Apple iPhone launch. Whether you were in New York, Chicago, or Seattle. You woke up early in the morning, you got yourself your favorite cup of coffee. You log in into T-Mobile app. You browse through the product catalog, you found a device that you wanted to purchase, added it to the shopping cart, submitted, boom, done. Great experience, right? Low latency, smooth and fast. No Here is something maybe you didn't know or you haven't even thought about it. At that exact moment there were millions and millions of other customers. Doing exact same thing. And the platform behind it. It didn't even break a sweat. So Here is where the story gets very real. Just please look around the room. We are all IT people here. Builders, engineers, operations, we all share one job, keep those workloads running all the time, no matter what. So let me take you for a couple of seconds into Glenn's, Nick's, and Whippin's world. Think again. Picture that it's a major device launch day. 5 a.m. Pacific Coast hits. The East Coast people wake up. And those shopping carts start lighting up. Look at the transactions. They don't just increase, they just explode in minutes. We are talking 10 times more than the average traffic. Think about your Kubernetis clusters, scaling really, really hard, your Dynamo DB tables, your read and write spiking. What about your databases, your RDS, MySQL, Posgress. The quarries are piling up faster than you could ever have more capacity. And here's the thing, it's just not one app. It's T-Mobile T-Life, it's T-Mobile.com, and thousands of retail stores across the whole country. Hitting exactly the same back end, exactly the same APIs. Simultaneously coast to coast. So, think about yourself. Those are the days that define your career. Your company is counting on you. Your customers are counting on you. Trust me, nobody wants to be that engineer in a hot seat, explaining why the checkout page timed out during the most important launch of the year. So here is the multi-million dollar engineering question. How do you design, build, and operate a platform that never goes down? That scales to infinity and delivers the same flawless experience coast to coast. Glenn, Nick, and Vipin, you guys built something incredible to answer that question. Glenn, do you mind helping us understand how you actually make it happen? Absolutely. Thank you, Matt. What an amazing intro there. Uh, it's, it's amazing to have our partners speak about it like they pretty much have built it, and I know you've been part of this journey, so thank you. Folks, I would like to take you on a journey. March 26th, 2013. John Leger The T-Mobile CEO at that time said, Love your customers. A simple statement. It came from a genuine sentiment. This was called the uncarrier movement. For those of you who don't know what that is, it pretty much revolutionized the whole landscape of the wireless industry, for example, we were the first. To take away all contracts from the US, we were the first to provide unlimited talk, text, and data. And this journey continued for a while. We started out on this journey about 12 years ago. At that time, We had 8 commerce platforms and multiple disconnected customer experiences. Now imagine this, we kind of joke about it now, but back in the day when John was about to make an announcement. All of us IT geeks would be huddled up in a room, the uncarrier war room, and talking amongst each other, wondering what is he going to sign us up for now. Those launches were announced first. And built later. It was crazy. Those days were really crazy. We call it the chaotic days, right? Imagine the complexity around an industry changing capability that had to be built in 8 commerce platforms and multiple disconnected user experiences. Right in the middle of that chaos evolved. Our whole mission, which was the T-Mobile digital transformational journey in commerce. We had one vision, and that was to take it from 8 commerce platforms to 1 platform that rules it all. Our digital share of commerce at that time. 0.4%. That's right, a measly 0.4%. We built this platform from the ground up, fully headless, built and powered by elastic path. 100% cloud native, all on AWS. All right, since we are always about show of hands, a quick show of hands, how many in the audience here have been through a major transformational tech transformational journey? That's right, we all have been through that now. Keep your hands raised because that's the next question. In case you've been through this journey and that journey has been completely painless, you can keep your hands raised. That was a trick question. I wasn't sure if people would respond to it, so that was awesome. You're right, transformation is extremely painful. And most transformations come with its own fair share of war stories, and we at T-Mobile have our own fair share of war stories. The uh Matt here talked about some of the key events that take place within T-Mobile and so we have a few events that take place each year. For example, the Super Bowl is one such event. The T-Mobile ad places and folks dash right into the website or on the app and we experience what we call burst volumes of traffic. But the one event that we at T-Mobile obsess about is the Apple iPhone launch, also known as NPI or New Product Introduction. So here's my war story. NPI of 2018. It now looks like a long time ago, but that was one of the most difficult days of my professional career. Our platform at that time. was not quite mature enough to handle the loads coming from a large event like the iPhone launch, and no matter what we tried. With respect to loading the catalog into our platform, it just kept crapping out. Yeah, crapping out was a technical term we used a lot those days. It got to a point where our CIO actually asked us, Hey, should I be calling the business and let them know that we won't have an NPI this year? So talking about that actually gives me goosebumps because uh like I said it was an extremely difficult day. Well, we survived that day because we're here. But we made a promise to ourselves and to our customers to never ever go through that again. Out of that resolution was coined a very popular term at T-Mobile, YAML. It's not the Yamal that everyone here is familiar with. For us, it stands for yet another market launch. We decided to take a high stress, high anxiety event like the Apple iPhone launch and turn it into a boring Yamal. As we progressed on this journey, And for those geeks in the room here, my My awesome team here is going to get into the gory details. We started seeing light at the end of the tunnel, and as a really smart TPM on my team joked and said that light is not a freight train anymore. For those of you here in the audience that are about numbers, and I'm a numbers guy meaning for the life of me I just can't remember numbers, but I do recognize patterns and so what we see here is what I call the pattern of force. Our journey took us from below 0.54%. To 0.4% to 4%. To 40% to now while I can't quite share the exact percentage we are at right now because that's not public information. The gap to getting to 100% is shrinking rapidly day by day. In fact, we are extremely close to being 100% digital on commerce. Folks, T-Mobile has now evolved from a telco to a techco. With that, I'm going to hand it to my head of commerce, Nicholas Kris. Take it away, Nick. Thank you, sir. Appreciate it. Hey folks, So, uh, as Glenn explained, uh, you know, we had a bumpy start. We definitely did not have it all perfect and figured out from day one, but what I think we had going for us was we've always had this willingness to try something different when the standard approach wasn't working, and then I think we've always had sort of an obsessive drive to just keep improving, no matter how bad it is, just keep improving. And I call this philosophy uh blue collar architecture. And so what that means is we're not overly concerned about appearances, but instead we just focus on practical and gets the job done. So today what we want to do is focus on two aspects of our platform that I think like do a good job of illustrating this approach. Uh, I'm gonna talk for a bit on our Active Active system and then I'm gonna hand it to VIPin and VIPI's gonna talk about our release process which makes use of ephemeral cloud environments. So before we get into that though, I just wanna give you a little more background on kind of like what our platform is, where it fits into the whole equation here. So, uh, as Glenn kind of alluded to, we're not the UI, uh, we're the core APIs that power all of the various UIs so you can think about like, uh, products, pricing, promotions, and of course, uh, uh, carts and checkout, right? And so more often than not though, uh, we are the ones in the whole ecosystem that are either making it or breaking it, and we take it really personally when it does fall short. Uh, so now let's get into the active, active journey. So we're gonna make you guys do more uh show of hands here by quick show of hands, is anybody here today running a platform that's multi-region active active? Oh, good number. We definitely like to compare notes, uh, after the, after the talk. Uh, who's doing, uh, active passive? Anybody? Yeah. And anybody in any of those groups doing using Golden Gate? OK, good for you. And who's generally happy with their setup these days? Half a hand. OK, that's great. Yeah. Uh, so where we started was, uh, Active passive, and we used Golden Gate to replicate two SQL databases across regions. And I have to tell you that was painful. Uh, the releases were complicated, uh, especially when there were database schema changes involved. Uh, it was brittle to operate. There was always sort of this drain on performance of running that system, and you definitely did not have any horizontal scaling, um, but that's not even the worst part. The worst part is when you needed it most, we were always too afraid to use it. So like if you're talking about like resetting a, uh, uh after a stack failover or after a release rollback, um. We were so afraid of doing that because we didn't want to have to worry about the rethink later that in the end we always ended up just trying to fix forward and avoid that scenario. And so finally after several years we just got fed up and we said we're going to build our own process from the ground up. So a key insight we had that we used in our approach was that we're actually not the source of truth for as much data as we worried about in the beginning. Uh, some background for this, we kind of think of all the data that we deal with in 3 big buckets. Uh, so one bucket is config, and that's the simplest. It's basically for the most part the same on all the copies of the running of the application. Then we have what we call context data. That's like, uh, mostly for existing customers if you think about any information that might affect the pricing, the promotions, eligibility of what we're, uh, you know, selling in the, in the cart. Uh, then finally there's the cart itself, and so anything that we need for you to be able to check out must be in the cart. And so with the insight that we really only needed to worry about that last category, that that small subset of data, we want to build a system that basically streams just our cart changes. So we use kinesis with lambda and we basically stream anytime a change happens on that local stack cart to a Dynamo DB table which we refer to as the global table, global cart. And so, uh, basically, as you're shopping, uh, any car changes that are happening are streamed async, and because it's async, it's not affecting your performance on the stack at all. And so let's walk through how it works. So as a user when you first start shopping, uh, you're gonna hit one stack and our traffic routing component is going to keep you mostly sticky so you'll continue to be on the same stack. Now that's not critical. You actually would be fine to bounce between stacks, but for the most part that's the, that's the approach. Uh, and then as you're shopping, you're making changes locally in the as the system's working, it's just streaming out any changes to that cart and. On a regular normal day, that's it. Like it sounds too good to be true or like there's a big piece missing, but for the most part, like uh when we're in normal operations mode, there's not a lot that we are doing with that data once it's in the global cart. So let's talk about when it's not so good of a day. That's I think where the scenario gets interesting. So let's imagine for example that there might be like a regional outage and so now what we want to do is fail over 100% of the traffic to another live stack, right? So in that scenario for us to switch over, it's simply one very small config change in that traffic router component that says we no longer want to do 50 50%, we just want to put 100% of traffic on the other stack. And so what's going to happen now is as you were in the middle of shopping and suddenly now you're you're hitting a new stack, uh, we're gonna be checking and we're going to notice that the global cart has more recent changes than your local cart and so then we call that rehydrating where we will just pull those changes in and then after that you're gonna basically continue operating as normal. And so because the process is very fast, even if we suddenly end up having say 50% of our current active users all rehydrating at the same time on a stack, it's still very negligible from an impact. And so in the worst case scenarios where we have you know, very chatty scenario, you might get that initial request for each customer taking a little bit longer, but then once they get through that part, it's back back to normal. And so, uh, you know, with this set up we don't really think anymore about primary or secondary or like flipping backward or forth. The only thing we ever think about is what percentages of traffic do we want to route to which copies of the application stack. And so besides the sort of reliability scenarios, we also use this for our zero downtime blue-green releases and so for us a deployment or if necessary a rollback is an action that really takes less than a minute because when you think about it all you're doing for the actual deployment is making that traffic change, right? And beyond that, uh, we actually use this as a foundation for a lot of other very cool things, and uh I'm gonna let Pippin kind of get into that, all right. All right. Uh, thank you, Nick. So once we got to active, active, uh, we soon moved from blue-green. To can retesting, and for those who do not know, this is where, when we go live, we introduce a small amount of traffic to our new release. And then we wait, let it bake in. And once we are comfortable about its health and its customer impact, we open up the traffic for the rest of the customers. Soon after that, we also got to a point where we used AB testing configuration. This is a case where we set different business scenarios on different stacks. We compare and contrast the customer impact and KPIs, and once we are comfortable and we know how it's impacting the customer, um, we make an informed business decision. On which configuration we want to go forward with. Then came our 4 stack releases, so we used. 4 stacks in our go live, where we have a stack assigned for each combination of Release and Region. This fortified our release because it made us capable of handling issues that might be rooted to either a release. Or a region. So last month, any given Monday, we had no problems, right? Now, four-stack release might sound a little rich, right? And that's because an environment is an expensive commodity. It takes us a lot of time and effort to manage a stack. And this is absolutely this was absolutely true for us at one point in time. Given our environment size. At any given time, our core application has almost half a million lines of code in production. And we are not even counting the supporting applications, for example, our test automation suite. There are almost 5000 G Gherkin scenarios, either validating partially or fully our current business scenarios. It is about 125,000 lines of code, and this is not counting the test data that we store as JSON files in the repository. If you count them, it's 1.5 million lines of code. It used to take us almost 4 to 5 weeks. To create an environment. And the idea of deleting it after a short run. was not even conceivable. So we treated environments as fixed and tried to work around it. Which means many long-running environments. Specifically assigned to certain parts of the whole release process and we move code from one environment to another as part of different stages where we go through those painful environment refreshes. That means one refresh and then functional testing in the release branch, in the release environment, then another refresh and then performance testing on the perf lab. Yet another release, finally to the Blue stack, do a quick sanity test on Blue Stack and then open it up to our customer as the new pro environment. Now, another question here. How many of you are really unhappy about how much time it takes for us to get an environment ready after a new release and get to the actual part which is the validation of the new features. Yeah, And that's one of the main reason for that is essentially because a refresh usually doesn't cover the whole environment, just parts of it. And the long running configurations and data. are honestly Uh, go, go far apart. They drift between environments, so the, so the environments that are supposed to be identical are really not. And let's consider us in the perspective of Blue stack. What that essentially means is we take a stack live without doing a full regression testing, end to end regression testing on that same stack. And canary testing is where unfortunately, we are using our customers as as uh guinea pigs. So obviously that's not what we want and we attack the root of the problem that our environments are fixed by fully automating the whole sack life cycle. And now we're starting a new cycle every sprint, for every release. What this means is once a stack is created, we let it live in development and release evaluation for 2 weeks. Then it goes live, and it serves customer traffic for another 2 weeks. Then, when the next stack is ready, as part of the next cycle, we suspend the existing stack. And then for a short while before we destroy it safely. The whole process lives as infrastructure as code. Which means it's predictable and reliable. This also means that any kind of change to this process is code reviewed just like any other functional code that goes to production. So what does our automation look like? What we aspire for is a lambda driven. Worker-based architecture, but for now, it's a big Git Lab pipeline. It has many jobs. Trying to fulfill some kind of a a purpose, um, some different types of purposes. One of them is, for example, validating state. Now that is essentially checking if the environment at any given point. It is up to the mark for us to move forward. Uh, to the next state. Then there are others that are essentially doing the main part, which is creating and destroying resources. Then there are those that load data or run some kind of test execution. Um, at different stages. Than those that are owning the go live activities. Many of these jobs. Accomplish their work by triggering more downstream pipelines. OK, let's talk about the resources in our environment. I would want to break this up into two parts. The ones which are in the scope of the environment and their life cycle is the same as the environment, and the environment itself, the stack owns them. Um, like our application pods and our data stores. Our data stores are created from scratch. And all the data that they hold are either generated or loaded at that same time. Well, most of the data stores, and I'll get to that. Now, uh, the other kind are the shared resources. And as we talked about just before, one of them is our proxy. Which our customers use to connect to us. Any time we want to take a stack live. We just connected as a target to that proxy. And then control the flow of traffic all the way to 100%. And as Nick mentioned earlier, if in any case, if there's a rollback required, we just switch back to the old stack. The other key resource that we share across these stacks is our global Dynamo DB tables. Especially for cart information. Again, as we mentioned earlier, the cart information is the only kind of information that we, as DCD or digital commerce domain, truly own. All the other kind of information come from our customer, uh, from our partner domains. And cart information needs to last longer than a stack life cycle. Now imagine this scenario. You are switching to T-Mobile, all the family lines on your account. Yep, And It only you just have to, and you get these lines for just $25 per line per month. And it just takes 15 minutes for you to switch. On top of that, what you get is one month. To choose and decide which iPhone 17 would you want and your family would want. On us, no trading required. Of course you will use the whole time to choose your phones. Over the next few weeks. You will keep visiting the cart with your family, adding and swapping different phones based on color or sizes that your family wants. Our stack does not last for 1 month, it just lasts for 2 weeks. It would be a terrible experience if you come in. To log in on your perfect iPhone configurations, only to find your cart empty because our stacks changed. We want to avoid that, and that's the reason why we Connect the stacks to Global Dynamo DB table for our cart information. Any time a candidate stack is ready to go live, we connect it to these tables before bringing it to our customers. So that's our environment. It's actually 3 environments in one. Just that, just like that uh 3-headed dog Cerberus. Instead of moving code. Across environments. and incurring the penalty of Repeated error prone refreshes and the risk of missed use cases or tests due to code and config drift. We create a stack for every release in every sprint. And then move it across the different stages and bring it, bring the same stack to our customer when it goes live. OK, so what other advantages do we have, apart from it being ephemeral, deterministic, and light? It does not cost as much as long running environments. All right. What else? For that I'll ask another question. How many of you have to do some kind of a disaster recovery exercise in production from time to time? Yep. In our case, we do not do an explicit exercise, it's just part of our normal operations. It essentially happens every sprint automatically. And this guard dog, is also secure by design. Because the secrets are just another resource of the same stack, sharing the same life cycle. So instead of having a special project every 6 months to rotate production secrets, our secrets get rotated every sprint. As part of our normal operations. So to sum it up, By treating our environments. As Uh, as, um As a real, short-lived and fully automated. Assets, we have changed that scary three-headed monster into our favorite guard dog for releases, reliability, and security. With that, I hand it back to Glenn. Thanks, Whipping. So every time I hear Cerberus, our favorite three-headed guard dog, it, uh, it makes me realize, it takes me back to that day which I talked about earlier, NPI of 2018. When things were just completely crapping out and I look at today and I realize the journey that we've gone through and how fortunate we are to have traversed this amazing journey from back in 2016 when we first started. Folks T-Mobile is now a deeply. Data informed, AI enabled, digital first company. It only makes sense for us to have AI embedded into every future activity from here on. And like our CIO Jeff Simon says, we like to sprinkle a little bit of pixie dust in everything, pixie dust meaning AI. But I'm going to uh leverage a very corny phrase since we are here in Vegas and that is. What happens in Vegas? You don't have to repeat that. About 2 weeks ago, right here in Vegas at the Las Vegas Grand Prix, T-Mobile made our latest on-carrier announcement. And that again has revolutionized the wireless industry. 15 minutes to better. For those of you who haven't seen it, I would encourage you to go look it up. There's a whole bunch of cool features that we have launched when we made that announcement. But the one that I'm super proud. To talk about is what we call the easy switch. It's essentially a feature that heavily leverages AI to smash a very, very painful customer experience, and that's the switching experience when you switch from your existing carrier to a new one. As you see this video playing back, which is the easy switch experience, what we allow our prospects to do is log into AT&T Verizon. We go in there using Pixie dust and we pull in your existing information, plans, services, we take it through AI and we come back with a recommendation on the best plan that we think is right for you. We also show you why we recommend that plan. We also walk you through all the benefits, including the cost savings that you're going to go through when you switch. And finally, using pixie dust. We also show you exactly which phones you should get. In some cases, we ask you to stay on your existing phone because it's a newer phone. Essentially, we set up your card for you and we make it extremely frictionless to switch to T-Mobile. I know the campaign talks about 15 minutes, but in reality, folks, we have experienced folks that have switched up to 5 lines in as little as 7 minutes. Now imagine this, a painful existing experience where someone wants to switch from one career to another. You spend a whole weekend at a store with your family, bitching and moaning, going through that experience, and here we are today where you can switch in the luxury of your home. In the comfort of your couch. In about 7 minutes. That whole thought process is just mind blowing for me. All right folks, thank you so much for joining us today, and I know you're thinking, what should your takeaway be from from this presentation. So here are 3 things that I would offer up. Resilience is built, not bought. Automation Is the new scaling lever, and finally, remember the pixie dust, AI driven experiences is the new normal. Thank you folks, we really appreciate you joining us today and we are going to be hanging out in the back there if you wanna come chat with us if you have stories to share, we can also talk about other stories that we couldn't stage, you know, share on stage but uh thank you once again, really appreciate it.