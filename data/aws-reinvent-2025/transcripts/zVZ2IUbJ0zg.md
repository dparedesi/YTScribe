---
video_id: zVZ2IUbJ0zg
video_url: https://www.youtube.com/watch?v=zVZ2IUbJ0zg
is_generated: False
is_translatable: True
---

I'm Gidris. I'm a specialist solutions architect at AWS. And I apologize for my attire. It's hard to put anything more formal for the presentation. You see this customer obsession in action, we had heated discussion over API gateway roadmap for the next year. And uh join me, my opponent, Srijan, who is product manager on API Gateway Service Team. And today we will, we will talk about API management on AWS. If you came for something else, it's not too late to leave the room. And as, as everyone who had been working with the API gateway for a long time, you all remember like yesterday, it all started back in 2015 with this simple what's new on AWS post that introduced API Gateway as a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. And this was APA Gateway 10-year anniversary celebration party in Seattle this summer. We all know it's just the age generated picture, though on a serious note, it was really worth celebrating. Over, over all those years, API Gateway released numerous new features, and this is just a small subset out of 80+ feature releases that I counted while walking through documentation history. And also over the years, it grew into tier zero service that handled 1 trillion T as in tango requests during Amazon Prime Day 2025. And before we go into details, I'd like to remind you a quote from our CTO who shared it at Reinvent 2001 while summarizing, uh, lessons learned of building APIs for a long time. APIs are forever, and indeed they are forever, and we will keep building them, maintaining them, improving, and new technologies such as an AI or or any other whatever comes next. They bring new aspects to the picture of maintaining, but they do not change underlying foundation. And that's what we will talk about API management on AWS and its future. We will touch on a few, on a few new releases, and along the way we will keep in mind two main aspects that AI brings to the picture. We cannot, we cannot really, you know, escape it, so let's talk about it in depth. Building and operating APIs with AI and building APIs for AI consumption in one shape or another. We will follow through 10 stages of API life cycle. We'll talk about high-level activities in each of those stages. Uh, we will focus on AWS services that help with those activities. Uh, we are not gonna go deep into API gateway services, service features and then settings. Uh, you all work with API gateway, hopefully you know them to some extent. Some of them will touch, but it's not a deep dive into API gateway particular aspect. It's more about the entire path and how to implement maintenance and governance. If you're looking for deeper dives, take a look at, uh, for example, Eric Johnson's deep dives from previous reinvents. Those videos are on YouTube at, uh, on, on AWS, uh, channel, and this QR QR code points to survellus land. It's portal managed and owned by developer advocates of AWS. Uh, there's a guided path with deep dives, additional resources linked from there, uh, workshops, uh, uh, latest blog posts, uh, example implementations, our partner links for the areas where you may use our partner solutions instead of using AWS. Also, this session will be recorded and recorded and posted on YouTube, so you can go back and check if you missed something afterwards, and slides will be published on AWS at some point in the future as well. So we have a lot to cover. Buckle up. Let's start. First, we plan and design. And this involves working backwards from business needs identifying API contract that you will have with your customers and that's where we've been talking about API driven development for years. It's not specific to API gateway. It's not specific to any other service, to be honest. It's about developing API first, uh using API first methodology and decoupling APIs from back end. When they become byproduct instead of being byproduct of some back end implementation bolted on on top of it as an afterthought, this approach allows you to build that contract how your APS, how your systems will talk to other systems and services. And there are a lot of considerations to make and questions to answer while you're planning and designing your APIs and in many cases they do not depend on technologies that you will use or services that you will use for implementation, for example, what kind of, uh, what kind of approach you're gonna use centralized or distributed? How are you're gonna implement your visioning strategy? Do you have any particular authentication authorization requirements? Maybe you are in the regulated industries, how your topology will look like. And so on and so on and so on. And then when you add AI to the picture, a new set of those considerations come into the picture. Are your APIs documented well enough for LLM to understand or get right your intent? Are your error messages rich enough for AI agent to retry error with enough information how to fix the previous request? How do you design your APIs with resources versus intents in mind? How many of the requests you have to make to perform one action? Uh, how do you deal with large responses? Pagination, returning part of the data set, and, uh, returning, uh, cursor to the next one. How it will be understood by a requester that is a machine. And so on. So keep all this in mind and then when you discussed, when you designed, when you know what you will do next, that's where developing and testing comes in. So the results of the planning and design serve as a basis to develop, test, and documented APIs and there are two main approaches that we typically see customers following one. Simplest one, you just simply go to AWS management console, navigate to API gateway, and start there. You create resources, you specify methods that you will use for those resources, integration targets, transformations, validations. You just click and deploy it. You try it. And define which stage it will go to and so on. So all this is fine and nice and easy to use. Until you figure out that you cannot scale with this approach, you cannot automate it easily, so it's not repeatable, and that's where customers would switch or even before this point switch to infrastructure as a code where AWS uh tools like AWS Cloud formation, SAM, CDK, Terraform would be used to build your API infrastructure. And also in addition to those infrastructures as a code tools, you would use open API specification as a rule of thumb where you define API itself and then either refer from that template of your infrastructure as a code or simply inline it depending really on what what your tastes are and what what kind of tooling you are using. And this open API specification also is used for integration of API gateway with others, with other services and systems that are, that can be used to develop APIs. Because API gateway can both export and import open API specifications with API gateway-specific open API extensions. There are plenty of open source solutions, partner solutions that use this approach that allows you to design, build everything outside of API gateway, and then simply through this import into API gateway way, those external systems would deploy your APIs into AWS. So that's one of the ways. Now, API development can use AI or AI assisted development tooling, uh, as any other code development activity. Uh, you can use, uh, AI assisted or agentic development tools. They can help you through all development activities starting from spec-driven or prompt-driven development through, uh, refactoring, migration, testing, and this is a screenshot from, uh, Quiro, uh, Amazon product in, in this space. And often in addition to what's available in your agentic tooling but that comes from LLM training set, best practices public available publicly, quite often you need to extend existing AI tooling and that's where you create. Central knowledge base where you load your internal organizational best practices, requirements, style guides. And then make it as a developer tool as an extended to the developer tool using MCP model context protocol, and you have that guidance, internal guidance available in your uh development environment and this sample repository can be used as an illustration how to start doing that, how to implement API development related tasks. And you can use the same knowledge base to build more event-driven approach for your rules enforcement or nagging your developers into compliance with your rules by simply building inspector agent who uses that knowledge base and on every API gateway. Uh, deployment we'll just simply collect configuration information using API gateway specific tools, collect account information, maybe some, some additional bits and pieces, and then provide generate improvement recommendations that can be sent to the owner of that resource automatically on every deployment. And if you go to that, uh, to that, uh, repository, you'll see both implementations available. There are samples, starting samples, and it doesn't have to be specific to API gateway. You can do that with any service or any technology. And a quick sidebar, I mentioned that I will not go too deep into individual features, but Uh, you know, you know, many of them and use many of them, but I cannot skip this particular one just to make sure you do, you do not miss it. We released it just a couple of weeks ago. And it allows you to stream responses from your lambda integration, HTTP integration. It can be private integration by simply enabling HTTP streaming in your integration target in your back end. It can be containerized workload. It can be lambda function. And changing changing response type in API gateway, it's as simple as switching from buffered to stream transfer mode, and it will start streaming that response as soon as it becomes available. So you can use it to improve your user experience. It allows you to get to improve time to first buy it so it's, it feels more responsive also. Package comes no maximum, no up to limit on maximum response size. So instead of using that 10 megabyte limit that you have right now, if you use buffered response, you can stream as as much data as you can push through that wire in the 15 minutes. That's maximum streaming session length. So it allows you to work with long running integrations like uh LLM chatbot implementations where you start getting like in this example, you start getting responses right from the beginning and stream sentence by sentence by sentence. You can stream payloads that are large that you wouldn't fit into 10 megabytes that typically uh your API gateway would not allow to. So go try it and use it. Again, Back to the programming. Testing Fastest way to test your API when you developed it is just simply going in that in AWS management console into API gateway console and. Uh, click on the test tab. When after you selected the resource and method, provide uh payload or query sing parameters, headers and so on, uh, click on the test button and then you see execution log. You see how that request looked like when it was sent to integration target, how it was, how it looked, how it looked like when it came back, what transformations were performed along the way, and so on. Again, it's easy to use at the same time it's not that easy to automate or scale. So that's where third party tools come into the picture. It can be as simple as open source solutions for style guide compliance, for lining, or you use testing frameworks that you have in place already for other parts of your stack. Usually they would have something for HTP response testing, or you can go deeper with API clients, automation, uh, headless browsers, and so on, and security and penetration testing. Almost all security partners that work with the AWS would have something for API security and penetration testing. If you have any security partner in-house, take a look at their offerings. It may be available to you out of the box and maybe even at no additional cost, but I cannot promise that part. Now when it comes to AI, it really, it is really helpful to generate data for your tests. It really, it is really helpful to go through your testing, harness and see. What edge cases may not be addressed, just ask those questions, uh, coverage analysis, uh, response validation, threat modeling, uh, to some extent, uh, penetration testing. There are large language models that are trained specifically on, uh, for, for API penetration testing if you look out there. And as MCPs become new APIs or proxies for existing APIs, you, you have to test them as well, and that's an entirely different topic for the discussion. We will not go in details and in this talk. And there are plenty of partner solutions for testing as well. So, take a look at the marketplace, uh, at our partners who provide those solutions. Now, Securing APIs is the next step, even though it's marked as a separate activity, it's not exactly one step in API life cycle. You have security pretty much baked in into every single of those steps. You think about it. And usually we start with access control for the data plane. Who can consume APIs and how they can do that. And that's where we use authorizers to implement it can be IM authorizer, it can be lambda authorizer, it can be cognito authorizer, and most of the customers do that day in, day out, but quite often we overlook control plane. Access control. Who can modify your? Who can deploy a new version of your? Are you allowing those APIs without proper authorization and so on, and that's where you use IM policies and AWS organizations service control policies, SAPs for it, and we'll come back to those in a few slides when we talk about governance, but these two should come hand in hand. You should not focus just on data plane access control. You should also think about control plane, making sure you know who is doing what to your APIs. And in addition, I would love to talk a little bit about less. Areas that we focus not as often that help to secure our APIs at the network level and you see this entire list of services, let me quickly or features, let me quickly walk through them, how we can add additional layer of security to start with, you can use web application firewall. uh, with managed or your own custom rule sets, IP sets, reputational IP sets, and rule sets from, uh, managed by Amazon or by security partners you can get them on Marketplace. They integrate directly in API gateway. There is no need to add cloud front distribution in front of your API to enable web application firewall. You can use the same ACLs access control lists. That you specify for your distributions, you can use them for your API gateway as well. And as you see, every request that is coming to API gateway, first of all, it will be inspected by a web application firewall, and only after that it will reach your APIs. So if you want, let's say to reject all the requests from Geos where you have no customers or where high, there is high risk IP range that, uh that is known for malicious activities and you want to prevent uh requests coming from those IP ranges, you simply use web application firewall. Next, you can use resource policies. You can specify, allow, or block VPCs, accounts, and entire ranges of IP addresses using resource policies, and they come hand in hand with private endpoints. So you can make your API endpoints private by simply changing API endpoint type, and from that moment on, when you make it private, it is accessible from your private VPCs only. And you use the research policies to specify which VPCs those requests can come in into your, into your API, which accounts, which cedar blocks, which API ranges, uh, and VPCs, those requests can come in into your private API endpoint. It can be your VPCs, it can be your partner VPCs. Really, it's all about specifying where that request will come in. Then another option. You can enable mutual TLS between your client and API gateway and also client certificate, mutual TLS between API gateway and your backend integration. If you want to ensure that request came from API gateway to your backend integration, you can sign that request using API gateway self-signed certificate and just simply check that it came from API gateway. Now, by the way, cloud front. Announced, it was last week, uh, MTLS support at the edge in cloud front distributions. So now you can combine MTLS earning on cloud front. Uh, and use for that distribution origin as a VPC origin pointing to API gateway. So in that case you can offload if you want more global presence of MTLS support, you can offload that to the edge, the cloud front while keeping API gateway as an origin for that response. Next, you can use private integrations uh to expose your data source or your business logics through API. It does not require to make your business logics public, so you enable that private integration by using VPC link feature that points to the network load balancer in your private VPC and from that moment on, anything that you can reach in that private VPC or anything that you can reach through that private VPC. You can use as integration target and in this example we have AWS Direct Connect pointing to the some workload on premises. Let's say you have some legacy application running on old mainframe that is still sitting in your basement. You can expose it through your API gateway in AWS uh using uh network load bouncer in your private VPC and VPC link to that network load bouncer. And throttling and usage plans as part of the security help you, uh, they help you to not just make sure that uh you have resource sharing and more fair between different size of uh your customers from request point size. So you, uh, also you, it allows you to not to overwhelm backend the resources. If you look at those different levels of throttling and metering, you see that uh there is per client and method and per client level of uh throttling. There, there's also per method level of the throttling where you do not care which client that request came from and how it helps you with the security. If you have some legacy application, let's go back to that, uh, mainframe in the basement that is reachable through your direct connect. If it cannot handle more than, let's say 1000 requests per second, you don't want to send more than 1000 requests per second its way, no matter which client is sending that request. So that's where you would set throttling permitted. That is implemented using that uh on-premises legacy application running on some old hardware. And we just announced a few more security features in API gateway. Uh, one is enhanced TLS ciphers inversions. So this was announced what, November 19th. Uh, which allows you to configure TLS policies for both default endpoints and uh custom domains. And it provides a wide range of security policies from minimum TLS 1.3 to post-quantum cryptography support. It also allows you to track TLS usage, which of those policies are used by which customers by adding additional fields, contextL version, and context cipher suite. To the logs, to the access logs, so you can always see before, let's say before retiring or deprecating TLS 1.2 support, you can take a look at your logs, so your access logs and see is there any clients still using it. Maybe I need to reach out to my partners or my clients to make sure that they switch to the newer version before I retire so I do not receive those calls in the middle of the night that we cannot access your API anymore after your latest deployment. And this magical time of the year, we got even more of new features. One more new feature is ability to use application load balancers in REST API private integrations. A few slides ago, I talked about network load bouncer being target of VPC link. This feature allows you to use application load bouncer. Maybe you have additional routing needs. Maybe use ALB as ingress controller for your Kubernetes cluster. Maybe you use it as a. Uh, as a load balancer without the scaling in front of ECS workloads in any of those scenarios, if you use application load balancer, you can use it directly as a target of VPC link with this new feature. Before, you used to have to. Put network load bouncer and then put the application load bouncer into the target group of that network load bouncer. So now you can simplify your architecture. You can reduce latency because you are eliminating one more network hub between these two load bouncers, reduce cost. And last aspect of API security and management, governance. And security controls that work as a technical guard guardrails, for example, how do I enforce a particular authorizer across all the public APIs and production account? How do I prevent deployment of public APIs and developer accounts because they do not, they should not expose anything publicly anyways. How to ensure that proper logging is enabled, how to make sure that MTLS is enabled on all custom domains, and all those rules can be implemented using governance. It's quite a large topic. We will not cover here in depth. I would suggest you to follow the QR link where we have guided path. It's linked from the guided path that I mentioned at the beginning, but Uh, this is set off. Uh, deeper dives into services listed here, use cases, implementation of examples of how to implement those controls using IM policies, permission boundaries, service control policies of AWS organizations, uh, various services in security space, and so on. So take a look at that. And to summarize, I'd like to remind you main principle apply security at all layers. API security is not limited to API gateway service or API service in general. Here's simple microservice implementation, and if you look, each of those components have their own features, their own settings that are specific to security. And if we add even more components, if you, for example, add a simple application with static content hosted out of S3 bucket, as simple as it can be, you see that each of those components have their own features, and you have to keep in mind all of them to make sure that your workload is as secure as possible. So use governance to enforce those best practices. Also, use AI to review your infrastructure as a code template. Just simply ask, is there anything I'm missing in the security features that I should enable or configure in that template? And use those agents I mentioned proactive AI agents inspecting your resources. Keep an eye on announcements that we have during the year or during the reinvent. Maybe something will help you because there are plenty of announcements in the next 4 days. Maybe there is something as well that will help to do that. OK, I'm done with development. How do I deploy it? When you use infrastructure as a code, your API deployment is no different from any other API from another code deployment, and it can be pipelines that you have like Jenkins or AWS CodeDeploy, or maybe if you use something like internal development platforms, open source, Project Canoe comes to mind, maybe there is ArgoCD or Flux on top of Kubernetes cluster that runs your deployments. Maybe you use Kuberne. This instead of Terraform to using open source projects like Crow and or Croplane to deploy any of it will work. So just take a look at what you have as a standard in your organization. It's already present there. Use it. And I'd like to remind you of one of the recent feature releases in that context, uh, routing rules. They enable you to route API requests based on the HTP headers in combination or independently, uh, with the URL paths. And why to talk about it in context and deployment of deployment, it helps implementing routing strategies for your versioning, controlled rollouts, AB testing, and sometimes overlooked aspect. You can use custom domain names with routing rules or base path mappings. To build centralized APIs while deploying individual endpoints separately. So you see in this example, I have 3 different stacks. Stack 1 is products endpoint implementation, stack 2 is orders endpoint implementation, and stack 3 is shipping endpoint implementation. And those 3 stacks can be managed, developed by different teams using different technologies. They can follow their own deployment cycles. You don't have to coordinate anything between those teams as long as they are deploying to their own. Uh, their own, uh, endpoint. And also, it works great for blue-green deployments. Let's say we have two routes with dedicated stacks, one for products, one for orders, and we need to deploy a new version of product stack without affecting your operations. You don't want to have any downtime. So, what do you do? First of all, you create new stack, new version of products stack. You test it, you see that it's behaving as expected. You maybe need to pre-scale some resources in the back, maybe use some cluster of EC2 instances for that matter. And then it's just a small change in configuration where you change routing rule to point for all incoming products requests instead of stack version 1, point to stack version 2. There is no downtime. A few seconds later after this change is propagated through API fleet, API gateway service fleet, you see that now the entire load is coming to stack number 2, that new version. You observe your errors. You observe your logs. You see that everything is working as expected, nothing is failing. And at that point just simply delete that old stack or you can keep it around, especially if it's surveless, if it doesn't cost you, if you want to be on the safe side, you can keep it for maybe a day or so. I don't know. It really depends on how you manage your deployments. And again, there's no downtime to switch between them back and forth. If let's say that V2 did not work, if there are any errors after you started pointing production loaded, you can always change that routing rule back to the original one and use the old stack that you had. OK, you deployed. Well, it's scale. So, API Gateway is a managed service. It scales for you automatically. Think about all those prime days, uh, Black Fridays, Cyber Mondays. Today. Uh, Super Bowls and so on, it will scale definitely. What you just, what you need to do is just keep an eye on your quotas and their utilization to make sure that you increase them if needed, but otherwise your back and scalability would be a concern that you have to focus first on. Because anything behind API gateway, this is your responsibility to scale. So you can use, I think we announced it. Last week, or, yeah. Last week, uh, that, uh, quota service now allows you to increase quotas automatically. That's one of the tools. Maybe it will work for you for depending really on architecture that you have. Maybe some predicted, predicted, uh, uh, predicted scaling using machine learning algorithms that we have and outer scaling would help. Uh, maybe you would use, uh, AI tooling for monitoring and troubleshooting to make sure that you achieve that scale. All this is no different from any other workload, API fronted or not. Now let's talk about other aspects of API scaling that we quite often overlook. First of all, shed the load. The easiest task to do is the one that you do not have to do. So this is a screenshot of the article by one of our principal engineers, uh, published in the builder's library. I highly recommend to look at it when you have time about how Amazon implements load shedding, but even without going into this more detailed approach and more elaborate approach, you can start doing this by simply rejecting requests that you know that should not go into the back. For example, You know that your backend integration is expecting particular headers and query string parameters to be present, otherwise it doesn't know what to do. You can enable a validator in API gateway if that header is not present or if that query string parameter is not there, you reject it at the front door. Don't even pass to the back end. Same with payload schemas. If you know that this payload has to follow or must follow a particular adjacent schema, just configure that validator on API gateway, and then every request that does not match payload schema will be rejected at the front door. Your back end will not even know about it. Implement rate limiting. Implement AWS web application firewall rules. Maybe that incoming high load from some strange geographical region is the beginning of the US attack from the region where you have no customers whatsoever. You can do that as a rule right from the beginning. You don't have to serve it there. So make sure that you shed load early if possible and do not allow it in your back end. Now, when it comes to backend, there are a few, few ways to reduce load on on integration targets. First, caching. If your API responses. can be shared across requests. For example, it's maybe hourly discount list. OK, it doesn't change from request to request, cash it at the front using cloud front, you reduce load, you reduce latency, you reduce cost because it does not reach your API gateway. If you cannot use cloudfront for whatever reason, you still can use API gateway cash to do the same. You reduce latency, you reduce load on the back end, though at that point you do not reduce cost because the request reached API gateway, you're paying for that request. Another way is reducing the response data set. You maybe you do not need to return 1000 lines from your database back to the front end because it will show maybe just 50 of them on the first page. Add filters, add pagination, just return the cursor to the next page that should be returned on the next request. Reduce request data set if your workload, if your use case requires multiple actions, small actions calling multiple resources in a sequence in parallel, maybe you can batch them. Maybe you can send it as a single payload again, you're reducing payload on the back. Also think about implementing that more as an intent instead of a resource style API. Instead of user get put, delete, post, maybe manage user operation where you can pass a lot of information in one request and behind the scenes it will call different microservices, maybe that's an option. And also think if there are requests that can work as a synchronous call, maybe they do not have to be implemented immediately waiting for the uh while the user is waiting for that response. Maybe making an order or putting an order into your system does not require that order to be processed immediately. Maybe acknowledgement that we got that order like in Amazon. You make an order, you get acknowledgement, and then 5 minutes later you get an email that OK, your order is packaged and ready to ship. Maybe not 5 minutes. It depends on where you live and what you ordered. But still, that synchronous processing where you can process that queue of the requests coming in at your own pace allows you to scale way more flexibly with the back end uh constrained resources. And lastly, do not build too big to fail. Do not bet your entire business on a single deployment. Think about multi-accounts, cell-based architectures. They will allow you to have better blast radius control. They will allow you to make it easier to control your quotas. And if your client base is geographically distributed, you can use for the same reason, you can use multi-regional deployments where multiple instances of API gateway will serve subset. If one region goes down, other regions are still serving, or if you for whatever reason had bad deployment in a particular region, you do not stop your business from functioning. OK. How do we know how we are doing? And in AWS you all know, we build observability on a couple of services, on 3 services, cloud watch logs, cloud watch metric metrics, and X-ray traces, and APA gateway is no different from that. I'm not gonna go into all those services. I'll just want to remind that there are also open source services either managed and hosted by AWS or by yourself. There are partner solutions. There are plenty of observability partners in the space, and most likely you have at least 1 or 2 in your organization, if it's large enough already. Take a look at their offerings. Large number of our observability partners, they have API gateway-specific reports, dashboards, analysis, and so on. Look at them. They may fit your needs and just let's walk through what you should look at. First, API metrics. They are published per API, per stage, per resource and method. You can enable a different granularity depending on your API type, endpoint type, rest or web socket, you may have slightly different metrics. And do not forget about custom metrics. Let's say if you have some, your, your stack is serving some business purpose. So knowing what it did, how it did makes sense. So easiest way to do that, emit embed emit those metrics using embedded metrics format. If you haven't used it, look it up. It allows you to include structured log with the metric data into your logs that you send to Cloudwatch logs. Cloudwatch service will take that metric value and pass to Cloud Watch Metrics. It does not cost you extra uh to pass it from one service site to another. And it's much cheaper than using cloud watch metrics, custom metric publishing API. And also create alerts on those metrics. Send the email, execute lambda to remediate issue if you see that something is wrong and you can automate it, and so on. And then logs. API Gate Week has two types of logs that you should enable. One is execution logs. They go to Amazon Cloud Watch. You can specify level, error info. You can enable full request and response in those logs. One thing to keep in mind when you are enabling full request and response payloads that they may have personally identifiable or sensitive information in general. So now you have to keep in mind who has access to those logs in Cloudwatch and make sure that you still are good with your regulatory compliance. And these logs would be used for debugging and, and uh troubleshooting. Another group of logs, access logs, they go to cloud watch logs or, uh or, or Amazon Data Firehose. You can customize log format. You can, uh, it supports multiple formats like CLF, common log format, JSON, XML, CSUV. You can specify which fields should be included into those logs. You cannot log payloads and access logs, so there's no need to, to, to care about this. And you would use it for analytics and getting insights into usage of your API, who is making what kind of requests, where those requests are coming from, what are most popular API endpoints, where are drop-off points, where customers are leaving you, and so on. And lastly, do not forget about AWS cloud re logs. So that's where all the actions. That are taken by user service, or role on AWS service are logs. So this is your control plane logs. That's where you know, we're looking at those logs, you know who did what to your API gateway endpoint, who changed that particular feature, uh, or, or the, the particular configuration value and so on. Now one thing that metrics and logs are useless if you do not have tools to react to them when needed, and that's where one of them I mentioned already, enable alerts, react to those alerts, maybe you're sending to external system those alerts that has different rules for your escalations and so on. Another tool available is AI operations in Cloudwatch, and this is an example where I simply specify, look, I, I see that something is wrong. I got an alert. I see that something is wrong with my service, and here is a time window where I observed it, go figure. It will go, it will dig through the logs, correlate across multiple services in your stack, come up with hypothesis, and maybe with the remediation steps. It does it in quite an efficient way. And then another quite often overlooked tool is old fashioned command line with an AI agents or agentic uh additions into those tools, like, for example, Kiiro CLA. You can simply ask a question, for example, why do I see increased number of 500 HTTP response statuses by the API gateway in this stack? And let that agent to dig into it. They have extensions. They integrate with the inspection tools, observability tools, run books, insights. They can correlate logs. They can come up with remediation, and so on. You may be really surprised how well they work in troubleshooting and proposing your remediation steps when you ask them. And once APIs are in use, metrics, logs, operational picture, that's one side, but you want to have some kind of analysis to get insights what can be optimized, what can work better. How do I do that? How do I improve my, let's say, my user path through the system? And that's where a few tools come into the picture. There are plenty of the observability tools as I, as, as I mentioned in partner space, but also. First tool that will help is Cloud Watch loginsights. It enables you to correlate logs and query logs in batch across multiple log streams. Uh, and one thing that I really help, uh, really like about this tool is ability to translate from human to query language. I keep forgetting syntax. Uh, and whenever I need to ask for something, for some data from that tool, I just simply tell it in English. It translates into query, uh, and even better, when it returns results, it does analysis to spot patterns in those logs returned by the response by that query and generates a paragraph or two summary of what could be going on there. I can spot check logs or just simply trust whatever was there. Now, if you want a deeper dive into analytics, if you want to understand who is doing what with your APIs, next step is DAY deep insights using access logs. In this particular example, in, in this repository, we have implemented logs streaming into Uh, into data fire hose that delivers to S3. On the way, they get enriched using enrichment lambda. And then after that, when they delivered into S3, we use Amazon Latina and Quicksight to query and visualize, analyze those logs that were delivered. So you can. You can dig into all those more business-specific questions than operational questions. And if you need just a quick, uh, quick glance at what's going on with your API, you can always use service management console. If you want to visualize entire application metrics, you would go into cloudWatch dashboards and create your custom dashboard, which can be done as part of your infrastructure as a code template. And don't forget, if you are putting together that dashboard for the service, don't forget to include your custom metrics. Let's say how many trinkets I produced in the last hour, how many sales I made in the last 15 minutes, and so on. And more and more often, customers treat API gateways or APIs as products and look for ways to monetize them. And typically, they have two models. One is paying for consumption. It can be per API call, per use, per transaction, revenue sharing, or subscription-based. And there are a few ways to implement that using AWS services or self-managed services. The easiest one, just issue API keys, associate them with usage plans, and sell those API keys on the marketplace. That's integrated directly into API gateway. There is no need to build anything except for configuration. Another option, group your APIs into products and sell them as part of your product, uh, uh, as, as part of your overall uh family of products as a separate SKU. Now, when it comes to pay per use option, you can go back to that analytics example and just generate, uh, generate a report that lists all of your API keys, tenants associated with that key, and how many requests were made during that billing period that you're generating that, uh, uh, particular report for. Now, one thing when it, when we talk about monetization and if we bring in AI into the picture that changes that equation a lot is how do you monetize AI traffic. And I do not talk about AI traffic that is requesting AI, let's say some LLM requesting the tools, requesting data from your API. It's about APIs that you build that are baked by some, uh, that, that are backed by some workload including, that includes large language models or agents. Because each request may be different. You don't know how much it costs you for that particular request to get a response from your back end, from that LLM or agent in the back at the beginning. So, when you look at those, at those access logs, you don't know how, how much it costs. So, how do you make sure that you do not go under uh by simply charging too little. What do you measure? Is it a request that you measure? Is it tokens, outcomes of those requests, maybe tool use, amount of data returned. So this is a problem that you will need to solve for monetization if you have anything that is fronted by API but using AI in the back where you do not have a deterministic approach on how to calculate that cost. And finally, let's talk about the last part of the APA life cycle discovery, and please join me in welcoming Srijan. Alright, uh, thank you so much, Gadrius, uh, now let's talk a little bit about the last part of the API life cycle, which is discovery. Now, as your team size increases and as the number of APIs in your environment explodes, the first problem that you start facing is around fragmentation of APIs. So there are APIs across multiple accounts, and uh this API sprawl is contributing to a lot of problems in your environment. So, uh, let's talk a little bit about the three big problems that API sprawl and poor API discovery leads to. First, it slows onboarding. When new developers are trying to onboard onto your systems and trying to understand your API ecosystem, uh, they, they find it very difficult to, to, to understand the entire surface area of your APIs, and that results in a poor onboarding experience and slows them down. And the second one is around it hides risk. So you cannot really protect what you cannot see and what you cannot discover, so you will have a large swath of APIs that are undocumented, unknown, undiscovered in your environment, which leads to a compliance risk or a compliance exposure that you do not have control over. And finally, it. Inhibits reuse. So your developers are forced into a digital treasure hunt each time they're trying to find the right tool to do the right job, and uh and when they do not find the right tool, the first thing they do is they reinvent the wheel again and they deploy another API service for it, which prevents reuse of your APIs. So, uh, on November 19th, we announced the launch of Amazon API Gateway Portals. Portals is an AWS native fully managed developer portal. That allows you to manage API chaos and streamline discovery across your AWS accounts. Uh, you can control documentation freshness, and you can enable scoped access to your developer portal, allowing your API consumers to consume exactly what you want them to consume, uh, and you can do this while staying, while keeping all your configurations inside the AWS boundary. Uh, and with no infrastructure to manage, so that is, those are the cool parts about it. Uh, you can set up a branded API gateway developer portal in minutes, so developers can gain access to, to, to the APIs that you have. They can test it out, they can try it out, uh, and security gains access to visibility across all of these APIs that you have across accounts, and the platform teams can cut governance drift. Um, moving on, uh, we, let's dive a little bit deeper into what portals allows you to do. We, um, Gidris was talking a little bit about how APIs need to be treated as products, and that is exactly what portals allow you to do. You can bunch or group APIs together to form products, and, uh, you can also share these products across accounts using AWS RAM. Um, so that the vision is, so that it allows you to move towards a single developer portal across all of your, uh, enterprise. Uh, you can, uh, you can also to get you off the mark from a documentation perspective, what we allow you to do is we pull in the documentation from the API gateway runtime for the APIs that you're adding to the portal, and you have the option to overwrite that documentation and add your own documentation. You can also, uh, you know, enable or disable the ability to try a specific API out on the portal, uh, using you when when you create the product itself. Next, let's talk a little bit about the portal itself. Uh, the portal itself is where you, uh, collect all of those API products that you've created and you publish them. Uh, there are a few things that we allow you to do on the portal itself. Uh, we allow you to scope access using Cognito user pools. We allow you to, uh, choose between a custom domain name for your portal or an API gateway a generated one. Uh, and API products that are created on your portal can be decorated on the portal itself, and we also allow you to, um, uh, configure the, the branding of the portal so that it aligns with your, uh, organization's, uh, branding guidelines. And finally, we, you can, you can publish the portal and before you publish, we allow you to generate a preview so that you can look at the portal and understand if this meets your requirements or if it, if the look and feel is exactly what you want it to be, if the products are properly aligned and all of those things before you go and publish the developer portal. So API gateway uh portals is now available in all regions, all commercial AWS regions, um, and, uh, finally. Every feature that we have discussed today, right from response streaming to enhanced TLS policies to private integration with ALB routing rules. Uh, and portals, uh, forms is, is a commitment towards a larger promise to give you the most trusted and most secure and scalable comprehensive API platform there is where your API developers spend time innovating and not managing infrastructure. Security and governance is built in and is not bolted on. And scale is automatic and it's not a weekend project for you folks. So the API economy is here and it's continuing to grow with the advent of AI agents and whether you're a startup that is trying to build their first API or an enterprise that has that is managing hundreds of thousands of APIs across your accounts, uh, the innovation is already there, the tools are already there. The question that we are here to answer is what are you going to build and how you're going to build it. Um, so thank you so much everyone for coming. Uh, we have, since you're at Reinvent here, we have a few other sessions related to API gateway, uh, over the next 4 days. Uh, the next one that I'll be speaking at is around develop a portal. If you want to dive deeper into the developer portal, I'll be talking a little bit about building a culture of API excellence with portals, uh, and we have a few other, uh, sessions there listed as well, which will help you migrate over to API gateway and a few other things. Um, and we have some service resources. Gadrius has been pointing you to it on each and every slide. Um, so there are some guided paths and board, uh, and documentation available as to how you can build your architectures, uh, around API gateway. So, thank you so much. Please don't forget to, uh, uh, complete the survey of, for this session, and we look forward to having this conversation and continuing the conversation with you, uh, off stage. Thank you so much.