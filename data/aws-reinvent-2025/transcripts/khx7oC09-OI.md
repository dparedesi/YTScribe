---
video_id: khx7oC09-OI
video_url: https://www.youtube.com/watch?v=khx7oC09-OI
is_generated: False
is_translatable: True
summary: "This session provides a comprehensive deep dive into high-performance storage solutions for AI/ML, analytics, and HPC workloads, addressing the critical \"storage bottleneck\" that often leaves expensive compute resources like GPUs idle. Speakers Ati, Senior Product Manager for Amazon FSx, and Manish Dalrija, Principal Product Manager for Amazon S3, structure the talk around two main customer paths: those \"lifting and shifting\" legacy file-based applications and those building cloud-native architectures directly on Amazon S3. For the former group, the session champions **Amazon FSx for Lustre** as the premier solution, offering a fully managed, POSIX-compliant file system that provides the familiarity of a file interface with the scalability of the cloud. The speakers detail how FSx for Lustre achieves sub-millisecond latencies through zonal storage architecture and client-side caching. A significant focus is placed on its capability to scale throughput to over 1 terabyte per second and its support for advanced networking technologies like Elastic Fabric Adapter (EFA) and NVIDIA GPU Direct Storage. This feature is particularly crucial for deep learning, as it allows compute instances to bypass the CPU and communicate directly with GPU memory, enabling up to 3200 Gbps per client throughput.

For cloud-native workloads, the presentation highlights **Amazon S3 Express One Zone**, a high-performance, single-AZ storage class designed to deliver 10x faster access speeds and 80% lower request costs compared to S3 Standard. Dalrija introduces a \"mental model\" for optimizing S3 performance, distinguishing between \"overhead\" (time to first byte) and \"data transfer\" time, and explains how Express One Zone minimizes the former for latency-sensitive applications. He details architectural innovations including directory buckets that automatically handle partition scaling to support millions of transactions per second (TPS). The session offers practical optimization advice, such as using session-based authentication to minimize request overhead and leveraging the AWS Common Runtime (CRT) to manage parallel connections across multiple network interfaces efficiently. Purpose-built integrations are also showcased, such as the S3 Connector for PyTorch—which accelerates checkpointing by bypassing local NVMe storage—and the S3 Analytics Accelerator for Parquet, which uses byte-range gets to prefetch metadata. The session concludes by demonstrating how these two worlds converge: customers can link FSx for Lustre file systems to S3 buckets, effectively using Lustre as a high-speed cache that provides a file interface over massive S3 data lakes. Real-world success stories from Shell (achieving 100% GPU utilization), Meta FAir (sustaining 140 Tbps), and LG AI Research illustrate the tangible impact of these architectures on reducing model training and inference times."
keywords: High-Performance Storage, FSx for Lustre, S3 Express One Zone, AI/ML Workloads, NVIDIA GPU Direct
---

Hello, everyone. Uh, my name is Ati. I'm a senior product manager for Amazon FSX. Uh, I'm joined by Manish Dalrija, who's the principal product manager for Amazon S3, and Mark Roper, who is the principal engineer for Amazon FSX. Um, Mark will join Manish and I for Q&A after the talk. All right, um, so all three of us have spent years, uh, working in the working directly with customers running high performance workloads, um, and pushing the boundaries of what's possible with storage. We've also, what we've, we're going to share a little bit about what we've learned about high performance storage for AIML analytics and HPC workloads. Uh, we're going to walk through some real customer use cases, um. Real world examples dive into technical capabilities that enable performance at scale, um, and introduce exciting new features along the way. Let's start with the fundamental question. Who needs high performance storage? And the answer is a lot of different workloads across a wide range of industries. So think about machine learning teams running large language training models on massive data sets, data analysts querying petabytes of data interactively, or think about researchers running weather simulations or drug discovery with tens of thousands of cores, and the list goes on. But what ties all of these together are two things. First, that these are compute intensive workloads, so they require hundreds and thousands of cores, uh, CPU or GPU GPU resources, and second, they're all data intensive. They depend on fast, reliable access to massive, uh, set, massive scale of data. And that second part, that's where things really get interesting. Let me show you how. So we frequently hear from our customers that. They love that with AWS they can spin up compute clusters, run their workload faster than ever before, and then spin them down once their workload's done and stop paying for those resources. This is the magic of the cloud. But so ideally you want this. You want, as you add more compute, more CPU, more more GPU resources, you get proportionally more work done. Ideal linear scaling, uh, beautiful. Here's the problem though. What if you have a storage solution that cannot keep up with the performance requirements of your workload, and in that case, your the work done or your throughput plateaus, and you can keep throwing CPU GPU resources, but the performance, uh, will not scale linearly with it. And the reason being that all those compute instances are now competing for access to the same data store. And that data storage has now become the bottleneck. This is specifically painful because we see that 90 to 95% of spend on these workloads is compute. So when your compute is sitting underutilized waiting for data, your time to run gets shorter and your cost goes up. So in an, in an ideal world when you're architecting your solution. You would want your storage to scale linearly with your compute so that it never ends up becoming a bottleneck. Before, uh, and before diving right into solutions of what we've built at AWS to address that very bottleneck, I want to acknowledge that customers come to AWS from two different paths. On the one hand, we have the customers that are running HPCML AI workloads on premises for years. Their workloads are based on file, file-based access, and they are. They want to maintain that paradigm and move to the cloud, uh, gaining the benefits of the cloud, and on the, on the second hand, we have the customers who started in the cloud from day one. So that means that they have their data stored in Amazon S3 and their applications are built around S3 APIs. So, just with a show of hands, who here is using file-based applications today? Oh, awesome. And who here is using Amazon S3 as their data lake? That's pretty good. That's a great mix of customers, and this is also very, um, very accurate representation of what we see with our customers, um, in production today. So we're going to cover each one of these, but let me start with Lift and Shift file system customers. So the question remains, why do file system remains the preferred choice for so many of these high performance workloads? Couple reasons, but the primary one being familiar interface, so researchers, data scientists and developers know how to work with files and directories. It's just more intuitive. The second is pause permissions. So when I say pause permissions, file file systems give you granular access. When you have multiple users accessing the same, same data, you want to give control. You want to make sure that you're controlling who gets access, who gets access to those files, who can write and execute those files. And the third is consistent data access. Again, if all of your users are accessing the same file system, you want to make sure that the data that they're reading is consistent. Consistency is guaranteed. There are no stale reads, and that is where file system is used. Back in 2018, we found that many customers wanted all of these benefits of file system, along with the ease of use and scalability of the cloud. And that is when we launched FSX for Luster. Uh, FSX for Luster is built on the open-source Luster file system. So, any of you who've heard Luster before? OK, that's pretty good. So Luster is the open source, uh, high performance file system. It is one of the most popular high performance file systems. It is used by national labs across the world. It is used by MLAI and HPC applications, uh, on premises as well. So it goes from applications range from training and inference all the way to weather modeling, um, and genomic analysis. And um So what we've done is luster is really powerful, but it's also notoriously complex to manage. What we've done is basically taken the all the benefits of the fast and scalable luster file system and combined it with the management and ease of use of a cloud solution. So we've offered a fully managed, fully elastic and fast FSX for luster file system. Let me walk you through all of these, one by one, starting with fully managed. So what does fully managed means? Fully managed means that this file system is tested and operated at an unprecedented scale. Uh, we're also consistently monitoring all the hardware resources that are underpinning the file system. So assume that one of your server, uh, runs into a hardware failure, and in that case we automatically monitor it, uh, automatically detect it and replace the server with a healthy server to keep your file system healthy at all times. We've also done the heavy lifting of ensuring that your file system is built on the latest and greatest technologies at AWS and the latest and greatest technologies offered by the open source cluster community. And make those available to you with full API support. All right. I want, now I want to talk about elasticity. We've made luster fully elastic for the first time in the cloud. Uh, we were hearing growing pains from the customers as they were scaling their workloads in the cloud. Growing pains such as data does not grow linearly. So for instance, if you have ML training runs, you generate terabytes of checkpoints, and then you clean them up. Your simulation spikes, and then you wrap those projects, so your data requirements are always going up and down. And this, there's this constant battle that you never want to run out of storage, but at the same time, you don't want to pay for unused capacity, right? Um, that's one thing. And the second thing is that not all your data is active data. You're not actively using all of that data to be stored on the SSD file system. So you need it for your hot data, but you still need your terabytes, your checkpoints, and your results stored, but not on the fast SSD because if it is stored on your faster SSD tier, it gets real expensive real fast. So think as you're scaling your data set to petabyte scale. As you go to petabyte scale, it gets really, it gets really hard to just operationally manage and the economies don't scale either. That is why we launched FSX Intelligent hearing earlier this year. So with FSX Intelligent steering, we offer virtually unlimited storage capacity, which means that the data that the storage capacity grows and shrinks automatically based on your usage. Um, you don't have to worry about it. You'd never run out of storage and you never have to pay for sto for storage capacity that you're not using. Second point is intelligent tiering between storage tiers, so we keep all your active data on your fast SSD and automatically tier your less accessed data to the colder, low cost tiers, and we, we manage all of this. You don't have to worry about it. We do it intelligently based on your access patterns. And finally, the economics of it are quite compelling too, um, for the colder, as an example, for the colder tier, you pay 0.5 cent per gigabyte per month and to put this in perspective, your overall, uh, solution comes down to be 34% more price performance compared to your HDD solutions on premises. Now, now let's talk about speed. These are the performance levels we are delivering in production today. FSX for Luster is the fastest storage for GPUs in the cloud, and it delivers the lowest latencies in the cloud. This, this performance metrics are what keeps your compute fully utilized. Let me walk you through how FSX for Luster delivers this performance so that you can customize your file system based on your workload requirements. Each of the dimensions shown here, you can, you can play with them, uh, independently, and that's one of the, one of the ways you can even make it even for price performant. So here's a simplified view. On your left you've got your compute instance that's running your workload, and on the right you've got your storage, you've got your file system. File system has two kinds of servers. It has the metadata server and it has the storage server. So your metadata server is basically handling all your metadata requests. Think about creating. Creating files, listing directories, managing permissions. Your metadata server takes care of all of that and then we have the objects object server. Object server is what handles your actual data reads and writes, uh, to and from the file system. All both of these servers are backed by storage disks, and when you talk to storage, it, the your compute instance basically talks to this metadata server and storage server over the network. Now I'm going to show you how each of these components comes into play when we talk about performance and how you can, how you can change those based on your workload requirements. Let's start with uh throughput. So when you're using multiple client instances, you're running thousands of training processes, simulation jobs, they're all accessing the same file system simultaneously. This means that you need more throughput, and by throughput I mean how many bytes per second you can read or write to the file system. So, to deliver this throughput, FSX4 luster file system basically scales out with multiple storage servers working in parallel to serve your requests. This parallel distribution across hundreds of storage servers is how we deliver over 1 terabyte per second of aggregate throughput. So when all of your compute instances are talking to the file system, your requests are going to not one but multiple servers in parallel, and you're getting the, you're getting the power of those multiple servers every time, and that's how you get over 1 terabyte per second of aggregate throughput. This enables fast parallel data access to hundreds and thousands of cores. All right, now most high performance workloads are heavy on throughput and relatively light on metadata, but recently we started noticing that customers were using FSX for luster for more metadata intensive workloads. So you can think about home directories, user research workstations, and interactive applications where you're constantly listing directories, opening files, checking permissions. So last year we launched a capability to scale out your metadata servers very similar to your storage servers. Again, your metadata servers can be scaled out independent of your storage servers, um, and your storage capacity. What this helps with is you get 15x higher metadata IOPs when compared to before when you could not scale out. And we're further doubling down and making sure that you don't run into metadata bottlenecks. Um, recently, we made an update to the Luster software which enables 5x higher directory listing performance. Now I walked you through aggregate throughput. I walked you through metadata IOs of the file system as a whole. Now I want to talk about what happens when you have, what happens with a single compute instance interacting with the file system. Why am I even talking about a single compute instance? So we have a file. We have seen that EC2 instances, uh, on Amazon and across the board compute instances have really improved when it comes to their network bandwidth. We've gone from 25 gigabits 200 to now 3200 gigabit per second with P5 instances. And all of this performance is great, but it, it's not, it's not amazing when you cannot leverage it when talking to storage. So when you're looking at a traditional file system, your traditional file system generally talks to your compute instances over TCP network and it only. Only interacts with one NC on the compute instance, which was true for FSX for Luster as well for two years back. Um, you're only interacting with one NC and because of that your throughput gets limited to 100 gigabits per second, and that means you're leaving so much throughput on the table. So we, what, what we did was we launched EFA, uh, EFA support. EFA is elastic fabric adapter which is built on Amazon's SRT protocol. What it helps you do is it helps you communicate with the compute instance across multiple NCCs, and it bypasses the operating system layer completely. And how that is helpful is now you're not using a lot of your CPU cores in figuring out which NIC to communicate with. And it's much more faster. It can scale up to 700 gigabits per second, um, and obviously knowing the power of GPUs, we wanted to ensure that we're utilizing it to the most optimal levels. We also support Nvidia GPU direct storage. Um, how many of you have heard about Nvidia GPU Direct storage? All right, we've got a lot of storage folks here. I love that. Um, so for the Nvidia GPU direct storage, what happens is your compute instances can basically bypass the CPU and talk directly to your GPU memory. Um, think about a general data path going from your storage to your CPU memory to your GPU memory. That's multiple hubs that are that need to be managed. But what happens with GPU Direct is it allows the file system to communicate directly with your GPU memory. Allowing us to give you a throughput of 3200 gigabits per second, um, per client. So 12 x higher. All right. Finally, let's talk about latencies. When we built FSX for Luster, we architected it to offer the lowest possible latencies on AWS. Um, when I say latencies, we're talking about how long it takes for a small operation to, to perform, so it affects the overall responsiveness of your storage solution. A lot of workloads we work with large compute clusters processing data, while at the same time end users or researchers are also manipulating that data interactively. And for these human in the loop, uh, use cases, having fast responsive storage really matters because it should not feel like you're sitting for seconds or minutes just to wait for your file to open um or do an operation that really matters for a researcher. Now how do we get sub-millisecond latencies beyond just having your SS beyond just having an SSD disk for your active training data? Three things. Um, first of all, we allow you to FSX for luster is a zonal storage offering so you can keep your file system in the same AZ as your storage, uh, as your compute. So that means just at a very low level, the distance that the data has to travel is really minimized. And that helps in improving latencies. Second is we do point to point communication, which is single network round trip. Your clients your client server can directly talk to your file system server without multiple network hops. What you would have seen with most storage solutions is there are multiple network hops through the server or there are load balancers. And while they're there for like very good reasons, they do, they do have an impact on the latency of the storage offering. And the third one is client side read and write caching. So with FSX for luster, I like to say that we consider client as part of the file system. So anytime you read and write, if you're reading and writing, say similar files over and over again, FSX for luster can cache those on your client. So in that case you're basically having no network hops at all and even lower latencies. So FSX for luster allows you to get about 0 to 1 network hops and some millisecond latencies. Um, I've thrown a lot of theoretical information at you, and now I want to share how this happens, how this works in production. Shell is a great example. Shell had a GPU based on-prem environment where they were running into infrastructure bottlenecks, so they decided to burst into cloud with FSX for Luster and EC2. They wanted to leverage the ability to scale compute and storage up and down based on their user requirements. Now how this helped was they were able to increase their GPU utilization from less than 90% to 100% in the cloud. So with FSX for luster, Shell was able to fully utilize the compute resources at full throttle. And these 1011 point percentage difference might not seem a lot, but those of you who are using P5 instances, this really, this really accumulates really fast. And that concludes the file section part of the presentation. Now I want to hand over to Manish to share the options and optimizations for S3 Data Lake customers. Thanks Ati. Where are my data lake customers? Great, how many of you have heard of Express One Zone? Awesome. A few So we'll talk about how do you reduce storage bottlenecks with cloud object storage. Many of you already have petabytes of data stored on S3 data lakes. Now this is because, um, you love the durability, the scalability, and the cost effectiveness. Now this is why S3 provides you with purpose-built storage classes, right? You're starting from, um, S3 standard, that's for your frequently accessed data, Glacier for your long time archival, and intelligent hearing when you have changing data access patterns. Now, this makes it very easy for you to source your data directly from S3. But here's the critical point Your storage must keep up with your compute and as Aditi mentioned earlier, compute is your most expensive resource and when the storage cannot keep up, you will waste money on idled CPUs or GPUs. With that said, I want you to, uh, focus your attention on one particular storage class on the left. This is Amazon S3 Express One Zone. This is our fastest cloud object storage, and it's built specifically for performance critical applications. It will give you 10 times faster access and 80% lower request costs than S3 standard. Now, let's look at what makes it different. First. Express One zone is a single availability zone uh storage class, and we built it specifically to deliver single digit millisecond consistent single millisecond digit uh digit millisecond access for your most frequently, uh, accessed data. Now it instantly scales to hundreds of thousands of TPS with a new bucket type called directory buckets. Now for the rest of the talk we will focus on S3 express one zone, but some of the techniques that we talk about will apply to S3 standard as well. All right, so before we dive into optimization techniques, let's think about a mental model that we will use. So when your client sends a request to S3, there are two parts to the request. The part on the left that's in white is called overhead or time to first bite. This is where your client sets up the connection, sets up the authentication. There's the round trip time. S3 is locating where your data is and then serving it up. No data is being transferred during this time. And then there's the second part in pink. This is where the actual data transfer happens. We're going to use this mental model for how we can evaluate your workloads. Now there are, there are 2 workloads on the screen right now, um, the top and bottom, they have the same overhead. This is the part in white. But notice the difference in the data payload. On the top, you will see a much smaller data payload. When your transfers look like this, your um your, your workload is latency sensitive. So think of KV caching, your shuffle charting workloads, your real-time inference serving, and this is where your overhead matters a lot more. And on the bottom you'll see a larger data payload compared to the overhead. Now when your transfers look like this, you care about the total transfer time or your throughput. So now that we have a mental model, let's talk about how we can optimize for different parameters, different performance parameters. Let's start with latency. And we're gonna talk about two different techniques that you can use with S3 Express One zone. The first one is co-locating your compute instances with S31 zone directory buckets. When you do that, your data travels shorter distances. You have a few, uh, much fewer network, uh, hops, and what that lets you do is reduce the latency that you have to the storage. Now co-locating might not always be possible. Now in these situations, you can still take advantage of the latency optimization. But you might pay a little bit of a penalty when you go across AZ. So if your computer is in an adjacent AZ from your directory bucket. The second technique is session-based authentication. Now this is a new, new, um, technique that we introduced with directory buckets. Now with traditionally with S3 standard buckets what you would use is IM authentication. With S3 directory buckets you would use something called session, um, a session-based authentication. You would create, um, you, you would call it an API called create session. This will give you a token. Your application will cache this for a period of time and reuse that so that you don't incur a latency hit when you are authorizing, uh, with a directory bucket. Now we understand that not every application can change overnight so you can still start with IM authentication and then move over to session o when you want to optimize for latency. And whenever you're diagnosing high latency, we always ask customers to look under the hood in the library into their libraries and make sure that session auth is enabled. OK, so latency often goes hand in hand with transactions per second. A lot of the, the workloads that you run with small latency. Um, require, uh, are, are request intensive, have request intensive operations. So think of, uh, large scale analytics or shuffle charting. Now with S3 general purpose buckets, if you're familiar with how do you optimize for higher TPS, you would do that through prefix management. And the way S3 works is you, um, it scales by with with prefix so it adds TPS capacity when you have constant load. Constant load and and it it will basically scale up after a period of time but it takes time. With Express One's own, you don't need to do any of that. We handle the partitioning under the hood. And uh for each directory bucket, we'll give you 200K reads per second out of the box. And of course we're continually innovating on your behalf, so this year we added optimizations to go up to 2 million transactions per second. Now, a couple of quick tips, um, when you're working with directory buckets, um, you should try to keep your directories, uh, dense. What does that mean? Right? So you should try to flatten your directory structure and keep the objects in the leaf nodes, um, and put a lot more objects in the leaf nodes. And the second tip is, um, don't add entropy. Typically we advise you to add entropy in your prefixes for your S3 standard buckets. Here, this will actually slow you down. All right, let me share with you how this really works in practice. Tavali is an AI infrastructure company, and they're building a web access layer for agents and large language models. They managed a two-tier caching system, and their costs were rising. They would have had to double their investment to change uh their hot double their investment for their hot cash layer. Instead they chose S3 Express One Zone because it delivered the low latency and the cost efficiency they were looking for. This is for their hot cash layer. In addition, it just scaled automatically as their data grew. You can see the results. They were able to cut costs by 6 times. While improving performance and reliability across millions of user requests. So this is um latency and transactions per second in play. Now let's talk about high throughput. Now many of you are working, uh, running workloads like um large scale ML training or research workloads. This is where data loading and checkpointing becomes important. You need massive throughput so that you keep your compute um fully utilized. So one of the techniques we're going to talk about is parallelization. Now since S3 scales horizontally, the way to increase throughput is by breaking up your requests across multiple connections. In this example over here you can see that each connection can achieve 100 megabytes per second. Now, if you need gigabytes per second or terabytes per second. You open up multiple connections. So how do you accomplish this with APIs, right? So when you're downloading, use um byte range gets. This is where you will download parts of the object, and then you'll have to assemble that on your compute node. If you want to upload, you use a similar technique called multi-part uploads where you would upload um individual individual parts and S3 will assemble that for you. All right, so I'm very excited about this example. This was mentioned by Andy in the keynote yesterday as well, um. Meta Fair is a research organization. That runs research workloads including LLM training across thousands of GPUs. They wanted to speed up their checkpointing and data loading. And uh what they did was they they used S3 Express 1 zone. We scaled up an AZ with 60 petabytes of data. In one AZ where their GPU clusters were located. And they were able to sustain 140 terabits per second with over 1 million transactions per second. Isn't that neat? OK, so let's talk about the client site optimization. Um, we, we saw that some customers were not able to fully utilize the compute resources using the techniques that we just discussed. So we built the AWS Common Runtime or CRT um, anybody heard of CRT over here? Oh, not many people. This is great. Um, so let's talk about it, right? So it's a set of open source libraries and, uh, what we did was we embedded these libraries right into all our SDKs, all our clients, all our connectors, so you get performance improvements with S3 right out of the box. The great thing is you don't have to change much. An example here is that CRT will deliver up to 6 times faster data transfer with CLI. Using some of the techniques that we just discussed, right? Um, just keep in mind that, um, for PMTRN, the, the larger instances, we enable the CRT by default. However, if you're using some other instances, please look under the hood. You might have to enable this and opt into CRT so uh so that you can take advantage of some of these techniques. So similar to how Ati spoke about uh multiple NCs and how you can increase the per instance throughput. We have a similar feature in AWSCRT. So instances such as P5 and P6 can achieve 800 gigabits per second theoretical bandwidth over their ENA network interfaces. Right, so this CRT feature allows you to distribute your connections, your S3 connections over all of these network interfaces. Now this is particularly important if you're using EFA for your um other uh for for your computer intensive workloads. It prevents EFA slowdowns. And your compute and storage traffic can both run optimally if you spread the load. How do you do that? This is, it's, it's as simple as just setting the config and picking the network interfaces that you need using CRT. Let's look at some sample results. So we, we ran tests on a DL 24 Excel, um, instance. We downloaded 1030 gigabyte files. When we added a second Nick, we were able to almost double the performance. That we could we could get almost double the throughput with 4 nicks we were able to get about 2.5 times the performance compared to a single nick. Now, the scaling doesn't look linear, but you will get some meaningful uh throughput gains when you when you add multiple nicks. But the more important thing is that you can spread your traffic or even pin a particular Nick to your process so this will help you optimize how you, you control this will help you optimize your, your, um, your workloads when you're when you're using a, um, much larger instance. OK, let's talk a little bit about integrations, right? So this is, these were all libraries that we spoke about, but we heard from customers that they didn't wanna change their application and they wanted to take these best practices and use them with the existing frameworks that they had. This is why we develop purpose-built integrations. So that you could take performance off as 3, we're gonna only talk about 3 of them, but we have a lot more, um, that that a lot more integrations that that we have developed. So the first one is the Pytorch connector. And when you use Pytorch, you have to build your own data primitives to load and save data when you wanna load the data or you wanna run checkpoints. So we wanted to give you a high performance integration that took care of a lot of this. This is where the S3 connector for PyTorch comes in. Now, um, this gives you data set, uh, this gives you built-in Pytor's data set primitives, and it supports both map style data sets. This is for your random access data and iterable style data sets, which is when you're streaming sequential data. Now for checkpointing, the connector automatically integrates with the Pytorch distributed um checkpointing support. With this, you can checkpoint directly into S3 Express 1 zone, and you can bypass the local NVME. What we found was that you could get up to 40% performance increase by bypassing the local storage and going directly to S3 Express 1 zone. Uh, new for 2025 we've um accelerated partial checkpoint loading by 60%, so this is another example of how we're innovating on your behalf. Uh, how many people here work with parquet data sets? Awesome. Quite a few. So, um, you probably already know that your query engine has to read metadata from parque files, and these, um, these, this metadata is stored at the foot of these parquet files. This metadata stores information about the file structure and what the schema is. So when you're scanning across multiple of these files, what you will see is that the metadata will slow down your query execution. So this is where um the S3 analytics accelerator accelerator comes in. This is an open source library. It prefetches metadata using byte range gets. It, it reads the, the, the footer. And um it also caches this footer for subsequent reads. So this eliminates quite a bit of overhead. We tested this with um TPCDS. This is a benchmark that you can run, and we saw that you were that we were able to achieve up to 27% performance improvement um just by using the analytics accelerator. Another integration I wanna talk about is Sagemaker. If you're not familiar with Amazon Sagemaker, it's our fully managed service. And it's built for building, training, and deploying machine learning models. Now when you're running training jobs, um, you need to load your training data, often, um, you know, millions of, if not, uh, uh, lots, lots of small files, um, like images or text samples, and this will generate millions of requests per second into storage. Now this is where our S3 express one zone integration with SageMaker fast file mode comes in. Uh what fast file mode will do is it will stream data directly from Express One Zone into your training instance, bypass the local disk, and accelerate your, um, training performance. Now since Express One Zone scales to 10 hundreds of thousands of TPS, um, you should be able to see meaningful performance increases with this approach. Great. So earlier LDT covered um how you can optimize interfaces for high performance file interfaces for uh high performance workloads. We went through some optimizations for object storage, but what if you already have a lot of data in your object store? And you want to use a file interface to access that. So this is where Aditi will show you how you can bridge the two. Thanks, Manish. All right. Um, I'm only going to speak for 5 more minutes, and this is the favorite part of my presentation, so listen up. Um, So I, I shared a lot about high performance file systems, luster file systems, fully managed, fully elastic, uh, in the beginning of my presentation and. If any of that resonated with you, for you felt, oh but my data is already stored in S3 data lake, then don't worry, we got you, um, we basically have the option to connect your file system with your S3 bucket. This is also just a general paradigm we use within AWS that we want to meet where you're at and we want to minimize the work that you have to do to move to the cloud or run your applications in the cloud. Um, let me show you how this works. So with FSX for luster, you can create a file system and link it to the S3 bucket under the, under the covers. This is your S3 bucket. It has all the objects, all the data stored in it. When you connect your file system to the S3 bucket and try to read and write data from your compute instance, what it does is it immediately takes all your metadata from your S3 bucket and shows it as file metadata on the file system. So for you it would seem like the data is actually on the file system. When you try to read and write data onto the file system, FSX for luster will automatically go and fetch that data from S3. Um, now, and another interesting fact here is that we take care of bi-directional synchronization. So any changes you're making to the file system, we are exporting those changes to the S3 bucket, and any changes you make to the S3 bucket, we import those changes to the file system. Um, completely hands off for you and fully managed on the back end. You can think of this as a file system cache in front of your S3 bucket. Um, we can look at, let's look at a performance when a file system is in front of S3. We took a Cggle liver data set, stored as a public S3 data set, and measured the time to do patient classification with different setups. So the first one that you see right there is is where you have your data stored in S3. You load it to your local storage, and then you train against that local storage. What we did was we we tried to do a test where we used FSX for luster as a cache between your compute instance and your S3 bucket. In the first run of that model we saw a 67% increase in performance. Now that increase in performance was coming from two things. The first one, you, you see that green color over there? That is the data loading time where your compute instance is copying the data and it has to copy the data, wait for it to be completely loaded before it can start training with FSX for luster, you spin up your instance and you start training immediately, so we save on the data loading time. And second is just pure performance that a file that a fast file system delivers, which leads to this cumulative impact of 67%. Now this was the first run where FSX for Luster was still going to S3 to get that data for you. Um, in the second run onwards you would see that the performance has improved by 83% and that's because the data is already cached onto the file system. So Finally, one last customer example to bring all of this together for you, um, LG AI research. This AI research lab within LG wanted to create a foundational model that mimics human brain, so they decided to build their own model using Amazon Sage Maker and FSX for Luster. On the left you see that they store machine learning training data in their S3 bucket. And for performance and a file interface, they created a luster file system linked to that S3 bucket and their Sagemaker training jobs was directly talking, uh, was directly accessing their data through the file system. When workloads finish, model artifacts get written back to S3 for long term storage and for inference, so they use S3 for longer term storage, FSX for luster for their highest performing hot data to accelerate their training runtime and getting the best performance out of their instances. This concludes the presentation we had. We basically try to cover high performance storage options depending upon whether you're looking to lift and shift from on premises or you prefer a file system interface which is FSX for luster, or if you have an S3 native application where you can use S3 Express one so. And then if you already or if you prefer file system interface but your data is stored in S3, uh, then you can basically mount your file system to the S3 bucket. Thank you all for attending. Um, if you haven't already, please complete the survey. You should get a notification in your app. Um, Manish, Mark and I will stick around to answer any questions you have, um, on. Anything related to high performance storage. Right, thank you.