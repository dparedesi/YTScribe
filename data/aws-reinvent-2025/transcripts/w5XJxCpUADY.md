---
video_id: w5XJxCpUADY
video_url: https://www.youtube.com/watch?v=w5XJxCpUADY
title: AWS re:Invent 2025 - Agents in the enterprise: Best practices with Amazon Bedrock AgentCore(AIM3310)
author: AWS Events
published_date: 2025-12-04
length_minutes: 59.92
views: 83
description: "As organizations scale AI agent development, robust enterprise architecture patterns become essential. In this advanced session, we'll explore how Amazon Bedrock AgentCore enables teams to build modular systems using their preferred frameworks while sharing tools through MCP gateways. Learn about A2A collaboration, shared memory, identity-based access controls, and integrated observability. Discover practical strategies for secure runtime deployment, standardized tool integration, evaluation fra..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Hi everyone, uh, if you've ever been in, in an agent demo that went perfectly and then you thought. You cannot hear us? No, cannot hear us. Cool. OK, OK. Hi everyone, if you've ever been in an agent demo that went perfectly and then you thought. How do we actually do this at scale across many users? How do we do this securely and how do we do this in a way that we can trust that it performs well at scale? Then you're in exactly the right room. Quick raise of hands if you don't mind, if you are a developer or an engineer working on agents. Cool. And if you are representing the platform team that is enabling infrastructure for the developers. OK, love you folks, and If you are from the business side supporting or using those initiatives. Very few, cool, um. I'd say we have about like probably a 50, 2030 split. Thank you for uh all for being here. The session is built with these three personas in mind and um we'll share a lot of best practices about what it is that it takes for all of you to help bring agents into production. My name is Kosti Vassilagakis, and I'm a product manager lead on AIDS and Core. Um, I had the pleasure to be with the team since the early days of the vision. We've built the PRFAQs and we launched the service, and we've iterated with many hundreds of design partners that we helped not only build the service but also bring them into production. So very excited to be sharing a lot of the lessons we learned. And with me on stage is Maira Ladera Tanke that is a tech lead on the Agenic AI team and has helped many hundreds of customers go from POCs to production. I'm sure she's super excited to share a lot of the lessons we've learned so, uh, to, to date. Later, uh, in the session, we have the pleasure to host Phil, uh, Norton and that from the Clear from Clearwater Analytics. He's a senior developer manager and Clearwater Analytics is a unified investment platform that has more than 10 trillion assets under management, and they have a lot of agents already in production. I'm sure you learned a lot from Phil. Today's session will cover what separates POCs from production, what are some of the best practices and lessons learned as you think from taking, uh, your agents into production, and of course we'll hear from Phil a lot of, uh, the insights that he has to serve. This session is full of insights plus demos, and the pace is probably quite fast. Please keep your questions, and we'll we're gonna be very happy to answer them right after. Now let's hear what we from Ia what we're hearing from the field. So our customers keep telling us that moving from POC to production is difficult. They keep describing this POC production chasm. Why is it so difficult to go from your demo to a production application that scales across different users that provides the governance that you need? Well, the key to that is in 6 different. Capabilities. First of all, you need to have accuracy. You need your agent to work in real life with real users. Users tend to not behave like the developers expected. Users tend to want more once they see how cool our applications are. So the more we see generative AI evolving, the more we see Agente AI evolving, the more we also see our. User experience changing and user expectation evolving and keeping this accuracy is extremely important. Then scalability. Working across different users, working across different domains, you want to have your agents scaling and you want to have your agents scaling seamless while keeping this hyper personalization. The beauty of generative AI is that you can have your application being tailored to your needs, being tailored to customers needs, and being completely different yet similar, right? That. Brings us to the memory part of it. So handling memory across users, handling memory across different agents, but handling this memory securely. So security is a crucial part of the. The journey to production and having those agents being secured in production is essential because those agents are accessing our life systems. They are accessing real data, and this data access needs to be secure. Then cost like models keep getting cheaper and applications keep getting more viable, but we are talking about a lot of tokens and agents can reason, but they use tokens for that hosting infrastructure. All of this requires cost and you want to be able to. Understand where the cost of your application is so that you can control the cost and you cannot understand the cost, you cannot understand the accuracy of your agent. You cannot do any of this without an observability pipeline. So you need to be able to understand what your agent is doing in order to Create these agents to evolve these agents, so this is uh the crucial part of the job and this comes to the last part of the challenge which is monitoring because after you create your agency, you have the observability of everything that is happening, you want to monitor it in production. We are seeing a transformation on how things are working. So we are seeing open source moving really quickly and we are seeing like in months the technology completely change. So how can we embrace the open source while keeping all of those requirements, right? So how can we embrace open source securely? How can we embrace open source scalable with monitoring, with observability, cost control, all of it. That's the biggest question, right? Sounds familiar. I, I'm sure it does, and that's actually what we've been hearing from our customers, and this is why we built Agent Core. These are the very reasons we decided to invest in genic infrastructure. But before we, uh, discuss the best practices, let me, uh, give a quick overview of what Agenore is. And let's start with Runtime. Runtime is a secure serverless hosting engine for your tools and agents. You can build any agent using any framework, for example, line graph, Lang chain, uh, crew AI, and strands, and you can leverage any model from Bedrock, of course, other models like, uh, OpenAI, Gemini. And you can host all of them and uh satisfy real time and long running use cases. You not only have an endpoint that you can have interactive chats, but also you can host long running, uh, uh, deep research as well as coding agents, and run time is MCP and A2A compatible, which means you can, uh, host your tools using and have an MCP interface on top of them, or you can allow your agents to collaborate using Google's 828 protocol and. Because agents want me to remember, we built also memory. Memory offers short term and long term capabilities where short term is within the session. Long term it means across session you can have the agent remember facts and summaries across the interactions that they had with that particular user. And because an agent is as good as the insights and data that it has, it is access to tools and for this reason we uh built gateway where you can expose any internal API, any lambda, any MCP server into that gateway, and you can expose that gateway to any number of agents. But how do you control who has access to the right agent and right tool? And this is the reason why we build identity. Identity is helping you use your workforce credentials using, for example, Octa, Entra ID, Cognito to establish who has access to runtime and gateway, but also who has access to third party resources from the gateway, like, for example, AWS resources, as well, third party like Xira, Slack, etc. And because you want to enforce certain rules on those tools we built and launched yesterday policy where you can define in real uh you can define what exactly controls you want. For example, you shouldn't allow for that particular user to uh proceed with a transaction of more than 100. You can establish that all with policy. And many of the tools that customers are using tend to be a little bit similar, so we decided to invest in some tools. We offer, for example, browser and code interpreter. A browser is just a headless browser for your agents to navigate the web, and a code interpreter is an isolated engine to for your agents, once they generate code to execute JavaScript, TypeScript, and Python code in a fully secure fashion. And obviously as Mayor said, you need observability. You need to know exactly what the agent did at the end to end session, and this is why we launched observability. And yesterday you might hear that we also launched evaluations because you need to be able to know how well you're performing, how well your agent is performing based on your own metrics, and we'll dive in all of them around the best practices, but that's the overview of um agent core fully modular and fully managed. Wow, that's a lot. How do we get started? Well, at AWS we always say that we start from the business problem, so we work backwards from the problem, and I'm gonna add here that we should start small. We want to define the use case. We want to define what the what the agent should and should not do. We want to define like how the agent should perform the tones and everything. And which kind of APIs the agent can access, but we want to start small. We want to create a proof of concept. We want to fail fast. We want to validate what works and what doesn't work so that we can create the best possible agents. I want to say here that starting small does not mean thinking small. So you think big. You prototype and you interact on this agent to create your applications to solve your business problem. OK, so let's ground ourselves with one very common use case, um, that of financial analyst assistant, and I'm sure many of you are building similar cases and let's imagine three personas which we have all of them in the room. Um, John is the developer that wants to build that assistant that pulls in data from internal external data sources, does analysis, and returns back reports so that the analyst can leverage and. Uh, um, do whatever they need to do, and Anna is the user that is actually gonna be consuming that, um, agent, and what she really cares is that that agent works perfectly. That agent remembers her preferences, and also that agent that doesn't leak any information between the user sessions that she she might have versus other users might have. And then you also have Michael, the AI platform leader that works with John to establish the um mechanism and the processes that are being used to build and uh scale agents in the across the organization and. Michael really cares about establishing security policies, approving certain tools, and being able to centrally monitor and understand what those agents are doing. So there's 3 people, 3 problems, 1 use case. So where do you start? OK, let's start from the business problem, right? Let's talk with Ana. So Anna is the user of the application. She knows what she wants to do and she also knows what the agent should not do. So now Anna is a financial analyst. She's gonna ask a financial. What is the biggest market of Q3 in? The Asian should answer this question, and it should answer this question with the UK was the biggest market and in the background, the Asian should go to the live systems and get the data to reply to the question. However, we are. Like we are defining the scope of this agent and we are defining also what the agent should not do. So we are gonna ask Anna what should be out of scope, for instance, how much is my colleague earning that is completely out of scope. The agent should say I. I'm not here to reply to this question. Like I am here to help you with financial questions, right? So we need to set the guardrails for the agent for keeping it on the scope and to do that we also should define those queries beforehand. Rule number 2. Set observability from the beginning. I know you think observability is something that it's production close to production, but unless you said at the beginning, you won't know what your agents are doing. And as you set your observability, there's a few recommendations that we have for you. Always use open telemetry compatible traces. This, uh, all agent core services are emitting logs, uh, in open telemetric, uh, compatible way so that you can understand what agent did, for example, what were the models, what model was used, how, um, what APIs were invoked, what, uh, how many tokens were emitted, and. So that you understand at each session level we also recommend to enable trace um level debugging so that you know at that particular session what did the um user say, how does the reasoning steps, what exactly API calls were used so that you can always be able to remediate if something was wrong. And do use dashboards because we on the agent core site we offer a lot of um GAI observability dashboards for agent core specifically but you're free to use any of your uh favorite um observability stacks if you're using Dyna trace uh uh data dog if you're using line fuse lines you're fully um allowed to and permitted using open telemetric traces to leverage that observability stack. You're not tied to agent core. Observability is crucial throughout the personas of our Asiantic development. Anna wants to know that what she is getting as information is correct, that the right systems were accessed, that the right information is returned. John wants to debug his agents. He wants to understand when things are going wrong, where. Is the issue. He wants to be able to change things and to fix problems, so he wants to use observability for debugging. Well, Michael has the bigger organization picture. He's on the platform team. He wants to know the use case. He wants to know like costs. He wants to know what is working, what's not working. How much each team is spending, so he wants to have the bigger picture. Observability goes throughout the different personas, and if you want to be successful with your use case, you also need to plan for the observability throughout the different personas to have the right data available for the right tasks. Rule number 3, be prepared to expose tools and create tools as well as expose internal APIs to your agents. Um, as Phil will tell you in a bit, um, context is king, and those tools can allow you to get the right context at the right time for your agent, can give you accurate data, but as you do that, remember a few things. Ensure you use clear descriptions and, um, specify the parameters that are required for each one of those tools. Might find it funny, but, um, the amount of issues that we have been hearing customers face because of not writing explicitly what is expected on that particular tool, for example, if an Jason schema is expected in a specific way or a date format is expected in a specific way, provided, uh, if you, it will help reduce ambiguity of the, uh, for the agent and it will help reduce the tokens that are utilized and the amount of reasoning steps that are needed. Also, uh, set error handling, uh, guidelines. Make sure you have some retry logic, some tool fallbacks, as well as have proper user notification if things fail. And do you reuse MCP servers, for example, there's in the ecosystem so many from Slack, Jira, GitHub leverage them, but not only the external ones, but also internal ones. There are so many I'm sure parts of the organization that are building MCP servers for specific tools, and you can definitely reuse them, create a wiki, leverage the platform team to expose that wiki, and create examples to showcase how exactly to leverage that particular MCP server. Unless you collaborate and uh serve those assets, then innovation will slow down. We highly recommend you reuse those assets. Now let's go towards our first demo. In the first demo we'll showcase how John the developer can leverage multiple uh uh frameworks, strands, and line graph, and we'll show you how you can expose a gateway with various tools so that um that financial assistant agent can read from those tools, get some insight, create analysis that in code that can execute in code interpreter, and then everything will be flowing into observability. And one thing I forgot to mention. Whatever comes from the gateway will have some identity and force controls so that the end user will only be seeing the data that they are allowed to see from those tools that they are invoking. So this is real life. John has worked with different teams. He saw that the gateway has already what he needs. Sorry, so he saw that the gateway already have all the information that. I just Already has all the information that he needs so he can reuse this information so he is connected with a cognito authentication. He already has the permissions that he needs to use this gateway. It is connected with a target lambda function that has all the functionality already implemented, so John can focus on testing strands versus lane graph. He will import the required libraries for code interpretation for the. He will reuse system prompts. He will implement his own tools as well, so he will extend the functionality from the gateway with some local tools, and he will just start testing. So he will test. Strands he will see how the agent is behaving there. He can ask questions like what was the Q4 revenue. He can get this from his local tools using code interpretation. He can get all those aggregations. He can also ask questions from the gateway. So what is the budget for a certain project? This is all available in life information that is connected in the gateway. He compares that with link chain, gets exactly. The same response because it is using the same prompt. It is using the same tools. He implemented the same tools and because John created observability from day one, he can see how the frameworks are behaving. So now the choice of framework is his own. The choice of framework here is based on his developer experience. So he decides to go with strands because he got all the information required to choose his framework. Awesome. So let's recap what we just saw. We saw an agent build in strikes fully locally from John's laptop and John added a code interpreter to do data analysis, a gateway to expose a variety of tools with an identity, a primitive to enforce who has access to those tools and. Nothing yet has been and observe there is also observability, but nothing yet has been deployed and is ready to scale, and I'm sure John will will get there. But the point we want to make here is that every bit of agent core is fully module. Whatever use case you have, start with the modules that really make sense for you, and then you can expand or reduce however you actually need. Nothing is truly tied together. It's a nice integrated experience, but you're not forced to use everything. OK, so now how can John start interacting with his agent to improve it, right? He needs to evaluate. He cannot improve his agent blindly. He cannot improve his agent with some queries that he's thinking. He needs to have evaluation, automated evaluation. He needs to start getting ground truth. He needs to start getting all the queries that Ona expects him to have on his agent. Automated on his evaluation, he needs to set all those metrics, gather diverse sets of examples, think about how to say the same thing in 234 different ways, and consider the metrics. So the technical metrics are great latency, accuracy. How correct an answer is, all of it is great, and John is going to use that, but the business metrics are the ones that will give Anna the trust of the agent, that will give Anna the certainty that this agent is solving her business problem. So John will work with Ana here as well to create these metrics. So agent development is an interactive process and evaluations follow the same features requirements type of process where one needs to understand what is expected and what is not expected and that interaction behind me between Anna and John shows how a traditional feature. Requirement is happening but for the evaluation space for example Ana is stating that if she's asking specifically about Germany she shouldn't be given any information about France and John understands that needs to evaluate accuracy or needs to evaluate if there is citations given uh back from the agent and Ana also says I want. A response that is fast. I want a response that uh is not only good but also fast. So John decides to also establish a latency and cost as mechanisms to evaluate that particular response and um. The conversation is exactly why we built its evaluations. You define what good is, for example, accuracy, citations, latency, cost, and we continuously score in real time those respo uh, the, the agent interactions so that you know how well your agent is doing relative to those, um, metrics. And as you do that, be ready to swap your models, be ready to add more tools, be ready to edit your prompts. And be ready to Use multi-agent architectures, so. Agents behave pretty much like us. We have specialists. We have supervisors. It is a very similar organization as we have in our enterprise. As your solution scales, it becomes harder to handle all of this information for the agents. So we need to define clear roles and responsibilities for this agent. We need to orchestrate between the different agents. In the correct manner you can have sequential types of orchestrations when there are dependencies between the work that each agent needs to do. But sometimes things are independent, so you can optimize your latency with parallel types of orchestration. All of this needs to have the context and memory in mind. You need to know what each agent knows in order to architect good applications. And of course I'm going to say that many times today, you cannot have a good application without observability, without monitoring everything that is happening in the background. And let's look for example at John's uh financial analyst assistant. We started with a few tools and now we have 10 tools to read Excel files, generate growth trajectories, calculate statistics, create visualizations, and that's. Natural, uh, the more you want the agent to do, the more tools you need to expose to them. But as you do that, first, the code gets a lot more complicated, which means that you have to have a lot more error handling scenarios. You have to have, uh, cater for a lot of AIDS cases. You need to establish a lot more conditional logic, and obviously the code becomes a lot harder to debug and understand eventually. And as you expose more tools, as is natural, agent has more options or more, more areas to more reasons to be wrong and it's not just the tools that can be wrong can mix the parameters that they use for a different tool or it can mix the sequencing of the tools that they're, um, the agent is using and because of that, the agent becomes a lot slower and more expensive. All those descriptions and tools have to be mounted into um the context so you use a lot more tokens and because you use more tokens you need the agent will do a lot more reasoning steps which will take a lot more time so that's very natural and not a failure at all but just a signal that you need to start thinking about breaking your agent in multiple. Agent, uh, different agents. So for example, in, um, John's financial analyst assistant we can think of three agents that might be required one that does the data retrieval, one that does the analysis, and one that does the reporting. And all of a sudden we can start optimizing for a lot more focused. Objectives which will help simplify the code will help make the agent a lot more accurate just because the objective is just 1, not 10, and will also make it a lot faster and easier to run. Of course that is only if you do it at scale. If you have 23 tools and you break them up, you probably have the overhead of orchestrating, but if you have tens of tools and you break them up in. The right agent, then you start getting a lot of meaningful speed and accuracy improvements. Let's take a break here to talk about multi-agent collaboration protocols versus patterns we hear a lot about H2A right now, which is agent to agent communication. H2 is a protocol, so is, so is HP. It defines how agents talk between each other, so it cares about the message form. The API formats like the agent card, all of it that makes each way great. Well, when we are talking about multi-agent collaboration, we also have to think about the patterns of the collaboration. So how is the information going to be handled between the different agents. So the pattern defines how the agents organize this communication. So it's about. The organization is about having sequential types of patterns, hierarchical, peer to peer, so like how your agents are organizing so it cares about the workflow and the coordination parts of it. So it's always good to remind that you can do H2A with sequential pattern. You can do that with a hierarchical with peer to peer. It's about how you are implementing those things. Rule number 6, scale your agents to multiple users, uh, in a fully secure fashion and also in a personalized manner. Um, obviously local development can take you that certain to a certain extent, but if you want to really go to many users, you need to think about a lot of requirements. For example, always isolate context and sessions. For example, when Ana is interacting with that with an agent session, she doesn't want the information to leak to a different user with at a different session. And for this reason within agent or run time we um offer what we call full isolation and we dedicated a micro VM at every session not only to maintain persistence but also to ensure that whenever that session ends we completely clean up the CPU we completely clean up the memory and clean up the file system so each end user gets their dedicated micro VM for full security and. There is no search date, no data leaks. Secondly, make sure you set up user specific memory and especially short and long term memory where the sessions with. That you execute can remember the agents that you execute can remember what happened across session, what happened session in many ways. For example, what facts that user wants, what preferences they have, what was exactly the topic of the conversation from previous, um, uh, memories, and you can name space them using user IDs so each user can get their own dedicated memory that nobody else can access. And third, make sure you have policies on top of the tools and um and access towards agents and uh and and and and the gateway. So for example you can leverage your workforce credentials like Octa tra ID to enforce access right to the to to the agents, but you can also establish using policies on who can do use whichever tool under which conditions. So policies come very handy for that and um lastly make sure you um host your agents and your tools separately. So run time can host both tools and uh and agents and we recommend our customers to leverage for each agent or part of the agenttic system split it up in different agents and tools and host them separately because that way you have a lot more um uh visibility into what part this particular agent did as well as you can reuse those agents across many agenttic solutions. For example, in the next, um, uh, in the next demo we'll showcase how we use the same, um, agent but split up into 21 is gonna be a PM agent. The other one is gonna be financial agent. It's gonna be an agenttic system fully hosted on runtime again leveraging code interpreter to execute analysis, but after it has leveraged, um, gateway and proper identity controls to fetch the data and. At the same time we'll showcase how you can use memory so that the agent can remember for that particular user what happened in previous session and what they they they they have served before. OK, so let's look at how John's codes evolved. So he is now importing the necessary packages to handle memory and to deploy that in real time. So he got his Bedrock agent card application. He is changing his tools to become agents. So now those agents will be his tools. He's doing multi-agent collaboration. He's setting the entry point for his runtime so that he can host this agent. He's handling authorization from Cognito here as well and passing that to his memory so that now his agent can handle like different memories for each customer. You see, on runtime he can have different versions. He can connect his identity to it. On memory he can have different types of strategy as well. In AI he can see this being exposed to different users, so I'm creating my own user here, asking my questions, having my responses to my agent, and getting also like the visibility of the tool. So I get my observability here in real life you check this observability as necessary. John can also observe everything on. Uh, Agent car observability together with Michael, so all the visibility is there, but uh like it is out of the box for for them, right? You have token latency all of this, but now Costy also wants to use the same agent so Costi is just gonna log in here. And now it is a Costis session, so the agent will behave for Costi now and it will start creating the memory for Costis. So Costi can ask his questions as a product manager, can get his response, can use this agent for his own work. So now you are getting this hyper personalization as well. So just taking a look at what we did here, right, like we are deploying this agent at run time. We are still experimenting, so we did this uh deployment of all the agents in the same runtime we would expose them to different runtime as we move forward as we get more trust in our agents. We are adding this memory and we are connecting also the user to the memory. Uh, we are getting everything we already had before, so we continue having the gateway. We continue having the gateway connected to identity for getting the identity of the tools, but now we are also connecting this identity to runtime so that you can also get the identity of who is accessing the agent. And yeah, like observability now becomes across all of those different services, out of the box. So you didn't have to do anything extra for the observability, it is there for you. Rule number 7 Do use code whenever possible. I know agents are exciting, but, uh, agents are not a hammer. Uh, models are non-deterministic. Agents are completely stochastic, and of course user behaviors change all the time. So, uh, whenever you need to do calculations, you need to do forecasting, you need to do, uh, um, anything that is very determining, don't rely on your, uh, on the LLM and the agent to perform. So reserve the agent to do the reasoning tasks and keep Uh, functions within as as tools or as external internal tools or external tools that the agent can use, for example, calculations, validations, or like rules based logic. There are great things that you can completely make create a code that is fully deterministic and run super fast and cheaply, and you should leverage that and use agent to orchestrate the. What exactly tool or what exactly function is used on top of that and you should always measure cost versus value for example, if you know that there's um it's just cheaper and faster to use code to perform an action, just build the code and have the agent orchestrate on top of the logic. If it doesn't, then use the agent so it's, to be honest, one of the most common mistakes that we see customers they just. allow the agent to do pretty much everything which allows of course a lot of new LLMs allow for calculations and all that, but still you cannot always rely that they will do the calculation correctly. So do expose that calculation to your agent. Just, oh sorry, just bringing a classical example here, uh, that I've seen with many customers, which is the current date, right? Like current data is even some native tools for framework. But it is a classical thing that you can just pass as an attribute to your context. It is super easy to calculate and it avoids one orchestration loop. So now you are using less and you are passing this information to your agent. So the agent can still reason about this date, can still know what's tomorrow, what's the day of tomorrow, but you don't have to call a tool to get this information. Other things that you can do with code instead of having the orchestration, authentication of users, getting user profiles, combining different tools that should always work together, you can do that in a code because that will be faster, cheaper, more deterministic. So let's use agents when agents are bringing value because agents bring a lot of value. Rule number 8 test, test, test, test, test, and never stop testing. You think that going to production is actually the end, but to be honest, we think that that's actually the beginning. Uh, as I mentioned, models are stochastic or user behaviors are changing, and we recommend that you implement continuous testing pipeline. So create a few scenarios or many tens of scenarios, for example, in the Jones case, they should, whenever they create an agent update, they should run that pipeline. Um, of testing so that it can determine if the agent version is good or not and if it's, uh, maintaining accuracy or increasing accuracy then the version should be uh pushed but if it's not, it should be pulled back in a similar fashion you should use AB testing um you should have in production end points that only few users are using so that you can understand how those agents are performing. And agent or run time does offer you with does come built in with versioning endpoint management and rollback. So whenever you see and the beta is performing well, we can swap up swap towards it. But if it's uh not performing well, we can bring out all the traffic to to the alpha. And same thing you should do in real time you should always monitor for drift using evaluations if there is any degradation of performance for your important metrics, you should bring roll back to the previous. You you can allow to get alerted or you can roll back to a previous version so that you always have the right experience for your end users. And here's the last demo in architecture that we're gonna cover. Um, here you'll see how easy it is to add evaluations into an existing, uh, architecture that we have already covered. Uh, it's you define what is good you define your metrics and as your those metrics are coming in along with your observability for example, in observability you have operational metrics now you'll start seeing performance metrics for those agent interactions. OK, so here is how it looks in real life. So you set your evaluation configuration. I'm gonna pass some metrics here like built-in metrics from agent car evaluations, and then that's it. Like you just continue to invoke your agent and that will automatically create those dashboards for you so you can see it over time, you can see the detailed overview for each. Like each trace of your agent with explanation for your metrics as well and uh you can see like all of those different metrics in the trace in like the session when you're thinking about a goal success and also in your parameters. So inside of the trace going like to the two invocation looking at like 2 selection 2 parameter selection accuracy you can see that. Out of the box for you and you can also create your own evaluation metrics if you would like using our custom uh evaluation metrics. So a lot of simplification of the work to have an evaluation running uh in production for you. So here we have 7 of the 9 primitives that we offer and it's a core all working together in a fully modular fashion and uh what we hope you uh saw is that the agent developer just built the logic and they didn't have to build agentic infrastructure to host those agents. They didn't have to think about authentication flows. They didn't have to have build monitoring pipelines. Everything is there for them and that's what we consider also what production looks like. And that takes us to the last, uh, rule scale via continuous update and organization setups. For example, make sure you consolidate the responses from, uh, um, the models so that you can understand and flag what is good, what is not. You can also get CISA scores so that you can very quickly understand the satisfaction with thumbs up, thumbs down of the end user. Use that as a way to test your agents and. They'll definitely consider if you don't already have to create a platform team or leverage their existing platform team that's your mechanism to ensure that your organization has reusable assets as a mechanism that is leveraging the common approved technologies to push code and um um deploy code and make sure you foster collaboration across the team. At the end of the day everyone is is excited to build with agents, but unless we all collaborate, we won't innovate as fast as we should. So let's just look how an AI like Asiante AI organization set up can look like, right? You start having different use case teams. Those use case teams, they will innovate, they will create different use cases for their business. They will try different functionalities, but they will also use reuse different functionalities from the platform team. So the platform team is the place where you are going to get everything centralized, so you get like different tools exposed, different agents. From the use case teams you also do the governance of everything so you can see how the agents are behaving. You can see where the bigger costs are, the bigger challenges, where the customer is happy, where accuracy is working and. I like to say that Agentec AI is the next step of software engineering, so you have all the dev ops capabilities still there. They are still very, very important. So you need to set the dev ops. Pipelines. So you still have pipelines. You're going to have version control as Cassie was mentioning. You're going to roll back when things go wrong, but you have to have all your software engineering still there with the extra step of Asiante AI. So when we look at a platform and different teams collaborating, now you are going to have the capabilities that are created by the platform. Those are like the organizations that go across the enterprise that the platform wants to work and invest on, and you also have the different teams, the agents that they are creating. That they are exposing to the organization. If you look at a budget team, the budget team will create the best agent for doing budget analysis because they have the domain knowledge. They are the experts on budget analysis. They should also be the ones creating the agents to interact with their data, but this is such an important thing to do that. Teams in the organization want to reuse their capabilities, so that's where the platform also comes into play. So Michael is your best friend here, right? Like you want to be close to the platform team and to work with them. So John is going to work with Michael now to expose the agent via the platform so that the next team can use this agent. Now what we discussed today, um, in a nutshell, these are the 9 things we have been hearing from our customers. We see them struggling and I have a quick, uh, um, quick recap. First and foremost, start small, define one use case, define what is good, build it, learn from it, scale it to 5, 10 users, then move it on to a larger audience. Always use observability. Make sure you understand using open telemetry compatible, uh, logs. Understand what is happening so that you can depart and improve and expose. Plan to expose tools to and APIs to your agents. Make sure that you are clearing your descriptions. Make sure you're, um, uh, avoid any ambiguity and reuse MCP servers. Split your agents into multi-agenttic architectures so that you can focus on making those subcomponents really good at what they are supposed to do and when you scale to multiple users so you isolate contexts and ensure that you use uh user specific memories and always use identity to enforce who has access to what. And use code whenever possible. Use the deterministic, always good fast code whenever you need to perform calculations and you know what exactly is expecting from the code and test, test, test again. This is where you can leverage evaluations. You can have your own test rate so that you can always know what is happening. And lastly, make sure you keep all the interactions, the satisfaction scores, so that you can improve your agents over time and do leverage your platform. These these are your friends that can help you scale organization and innovation. And last but not least, don't start with flying. Crawl, walk, run. We're super excited about agents. We're very thankful you're here. We're very thankful you're you for whomever is using Agent Corp, and we're also very excited to be hosting Phil Norton from Cya Water that can share a lot of lessons from, uh, Clearwater Analytics. Thank you. OK. Alright, thanks Costi and Myra. Uh, alright, so my name is Phil Norton. I'm from Clearwater Analytics, uh, and I'm here to talk about our agent Core use case. Uh, so real quick, Clearwater Analytics, uh, so we are a public fintech company. Uh, we provide financial accounting and reporting for institutional investors, uh, and we have a very large SAS platform, uh, that we've built on AWS. Uh, so yeah, Phil Norton, I'm a, I'm a software dev manager. My illustrator here is GPT 5.1 because he also likes Art Deco. Uh, so let's get going. Let's, uh, let's see what we, uh, see how we're using this. All right, so from the beginning, let's start, let's start from the beginning. This is our journey, uh, in the, uh, agentic, uh, AI, uh, space. We were early adopters, right? Um, June 2003 is when we, we first got together as a group, right, when people started seeing how, you know, GPT 3.5 came out or 3 maybe maybe it's 3.0, uh, and, uh, and, and everyone started getting excited, so we, we got the group together. Uh, so first thing we did, right, sort of a standard stack link chain rag data access, uh, and app awareness, um, we, uh, had our first production agent internally in July of that year, and externally we got them to clients, uh, in December. So let's go through our use cases. Um, we kind of started from the beginning. We started, uh, you know, crawling. We started with internal knowledge bases and standard operating procedures. Uh, this is really the sweet spot, uh, of Gen AI. It does a really good job with, uh, with RAG and explaining things. So then we built, uh, a chat agent, uh, that, that works inside our application. Uh, then we started adding on Salesforce ticket support, right? So assisting client services with handling some complex, uh, financial use cases, uh, questions coming from clients. Uh, and then we started getting into the data. Um, this is a really big use case for us accounting data analysis, anomaly detection, visualization. It's a big use case for our clients. Um, they all have different requirements, uh, for looking into the data, finding problems, cleaning the data, getting their books in order, uh, and that's a really big use case for us, um, so, but we don't just do that, you know, by hand, uh, we don't just tell the agent what to do, we schedule it, we automate, uh, a lot of those things, um. And uh and finally, um. We of course like everybody else, uh, we're using automated coding agents and uh code and, and we have code review agents. So in addition to, you know, interactive coding, uh, we can also put in Jira tickets, uh, and have the agents, uh, pick up and write code for us and, and then review the code. Uh, and finally, one of the more complex, uh, workflows is financial data intake. Uh, a lot of data providers, uh, uh, provide data in PDF, uh, and so we have to use, uh, more advanced techniques to get that data, uh, into, uh, JSON format so we can put into our accounting engines. Uh, all right, so let's talk about our Gen AI apps. This is the part where I get to brag about all the cool stuff that teams have built over the last couple of years. Uh, all right, so our brand name is Quick, um. And uh first up is quick chat. So this is a this is a web component that we can embed in our production applications. Uh, we've got it in 5 applications for now. This is the original uh flagship uh reporting engine, um, and we've got 55 more instances and we're building more every, um, every month. Uh, this, this is aware, uh, the, the agent is aware of, of the report that we're on, the account that we're loaded into, things like that. You can also switch which agent you're chatting with. Uh, next up, uh, we can create our own agents. Uh, this is how we've built 800 agents, um, and this is available to our clients. It's no code, uh, uh, you know, you just, you just fill out, write your prompts, give it tools, give it knowledge, uh, and you can create your own specialized agent. Uh, next up is quick flow. This is scheduling and triggering, um, uh, either single or multi-agent collaboration, uh, so you can put together your prompts, uh, kick it off like Monday, every Monday at 8 a.m. and get the results in your inbox. You can go back and you can see what the, uh, what the agents have done. Uh, and, uh, quick orchestrator, this is the killer app. Uh, this is kind of like Lane chain. Uh, you can, uh, plug together, uh, different agents. It's multi, multi-agents, um. Uh, work flows, uh, and you can see there's this, uh, it's a nice visual interface, and you can see even as it's executing which step it's on, which decision it made, it's kind of like you can build a flow chart, uh, that a human can follow. You build a flow chart here, now the AI follows it, right? It, uh, it's a very natural progression for, uh, for, uh, financial people. OK, so, uh, I'm actually going to, uh, let's see, yeah, let's let's let's let's, let's talk about to get to the next level. So Um When we were looking at getting to the next level, let's talk about what we needed. So we need scalability, right? We need, we need zero downtime deploys. We're getting into like 4 hour workloads. We have very large reports that take a long time to generate, uh, and then some more processing on top of that. So when we do that, we, we wanna make sure that we're not, um, you know, having pods drop out, uh, and, and kill, kill the run time, uh, in the middle of it. Uh, we built 500 tools before MCP was a thing, right? So these, uh, these had been put together in a single, uh, a single fast API server. So we wanna move those, uh, we wanted to move those to MCP servers. Uh, we wanted to maintain rapid follow-ups, right? So maintain context when talking to data, uh, so that you can, uh, interactively ask more questions without having to go fetch all that data. Uh, and of course, uh, any platform we wanted to choose has to support backwards compatibility, right? We've got, uh, a very mature system. We've got custom, uh, role-based access control that we wanted to keep. Uh, we wanna make sure that we can maintain our behavior. We have a homegrown agent. Uh, we've moved off of Lane chain into our own thing. Uh, we want a connectivity to the rest of our AWS stack. We use a lot of different features of AWS. Uh, and of course we, we needed access to all the, uh, all the major models, uh, that we, that we use, uh, light LLM for. So all the state of the art models we wanna make sure we keep those. Uh, and, and we would like, we wanted to migrate to a better rack solution. Uh, finally, uh, we, uh, have our own observability solution, um, and so we wanted to make sure that we could, we can maintain that. So let's take a look at where we were before, uh, pretty standard stack. Uh, we made a lot of use of EKS, um, but we had a lot of shared workloads, uh, so you know, for one thing we've got, uh, agent runtime pods, we've got data processing agent like a sub agent, uh, we've got pods running tools. So of course I talked about the zero downtime deploys when these agents run for a long time. Uh, we deploy all the time, all day, every day. We don't just want to wait till it's safe to deploy, we just go. Uh, challenge number 2 is noisy neighbors. Um, if you work with Python, async IO, uh, if somebody does a synchronous, uh, call, um, in a shared workload, uh, your noisy neighbor is gonna screw up, uh, your, your, uh, your agent run. So that was another problem that we had. Uh, same thing with memory. Somebody sometimes, uh, tools can be sloppy with memory and, and take out other workloads. Uh, after the move to Agent Core, here's what things look like. Uh, so for one thing, uh, we're able to get rid of that SQS layer, um, with the direct as sync invoke, uh, so you know, fewer moving parts, fewer things to break. Uh, we make use of Agent Core runtime, uh, because that's a very natural progression from moving from Kubernetes. We just move our pod over and it runs. Uh, and we have our sub agent that runs, uh, in, in runtime, and we have MCP servers, and again, uh, you know, uh, what, what's nice about this technology is that everything runs in isolate in isolation, right? We have no more noisy neighbors, uh, and, um, so maybe some people are wondering about startup times. Uh, I can tell you it's, it's negligible. Don't worry about it. OK, so why do we choose Agent Corp, right? Like I said, 0 downtime deployment, this micro VM perfect solution. Flexibility in tech stack. Like I said, we have all this stuff that we've built, we just wanna bring it right over without having to rewrite everything. Sticky sessions, uh, our data processing agent, again, we can just keep that session there and you can, uh, have repeat questions. Uh, we, uh, we wanted to be able to accelerate data access tooling, right? We wanted an easy way to create MCP servers so that all the different dev teams could own their own MCP server. Uh, and memory, uh, we haven't really built memory quite yet, uh, but it is coming very soon. The other thing I'd like to mention is we do, uh, recently came up with a, a nice use case for, uh, for making use of Agent Core browser. So, and the real big selling point is this is undifferentiated heavy lifting, right? So we can manage all this ourselves, we're smart, we're good engineers, but Agent Core just made it so much easier to do. OK, so, um, I figured I'd follow up with some of the best practices that we've figured out, uh, along our journey, uh, so that hopefully I can share those and keep you all on a happy path. So first of all, context is king. I'm sure you've heard this before. This just don't forget this. This is a really big thing. I found that I LLMs are highly reliable if they've given, if they have unambiguous context. Uh, so a rule of thumb for context is like when I see these big prompts and I read through them, if I can't understand this by glancing at it, if I can't read through it and say, well, what's the ask? What are you asking the agent to do? If I don't understand, the LLM will not understand either. So that's really your first, your first test. Uh, and I, and I would say the hallucinations are pretty much non-existent if you have proper context. But That's what you can control. Your users are going to throw the kitchen sink, all sorts of weird stuff at your agents, so you don't really have control over that. And LLMs will always do their best, but again, poor context means they might have hallucinations, so how do you manage that? So there's really two main ways that we interact with these agents, right? Chat and automated workflows. In chat, you want to instruct the LLM, ask for clarification. So if somebody asks something that's really unclear, then the LLM says, I'm sorry, I, I'm not sure what you mean. Did you mean this or did you mean that? So you wanna give it an out for that. For automated workflows, you don't have that interactivity, but what you can do. Is you can have, you can have the agent output some metadata about, you know, what it came up with while it was processing when when we use like a confidence and a rationale. So if the uh the agent comes back and says well my confidence is only 5, I'm not sure about this because you know some of the fields had different names. I wasn't really sure what to do well then when that goes back in your automated system, then you should flag those low confidence answers, follow up, figure out what was wrong, uh, and deal with it there. Um OK, so let's talk about rollout and best practices because we're building these these work flows for people to use. Uh, first of all, this is a culture change problem, right? We're asking people to change the way they work and people don't like change. So you have to identify what's in it for me, right? What are their pay points? How's your life gonna be improved? And then training, training, and more training. You need to, you know, schedule time with your users, show them how to use the tools, show them what it can do for them. Uh, second, build narrow use cases. So Gen AI is a general purpose solution. You can do pretty much anything with it, right? But that's really hard to understand. Um, so what you wanna do is you wanna focus on very specific tasks, right? So you've got this huge workflow that people do, uh, but just focus on one part of it, right? Get that automated. Probably we wanna focus on the most boring part, get that automated, makes, make their life better, and then they find the next piece and then go on to that. People can really understand that. Uh, and then finally monitor everything. Observability is key, so figure out what's broken and fix it. Figure out who are your champions, who are the people who are really, you know, gung ho about this, empower them, uh, who's on the fence? Persuade them, and who are your detractors. Listen to them. You probably can't change their mind, but at least you can understand where they're coming from, and then they'll change their own mind when they see themselves falling behind. All right, that's all I had. Costi, back to you to close it out. So much it's uh it's awesome um I just wanna say a big thank you to all of you for being here. These are the uh resources uh that you can leverage. This is the documentation code samples and we have workshops anything you need, very happy to answer your questions, post the session, but again, uh, very big thank you for being.