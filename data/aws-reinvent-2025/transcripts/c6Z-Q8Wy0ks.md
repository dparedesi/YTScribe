---
video_id: c6Z-Q8Wy0ks
video_url: https://www.youtube.com/watch?v=c6Z-Q8Wy0ks
is_generated: False
is_translatable: True
---

Everyone, thank you for coming. Good morning. You probably guessed, I'm Taz, tech leader for AWS, uh, data analytics. And I'm Shika Verma. I'm the head of product for, um, data analytics at uh AWS. Very excited to have you all here. Let's get started. Lots of content. Everyone ready? I know it's a little early in the morning, at least for me. Let's wake up. OK. So, in terms of the agenda, what we have for you today is first, uh, we will be level setting what exactly data foundation means because you ask uh the person next to you, you'll get a different definition depending on the day, on the time of the day. So we'll level set what that means. We'll in fact take it to the next level, which is Data analytics, sorry, Data foundation, what it means for data analytics and what it means for agentic AI applications. We'll talk about how you can bridge the gap between data and AI using the AI Ready Data Foundation on AWS. Examples of this will include uh AWS MCP servers followed by an in-depth look of how you can use AWS data analytics to manage context. We'll also talk about event-driven architectures where your application may have multiple agents. And then, you will bring this all together with the most critical part for data and AI, which is how do you make your data ready for AI. We'll talk about Data governance, we'll talk about metadata management, and that's essentially data readiness. So, sugar, shall we begin? That all sounds good, theoretically, but I think we need to root at an example where we can show everybody how we can actually do this live. Don't you guys think? Yeah, so we should use a demo to demonstrate what we have. Can we do that, Taz? Can we use our own data for the AI agents? Your wish is my command. I have a demo for you. We are a travel agent company. That's the premise of the demo. We'll use that to showcase how you can leverage the data foundation on AWS to build an agentic AI application and get the most out of it. All right, that sounds good, guys. Thank you. Let's do it. So let's begin by level setting on what is the data foundation. On AWS this is essentially for data analytics. This is what it means when we say data foundation, right? This is Having access to data tools that allow you to deliver on basic constructs of data analytics like data storage, data processing, data integration, data governance, and all of this with the price performance and the ability to scale as and when you need it. This is at its very core is what we mean by data foundation for data analytics. And these foundations, constructs basically, they come together to give you a typical data pipeline that you see over here with the different components coming together to give you an output that is important to your business. But what does this mean for AI? What we are trying to say is, when you are working on building AI applications, right, It doesn't actually have to mean that you have to look for something new or build something new in terms of data foundation. You should be able to leverage what you already have, and that's essentially the crux of this particular section over here. You should be able to evolve and adapt to build your agentic AI applications because data and data engineering are key AI differentiators. Even though that might be a back end aspect of urgent care applications, it is still critical. And here's that story in two pitches. Shika, what do you think? What do we think guys? Sound OK? Like, do we think data is a differentiator for AI? Just show of hands. Yeah, pretty much everybody. But do you guys ever say that data engineering is a differentiator for AI? Anybody says that? I mean, we believe you when you say data is important for AI, but let's be honest, when we talk about AI, is data processing, data storage, data governance, is that top of mind? Yeah, top of mind. Nice. We have a great crowd here, and that is essentially the point we are trying to make over here, which is data is critical for a successful AI application and needs a strong data foundation. And it is, it should be made up of data tools that allow you to extend the capabilities to build the AI application. All right, Taz, uh, all right, Sugar, shall we proceed with the next section? Yeah, I think you have a good story to tell here, and I can see that our, uh, audience is engaged. So folks, I'm gonna leave you in Professor Taz's hands for a bit, right? He's gonna show us how this is done, and he'll show us some real data with real AI agents, and then I'll come back on stage to talk about a few other things. Go for it, Taz. Thank you, Shika. OK, so really quick. An AI ready data foundation or an AWS, at least for businesses, what does that mean? Essentially what we are saying is it's a foundation that lets you fuel AI innovation, speeds up efficient delivery with little to no impact to your bottom line as a business. That is the data foundation that businesses get from AWS, but we want to focus on what do developers and builders get out of this, and that is what we'll be covering for most of this talk. And this image here is actually a testament to the AIA Data Foundation on AWS. Really quick, any guesses as to what this image is? Project Rinium. Thank you. So, yeah, this is our uh project Rainier uh uh cluster compute that was recently launched, one of the world's largest AI compute clusters. It's, uh, uses a lot of core components of the AIAD Data Foundation that we have on AWS. So, for the most part of this talk, we want to focus on how can data personalities like data engineers, data analysts, data scientists get the most benefits from an AI ready data foundation. So we are recommending two approaches essentially. The first is quite fundamental. It's actually understanding how your data foundation constructs, such as storage, processing, ingestion are influenced when you build an AI application. And the second is more, it's a more prescriptive approach about deploying the AI ready capabilities that the data Foundation has to accelerate your AI development. The first approach is best understood by looking at the evolution of AI. Now this has rapidly progressed from something simple like assistance to more complex autonomous systems. We started with Gen AI assistants that follow predefined rules to automate repetitive tasks, then quickly moved on to AI agents that are goal-oriented, capable of handling a broader range of tasks, and adopting the changing environments. And then today where we are at is we are moving towards fully autonomous multi-agent systems that can make complex decisions. The evolution of AI has a direct impact on Your typical data pipeline. They also evolve in, when used in the context of building AI applications. The very first thing One of the very first things that AI introduces uh is the need for additional data sources. And these are primarily in the form of unstructured data. Now unstructured data is not a new construct. However, it is new in the sense that it doesn't conform to a preset data model, uh, it doesn't have a predefined schema. And this is new territory for most data personalities. Functions like metadata management, data governance for unstructured data takes a whole new meaning and has a big influence on things like data processing, data integration, and data storage. The the next construct that gets influences around data processing, right? This is an area where you would start working uh on aspects of uh training or validating your data to move data from data warehouses or data lakes to improve the model accuracy or you could be running inference for your continued pre-training approach for near real world scenarios or managing vector data for a rack-based application to provide real-time context. And all this data processing requires advanced forms of data integrations and data storage. Next, some AI applications also incorporate techniques such as including human feedback to optimize theML models. These applications need to capture user input in real time for a highly personalized user experience. Capturing this information in the right kind of storage with efficient pipelines helps with latency and accuracy. And then finally, because an AI application opens itself up to a much wider variety of data sources and users, data governance becomes a function of the entire end to end pipeline. Behind the scenes, governance functions such as data privacy, data quality, data cataloging are required to deliver a comprehensive data governance for an AI application. OK, so that was the first part of how data personalities can make the most, right? Have a macro uh uh vision of what it means to the data pipeline. The next aspect is a little bit more prescriptive approach, like taking a deeper look into how to make the most of the AI ready Data Foundation. And that is about leveraging the enhanced capabilities of that foundation to drive AI developer and end user experience. And we thought it best to cover with an example agentic AI travel reservation application just like I promised Shika. All right. This is uh an airlines reservation agentic AI demo application that we built. I asked the agent to show me all available flights from JFK New York to Los Angeles, California. The agent starts thinking about it. And goes to work. For the purpose of this demo, you will see uh we are also printing diagnostic information, so that tells us. What the agent is calling, what it's doing, and for what purpose. So we give it a few seconds for the agent to do its job, and then You can see very soon that the agent will return several flight options for me, and those options will include things like flight schedules for different airlines. It will show me the ticket pricing again by each airline. Uh, in this case it was talking to a third party travel agency. It will show me user reviews from my colleagues in my organization. That have flown uh the same airline in the recent past. So like, that gives me a well-rounded, well-informed way of choosing the flight that I want to use over here. It's also telling me that based on my company travel policies, there are certain cases where I might need an exception to book that flight, right? I mean, we at Amazon have that policy. I, I'm hoping most organizations have something similar that they can relate to is when picking, making a choice that doesn't align with the company policy, you have to work through an exception process. So that's what's happening over here. And then finally, the agent is also uh making recommendations that are actually policy compliant and factors in my travel preferences. So this is the first part of the demo, right? Getting all that information. Now, we are calling this scenario one because there is a scenario two. This is where I feel I don't want to get in trouble with my manager, right? I mean, uh, I don't need to get an exception, and I go ahead and I ask the agent to book the uh American Airlines flight. Uh, I know that is compliant looking at the pricing aspect that it showed me earlier. It works with the company policy and timing wise, let's say for now it works for me as well. Again, the agent's thinking starts going to work. Um, similar to previous, uh, part, we are printing some diagnostic information that we will come back to later. The agent confirms the reservation. It uses my travel preferences such as uh my meal type, uh, the kind of seats I prefer, whether a window seat or an aisle seat, what kind of person I am, and performs a few backend operations like emailing me the e-ticket and stuff. OK. So that was the demo, and for the purpose of the rest of this talk, we will use this reference architecture of the airline's demo to help us work through what's happening behind the scenes. The user prompts, interacts with the agent, which in turn then works with a list of tools or API calls that integrate with different types of sources like the third party travel agency that makes flight data available, the user's internal corporate data warehouse and data lake which is on Amazon Redshift and S3, and a booking action with a third party. There are a few operational transactions happening behind the scenes that we will cover when we talk about multiple agents. So just for context, this is a demo that we white coded using Quiro and strand agents. Uh, people here are familiar with Quiro and strands agents? Nice. Uh, for those who aren't Quiro is AWS's AI IDE, uh, that turns prompts into specs and then into code. It's actually pretty cool. And then Strans Agents is an open source SDK that, um, takes a model driven approach to building AI agents. Strands supports all LLMs. Um, I suspect most of us have our favorite model. Mine is a cloud Sonet, and that is what we are using over here. There you go. All right. Before we jump into the details of the application, I want to take a slight detour over here to position what is generally called an agentic loop or a react loop. In terms of the transaction workflow between the agent and the tool and the model or the LLM, 1 may think that the flow is more or less linear. As in, the uh the user prompt invokes the agent. Which in turn executes the tool API. And then returns the results to the agent and then returns the final response to the prompt. Well, not, not really. At the center of an agentic AI application is a loop. When the user instructs the system to complete a task or a transaction, the workflow enters an event loop where it iterates until it considers the task completed or the question answered. This design pattern is typically called the reason plus act or the react loop and is the most popular design pattern for agentic AI applications. This is how we are illustrating it. This basic architecture allows an agentic AI application to accomplish tasks over the course of multiple event loop iterations. Now, while on one hand, this event loop means a more meaningful and relevant response to the user, however, on the other hand, it also means a slower and less efficient agent. It also means an unpredictable number of model invocations consuming an uncontrolled amount of input and output tokens. That impact both cost and performance. A good data foundation can help with minimizing that impact, starting with the AWS Data and Analytics MCP Service. Coming back to the reference architecture of our agent demo, let's look at the tools that we are using. These are either hardcoded API calls or function calls that you can pass as tools to the agent to involve. What this means is that the agent's capabilities are fixed at design time. It doesn't really have a choice or the ability to reason or plan which tool to use to call depending on the task. You're limiting its capabilities, and all the queries that it is using also need to be predefined for the usage over here. This is where MCP servers bring value. You let the model figure out what it needs through natural language. It also replaces the heavy lifting of managing multiple API SDKs and formats, which I, I can imagine is a production nightmare to most. MCPs enable true agent autonomy, the ability to iteratively call tools, get results, and decide the next steps in a react loop. Just a little bit of overdue on MCP servers. They don't really replace APIs. They add a conversational layer to the LLM. They act as a universal language, eliminating the need for custom integration layers between different AI models and tools. Too much structured data with traditional API calls tends to confuse the agent. I do want to point out over here that MCP servers are not always a one size fits all. There are operations that are better served through traditional API calls. For example, the MCP-driven agents struggle with large scale data processing with structured data. And complex data transformations even such as the the click get flight details function that you saw earlier over there we did not convert that into MCP. We didn't have an MCP server for it, but regardless, a direct API call serves a better purpose for those kinds of transactions. Similarly, operations that require deterministic and uh time sensitive outcomes like the booking a reservation, right, that function call, that is also a better fit for a direct API call instead of an MCP server call. It's very easy for these operations, the kinds that we just discussed, to exceed the LLM's context windows and drive up costs and latency. In our demo, we used the Redshift and the SV table's MCP servers. The Redshift MCP server acts as a bridge between your Redshift clients sitting on the agent and your Amazon Redshift infrastructure. Uh, it does, uh, natural language transformations and among, uh, among other things, it automatically discovers your redshift clusters whether it's provisioned or serverless, can work across multiple clusters, browse databases, schemas, tables, and columns through natural language, and execute SQL queries in a read-only mode which provides you with built-in safety protections. Likewise, with the Amazon S3 table's MCP server, you can interact with tabular data sets using natural language. You can list and create S3 table buckets, name spaces, and tables. You can also query and commit data to an S3 table through this MCP server. The AWS MCP Server repository is a suite of specialized MCP servers, and we invite you to check out all the publicly available MCP servers that can help you make your agentic AI applications more autonomous and less expensive. Moving on from NCP servers to context management, another area where uh data personalities like data engineers, data analysts, data scientists can leverage the AI ready Data Foundation on AWS. We touched a little bit on context when we discussed the react agentic loop. A simple way to think about context here is to think of it as the working memory of the model of the LLM, the memory that lets it interpret, reason, and then take action for the best outcome possible. Let's look at that in a little bit more detail. So while AI agents get sophisticated reasoning capabilities from models and LLMs, they face two key limitations. One, LLMs are stateless and unable to remember past interactions or maintain context. And two, the finite or limited context window that LLMs have restricts the amount of data that they can hold and use it to process at each step. Now, the kind of data that the model typically keeps within the context window involves three core areas. First is the instruction set, right? Uh, instructions such as the user and system prompts, the state of the conversation session that you're having, conversation history, um, learned lessons, user profiles, and so on. So that's the kind of data and information that sits in that instruction set. The second area is the knowledge base. So knowledge bases are, again, think of it like centralized repositories of structured and unstructured data. This is information that serves as grounding data for AI. They can include things like documents, FAQs, product specifications, uh, best practices from your organization. In our demo, we are using the company travel policy documents as the knowledge base and we'll look at it in a little bit more deeper in this very quickly. This is where the agent can tell me which tickets are within the company travel policy and which are not, and then we have data coming from the tools or the MCP servers that we just saw in this demo we bring the airline reviews from S3 tables. These are reviews that by my colleagues that were posted uh into that knowledge base that we use to make decisions. So the model needs to work with a lot of current and fresh information within its limited context window to be able to advise me on the right course of action. This is where context management or context engineering comes into play. This working memory as defined by a context window is the length of the maximum amount of input data it can hold or consider or remember at any point in time. And here's the kicker for that. Context windows are finite and tokens are expensive. We saw earlier how MCP servers are designed to be more context efficient and less verbose than traditional APIs. So next we will look at how best to manage the limited and expensive context window with the help of agentic memory and vector stores. All right, so let's quickly refresh our memory from the parts of the demo where the agent is retrieving and memorizing several data points including flight availability information, my booking reservation, and my travel preferences. So this is the agentic memory in play in action over here. Here for this function what we are doing is we're using the Amazon Bedrock agent core to serve the purpose of agentic memory. Agent Core, uh, some of you might know this, it's a modular service, uh, that provides you everything you need to go with agents up to production, including observability. In our example, we are using it to deliver personalized experiences with persistent memory stored in this particular service. It supports most of the popular frameworks and protocols like the strands agents and the MCP servers that we saw earlier that we are using for our demo. And then depending on the nature of your uh agentic application and the business case, if you're a shop that needs an over the shelf or off the shelf kind of solution like Bedrock Agent Core is a good choice over there, or if you're an organization that. To build something custom, something more specific to your needs, AWS has several other options that agents can use to build their agentic memory. Uh, these include things like Elastic cash for Valy, uh, and in-memory high performance key value data store. Uh, we also have uh Dynamo DB, a distributed NoSQL database, Neptune Analytics for knowledge graph-based dynamic relationships, and several others. So the agent can retain knowledge to learn and problem solve. Uh, it can deliver contextual intelligence through pattern recognition and personalized interactions by remembering preferences and past conversations. The next important piece of context management is knowledge bases, right? Uh, this is where unstructured data comes into the, into the front. In our demo we are using the company's travel policy documents such as PDF, uh, HTML files scraped through to build our knowledge base on Amazon S3, and then we are using Amazon OpenSearch as a vector store to do a semantic search to apply the correct policy to the booking. Again, like we did previously, let's refresh our memory on the information that our agent is using from the knowledge base to apply the relevant travel policy. We see a pricing policy around lower and upper bounds that was retrieved from the vector database to add to the context window. In this case, our agent is using the knowledge basis for Amazon Bedrock Service to perform this function. Now the way this working behind the scenes, this service provides the end to end rag workflow for us. You simply specify the location of your data, which in this case is 3. It selects an embedding model to convert that data into vector embeddings and then have the service point to a vector store which in our case is Amazon open search to create that vector index. Our agent fetches that context and adds it to the context window in the model, so for the model to be able to reason and use that information in the most appropriate way. A little bit about the vector engine for Amazon OpenSearch. It's a search service, delivers highly relevant and accurate responses. And provides the vector indexes of scales that are production ready and at this reinvent we are introducing the general availability of GPU acceleration for the open search vector engine by offloading vector search computations to GPUs. OpenSearch can achieve up to 10 times on speed and compute costs. Which is great for applications like RAG, retrieval augmented generation, and recommendation systems. And similarly, just like AWS for agentic memory, depending on your use case and your business needs, AWS provides additional choices for deploying a vector store for your AI application. Including the general availability of Amazon S3 vectors announced at this reinvent for cost-effective large scale vector processing or vector storage. OK. Besides personalized responses, another area of context management is to have a timely response. A travel reservation agentic AI application is expected to maintain the latest information of the user's travel preferences as they can change over time. This is where change data capture CDC events combined with streaming ingestion, storage, and near real-time processing provide current information to the AI applications knowledge base. Also, depending on the use case and business needs, AWS has several streaming and CD CDC options to choose from, including Amazon Kinesis Data stream, uh, AWS Glue, Amazon Manage Apache Flink for real-time aggregations. Moving on to the next aspect of leveraging your AI ready data Foundation on AWS, let's talk about event-driven architectures in scenarios where you have multiple agents that are providing an agentic AI function. Uh, multiple agents is where an agent needs to gather information from multiple sources, including other agents, tools and systems, and this is a very likely scenario in production when you build an agenttic AI application where you end up with multiple AI agents. Multiple agents, a good way to think about those is like a loosely coupled asynchronous architecture that is very similar to a microservices based application. Where multiple agents work uh independently but at the same time in collaboration with each other, moving data and events, making sure unavailability of one does not affect the other. This is scenario two, where, let's say I want to book a flight which actually violates the policy exceptions. So this will demonstrate how multiple agents come into play. Right. So, as I said, scenario two of the demo, where I am Uh, having to book a ticket that is that is outside of my company policy, but works better for my availability schedule. Uh, as expected, it comes back to tell me that this particular flight will need my manager approval. And as part of backend operation, what I'm doing, I request and get the approval from my manager. And then. We'll wait for the agent. It's doing a little bit of hard thinking over here. It's a policy exception. So, there it goes. So now the agent is telling me that I need an approval from my manager because this uh choice of flight is uh violating my company policy. I, like I mentioned, I get the approval from my manager through a backend operation, and then I tell the agent, yes, I have the approval. Uh, the agent takes that information and does some uh validation and then goes ahead and books the reservation for me. As you can see, it's performing the similar operations that we saw earlier, which use components of the knowledge base, the agentic memory, uh, my user preferences, uh, and also does a few backend operations. Let's take a look at those backend operations, right? These back end operations are basically additional agents that are collaborating with our main agent. There's a confirmation email notification agent, an agent that triggers uh a rule if a company policy on expenses is uh satisfied or not, and an agent that logs all transactions to a back end applic uh database. Now, our main agent could actually be made to handle all these tasks in one, but if you recall the react loop, right, agents can also get easily confused, and you're also working with the limited context window and the agentic memory. So On the other hand, having multiple agents, how does that help? Multiple agents performing a unit or a single task are easier to build, and you can build in resiliency with an event-driven architecture, right? This is something that looks like this over here for us. Using a service like Amazon managed streaming for Apache Kafka or Amazon MSK, which is a fully managed Apache Kafka setup, you can loosely couple all the agents while ensuring real-time exchange for low latency with built-in scalability where there is a need to add more agents, for example, and manage persistent events with multi multi multi AZ deployments and automated recovery. An event driven multiple architecture not only decouples and scales the agent task, it also provides a distributed processing platform for the agentic AI application with durable messages and delivery guarantees as provided with the MSK service. A little bit about the Amazon MSK service, it can process high throughput events, uh, enabling agents to respond to various patterns and real-time scenario changes. Asynchronous communication in Amazon MSK facilitated by event-driven architectures provides benefits such as decoupling, independent scaling, and improving fault tolerance through replication. And now, I would like to invite Shikha back to us to tell us how data readiness can be made, can be worked upon through the AI Ready Data Foundation. Shika. Thank you, Taz. How are you doing? That was a lot. To consume, right? How are we doing? Good, good. All right, I think Taz did what he, what he told me that he was gonna do. He was like, OK, I will use our data to create agents, and he created multiple agents. As we were getting ready for the session, we purposefully wanted to do more of a show rather than tell. So I hope it's working for you. Quick raise of hands if it's working for you. Yeah, OK, cool, cool. So. Who believes in this? Does this make sense to you guys? Yeah, so just like how humans need maps, AI needs metadata. So how does this fit into our overall picture of what we have seen so far? So we saw as use a lot of data and built-in context. We talked about context management and how context comes in through knowledge bases and through session logs and through data that is coming through tools or databases, um. Is metadata new guys? Like how many of you have dealt with metadata for like decades now, right? It's not new, but it is cool now. So I think metadata is definitely having a moment right now and I think it's great because all of the context that we're talking about is essentially metadata and data that is sitting in unstructured files like Taz pointed out the policy file we're. Gonna go through, um, more of the traditional data side of readiness which is, you know, data sitting in your data warehouses, data lakes, and how do you get that ready for consumption by our AI agents because that is where all of our data is sitting right now, right? So when Taz and I started thinking about our session we were like, well, all of our data is like literally all over and it's not even in AWS. Some of it is outside. How do we bring it all together? So if everybody accepts this, then I think we're going to go into The next side of the story, so. We have our data sources, right? Traditionally we've had data warehouses, data lakes, then came the lake houses, then all these different data sources where data is just sprawling. All of that comes together, uh, through the architecture that, uh, Taz is pointing out with generating more and more metadata, right? So we generate metadata as we enrich each of these data sets with more information. We generate metadata on data quality and data lineage as we get this all data moved into a form where our agents can use it and humans can use it, so metadata is definitely having a moment, and I would highly encourage everybody to think of this as a key foundation that you have to invest in, uh, for your AI initiatives. So what does really metadata help us do, right? So I don't think the story will be very new, but again I'm gonna emphasize that it's what we used to do for humans is a must now for AI agents because AI agents are not gonna call their friend, right? Like we want autonomous agents working on our data and on the context that we are providing them, so it becomes more and more useful for us to bake metadata into our systems. So let me just talk about this, uh, cycle for a second. So there's, in most companies, and I'm assuming that this will sound familiar to you, there's data producers, people who create the data or own the data, and there's data consumers which now is getting filled up a lot with, um, not just humans but also AI, right? Does this sound familiar to you guys? Yeah, it's very much the story that we've been living with, um, but then it's now vital for us to have the context because AI cannot work without the context, right? So we gotta make sure that we're building the right technology in the middle to bring these things together, right, to add the right context in terms of metadata, to have the right frameworks in terms of governance so that humans and AI can use the same systems that we have been using by adding the right components to it. Does this make sense? Quick show of hands. OK, cool. All right. So let's go ahead. Um, how many of you have heard of Amazon Sagemaker? Yeah, so we invested a ton in this, uh, platform in the last few years, and for those of you familiar with Sagemaker AI, which used to be the traditional machine learning side of our offering, we really expanded that that brand and portfolio to. Include all of our data analytics and AI capabilities into it. So that is what Amazon Sagemaker is. It's really the center for all your data analytics AI because we saw this thing coming. We're like, OK, we're gonna need to bring it all together, right, because we can't have these agents running a havoc through your ecosystem. So what Amazon Sage Maker is built on is a is a foundation of the lake house, which is if you're iceberg compliant you're gonna this is gonna be music to your ears because anything that is iceberg compliant can come into this ecosystem and can bubble up through Sage Maker's uh capabilities of data and AI governance as well as the unified studio which brings all of these capabilities together. So next set couple of demos that Taz and I are gonna show you. Are really using these capabilities to build the context and the metadata for your users and your AI first and then use that context to create an application quickly, right? So we're going a little bit behind the scenes here where Taz really showed you almost the front end application of the agent and the customer using it to build, build in, uh, to book a flight. Now we're gonna go a little bit in the back end and show you just how this gets done. All right, so at the center of that Sagemaker picture you saw a data and AI governance, right? So what does that really mean? This is the center for where all of your data, your models, your agents come together for availability across your entire organization, right? So I'm assuming most people here are, um, data engineers, software engineers, data platform owners. That sort of um um demographic is that, is that right? Can I, yeah, OK, cool so you guys deal with this, right? Um. So what Sagemaker really allows you to do is bring all of that together, then you build trust in it. So we talked about data quality and data lineage again, words that are not really new but are of immense importance in this new world because we got to get this right so that the context is all built in into our, into our data and metadata. And then applying the right guardrails around, you know, who can use this data. You don't want your notify agent using your policy data. You don't want your, you know, like it's the whole picture like in humans you can call and make the right controls happen, but in agents it's all the more important to set the right guardrails and the right workflows in the system itself. All of that, of course, is very transparent and auditable, so all of the metadata that gets generated through this, for example, at the asset level, goes back into S3 tables, right? So what does that mean? That your agents can go read that data directly, right? So if I put all the metadata that I'm creating through SageMaker back into S3 tables, you can read that data. Your agents can read that data and infer information from it without having to go through the whole stack. All right, so two key important things again, takeaways, right? So for data and AI initiatives in general, right? Like you want to apply the same framework across your data models and all kinds of assets, and you want to invest in metadata as well as a comprehensive data governance around it. All right, let me show you some stuff in action. So we talked a lot about metadata. Metadata is not easily available for the things that we've had for many, many years, right? Not every system that we own has useful business context attached to it. It doesn't, the tables are, you know, hard to tell for those of you who are. You know, in the SAP land, like the table name doesn't tell you much, you know, it's things like that, right? So what we need to do is really add the right metadata to all of our tables as well as columns, but we can't expect humans to be creating that, right? So what I want to show you here is for one of the tables that I showed you in the demo. Um, we go and create metadata automatically through AI. So again with Sagemaker, what we have tried to do is we are building the system for you to use for your AI initiatives, but we have also built enough AI into the system to make the system more, um, more productive, right? So in this particular case, I'm picking the customer reviews table. You can see that there is a quick generate description button up here. There is, it's a lot of, it's showing me the typical technical metadata that is readily available usually, and I can add to it. I can ski the schema of the table that I have. It shows some data quality. We'll get to that in a minute, but I want some more useful business context added to it so that I can feed it to my humans as well as AI. Right, so it's gonna take a minute. It's doing it live. We, we didn't wanna shorten this stuff for you so that you know that it does take a minute. But as you can see while it's loading, like on the rest of the screen, you have glossary terms, metadata forms which you can add additional ones, and there you go. The metadata is generated. So here I have an option to edit the metadata if I want, but I've seen pretty good success, so I'm just gonna accept it, and you can see that in the summary it's giving some very helpful pointers. So what does this data have? What can it be used for, right? Things that we would be like wanting somebody to put in the data so that we can query this information, we can use this information when you're trying to use it, and it just generates all of that in automatically, right? That's the power of LLMs that we are. Getting the benefit of right now, it also went to the column level and generated descriptions there too. So earlier, if you remember when we came to the schema, it was largely empty, but now you can see that there's even column level descriptions added. On the and I am accepting all of the descriptions here as well because I think they're pretty decent for my use here um, the other things you see on this, uh, screen are there is a data quality, uh, scores that are available as well, right? So this got pulled in by looking at the rules that I had set for the stable back in glue, which is how I. Brought it right, so those tables that is pulled in, I have a clear understanding here that OK, this data is only 60% good. You really wanna use it, but you wanna at least show that to your users up front so that they know what they're getting into, right? And they're not being misled. Now the lineage story here, Taz and I were laughing that it just shows one table because we didn't combine it with a bunch of other things, but if you bring a bunch of things together, it shows a beautiful graph of how this data was constructed. Anyways, now that I have put all that information, I published that asset back into Sagemaker catalog. So now it becomes readily available for another user to come in who's coming in right now and they search for customer reviews and that table appears. So now another user from another side of the world or another agent can come in and search your catalog for using this information by some keywords. And if I like it, you can subscribe to it, which means means that you're getting the actual grants to use this data for your systems, uh, which is again like, you know, something that is audited. You can, uh, query it, you know who's using what data. So again, pretty useful really quick demo on the behind the scenes of how this data got ready for task to use. So in this area we are really introducing a lot of capabilities. Um, some of you may be familiar, um, but may be new, so I'm just gonna quickly go through that. But metadata in general is getting a lot of investment across AWS. You'll see that we are adding metadata capabilities at the storage layer at S3. I'm gonna call out some of the key capabilities that we have added to Sagemaker catalog, which is really at column level. You can have enhanced descriptions. You can have metadata forms. It gives you so much more control over what kind of context do you want added to your data so that your agents and your humans have all the information available, right? Uh, you can also enforce that, right? So you can now say that for this use case, this particular table or column is not usable. That also gives you that feature with the enforcement of metadata forms as well as glossaries because you can, you can, you can spell that out a lot more clearly when the context is added to your data. Um, also, you know, again, it's really hard to create these things from scratch, right? So we are continuing to invest in our automation where we gave the automated business descriptions first, and now we are also, uh, giving the capabilities to add business glossaries automatically. They will give you suggestions. You can accept them, or you can associate them with a pre-existing glossaries just becomes super easy. And then like I mentioned earlier, we're putting that all back into S3, right? So then it's available at your, even your storage layer, the metadata itself is available for you to query and use with your agents. How many folks here have data outside of AWS? Most of you, right? Yeah, even I do, um, so very excited for this particular capability which is, um, if you're, again, if you subscribe to Iceberg, this is great, we are investing a ton in Iceberg, um, where this is Catalog Federation to external Apache Iceberg catalog. So for example, if you use data bricks or Snowflake. Um, you can bring in, read those tables from these third party, um, solutions, and you can bring them into into Glue data catalog so that it becomes available in the same Sagemaker ecosystem. That I showed you earlier, right? So you can, for example, bring in a table from Data bricks, you can catalog it, you can add the automated business descriptions to it you can do all of that from within Sagemaker if you're leveraging this capability. Super excited about this one. This has been a huge ask from most of our customers. All right, now that we have all of that data together, so now our data engineer, um, data scientist wants to build something on all of that data, right? So how do you do that quickly without having to now build like the traditional pipelines or the, um, you know, go to 5 different tools to do your things. So if you're familiar with SageMaker Unified Studio, we're adding a ton here, um, uh, we've actually in the last couple of weeks. So one is the one click on boarding of existing data sets. What this means is if your data is, is part of Blue Data catalog or you just did the external federation from Data bricks or Snowflake, with a single click of that, that data can be made available in Sagemaker invite Studio. What that means there is there is also a net new notebook there that we have added, which is pretty cool. I would. Encourage you guys to try out. Taz is gonna come show that to us in a minute, but this is a polyglot notebook, so you can have cells with SQL, cells with Python. They can correlate, and it has a built-in data agent in it, which again Taz used to, you know, help create the little application that we did which we'll just show you, which is really, really cool, guys. Like it can, it can create code of all types automatically. We'll show you in a second. It can also correct coding errors within it. So if you guys are playing with, you know, how many folks here are familiar with like cursor or Red surf or lovable or that breed of things, yeah. So I think you will love it if you dabbled in that space because there is a lot of good stuff here that you can saves a ton of time. Um, I was playing with an application last week and literally like. It created the code for me and then I'm like, what the heck is this? And then there was an error and it, I did the click fix with AI. It fixed the code for me. We're actually gonna show you that live in a second as well. So Taz, do you want to come back and show us how this works? Absolutely. Well, if you haven't realized so far, I'm the demo guy. OK. Uh, I know we threw, uh, we showed a lot of demos, uh, but, and this is the last one we promised, but this is the coolest one, and you can see how excited I am, right, with this. So, you earlier saw Shika, uh, talk about data agent data notebook. And what I want to show here is the customer user review data. How easy it is to do a subjective analysis of that data. Anyone here tried ever to do a sentiment analysis of a data set? That is human dependent. I mean, you can imagine the number of uh machine learning libraries that you have to use to get the code right, uh, and actually get the the right results. Um, to me, uh, it, I can imagine it can take me about 2 weeks of time. This demo took less than 10 minutes. Let me, uh, stop talking about that. Let me start the demo. It's, it's a little long, but pay attention. I, I think I can promise you that you'll enjoy this. All right. So, we are starting with the Launching the ah the IAM based SageMaker interface and quickly navigating to our customer review data set, right? And then we create a data notebook, just by the click of a button, preconfigured with a cell that is telling me Uh, what my data set is and a simple SQL query, so I can make sure I have the right permissions to my data set, and I'm looking at the right, uh, data in the right context. I go ahead and run that query and confirm I have the right data set. So keep in mind, this is the customer user review data that we are using in an, in our agentic AI demo. On the right side of the data notebook is the data agent that Shikha was talking about earlier. I want to test this data agent, right? I want to put it through a grind to see how good it is, right? And this is what we are going to do over here. I start with giving it something very simple by saying, give me the count of unique flights with reviews, right? I also intentionally use bad grammar to see, is it picking up my intent, what I'm really trying to find. It goes ahead, spins up a cell, it decides to use SQL on its own, and I run that query, and yes, that, that is essentially what I had in mind, what I was looking for. It gave me that data. OK. So, that was simple, right? Um, I want to test it with something more challenging. So I come up with something over here. I give the agent a complex task to do a subjective analysis of the customer reviews by asking it to do a sentiment analysis of the data that I have. So this is where I want to see, do I really have confidence in my user reviews, right? How's the distribution of positive versus negative? What does that look like? So, I go ahead, you can see I'm asking it very nicely. Uh, can you do a sentiment analysis for the reviews of my data? The agent goes into thinking mode. And then comes back with an execution plan over here. Waiting on it. There you go. So, what it's doing is it automatically decided to switch from SQL to Python for this task. I don't have to tell it anything. It determines the Python libraries that are needed and available for this task in my notebook environment. For example, uh, it plans to use, and I, and I know it's a little hard to see, uh, but take my word for it, it's, uh, trying to use the text blob library to process textual data and the transformer library that it found in my environment for natural language processing, uh, using pre-trained models for that sentimental analysis that I was asking about it. And in this case, it is, uh, decided to use the distal word model that is ideal for a sentiment analysis in notebook style environments. All right, and then the agent tries to show off actually by also proposing a visualization summary of the sentiment analysis, followed by a breakdown of the flight with the most positive reviews. We run the first cell over here and as you can see it is now working on the dependencies. It is thinking through it. Let's see what it comes back with. All right. It came back with an error over here. This error is essentially what is telling us the data frame that was passed to the cell from the previous SQL query is not right. Um, being who I am, a little bit lazy, I, I know what's wrong. I'm just don't want to fix it myself. I use, uh, the fix with AI feature that we have in this notebook. There you go. And folks, as it a fixing with the AI, I know we are coming up at time, so we tried something different here, uh, for this session. We did more show than tell. If this worked for you, please do give us feedback in the session survey because we'd like to do more of this if this worked for you. So while that is fixing with AI, here's a human announcement for you, right? It did its job. Shika. Thank you for that. And all right, it fixed it. It made the code change. And now it is telling me, OK, I've made that change. I think this will work. If you like it, accept and run. Uh, I do that OK, as in, in the real world, another error, right? And this time it's a little bit more uh challenging what that error is. It's talking about a KS library that is not compatible with what it's trying to do and the, the, the Python code is telling me, go ahead and install this to uh make it compatible. Um, again, I, OK, now I want to test the fixed with AI a little bit more. I tell it, uh, why don't you try to fix it? So this is what the agent does, the data agent does, right? It looks at the error, uh, and it knows that there's a compatibility issue, but it takes a different direction in fixing that error. So instead of fixing the compatibility issue, it actually pivots to a different library like PyTorch because it knows that will work for sure because that's the response that it is giving me. You can see it's made the code change line number 8. And again, like I said, the agent was actually showing off, it made the change in the comment as well, that I'm changing this library to Pytorch. All right. I accept it, I run it. It's running the code. It's running the cell. All right, I got one output that it cannot, and I got the results of what it was trying to do. OK, so that was the first cell where it was doing the uh analysis for the sentiment. Next, it's going to, um, do a uh uh a graphing of the results over here. Uh, again, it generated the Python code on its own. It's using MapPlot libraries on its own by itself. I simply go ahead, run it. This one runs without any issues. I got my data. Uh, I go from data engineer to data scientist to a data analyst, all in one over here. All right. And then the last query, which is essentially giving me the highest uh count of the best airline with the most positive reviews. Uh, in the interest of time, I'm just gonna move on to the next slide, which is the second to last slide over here. Key takeaways. I'm not gonna read this slide. We talked a lot about this over here. Key idea is you don't really have to build anything new in terms of data foundation. AWS has what it takes for you to build your agenttic AI applications. Uh, use, uh, uh, leverage the critical path between data and AI through purpose-built capabilities, some of which we saw over here today, MCP servers, context management, uh, vector stores, and data governance. And with that, folks, uh, if you have additional needs, we do have programs to help you get started. Thank you for your patience staying with us. I appreciate that.