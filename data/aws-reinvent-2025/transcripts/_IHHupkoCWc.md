---
video_id: _IHHupkoCWc
video_url: https://www.youtube.com/watch?v=_IHHupkoCWc
is_generated: False
is_translatable: True
---

Hey folks, uh, so I'm Brettie and I'll be talking to you a little bit about S3 tables and giving you an introduction to what S3 tables is at a super high level, uh, the value it brings customers, and then I'll turn it over to Venkatesh to talk about how Indeed is leveraging S3 tables. So S3 Tables was launched a little over a year ago at the last reinvent, and we set out to solve three primary problems for our customers simplifying security, improving performance, and reducing the complexity of maintaining the iceberg at scale. I'll talk more in depth about how S3 tables does that for customers in the next few slides. S3 tables was introduced as a first-class AWS resource and it's purpose-built for storing tabular data at scale. So how do we, how do we improve, uh, security with respect to tabular data in a data lake? If you think about objects being written to a data lake. You would have data and metadata files being written maybe to an S3 bucket and various objects coming in. You might have multiple tables in that S3 bucket as well. Well, trying to write security policies against individual objects doesn't scale well. Customers needed a better solution. And so with S3 tables, it allowed customers to write security policies against the table itself, greatly simplifying the policies that customers had to write. The next is uh compaction. So when we think about uh iceberg data and a data lake, individual objects are being written to that data lake over time. And a customer may need to come in and query against those objects. That query may need to read multiple objects at one point in time, that all results in multiple API calls to the service consequently increase latency. In the past, customers had to write custom spark jobs or run custom compute resources to come in and compact those objects, and compaction really just simplifies the data lake, reducing the number of objects you have to query to return a result, thus improving performance. S3 tables does that all for the customer automatically so you don't have to create or spin up customer custom compute resources to do this work. The service manages that undifferentiated heavy lifting on your behalf. In a similar fashion, you have table maintenance, so as. Objects are being updated or deleted in your data lake. It creates unreferenced files. Those unreferenced files and snapshots in your data lake uh do powerful things like the snapshots enable uh time travel and rollback in your data lake. But they waste storage space, and when they're wasting storage space, they're also increasing your cost. So once again, S3 Tables comes in and manages this cleanup and the maintenance on your behalf, so you no longer have to do those operations. So that's S3 tables at a high level. I wanted to just give a quick primer on the service and the value it adds to customers, and now I'll hand it over to Venkatesh who will talk in more detail about how Indeed is leveraging S3 tables to help people get jobs. Uh, good evening, folks. I'm Venkatesh Mandallapa. I'm the tech lead for the Data Lake team at Indeed. Uh, quick primer on who we are. So we are the number one job site in the world. We have 635 million job seeker profiles and 33.3 million employers and in over 60 countries and we have 11,000 employees. So our mission is to help people get jobs and the engine that drives this mission is data and we. At a global scale, we generate a lot of data. We need to process, store and process all that data, and my team, the Data leak team at Indeed, is a central data store that stores and processes all this information. Um, I'm gonna give a simplified version of Indeed Data Lake. Uh, so we have AWS Glue data Catalog, which is our central catalog for the entire Indeed. This is sitting in one AWS account and it's exposed to all Indeed customers, and, uh, it's the, the data itself is stored in S3 standard storage, and it's a mix of Hive data and which is in ORC format and Iceberg data which is in parquet format. And uh we have about 68 petabytes in Hive and about 20-ish in iceberg at the moment. We have a lot of engines that access the data, uh, Athena, Snowflake, Trino, Spark, and have a few more, but these are the primary engines that our consumers internally and externally use the data from Data lake to query the information. Um, some stats there, we have 87 petabytes total for data lake at Indeed, uh, from 5550 terabytes ingestion per day that we ingest into the data lake, uh, 15,000 plus stables in different schemas. We have 6 ingestion patterns. Uh, so this is how we get data into the data lake. We provide multiple mechanisms for our customers internally. To to register the data, to load the data, and to make it available for all our consumers, and it's not just that we have a lot of data, it's also being used by a lot of people inside Indeed and outside, and we get about 170,000+ queries a day. Uh, we have a mix of analytical ML operational data, and, uh, we have all kinds of modes of data, streaming, batch, EDL, you name it. So we kind of have this legacy architecture, legacy formats, and we want to simplify, modernize and make it more fast and efficient, both, both to ingest the data and to query the data. So we started with a goal, we wanted everything in Iceberg. Iceberg is the future. Iceberg has a lot of uh uh promises and features that is very interesting, skim evolution, maintenance, all that, and we, we made this a goal to move our entire data lake to Iceberg. And with the Iceberg we we decided to just use the standard S3 storage, the general purpose buckets, but then Amazon released the new S3 tables, and we looked at it and we thought it was very interesting, and the product matured over 6 months, and we did a cost benefit analysis and we found that it's actually really interesting for Indeed to switch 100% of data lake to Amazon S3 tables. And we also found out that the 6 ingestion patterns of how we get data into data lake can be reduced to 1 pattern, simplifying the code base for my team, uh, making it much more streamlined and much more maintainable for the data lake. So, what are the challenges we face with Iceberg in general-purpose buckets? So I'm gonna talk about that for a little bit here. Um, we had to run our own maintenance operations for iceberg tables in Data Lake, and that was costing us a lot of dev hours, 2,000+ dev hours per year managing the iceberg tables, compaction, uh, delete dolphin files, uh, and all, all the other maintenance operations that you have at Iceberg drops. So we rolled our own sport shops running on kiunities to do all this stuff, and we are finding it much, much more challenging to keep this up and running. We also have this ingestion patterns, the 6 that I talked about. On, on average, it takes about 1 day per customer team to actually get the data into Data lake with the standard iceberg in general-purpose buckets. And S3 rate limiting was a problem. Some tables are queried a lot and some tables are queried like every quarter or every day, and these different access patterns generated S3 rate limits for us, which caused severe incidents of data not being available because of these rate limits with the general purpose buckets. And we also do per object tagging for access control, so we have about 600 million objects in the data lake, and each object has object tags, which is evaluated against the resource and identity policies to actually provide access control. So, managing these object tags and trying to, trying to change the object tags or trying to assess the security of it was a nightmare for us. So Amazon S3 tables provides a lot of benefits and a lot of solutions for all these problems that we are facing. And it simplified the data governance of the access problem that we have. So with 600 million objects, if you want to reclassify the data, then we have to spend weeks trying to update the object tax. So one table today it's accessed by all of Indeed, but we want to switch it to only accessible by Glassdoor, which is a sister company, and that would have taken us weeks to do that. But with S3 tables, it's a resource policy on the table object for S3 tables, and we can simply go and do one operation on that and update the policy. Um, we evaluated that overall storage, maintenance, uh, compaction, you, you, uh, ingestion, when we add up everything, we are seeing about 10% annually for AWs cost compared to general purpose bucket iceberg tables. So this has been a huge incentive for us to push everything into Amazon S3 tables. And I recently heard that the maintenance operations have been dramatically reduced by Amazon S3 Cable team, and this is making us, uh, this is making this whole thing much, much more attractive for us. So we talked about how the onboarding experience was one day in the past, but now with S3 tables we can do it in minutes. Our producing teams can write to S3 table buckets with an API call and start ingesting data into data lake within minutes, and this is huge for our consumers to be very fast, very nimble, and very cost efficient. And reducing the maintenance operations and taking that away and let Amazon do it, we also found that we can return that for damn months back to the team and we can actually do product development. We can actually build things for our customers, so this is very exciting for us. So how do we go about with this migration? Because we have 87 + petabytes, and we can't do a big bang approach that's not gonna work. So what we did is we looked at the server access logs, the query logs, and we identified different workloads and categorized them into cohorts and batches and faces. And then we're, we're trying to do this incremental migration piece with a dual right pipeline, and this is a standard approach that people use during migrations. You still have your standard pipeline that writes to the existing production objects, and then you set up a separate pipeline, and that's what we're trying to do with the with with the incremental migration. And then we're going to develop some tools and APIs to automate the ingestion of data directly into Amazon S3 tables, and this is going to help us remove one step during the migration process. Um, so, as a result of all the assessments that we did with the migration planning, it, it looks out like this. We have the entire data lake, and we're gonna split that into multiple phases. The first phase is gonna be like move all the Hive data to Amazon S3 tables, and then move Iceberg S3 standard data to Amazon S3 tables and data producers update. And within phases, we're gonna break that into cohorts and based on different workloads like Spark, Trino, Athena. We have to, we can target our our migration along this cohorts and we can still further break this down into batches and do it at a terabytes or petabyte scale so that we know that we can understand what the challenges and the problems that we'll face by using this incremental approach. So, I'm gonna talk about the dual pipeline strategy a tiny bit. Uh, this is our current pipeline, very simplified. We have producers that are producing data. We have an ingestion pipeline, multiple 6 patents, and we store this in the data lake in a standard S3 storage and the AWS default glue catalog. With a dual ride, we're going to split the pipeline into two modes, one standard and one going to Amazon S3 tables. Amazon History tables has its own catalog, so we're going to have 2 catalogs with the same objects. And then we do the cutover process which is simply linking the S3 catalog to the default glue catalog. So all the objects that are in the S3 catalog will show up in the default glue catalog. The consumers that are accessing the default catalog will actually go to the S3 tables in the back end. And then we do a final cleanup. We remove the old S3 standard data, and we, the, the, the data cleanup from, and then we'll remove the pipeline that writes to the S3 standard. This is pretty typical in most migrations that people do. Uh, the only. The Interesting thing here is the catalogs are going to be linked between the two systems. So the consumers will always are going to default catalog. They're not going to be impacted. They're not gonna notice any difference between accessing their queries, their applications. It's all going to be seamlessly done because they're all hitting the AWS default catalog. We're only changing the back end storage systems here. So, some things we have learned so far with migrating uh some tables to Amazon S3, uh, we found that Amazon S3 tables is very tightly integrated with lake formation. And this was unexpected for us, and uh we, we don't use lake formation at Indeed, uh, we manage permissions quite differently. And trying to understand lake formations, trying to understand the identity, identity permissions that we have to update, the, the resource policies that we update, and uh that became quite challenging. So just a heads up, if you guys are not using lake formation, Amazon S3 tables is very tightly coupled. It's we made it work, but we had to struggle quite a bit and we had to get a lot of AWSs help. Um, query performance is going to be quite different because we have a lot of data in Hive and ORC format. Iceberg has a different planning mechanism, uh, so make sure that all the workloads that you have, make sure that you run that against the Amazon S3 tables, and they perform similarly. Uh, during the migration, we're also changing the partition strategy quite a bit. So we want to make sure people that are using the old partitioning for Hive still works for Iceberg and faster. Amazon S3 tables have some limits, uh, 10,000 table buckets per region, per account, 10,000 tables in the buckets, so a total of 100,000 assets that you could have. We only have 15,000 so far, but with the linear projection we could see it going to 20 and 25 and so forth in the upcoming years, and we want to make sure that we use as many, as little buckets as possible for, for just for the maintenance of it. Uh, backup restore is a little bit of a problem right now. Uh, we have to roll up our own backup and restore strategy for Amazon S3 tables, and we have to rely on, uh, some of the unreferenced file features and, uh, the policies on that to, to provide backup and restore. Concurrent limits is more of an iceberg problem. The more concurrent writers there are for iceberg tables, uh, things break, so we have to solve how many writers we have for each table. Um, which is also something that you might have to, you might, you will face. We have other issues, which is server access logs in the past, S3 standards, buckets. They come with this beautiful server access logging feature, so every operation is logged. It shows AWS roles, it shows the region and all that stuff, but S3 access tables has a different logging mechanism. It uses cloudWatch and cloud trail. So if you're relying on S3S access logs, you might need to update your pipelines to go to cloudwatch and cloud trail to export the data to the format that you want. So it's it's a small change in how the access logs work. We use Snowflake quite a bit at Indeed, and every asset in Data Lake, we want to expose that in Snowflake. And with Amazon S3 tables, it provides two rest catalogs, IRC catalogs. One is the Glue Rest catalog, one is the Iceberg Rest catalog from S3 Tables. So you can register both or one in Snowflake and expose everything that we have in Data Lake in Amazon S3 tables in Snowflake as well, and this is huge for us. Our customers really like Snowflake, so. So, some things that, um, this is the concluding slide, how it's progressing. Uh, we are pretty happy with what we're doing right now. We have about 50 petabytes that we're scheduled to migrate to Amazon S3 tables, and We are, we're doing canary tables right now, about 2.5 petabytes, which is ongoing right now during this Thanksgiving and December 1st week. Uh, we're always preparing for inverting or reverting the pipeline, the dual pipeline. If there's any problems at any second, we are ready to revert the, the table access, the table objects, everything back to the old pipeline, so no consumers impacted, no queries are impacted, no producers are impacted by what we're doing in the platform team. And The dual pipeline is obviously going to create higher cost and that's OK for us because what's more important for us is that our consumers are not impacted, not just the internal consumers globally people are using this data, policymakers, media, so we want to make sure that no one's getting impacted by what we're doing here. So finally, we are on track for a unified, modern and a cost efficient data lake, and we are very happy with Amazon S3 tables uh that it meets all our needs for here. Thank you.