---
video_id: 4s-a0jY4kSE
video_url: https://www.youtube.com/watch?v=4s-a0jY4kSE
is_generated: False
is_translatable: True
---

Hello ladies and gentlemen, thank you for joining us, spending your time with us today for a code talk. If you're not familiar, uh, a code talk is, it's not a recorded session, it's a live coding session. Lucas and myself, we're gonna be live coding for you today. My name is Cy Venom. I'm a principal solutions architect with AWS. Hey folks, nice to meet you all. I'm Lucas Duartz, a principal solutions architect here at AWS as well. Yeah, and today we're gonna talk about how you can streamline EKS operations with Agenic AI. This is very much gonna be an informal casual session. We wanna give you folks an opportunity to ask questions at the end as well. Um, we're gonna be going back and forth. Lucas and I, we've, we've done this, uh, a couple times before. We're excited, uh, to, to run through what we built for you today. Last year we did a similar session about using AI. You know this was before the time of agents this is even before MCP was getting popular so we built a lot of things from scratch this year we leverage tools that are available out there that make it even easier to build really robust troubleshooting agents from scratch, um, so that's what we're gonna talk about today. But first I think it's important to understand how the technology has progressed from where we were last year, maybe even two years ago, to where we are today. But we're gonna go for it, thanks. But before we talk about that, I just want to introduce to you folks a new member of our team, the cue agent. So essentially right now we have an agent that can be responsible to troubleshoot your clusters and also also remediate some issues based on. Root cause analysis using MCP using Agente. So what we're building here is not just an LLM and a chatbot because that's 2 22024. What we're doing right now is actually building an AI agent that can be your partner that can help you to. Accelerate your troubleshooting and actually reduce the mean time to remediation of your issues. So what we are trying to do here is to empower network and operations center engineers so they can use those type of tools to reduce the mean time to remediation because we know that Kuberne is a very spread out ecosystem. So that's the idea that we have for our session, right? Exactly. And here's the thing, we don't have anything running right now because our, our, our setup is completely clean, but we wanna show you what we're going to build just like a little bit of what's possible with the technology, the art of the possible. So here if you can see we have a Slack channel, right? The Slack channel is where we're gonna receive messages and everything, and then we have two, terminal tabs right there. One is running a memory agent. Stick to that information to the end because we're gonna build that up and the other one is that community is a specialist agent. So what's going on right now is the agent is looking today's lack. We have received an alert using alert manager and by the time we have received an alert, we have configured a troubleshooting agent to identify that alert and start to do troubleshooting based on that information. So if you can take a look, we are saying monitoring agent has been pending state for over. One minute and right now in the terminal tab we are doing the agenttic loop and we're gonna explain that later in the presentation but right now we are doing the agenttic loop to try to fix the monitoring agent problem that we saw earlier. Now the agent is doing the loop executing the tools until it is able to find the last response. So let's see why the monitoring agent was actually stuck in pending state for a long time. So in this case it's finishing the troubleshooting, doing the summary, and soon enough we should be able to have. A solution here in our thread. So of course in this case we're not having any human in the loop. We can have that later if you're using Git tops and everything, but just for the sake of the art of the possible, what we're showing here is that the agent got the alert from another manager. It consulted REM books that we have available in our vector database. It fixed the issue by itself because of the information that we gave to the agent, and as you can see here, we have the issue resolved. Oh Lucas, but it's not too dangerous to let the MCP and the agent apply that stuff in our production cluster. Of course it is. That's just a. The art of the possible, but we could integrate that with Git tops and everything, but that's for later in the presentation. But that's essentially what we're gonna be building today our new teammate. Yeah, perfect. And of course that was a recording. We just wanted to show you kind of what's possible, right? Like these alert messages that can automatically fire in your Slack channel seeing that the agent can pick, pick those up quickly and respond with. Contextual information and that's something important that we're gonna dive into is, you know, we all know LLMs are just as good as the amount of context you're able to provide them and that's even more critical for what what we'll show today. So realistically again I love that it's not a chatbot that's right 2024 we're going into 2026. We wanna go beyond that. It's not just a chatbot. We want a teammate and you might just be thinking, oh well, you're just wrapping chat messages in Slack. It's still a chatbot. By the end of today's session, I think you'll, you'll believe me that it's, it's more than just a chatbot, um, and, and we're gonna dive into that in a bit here. Last year we showed rag. This is retrieval augmented generation. It's important for us to understand this because then you'll see how far we were able to come with egentic architectures over maybe simple rag. With any sort of rag pipeline you have a massive set of, you know, source material and, and in this case it can be like Kubernete's logs and of course logs can go into the megabytes, gigabytes, terabytes, tons of data. It would be crazy to store all of that as raw data and. You know, LLMs, they, they understand in a different language. It's embeddings, it's matrices, and so the first thing you do with any sort of, um, kind of, uh, retrieval augmented generation is you take that data source, you chunk it up, you put it into embeddings, and you put it into a vector store. So that was the first thing that we did last year. Then when the user asks a question. We take that question and we first look for any relevant context in that vector store and so that's what you kind of see here with the, the, the context over here is you take that context and you put it along with the embeddings, uh, that from the original search then now that you have the context you pipe that into the original question and you pass it into a large language model. So by the way, that embeddings model, this is kind of like a lightweight model just to find the relevant context. The embeddings, uh, the large language model uses that context plus the original prompt, gives you a response. Let's just quickly cover what we did. Chunk the logs into vector storage. The user asks the question, Why is my app response time high? You take the relevant context, you pipe it in with the original input, boom, you pass it into like a, a bedrock LLM, like a big agent, maybe sonnet or haiku, whatever you want. And uh you you get a response. The problem with that is one, we are limited to the context window, right? Uh right can be really intensive and also we are using right here because the model itself is limited to the amount of training data that it was trained on. So now we need to inject that custom data custom logs to be able to do the troubleshooting. But this was last year's approach. This was last year, and there's actually one more downside of this, of, you know, that data source chunking and storing into the vector database. We were doing that often, sure, but maybe like once every 30 minutes, once every hour, we weren't getting live data from the cluster. So if an engineer just deployed a pod and they're troubleshooting it, that data might not. Have been chunked and stored into the vector database yet and if the issue is happening right now, how we are able to reduce the meantime to remediation if we don't have live data yeah and so what we saw last year was developers would find an issue they would go to the cube cuddle they would run some commands, get them data, and then pass it into the LLM. It's like a lot of manual back and forth and context switching and really this is why agents are here to kind of solve some of that. Strands is an open source SDK for building agents, and it's, it's a fundamental way that we're gonna build the agent from scratch today. Is there anyone here already using strengths agent or have tested strengths agent? Awesome to see a couple of hands raised. It's gonna be good, yeah, but perfect. No, and, and, and we're gonna get into why this this SDK is so good for building agents. Uh, they've really thought through the different use cases of what agents might need to do. We're gonna start using some of those things as well. In addition, we're starting to see this trend in the industry of not just building one massive agent powered by one LLM, but using smaller agents that use the right LLM for the job. And so. Look, really, uh, I don't wanna go too much step into this architecture. Just know that when a user prompts this agent, that agent can use other agents. It can use multiple LLM models. It can talk directly to your AWS resources, and it can hook into MCP servers and, and the way it communicates and the. Prompts that the agent uses to talk to one another, that's all customizable and that's what we'll show you today with a very basic first getting started with a basic strands architecture. Um, it's gonna be a Slack interface that is reading chat messages that passes it to an orchestrator agent, and the reason why we use two agents here will be clear in a bit, but we use an orchestrator agent that then routes that message to another agent. That's able to load uh information directly from the API directly from MCP, um, and, and what you'll see is that we preconfigured that agent to do a couple of things right off the bat. So let's get into it. We'll show the demo here, um, so we again don't have anything running just yet and, uh, you know, if we wanted to we can I'll actually just load up the ID and we'll walk through some of the basic, uh, elements of the architecture right off the bat. I think this is the first one that we wanna see here let's do that cool and just hopefully you folks can see that nice. I think that's pretty big. Um, right, right off the bat you can see we're importing the agent capabilities from the strands SDK. This is all in Python, by the way. Um, there's a lot of cookie cutter boilerplate that we have in here already that I'll quickly walk through that orchestrator agent that I talked about, essentially, uh, it receives messages from Slack and it's able to do a couple of things like right now we configured it with one tool. Again, cookie cutter stuff we haven't really cooked up MCP yet. It's not a very smart agent. We just manually coded this one we say you have troubleshoot Kates, right? So let's, let's jump into that one. And if we go into that essentially it'll open this file right here this is our Kubernete specialist, um. Some hints here saying we will connect it to MCP, but it doesn't have MCP just yet, so spoiler alert, yeah, ignore that for now. Uh, we've actually just configured a couple of tools in here, so, uh, oops, don't wanna do that, uh, a couple of tools here. So describe pod, get pods, and we've kind of manually, you know, programmed these in. Um, we'll, we'll talk about how these work in a second here. We tell the agent to use a specific, you know, bedrock model ID that's configured in, um, some config settings that we'll go through here in just a second. And a troubleshoot command. This, this actually does the troubleshooting itself, OK, um, let's go through some of the, uh, properties that we've set up. So I think the one interesting one here was the model ID. So we're deciding to use cloud, 3 Sonnet for this one, kind of a relatively older model but perfectly good enough for our use case. We also have some other properties in here like the AWS Reg stuff for Slack, that kind of thing. Um, there's a Slack handler. I don't wanna get too much into this. This is cookie cutter boilerplate code. It just takes messages from Slack. Guides it to the agent. This stuff is really well documented, I think. I think we coded this using Quiro, right? Just auto generated this. Yeah, I just sent the prompts to Kiro and actually kiro code everything. I didn't even touch the code for days is like handler itself for the agents. Yes, exactly, um, and one last thing, let's just look at the requirements file. These are all the, you know, SDKs that are and, and tools that are loaded, and you'll see us use these throughout, not, not that many here, fairly straightforward. Some of the strands pieces, MCP, that kind of thing. OK, should we do the first demo? I think so. Let's do the first demo and just remembering folks that the agent that we have right now doesn't have any access to MCP capabilities. So the only tool that it has access to is Describe pods and get pods. So what Psy is gonna do now is send a question to the agent. With an inform requesting information that we know that the agent doesn't have the capabilities for, but it's gonna try the best to respond in the best way as possible. So I'm gonna, yeah, exactly, I'm gonna ask it what name spaces are in my EKS cluster. That's me. I'm Lucas. You're Lucas. Uh, but let's quickly take a look at the tools that we configured, right? So, um, if we look at the agent orchestrator, we actually only configured, um, a couple of tools here, uh, describe pod and get pods. We didn't tell it how to get name spaces. We didn't, it doesn't know how to, so it's gonna try its best and before I show you what it responded with, let's actually see how it kind of thought through the process, um. It doesn't know how to list name spaces, so first it's gonna get pods and based on that output it's gonna try and intelligently figure out what name spaces there are. Not great. It actually doesn't figure out all the name spaces. It says there's 3, and if we look at the response, uh, you know, it says there's 3 name spaces. That's actually not true. If we go to K9S, you know, it's just a, a handy CLI tool to let us see what's running in our cluster. You can see there's actually a lot more than 3 name spaces, so we wanna make it better and, and. To do so I think it's maybe I'll very quickly introduce a new capability that we've just announced, um, I think it was last week, right? Uh, and that's 2 weeks, 2 weeks it's the hosted EKS MCP server essentially instead of having to launch the MCP server yourself locally. We do it for you and so that means instead of having to launch a proxy and tunnel permissions and run the MCP server yourself and scale it locally handling to process a lot of mess it's all in the cloud and essentially you just. It leverages IAM so you can, you know, identity access and, uh, permission so you can, you can use that to grant the right permissions to the right set of people who should be able to use it. So let's use that and, and actually it's funny right up until two weeks ago we were doing the old way. And as soon as this came out we thought, hey, we can really simplify the MC MCP configuration for a live demo, um, and we switched it over and if you're using EKS, if you have IM permissions you can just use it with your favorite code helper exactly. And so here's that new architecture, right? So that specialist agent you'll just notice there's one more box there's on the bottom left there, uh, the manually, uh, coded tools that we had from before and then now with MCP. So now the specialist agent can decide to use the MCP if it wanted to. Let's take a look at how we would configure that. All right, um. We'll go back into Kiro. We need to just switch the. Oh yeah, hit the demo button should be good, perfect, thank you. Uh, all right, let's, let's jump in here and we're gonna go first to the settings file because we need a property and a property that will essentially just tell it, hey, we wanna enable MCP, so we'll enable this again just looks at the environment variables, um, and then sets it and so now in the actual code we'll go to our Kubernete specialist which again remember from the PowerPoint. That we built this into the specialist itself so it knows the MCP exists. Uh, first we'll need to, you know, actually import the system libraries from, uh, you know, so we import MCP and this is essentially just using standard IO to talk to MCP. It's a standard protocol for working with MCP servers. Um, and Stres 2 already has the MCP client embedded on it so you can make your strengths agent be a client of MCP hosted servers or even local servers. You don't, I mean, it's, it's very, very few lines of code. Exactly. And so let me just get the spacing here right. I think you need to align the cell phone all the way up. There we go. Perfect. Alright, so essentially what this is doing, if MCP is enabled, talk to the hosted MCP server again. Obviously this will, uh, you know, resolve to US West too where we're running this demo. Um, that's the hosted MCP endpoint. One thing that we had to do in the back end was just set up the IAM role so that this app can talk to MCP and get the right info, um, and that's basically it, you know, launch the standard IO client. And that should be good. Let me save it. Let me go back to the terminal here, kill the Python process. We need to add, we need to add the deletion method, right, right. If you don't add the deletion method, the MCP, the connection will be open forever, and then we're gonna have a problem. But since it's a life code session, just remember inside to do that so we don't have that issue. There we go. So here's the clean up step. We're gonna make sure we clean up after ourselves. It's very important, important, OK, um, all right, we're gonna go back here and again I think, uh, you were saying this earlier, Lucas, but we're running this locally. Um, and so it makes it easy for live demo, uh, so we're just gonna start it back up, so I killed it and then I'm starting it back up now with MCP, and we should see an indicator that MCP is being used now because until then while we had the library downloaded we weren't actually using it. Um, of course you'll remember right up here we import it we added some code to talk to MCP, um, so now it's thinking through the process and oh. Here we go. I think it's because I made it very big. Yeah, just click on top of it. There we go. Perfect. Alright, well, the spacing got messed up, but you can see it. It says fast MCP MCP. We're doing it live. Uh, OK, perfect. Python app is up. It's listening for Slack messages, and let's ask it the exact same question again. That's me. I'm Lucas. What name spaces are in my EKS cluster? And it's thinking. And now it should, you know, it's using the trouble Kate's method again. And it's using a tool that I've never heard of. We haven't coded this list Kate's resources. We don't have that. It's not in our code and I haven't coded it, yeah, and, and in fact it's only in the read me, right, but. In our documentation for MCP, but we haven't coded this anywhere. It's getting that tool from the MCP server. It talks to that hosted endpoint and it tells our agent there's countless tools that it, it can use, but it says for name spaces this is the perfect one to use. List case resources. It actually gets the correct set of name spaces, and now fingers crossed, I go back to Slack, check it. Boom. Alright, so that's the first step, and, and, and just a few steps you've seen that we're able to make our bot that much more intelligent. It's able to pull anything from our EKS cluster, uh, that realistically the, the, imagine if we needed to create every single tool for every single possibility of troubleshooting in our agent. And would be replicating tools here and there. So what I like to think as an MECP almost got an API gated that we can centralize the communication, standardize the tools, and then reuse those tools across multiple different agents. Soon enough we're gonna be talking about how to break monolithic agents to micro agents for sure. Yeah, yeah, exactly, uh, you know, I think the analogy a lot of folks have heard. For for MCP is that it's like the USBC universal plug for for integrations but I, I, I really see this as a solution for like an exponentially complicated problem. Think about all the LLMs out there. You probably use 3 or 4 yourself and then you know all the tools that it could talk to, you know, maybe your emails, maybe an EKS cluster, maybe different AWS services. So think about it. So if you're trying to build integrations for 10 LLMs to talk to 100 different tools, 10 times 100, that's a lot of integrations. MCP is just an integration layer, so you build it once and all your LLMs can talk to any of the tools that are built. OK, so that, that takes us from an exponential problem to, uh, you know, an ON problem. Perfect. Uh, so we've added MCP. Our chatbot is that much more intelligent, but there's actually one thing that I wanted to show. Um, really quickly in Quiro, the way that we're actually classifying when we wanna respond to a message, uh, if we look in the prompts, these are kind of the prompts that we've configured essentially telling the orchestrator how to behave. We have some for the orchestrator. We have some from the, uh, the, the system prompt, basically telling it its purpose in life. This agent, you're here to solve problems for us, but we have this keywords file. And essentially we use these for kind of dumbly figuring out uh when we should respond so essentially we should respond only when the keywords are in the message and so if we asked it a question like what version is my EKS cluster. It's actually not gonna say anything. I think closer is actually gonna say something because it's only keywords. That's right, it's live demo. Let's do it again. Let's do it again. But if I asked it what version of EKS am I running, for example, it wouldn't actually respond, right, because we have like this manually awarded approach for for keywords and we wanna be better. We wanna use. AI we wanna figure out when we should respond to a message and that's because in Slack you might have engineers talking to one another, uh, and they might use one of those keywords when they didn't mean to we should we should find the intent of what the message was about if if an engineer is talking about a problem and needs a solution. That's when we should fire up our bot. Otherwise if engineers are just talking about their weekend or what they did, we should kind of ignore that. Um, keywords is one simple approach to that, but let's use AI. We're gonna expand our architecture a little bit here. Didn't change. Oh, here we go. OK, so we're gonna expand our architecture and in the orchestrator we wanna make it a bit more smart. When a Slack message is received, we're going to use a different LOM. Nova Micro to classify the message intent if it's for troubleshooting proceed otherwise exit and I think the analogy I like here is you wouldn't put like a PhD mathematician to run your hotel check-in process, right? It's overkill. You, you don't need that level of intelligence. You don't need that level of uh operations to. people to the right rooms we wanna use a more lightweight more efficient, uh, model. In fact, because the Slack Slackbot res uh responds or listens to every single message in Slack, you can see how that could exponentially get more and more expensive. Um, if, if, if we're not being a little bit more intelligent about when we respond, and so that's exactly why we wanna use something like Nova Micro, it's a very lightweight model that responds very quickly to respond to messages, and so now Lucas, I'll pass it to you and he'll show us how the magic is done. All right, uh, let's take a look into the, can you please switch to the terminal? Awesome. So let's take a look into the should respond method real quick as you can see if there is any keyword that was found on that list that we just defined it here. Then we should respond. So if you look at the list of keywords that we have very limited amount of keywords. So we have a limited validation method that we just created. So as I just said, we want to do that in a much better way. We want to be able to do a smart classification. So the first thing that I'm gonna do is I'm gonna add a new prompt for our classification that model that we are using and then I'm calling that classification prompt. So if you look at this prompt, it's very, very simple. Is this message related to Kubernitis system troubleshooting technical issues or requests for help? Then we pass the message. Then we're replacing this variable right here and I'm telling the model, please just respond with yes or no. Remember that we pay by the amount of tokens that gets generated. So if we limited the amount of tokens that the model can generate and still doing a smart classification. Should be good enough and should be fast, right? This is exactly what we're gonna validate. So we have added the classification prompt. Now I'm gonna go over to agent orchestrator and then adding some imports here. Let me just remove that import from number line 5 because we don't need that anymore. So Jason Boltle tree and the most important thing before invocation even sores itself already has the capabilities. Of intercepting messages before the message actually hits the agent. So what we are gonna do now is we're gonna implement a method that it's able to do a smart classification using Nova Micro before we send the message to the agent itself. So we don't even pay for the cloud 3.7 model that we're using on the agent. So this is what we call before invocation even. So before invocate the agent, please run this method right here. What I'm gonna do now as well is I'm gonna replace the construction method here with the one updated. Uh, with the smart classification that we did, so the only thing that is different than the other one is we are instantiating Bootle3 client just to be able to talk to Bedrock and do the Nova smart classification. We are the agent didn't change, and then here line 41 is the most important thing. So agent hooks at callback before invocation and then the method that I want to call, the method that I want to use to validate my message. And I'm calling that method that method callback message validator because it's very straightforward name to my method. What we're gonna do now is, as you can see, we don't have any method called callback message validator any function like that. So what we're gonna do now is we're gonna create that function and also the Nova classification function function. We're gonna go over, uh, those methods real quick so we can see what we are doing. So callback message validator. That's the method that's gonna be called once we send a message to our agent, then we're gonna trigger classify with Nova method passing the message, the less user message, and if you go to classify with Nova, this is where the trick is happening, right? So first we have the classification prompt. We are replacing the message that was a variable before with the current query that we're sending to the agent and then here is the magic. So max tokens 10 because he hasn't no. are less than 10 tokens. So because we're limiting the amount of tokens, we are able to get a really fast response and because we're using Nova micro, we can get that even faster. It's fast. So, so the idea here, no, exactly, and, and the idea here is, yes, LLMs are nondeterministic. You can ask it the same question multiple times and get a different response, but there are ways you can get around this and and that's exactly what this is doing. It's, it's making that response more. Deterministic. Now we know that like 99.9% of the time I bet if you ask this question, it's gonna come back with either yes or no and it'll likely come back with that same response. So really just routing exactly the logic that that this goes through and of course less tokens means faster response. And since every Slack message in the channel will be sent here, it's gotta be fast. And imagine if we have a channel with 500 people and for every message we need to classify with cloud 2.7 sonnet. They're really expensive, so Nova Micro here could be one another model that we tested was, uh, on top, a small language model on top of CPU. We run that as well on our, uh, uh, local comp like laptop. It worked, of course. Nova Micro is a little bit better, but I mean if you have good enough CPUs, you can also try to do it small language models on top of CPU if you don't wanna use. Use like novel micro and anything but going back to the code return answer so if the answer that the model returns is yes we should return true if it's different than yes then we're gonna return false and just one thing that I want to show to you folks right here, line 53 message classification we're gonna look for that information once uh we test the new implementation. That we have done, but before we do that, we also need to remove that dumb method that we created before. So should respond and then this one you're not using anymore, so we are not using the keywords static keywords analysis anymore. Now we are gonna rely 100% on Nova Micro with the before invocation even. That we have just defined it and let's see if it works and I actually like that better that we're using this before agent invocation rather than this manually implemented callback approach you know again I've mentioned this before but the folks that Build strands really figured out all of the things you might need to do with an agent. So before the LLM invocation, do this custom bit maybe after you wanna do something to format the message in a certain way. These are all the kind of things that we wanted to show you what's possible, OK, so you're restarting the Python app no errors, no errors. That's a live demo as you can see it's completely live so if something breaks. It's my fault. Alright, so what version of EKS am I running? I'm gonna send the same query again to the model and hopefully if everything works, if we did it in the right way, we should be able to swap a smart classification and we should be able to see a response because EKS again was not on the keyword list. So let's send it again. Message received generating response message classification there you go. So if you look at this, that's the log that I told you and the content equals yes. So before we didn't reply because EKS was not there, but now we are replying because Nova decided to say yes because it thinks that this query is related to system troubleshooting, Cer disinformation and everything. So now we did the classification and we should be able to see a response. From uh the agent that we didn't see before because now we are doing that smart classification mechanism, right? So as you can see here before we didn't have the smart classification now we have the smart classification so let me just try to ask something random yeah we should do the negative use case right? just hey folks, hopefully it works. Hey folks, what time we are going to have lunch today. And then if we pray. There we go. No, there we go. All right, so, and you saw how quick that came back too. Yes, it's an API call, but this, this model runs that fast where, you know, we, and, and if we looked at the cost of it, it's, it's one token or maybe, maybe a few more for the processing, but essentially, yeah. Output token one exactly output token one. So I mean that's how much you're paying for every execution again if you don't wanna use bed rock if you wanna use your small language model. You can implement that same architecture that we have done and as I said, I think these trans folks, they have thought about every single use case that we could do. We were doing the smart classification in a different way and Cy approached me and say, I think we already have that before the callback handler that you can use and then it was much easier to implement actually. But now before we move any forward, because if you look at the summary of a recession, there's an important concept that we talk about, righty. Yes this is uh something critical that you know and I think this is gonna be the crux of what makes these agents so powerful. And it's this concept of tribal knowledge and. Uh, you know, I, I kind of, I, I see this term being coined, you know, of course the context of tribes and software engineering has been around for a while. I kind of credit that to Spotify and the Spotify model of tribes and that kind of thing, but essentially it's this idea that the knowledge that's in a company, the knowledge is, it's, it's in the hearts and minds of the people that work there, and these people are talking over email, they're talking over Slack, and there's actually really interesting case here, uh, uh, an example I wanna bring up. Um, have you, have you folks heard of the Voyager One probe, the NASA? Uh, they, they sent it out in the 70s. I'm seeing a few folks nodding your heads. Yeah. Last year that Voyager probe had an issue. The memory chip failed and it wasn't submitting data back to ground control in Houston. And, and to solve it, we have a problem, yes, we absolutely do. And so the memory chip failed and. You know, to solve this problem, they had to bring up documents from the 70s. They actually went back and pulled out cardboard boxes full of old documentation and here's the crazy thing, they pulled actual NASA engineers out of retirement to consult with to solve the problem. And essentially with the minds of the people along with the documentation they were able to bypass that memory chip, solve the issue, and, and get data routing back into uh the their ground control and all was good right? but that's a concept that we know and we we we're all familiar with it's, you know, even when you have perfect documentation well there really is no such thing as perfect docs, but even when you have really deep documentation. And you have the folks that are working there, you know, you, you really have this concept of tribal knowledge that should be stored and it should be present in the places that your developers are already operating and right now for a lot of our developers it's Slack or Teams or wherever you're conversing and this is something that we wanted to build into the agent. Any time that an engineer talks about a specific design choice they made, what may be a version of the image, a tip, or like you should, how much you should define for memory and CPU, we know that it's not really easy to define that. Exactly and, and, and, and the thing is you shouldn't have your, you know, your superstars, right, your elite SREs constantly being bombarded with pings of hey Frank, like what was the version of that image you were using like and look, we, we want all of us to be as efficient as possible in the workplace and so when these conversations do happen, let's store them, let's store them in some sort of knowledge database that can be accessed when that question. Uh, you know, inevitably comes up 3 months down the line or in NASA's case 50 years down the line, right? So we, we wanna store this type of tribal knowledge somewhere and to do so we're gonna expand our architecture a bit. So this is what we had before and to add tribal knowledge we're gonna leverage an all new capability that we announced back in July as 3 vectors. So if you are thinking why they are using two agents layered on top of one another. Now you get a response because we're integrating multiple agents and if we have. One out those use cases in a single agent, trust me I've tried it, it would took much more time to respond in the right way and hallucinate would be a problem as well. Yeah, up until now that orchestrator agent was just taking the message figuring out if it's relevant, and then passing it to the specialist but now we actually have kind of a branching approach of first we have to uh think about is that message. Is it actually for troubleshooting or is this just a tip that we should store away for later use, right? And the cool thing about this is the longer it's implemented in your engineers Slack channels and wherever this agent is listening, the smarter it gets. It's storing these, these solutions for future use, uh, essentially any time there's some information being shared in Slack. Automatically take it and store it in vectors for a future, uh, recall. Oh, and by the way, vectors again we talked about this quickly before, but it's a type of database that works nicely with LLMs of course LLMs don't think in terms of tokens. They work with embeddings and number matrices and transformers and so vectors are a very efficient way of storing. Uh, human readable text, right, so you take a bunch of human readable text, you encode it into embeddings, and then when the LLM needs to scan through that data later, it's a lot faster to go through a vector database. But then it's able to kind of reverse engineer that back into human readable text and, and give it back to the end user. So this is how we're gonna expand the, the architecture and let me pass it back to you, Lucas, and we have not, and one thing that I just wanna add on what you said side. It's not either an agent or either an MCP. It's more a combination of different tools and capabilities to empower the agent that you can use. So we're using MCP for the live data, right? So the data that we need in that time that is happening right now in my cluster, then we use MCP to do it. But maybe some tips, run books and everything, we can still rely on RAG. We don't need to use MCP for that. We could, but in this case we're using RAG on top of another agent, so it's a combination of everything. It's not like if I use them. CPR agents, I'm not gonna use rag anymore. I think it's what is best for your use case and like every other essay probably would say it depends on the use case. Yeah, so let's try to implement the, the, the memory agent right now. Hopefully it's gonna work. So the first thing, yeah, the first thing that we're gonna do is actually Psy told us that we're using another agent. So we created that memory agent to be a standalone agent because other agents could rely on that memory agent as well. So I've created a Cuber troubleshooting chatbot maybe besides creating a database analytics troubleshooting chatbot, but we are saving and storing the information maybe in the same place and we can reuse those REM books across multiple agents. So the agent that we're gonna be building now has a server embedded that's powered by strands, and then we're gonna integrate that agent server with our current structure that we have orchestrator plus plus specialist using A2A. And for those who don't know, A2A is just a way to communicate. With different agents, JZone over HTP essentially it's nothing more than that, but agent to agent, agent to agent, that's it really easy to understand. So, so Lucas, I gotta say, man, as you're going through that, it sounds a whole lot like the 12 factor app microservices spiel that we heard what, like 1213 years ago now in the early 2010s of why we break monoliths into microservices. It's really the same. It's, it's kind of this revolution, um, it's a cycle of, of, of programming and. And it's the same reason why we're breaking our agents apart because look, the memory agent has its own purpose that might be used outside of the context of the Slackbot just like a microservice, right? What is a microservice? Something that should do that use case in a very good way and then if you wanna add other use cases on top of that, then maybe we start need to start to think about creating a different microservice. So for the agent is exactly the same thing. The agent should should have a purpose. I, we try it to have everything in a single agent. The response was really really awful. Once we break it down and then we use the agent as tools pattern with the orchestrator agent, we got a much better uh response and performance on top of the agents that we are developing so specialist agents, that's what I. I wanna say so I just have pasted the code right here. I'm gonna go over the code that we are building again. It's yet another agent, so it's just more of that other code that we have, uh, running. So we have the construction method we're declaring some variables just we are initializing bothto3 client to talk to our S3 as. database and then we are defining assistant prompt. So essentially the assistant prompt prompt is what you are giving to the agent. You should behave like that. You are an agent that behaves like that. So you are a Cuber needs troubleshooting memory specialist and then your role is store solutions and retrieve solutions. After that it's all like some fixes and send don't like use mark now and everything but to me. Uh, tools that we have is to store and retrieve and then using strengths again we are defining the agent we are defining the model that we wanna use. Lucas, I just wanna 0.1 quick thing out that you look at number 3 and some of you might be thinking only do what you do and nothing more. It's, it's, it's very direct. You might even consider it to be rude, but we've, we've seen, uh, official research papers released on when you, when you start using words like. Please or asking nicely for agents, they actually do end up hallucinating more often and not retrieving solutions and, and responding in the right way. So and if you say please, you're gonna pay even more for the tokens that were sending to the model. So exactly. So you do wanna be very direct with these models and, and they behave better when you give them those, those tight, uh, bound lines. Try to imagine that is a co-work that you don't like and be very. Don't do that. Don't do that. Alright, so the agent bedrock model ID, 2 tools. I'm not gonna go over all those tools, but essentially it's 2 tools only, but I'm not gonna go over the code because I don't have a lot of time. So one tool is to store solution. What it does store solutions in S3 vector database, and the other two is to retrieve a solution from vector. Database based on a query. Just question, do we have to do the conversion into embeddings, or does S3 do that for us? How does that work? About to say that. So S3 will not embed the data for you. You're gonna need to embed the data. So essentially embedding is just make the data in a way that the model can understand and interpret the data to respond back to you. So for S3 vectors it's just a vector database we still need to do the embedding. So in this case we're using Amazon Titan uh on top of Bedrock to do the embeddings and once we embed we cannot de-embed the embedding data. Right, so what we need to do is we need to embed the query that we're using to retrieve. So in this case the store, let's go to retrieve, we need to embed the data that we're using to retrieve there we go and then once we embed the data that we're using to retrieve, we can search for the embeddings because we cannot de-embed embedded query so we need to embed using the same model and because we're using the same model you can use that chunk of data to search for similarities in our vector database. That's exactly right. And, and it, and it, it essentially we talked about earlier, you don't wanna store the actual human readable text because it takes a lot longer to scan through it and find relevant data. It's just faster when you're embedding. And, and Lucas, how many LLMs are we up to at this point? Is that our 4th, or I think that might be the 5th. We're using Sonnet. We're using Novo Micro. We used a different version of Sonnet for the memory for embedding and Titan. For 4 different models exactly and, and look, I did like this is how LLMs are meant to be used. You pick the right one for the tool and it's more efficient and every agent that we had there could also use a different model. We're just using the same model for the purpose of I just want to export one single environment variable, but we could use different models for different agents with different purposes. So for instance, cloud Haiku is much faster than cloud Sonet 3.7. But it's not as smart as Cloud So at 3.7, so we need to do those balances and try to pick the best one for our use case. But in this case we have the memory server again. We have two tools one, store and the other one retrieve. And now what we're, what we're gonna do is we're gonna launch an agent server, but before we do that I'm gonna actually try to use those tools manually. So what we're gonna do is I'm gonna get the variable that I have here from a class. I'm gonna call agent and I'm gonna say something like. So this just invokes the agent as if you know if imagine the orchestrator agent was talking to the memory agent we're just simulating that for the sake of testing and so we're calling the agent directly and yeah OK so we're we're doing a real example so Lucas is copying over uh a real model recommendation that we have for the node exporter image. So what I'm saying is anyone deploying monitoring. Agent should use this image if you watch at the hook of our presentation, you probably know where we're going with that information right? so what we are doing now is we're triggering it manually first so hopefully this query right here is gonna trigger a store and then we want to create another query. Another agent invocation to retrieve the solution. So what I'm gonna say here is what node exporter image should I use? And this runs synchronously, so one after the other that'll give it time to, you know, store the solution. Yeah, so essentially the first one we hope. We're not telling which tool to use, but because we are using an agent, it should use the store and the other one should use the retrieve. So what we're gonna do now is I'm gonna go to the folder where we have that agent created there you go so we have the memory agent server. And you can already notice the difference here before we were just restarting that one Python application which had the orchestrator and the the specialist agent. Now we have a separate memory agent that lives separately so it is its own microservice. Lucas, you came up with a word for this earlier micro agent, micro agent. You heard it here first, folks. Uh, and so look, it, by the way, we do give you helm charts to deploy all of this and all the sample code as well. We'll share with you at the end so you could deploy this in a Kubernetes cluster so you're not doing it manually locally. I think it's just for the sake of the demo we're doing now, we are doing it locally because it's a. code session but it's all abstracting to helm chard. So if you just wanna take the shot and deploy this architecture, you can we're gonna give access uh to the sample code for you at the end. But going back to what we are doing, let's try to CSI if the tools are gonna be able to be executed in the right way. So we are hoping to get store and then retrieve the node exporter chip. So I just triggered the memory agent and you commented that out so that the server isn't starting we're just calling. Agent synchronously it's just a test, yeah, so the first two store solution key information problem team is using consistent corrected image that's the image that you should use, uh, and then for the second two to retrieve the information there you go. So the develop team recommendation is that one that is the one that we added into the code. So now we have that information in our S3 vector database. So what I'm gonna do now. I'm gonna go to a dashboard that of course was completely generated by AI and then I'm gonna refresh it and then we should be able to see another solution. That dashboard is just seeing is just querying S3 to check the solutions we have available in S3, but as you can see we have a total of one solution in that particular bucket if I search here for no exporter. We should be able to see the solution that we just persisted. So since using consistent and correct image for node exporter in Cuber needs monitoring deployments, that's the image that you should use. So we have that in the vector database right now because we tested locally our, uh, memory agents. So what we're gonna do now is we're gonna remove those local executions that we have just done and then we're gonna now, yes, uh, start the A2A server so. If you look at here we have the H2A server that is essentially a library that we have imported from strands multi-agent capabilities. We're passing the agent that we want to run as a server and then we do a serve behind the scenes is gonna start a uh unicorn process. So if you do Python memory agents server.pi now we have uh agent running as a server. So if it's running as a server you can expose to a load balancer and now you can use that agent with your other agent so you don't need to replicate that memory agent code again and again and again. So that's it for now for the memory agent server. Now what we're gonna do is we need to make the orchestrator agent communicate to the memory agent server. Right, and for that we're using H2A. So what we're gonna do now is we're gonna implement that H2A communication on top of our agent orchestrator and remember for the agent orchestrator we are running the agent as tools patterns so we have an orchestrator agent that is calling other agent as tools. So if you can see here we have just a troubleshooting to these tools that will trigger. The cubit is a specialist agent. What we're gonna do now is we're gonna create the memory agent tool that will talk to the other agent running its own server using H2A. So the first thing that we need to do is we need to import the library that we run, so H2A client tool provider, and this way we are able to use the other agent tools in our own agent without replicating those tools in our code. So this is the first thing that we need to do. Now another thing that we need to do is actually create a tool since we're using the agent as tools pattern from strands, we need to also create a tool to talk to our memory agent. So I'm gonna copy and paste the method that we have created here. And as you can see I have a decorator here. So this is particular for strands. If you have the decorator on top of the method, you can use that method as tools in your agent, right? So that's the way that the method is, is able to be identified as a tool. But most important thing, A2A client tool provider that comes from strand and then I'm passing the memory agent URL. If you can see here port 9000, that's exactly the same port that we have here for the memory agent running as a server. So we already have configured that variable. And then what I'm doing is I'm creating that agent here and this agent is just an interface so we are using this agent to discover other agents available in our environment. In this case we just gave one single agent but if you look at here we have a list we could pass multiple different agents and our agent would figure out which agent it should call how many times I said agent right now. Like I'm gonna say it a couple more times too because I think it might be worth really quickly I'm gonna switch here to the strands agent architecture just to show you what we did, right? So we have that orchestrator agent that's that big, big box over here and it kind of is able to discover all the other tools it's able to work with. So this external tools slash AWS that in itself can be additional agents. That in itself is the kind of swarm we're building here. Strands likes to call it swarms, but essentially it's all of these different agents that are discovering each other and only talking to them when they realize you need it. And so I think that's really the idea here is you can have all of these different tools the MCP server that we deployed, um, this memory agent, and then we have this actual Kubernete specialist agent and realistically it's, it's hard for us to even code. Like an if else chain of when which one should talk to the other, so we offload that responsibility in itself to an agent call shift to left, remove from NI analysis, and sent to the agent. Exactly, exactly. You got it. All right, so let's, let's get back to it and see how you did it. All right, so that's the memory agent provider. That's my tool that's gonna talk to our agent server. Last thing that we need to do is we need to add that to. And you were saying earlier it's the agent as tool kind of paradigm that's essentially what we did. We took an agent and we threw it in there as another tool just like we did with MCP and the troubleshooting stuff we just threw it in there, yeah, so if you look. At strands that's actually a pattern agent as tools that's the pattern that we're following and then we are adding Hway on top of that as well so that's essentially what we are doing we have an orchestrator agent that is responsible to route requests to other specialized agents. So now let's go back. I have added already the mem memory agent provider here as a tool so the last thing that we need to do is we need to change the system prompt so let's go to the orchestrator system prompt and then if you look at the system prompt we're not. Saying anything about memory or vectors or whatever we're just saying like we have access to that community specialist agent to troubleshoot that's the one you have go for it now what we are doing is we're gonna replace that orchestrator system prompt with something more. Aligned to the memory agent that we have created and for every agent we have a system prompt so in this case I'm saying hey. Always check memory first before doing any troubleshooting. If you are not able to find memory, then go and troubleshoot. If the information is good enough, then persist that information for me after you are done with the troubleshootings. And again, we can control that. We can split responsibilities here. We can make the agent fix, of course we don't want to do that in production. If we're talking about production, we can make the agent open a pull request for a GitOps repository. And we already have that if you want to see that come back to next year code session but for this year code session we are doing an orchestrator agent and then just passing the prompt here. So now I'm gonna do and hopefully everything is everything will work, I'm gonna trigger our main agent that's gonna be able to talk to the other agent running as a server. So I see now you've started them up as two separate processes, one for that memory agent, one for the actual main. Orchestrator plus specialist and so think about this if you were to deploy this in Kubernetes for example you could deploy each of these as individual Python pods and also scale them, uh, independently of one another as well. For example, the memory agent has to respond every single time a message is is pasted in Slack, uh, whereas the troubleshooting agent doesn't need to respond as often. So you could really scale in response to demand and load. Yeah, so let's see if it actually worked. So what I'm gonna say is I'm just gonna try to trigger the agent to persist a tip and advice that some of our DevOps engineers have done. So what I'm gonna say is. If you don't. Know how much CPU and memory define for your limits. Always define the same as requests and and that that might be true, right? So that's a tip that I'm giving and I'm I'm gonna say tip from DevOps squad. So hopefully we're gonna be able to classify this message. The agent should respond to this message because it's related to troubleshooting. OK, as you can see, we didn't fix the classification, did we? So the classification that's right. So the classification should know, um, right now if you look at the prompt for the classification model, let's take a look at it really quickly. If it's related to Kubernete's system troubleshooting technical issues, I bet if we updated this, and let's make it a little smarter, uh, if you have any tips to store, right, do that, let's do it. We, we have, we haven't tested this. Let's try. Let's, if it's, let's see if it, I, I think it's gonna work. Let's see. Let's see, is this message related to Kubernetes or request for help or any tips. Is it good enough? Pretty good. If it doesn't work, I'm gonna tag the agent that should cover all the tips, but you know, uh, if we wanted to get more precise in the future for troubleshooting tips or something like that, we could add that for for now we're doing it live. We wanna make sure that classification responds yes for a message like that. If it doesn't respond yes, then I can tag the agent and you can be sure that this is a live demo. Right, so let's wait for the community specialist agent and orchestrator agent to start. There you go now I'm gonna send exactly the same message, and if we do it in the right way, we should be able to see as one demo gods one time. Yes, there we go. See, so here's the thing again, it is nondeterministic by nature. This, that's how LLMs operate. But you saw with just that little bit of prodding of like changing that classification prompt. We were able to make it respond better for something like this and so we're not saying you're gonna find the perfect prompt that fits your use case immediately like of course for example OpenAI has been refining their prompts and prompt engineering their their base models for years now uh you're gonna want to keep optimizing and with that small optimization the classification worked and then if you can see the agent they talk to each other so first we send. A message to discover the agents that we have using the agent card that is available in our agent. So if you look at here, we use a HA list discovered agents and then we discovered a memory agent and then with this we also discovered the tools that the agent that we are talking to have and now we are sending a message. To the right tool we send a message to store solution in our other agent running as a server and if you can look at here we have one tool that was called on the memory agent that was to start a solution related to uh QOS guaranteed essentially the same amount of CPU and memory resources and then there you go you have a response here. OK, and by the way, I think in the real scenario it wouldn't even respond. The bot would just sit there silently listening for any tip and just storing it in the database. But for the sake of the demo we have it respond saying, hey, this is a good tip. We'll store it in the database. And if you're not trusting me, we had one solution before. Let's refresh it. Now we have a total of 2 solutions. Let's search for CPU limits. And see if we have that solution there available already. And so this is going directly to the three vectors database, right? Just looking at what solutions we have. Yeah, absolutely. So that's the thing I'm doing a rack here in the same way that we are doing for the agent. So we are like just retrieving embedding the query and then retrieving the solution. So that's the solution with the shortest. Distance. So essentially that's the solution that means the most for our use case right here. So if you can see, please define the same amount of CPU and memory, uh, that you have for your servers. That's great. It's like a confidence rating, right? It's like the, it's like the inverse of a confidence rating. So smaller distance means more confident. And I think you saw that second solution was at a distance of like 0.9, that node exporter tip, not very relevant for the the question that was asked about CPU limits. Yeah, so last thing that I'm gonna try to do, I know that we have just 5 minutes, so I'm gonna. Try to do now is I'm gonna deploy some failing pods and remember that first demo that we saw before fixing the memory uh the monitoring agent that's essentially what we're gonna try to do now so let me deploy the failing pods to our cluster. We should be able to see the uh demo app name space here and you still have the Python uh main orchestrator running right? or did we kill that? No, I killed that but I'm gonna, I'm gonna make it run again OK but essentially we have the pods not running. Let me just trigger the agent again agentic troubleshooting. And then I'm gonna trigger the agent again. There you go. So now if you look at this, we have a lot of issues going on. We have the monitoring agent not working. We have the back end API restarting with out of memory killed front end has some issues as well. And then we have a radis here. Let's try to debug and fix the monitoring agent using the recommended image that we have stored in the solutions. So before we do that, let's see if the agent is has started already. Both apps running there you go. So what I'm gonna say is hey folks, my monitoring. Agent on Zimo at namespace. It's not running. You know, there's usually I'm a developer. I don't know why why it's not running and, and you know there's usually some poor SRE that's always in this channel having to respond to these kind of messages, this guy who's probably overloaded with too many messages, maybe it's some of you in this room can relate to this yourself, uh, hopefully the agent can pick this up and not, not one of you, right. Love the emojis. Let's see. I just try to see if it's gonna fail or not, but there you go. Yes, so the first thing that it's gonna do because our system prompt, it's gonna search in our vector database to see if there is any solution related to this. Hopefully we're gonna get the right image because that's related to node exporter. There it goes. So the first image, the first solution, the, the closest one to what we search is a solution number one, so it was able to get in the proper way, but it's not enough information. So what the agent is doing is actually doing the troubleshooting by itself with that information in mind. If we had more information, it would use it. So I actually love this question you asked because it just demonstrated all the features we developed on the left side. It found the two memory solutions. It realized this isn't relevant. Then it went back to the original specialist agent, talks to MCP to get more information about what's failing. MCP gives it all the application logs and the pod logs, and you can see it all thinking, all orchestrated together and ideally figures out what the original problem is. Yeah, so right now it's just storing the solution because it was able to find. In the root cause. So if anyone deploys that agent again with that same root cause, we don't need to do the troubleshooting again. We can go straight to the issue. So our, that's, that's actually really cool. So it, it found the solution and decided to store it in our solutions database. Yeah, we didn't even have to code that in there. It just, it realized it and decided, let's just store this for future, for future use. And if, and if you look at this. We didn't got the response that we were expecting to get so we let's try to ask that again what is going on? What is the issue? You know we've seen this before when it when it stores the solution in the database it considers itself done. It's like found the solution, stored it, we don't have to respond but since we have, we still have the context on the threads and everything it responded super fast so essentially the root causes we are using the image non. Existing monitoring agent. latest, of course this image doesn't exist. So would you like me to guide you through the steps to fix the issue? No, I would like you to fix the issue. So I'm gonna say Kate's magic pot, please, because you said that you don't want me to edit, please. I'm gonna say please because I still, I watch a Terminator 2, so I'm gonna say please. So please, can you fix the monitoring. Agent for me using the recommended dev ops oops DevOps image. OK, wildest things, Lucas, because we have 15 seconds left, folks. I wanna share with you some resources really quickly. So scan this QR code. It's gonna take you to a GitHub of all of the container sessions that we're doing at Reinvent. So find our session here. So, uh, with CNS 421 find our session. And we'll have the sample code. We'll have all of the, the code that we showed today linked as well. It's all on GitHub for you to go through. We really appreciate you spending your time here with us today. So scan that QR. You'll find all of the relevant resources there, and we would love for you to leave us some feedback in the app as well. Thank you so much. Hold on, hold on, hold on, hold on, hold on, hold on, hold on. Can you go back to the demo? There it goes. So now we have the monitoring agent running, as you can see right there. And then if we go to alert managers, there we go, resolve it the same day we, we deserve it. Thank you. Thank you so much.