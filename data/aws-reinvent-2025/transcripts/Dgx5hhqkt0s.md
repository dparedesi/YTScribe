---
video_id: Dgx5hhqkt0s
video_url: https://www.youtube.com/watch?v=Dgx5hhqkt0s
is_generated: False
is_translatable: True
summary: "This session features the Chief Product Officer of Reltio presenting their \"Data Unification Platform\" as a foundational layer for successful Agentic AI. He begins by defining Reltio's core mission: unifying fragmented data from various sources—such as SAS products, legacy databases, and e-commerce systems—into a single, authoritative \"trusted profile\" for key business entities. Moving beyond traditional Master Data Management (MDM), Reltio constructs a dynamic, real-time relationship graph that captures a complete 360-degree view. This view integrates not just static attributes like addresses, but also complex behavioral data such as purchase history and web clickstreams. This pre-organized graph structure is pitched as the critical enabler for specific AI agents, allowing them to query vast amounts of connected data instantly without needing to perform expensive and error-prone \"stitching\" operations during execution.

Real-world examples illustrate the versatility of this approach. CarMax utilizes Reltio to maintain comprehensive views of vehicles, prospects, and salespeople, directly facilitating their business guarantee of a 15-minute transaction time. Similarly, Warner Brothers uses the platform to manage intellectual property, tracking characters like Daffy Duck across detailed webs of movies, merchandise, and licensing agreements. The efficiency of having this data pre-structured is contrasted with the traditional, labor-intensive approach where data stewards spend significant time manually resolving duplicates and conflicts. Reltio aims to reduce this manual effort by orders of magnitude through automation, mapping data quality directly to business outcomes.

The presentation includes two key demonstrations showing different classes of agents. The first showcases a \"Data Management Agent\" that autonomously identifies and resolves duplicate records, such as conflicting entries for \"Solar Turbines.\" The agent converses with a user to confirm duplications, enriches the internal data by cross-referencing live internet sources, and proactively fixes the issue—shrinking a task that typically takes 30-40 minutes down to under one minute. The second demo features a \"Customer Application Agent\" designed for hyper-personalization. It analyzes a customer named Sarah Anne, synthesizing over 70 data points from 11 sources to provide highly specific product recommendations. Crucially, the agent explains its rationale (e.g., citing her recent enthusiast behavior) and demonstrates advanced reasoning by pivoting to household-level recommendations, such as suggesting a kit compliant with her daughter's specific consent preferences.

The underlying architecture leverages AWS extensively, specifically Amazon Bedrock Agent Core and the Strands SDK. This integration provides managed runtime, security, and sophisticated memory management, allowing Reltio to focus on building value-added applications rather than infrastructure. The speaker emphasizes that this \"data-first\" approach is vital for longevity; it ensures that as underlying AI models evolve rapidly (from \"Model X\" to \"Model X+1\"), the foundational trusted data remains stable and scalable, guaranteeing consistent performance and \"no regressions\" for large enterprise clients."
keywords: Data Unification, Master Data Management, Reltio, 360-Degree View, Entity Resolution
---

Thank you, thank you. Hey everybody, uh, I noticed that I am now a chief product office. The R is missing. Uh, good to have everybody here. Uh, we're gonna, the game plan for the next 20 minutes is I'm gonna talk about Reltio, the company. We're gonna talk about our product. Uh, we're gonna talk about all the amazing movement that we're seeing by building agents on top of data that is, that is within RelTO. So what is RelTO? Let's start with that, um. RelTO is a data unification platform. The idea behind data unification is that for any company of uh a reasonable size they will have plenty of data about their customers, their suppliers, uh, different, uh, uh, products that they sell, and this data ends up being in different parts of their organization whether it's. In different SAS products whether it's a different databases, um, you know, you have product data in a warehousing system you have, uh, product data in an e-commerce system and so on. So how do you bring all of this together, unify this data to produce a single view of authoritative view of any of these nouns that matter for your system. So what Reltio does is the uh it brings together this multi-source data and gives you a trusted single view on which you can build your business operations at the highest level. Yeah, our approach has been uh to we started with a space that is called master data management. Raise your hands if you know what master data management is. uh, OK, fantastic. So the idea of master data management was to bring together multiple different records and really, uh, manufacture a golden record out of that. And this definition of golden record was fairly narrow, right? You were talking about, uh, the definition of uh uh an individual, their, where they live, their contact details, and so on. And as the space has evolved, this idea has become more less of a golden record and much more of a trusted profile which are these sort of very rich descriptions of customers or suppliers and so on which have detailed amount of information drawn from many, many different sources and this graph representation of these entities is maintained up to the second and it's available for consumption. Via an API or without via the real-time systems, um, uh, all the time, right, and so it's a it's an evergreen data foundation that a lot of other processes can be built on. Uh, what, where Reltoio sort of the way we approached it at Reltio is to think of all of this as a relationship graph, um, and different entity types, their relationships, and including a 360 view of all the behaviors that, uh, that might be associated with these entities. So in simple terms, if you're talking about a customer, uh, a customer has an address, they have, uh, they also have a purchase history with you, they have, uh, a click stream which they're used to, or, or they've browsed your. Side and therefore they have a click stream associated with that and so on so this complete 360 view in a way that is connected every entity is connected to everything else that really is the heart of what RealTO presents now, um, what the story you can also already see it coming is a graph is a very natural, uh uh uh data structure for large language models to interact with, right? So, um, on the left hand side you see. Many different data sources that can be used to bring data into RealTO, but the interesting bit is on the, on, on your right hand side where you see the ability to consume this graph either through applications or or by sending it into different data warehouses. But most interestingly our customers are building agents on top of this data and that's really been an amazing experience to watch. I'll give you two examples, um, CarMax. The biggest retailer of used cars in the United States, what they use Reltio for is that they, they've organized the data into 3 360s. Why? Well, at the highest level, the promise they have to their customers is somebody can walk into a CarMax, either buy or sell or buy and sell, and leave within 15 minutes. So they have a transaction business guarantee of 15 minutes. How do they do that? Well, to achieve that, they have to organize. By the way, they see a typical vehicle in the United States 3 times in the vehicle's lifetime. Yeah, it's traded multiple times. So it makes sense for them to have a 360 view of every vehicle they possibly can so that they know as much about that vehicle as possible over time. They also have a 360 view of prospects or customers that they've dealt with in the past and they have a 360 degree view of their employees and for them and a transaction is the combination of uh uh a vehicle, a prospect or customer, and uh a salesperson who's who's made the sale because their goal is not, not just. That the customer can walk out within 15 minutes, but also that they settle with the salesperson within that 15 minute window and the the the the way to achieve that business outcome literally is to organize the data that is friendly for that outcome, right? and that's, that's what's happening that's what they use Realio for now very, very different example, completely different industry. Uh, Warner Brothers, now what is a product for Warner Brothers? It's their intellectual property are all the characters, right? So if you're thinking about Looney Tunes, pick a character from Looney Tunes. That particular character has appeared in a certain number of movies, uh, or cartoons. They've appeared, uh, as merchandise through different suppliers. They've, they've, they, they, they've licensing agreements, right? So that, what that is what constitutes a 360 for Daffy Duck or, uh, you know, uh, uh, uh, uh, Roadrunner in this case, right? And having that data be ready to be then explored via large language models, that is what Warner Brothers uses us for. So two very different examples, but the point is the way the data is organized reduces the effort in actually ingesting the data and organizing the data, and it makes it, uh, the consumption of the data is much more front and center, yeah, so if we. Think about this space bringing together data from multiple different sources and assembling it together. This problem has been a problem ever since we've had multiple SAS systems or multiple databases, right? This data fragmentation problem and at the core of what we're trying to solve for is we're going to say we were saying there's 2 or 3 or 4 records that all indicate or or they're all related to the same individual, let's say. How can we pick and choose and connect these records so that we have as much of a trusted, uh, view of this customer as possible, right? You're using first party data. You're also bringing in third party data to enrich and get the output of this process to be as high quality as possible, yeah, but. It's not all automated yet even in, you know, 2020, let's say 2024, 2023, there used to be, there, there was still a lot of manual effort being put in and if you think about the larger the company, the more the manual effort, um, in, in trying to make sure that the data stewards, that's the particular role, data stewards are spending a lot of effort, uh, making sure that the data is as high quality as possible. Now of course data quality itself is a very interesting uh topic of discussion because you can spend a lot of money getting data quality to the highest level possible, um, and the ROI on that it is really determined by what is the application of that particular data, right? So data quality has to be mapped to the business outcome that that quality is looking for and so how much effort you put into data quality is sort of the second piece that's uh. Uh, that's been sometimes highly manual and sometimes very expensive, yeah, and the third thing is complex data models as I described the space is about bringing together different data models or data from different data models and combining it together so that you have an authoritative view of that particular entity and there's complexity in being able to map all these together and typically that's been done from a manual point of view, right? So these are the real problems if you think about, you know, you say it's very easy to say if you. If you have your data organized well. Agents are going to behave well or trusted data leads to trusted AI, but the how part is left unsaid a lot of times, right? And, and this approach that I'm going to share with you really goes to the heart of that how it's a very pragmatic approach to how to get trusted agents, um, and so the goal that we set out on is really to take this manual effort in organizing this data down by an order of magnitude or 2 orders of magnitude, automate as much of that as possible. Um, so here's what I'm gonna walk you through. I'm gonna walk you through a couple of demos pretty quick here, uh, but just to orient you on our product set, so at the, at the base, uh, everything runs on a unified, uh, data cloud, relative data cloud that, uh, any of the products that run on top of the data cloud inherit, uh, the scalability and the, uh, security aspects that are built on top of AWS. The two products that we offered uh uh a year ago are are multi-domain MDM and our intelligent 360 products, and the way to think about it is that multi-domain MDM encapsulates everything that has to do with entity information and then if you add behaviors, if you wrap that around to build a true 360 in real time, that's the intelligent 360 product. And what we're gonna talk about today is really the agents that the agent flow, which is the uh layer on top of these two capabilities that delivers uh fully functional agents in two regards. The first agent, and I'm gonna show you a demo of both, the first type of agent class of agent is managed for managing data itself. So the data that's in the graph that exists within MDM or within Intelligent 360, how can we manage that autonomously? I'll show. You an example of that and the second example is once the data is managed in as much as pristine of a state as you can make it, let's put it to use, right? So how do you, um, build an application effectively or an agent that's, uh, partially doing what a CRM would do or, or a, a customer success system would do but do that as an application that's built on top of data and this, this is sort of the data first application movement, uh, that's very much in play right now, yeah. So let's look at the first scenario. In this case, um, we are trying to solve for data management we're trying to solve for duplicate data and um. Here we go. So as you see, it's a conversational interface. There's really no mention of data, yeah, uh, uh, you know, anywhere in terms of, uh, the, the interface itself, and we're just asking a business question. We're saying, hey, we're seeing service delays in our top 100 segment, and we suspect that there is some data duplication going on. So the system at that point our agent is looking at the what does that top 100 mean and it's getting that definition from the underlying data platform and it's able to then propose an approach where it says um I found what that top 100 means for that each organization I'm gonna systematically look at why that duplication is happening yeah. Um, and it says it gives you a simple option you can pick, you know, as these, I'm sure everybody's very familiar with these sort of interfaces at this point, and it's identified that, um, there are two organizations that, that actually have some duplication in the data. Um, and then further it says, um, it proposes a couple of actions, uh, that are proposed for each one of those, and it'll it'll show us that for this particular organization Solar Turbines, there are, it goes attribute by attribute and says, OK, well I'm, I'm looking at two versions of, uh, information about this organization, and I can see that there are, there are some mismatches that are probably causing. Um, you know, for example, suboptimal shipping, like you're shipping to the wrong location and so on, um, and so we say, OK, we've, uh, by the way, it's also enriched this data by looking up the Internet, right? That's one of the requests we've always had from customers. There's such a rich. Uh, and, and ever updated source of data, the Internet, why can't that be brought in the data management, uh, platform, and so it's sort of done all of that. It's brought in this data and it's created this, this level of automation where if somebody had to go and look this up they would very easily take, you know, 35 to 40 minutes to go resolve this, uh, a data. Steward would very easily spend 30 minutes resolving this, and so this took less than 1 minute, right? So just the savings, savings in in terms of doing 100 of these a day, that's pretty enormous and of course, needless to say, the whole thing can be automated, right? Doesn't have to be conversational. Everything is API driven, so you can very easily wrap this up in a even next level of automation. Um, the next demo I'm gonna show you is around is, is more on that next level of building, being able to build applications given that that data is already organized in a proper manner. And in this demo what we're showing is there's customer data and there's product data and a call is coming in from a customer and our agent intends to um uh show the next best product recommendation or at least propose that to the agent that so that they can talk to the customer. of course there's no assumption. It's a human agent on this side of the phone or or or an AI, it doesn't really matter. The point is, can we produce a recommendation down to the personalization of one, right, to the, if given as much data as we have about the customer and their past behavior, can we actually build something that is very, very customized for them? So very similar to the, uh, to the previous demo we'll, we'll drop into uh the conversational interface, uh, and in this case the question we're asking is. Uh, uh, for a certain customer we provided a name Sarah Anne, and please provide three compelling product recommendations and base that on her profile and, uh, different relationships that we see in the system and different interactions that they've had with us, um, and so the system's looking for, um, searching for that individual and it finds a perfect match there and it's going through our MCP layer and talking to, talking to our API in this case. Um, and it says we found a unique individual. Here's some information about them. They're 46 year old married female from San Francisco, etc. um, and we're seeing from them that they have, they have purchased from us recently. There's a pattern there. They're actually an enthusiast. They interact with, um, our, uh, uh, uh, blog or some of our content, and here is, here is a set of recommendations that she would most likely be interested in. And you know, the thing on the left is is not uh interesting to me the thing on the right is more interesting because that is the why. Why is the agent recommending certain things? Yeah, uh, because at the end of the day we have to be able to defend our decisions to present something to a customer, right? Uh, so we go a step further, we say. Uh, this is great, but if you look at the household, perhaps she's interested in buying something for her household, somebody else, a gift for somebody else, not just something for herself. So what would you recommend in that case? And so, um, we look at sort of the, the next surrounding, uh, circle of, uh, uh, uh, level in the tree, and we find that the family consists of, uh, Michael, Olympia, and Sarah, and Olympia, who's the daughter, um, we have consent from her. She's expressed consent. Um, uh, uh, for new product launches, so we could potentially position something for Sarah in this case, right, and the system goes through and says, uh, for Olympia and Michael in this case there is a specific kit that we're recommending and then the rationale on the right hand side is why we're making that recommendation, yeah, um. It also, uh, it'll scroll here in a second, and to me what's interesting is, is also the suggestion to the agent who's having the conversation. There's a communication strategy, right? It, it is about how to present not just what to present, but how to present this, and that's based on a certain amount of data that's already. In the system behavior that we've seen in the past and we'll actually ask the agent we'll say how much did you consider how many, how much data did you consider how many data points did you consider while making these recommendations that it can, you know, it'll come up, come up with an answer and in this case I think it's, it's used 70+ data points from, um, multiple different 11 different sources and, and multiple interactions that it's had access to in the past and so what this shows you is. If you have data organized a certain way, then using agentic AI on top of that almost seems simple and magical. Conversely, think about what you would have to do if you're trying to create an agent that is. Delivering you this outcome, but your data is distributed in 5 different databases, multiple applications, right? You'd be stitching together all this information when you need it. And that becomes a very expensive operation. It becomes, uh, uh, a very, um, the outcomes become harder to predict because you're, you're on the other end there's a customer you're trying to solve that problem or present a, a proposal for the customer and your system is busy sort of assembling all this information together. Yeah, so that that to me is is the real power of a data unification system where you have a graph ready to be consumed at all times and you're not really spending any engineering resources once you've set up the graph it's, it's, uh, it's ready to go at all times. Um, so a little bit about how we built this, of course we are, uh, powered by AWS technology, so we, we were one of the earliest users of, um, Agent Core, and, um, and, and, and, um, the strands, it, uh, uh, the strands SDK. So, uh, the core of the agent is built out using strands, and, uh, for us, Agent Core as it has developed, it's become really this sort of super easy way to scale up and not have to worry about a lot of the non-functional aspects. Or uh you know have to worry about whether we'll have uh security taken care of or uh access to memory so we don't have to assemble memory and and you know, web access tooling and all that that kind of comes as a package and and we can focus on really the value that we're we're creating on top of that um uh the. In a sense, the model use doesn't really matter, uh, because all these models are so capable. I think as an ISV for us to create this, it, it, it really is about getting those nonfunctional aspects, uh, to be testable because today we're, if you're using Model X tomorrow it'll be Model X plus 1 and how to make sure that we're able to actually give you the same. Performance at scale because remember the customers, our customers are some of the largest companies in the world and so the ability to operate at scale and yet go from one version of a model to the next version of the model and guarantee no regressions, that's the sort of important thing that a solution like Agent uh core provides us, right? So we're very excited about some of the policy related announcements that that were made yesterday, um, and I'm really looking forward to incorporating some of that into the mix. Um, with that, uh, I've shown you a bunch of videos. Uh, if you want to see the real thing in action, we're at booth 1227. Uh, I'll be there for a little bit, happy to talk, but the Relto team is there, uh, and we'd love to take questions there. Really appreciate everybody's time. Look forward to seeing you.