---
video_id: Bw7H8PEiJrc
video_url: https://www.youtube.com/watch?v=Bw7H8PEiJrc
is_generated: False
is_translatable: True
---

So, um, welcome. Uh, this is, uh, Rahul Ghosh. I'm an applied scientist, uh, in the AWS GAI Innovation Center. I've been with Amazon for a little bit over 2 years now, and, uh, my day to day job is mostly working with our, uh, customers, AWS customers, and collaborating with them in solving some of their toughest challenges in bringing Gen AI to their. Production flow. Speaking of customers, we have our customer from Yahoo Finance, uh, the Deepin, who is going to, who is my co-speaker for today. Deepen. Hello everyone. Uh, just give me a thumbs up if you can hear me OK. Yeah, uh, so I'm Deen. I'm, I've been in the industry for over 17 years and I'm working with Yahoo Finance for the last 3 years. And my team at Yahoo Finance builds Gen AI and in general AI and data science powered features, and for the last almost a year we've been working with Gen AI Innovation Center, and we have Vidya, whose team we have been collaborating with. I'll hand it to Vidya next. Hi everyone, can you hear me? Just one, OK, yeah, good. Yeah, uh, my name is Vidya Sagar. I'm an Applied Sal manager in Generative Innovation Center. Um, Generative A Innovation Center is a program in AWS, uh, to idea for, to help the customers ideate and build, uh, GA solutions fast. So we are primarily a team of scientists, strategists, machine learning engineers, and partners across the globe where we have been working with thousands of AWS customers to build practical and innovative generative innovation solutions. So customers like DoorDash. Uh, BMW and, uh, various other club, I mean, with hundreds of customers we have deployed generative, uh, AI solutions in the last one year, primarily, uh, identifying new business problems, um, for the, for their, uh, use cases and building a practical and AI innovation, uh, solutions at scale, um, so that, that's our primary, uh, intro for all of us, and with that we'll have Rahul start the presentation. Thank you. All right. So, let's see what the uh session overview is. So, Just a moment. Let me. Yeah. So we are going to uh Yeah, so we're going to first, uh, explore how the uh enterprises are adopting multi-agent, uh, systems with Amazon Bedrock and the recently launched uh Bedrock Agent Core. Then Yahoo Finance is going to share their real world experience implementing such multi-agent systems, especially in the financial research and question answering applications, uh, working with, uh, uh, AWS services. We'll cover some key aspects including orchestrating and agents across such distributed environments that access multi-modal data from different sources, structured, unstructured data, and coordinating between these different agents and tool and API calling agents. And uh we'll also discuss managing the data across such diverse and heterogeneous sources and evaluating uh the uh performances of these multi-agent uh architectures. So the agenda is going to be like, I'll introduce some of the foundations. We'll first dive into the what are agent, multi-agent systems. We'll explore some key core concepts and principles that underpin these intelligent systems, especially in the context of deep research agents. Next, Vida is going to go through and take you in a deep dive into the AWS Bedrock agent core examining its architecture, its capabilities, and how specially it's unique value proposition is uh for uh financial research agents. And then finally we'll have Deepin who is going to embark on the practical Yahoo Finance uh journey where we'll see these uh concepts into action through real world applications and lessons learned. OK, so AI agents represent a tectonic shift in how we build, deploy, and interact with uh technology. So we define AI agents as autonomous systems that reason, plan, and, uh, complete tasks through iterative thinking and uh refinement. So agentic AI is a big step forward for LLMs, giving them through agency through tools, memory, and agentic loops, and with that they solve goal-oriented tasks using these components. So AI agents are transforming how we all work and live with technology. So what are the key technological advancements that are making these AI agents practical? Uh, so it's with today's, uh, these key technologies, it's no longer if AI agents are viable rather than how we can best, uh, how they can best succeed. So we see 4 key enablers to taking agentic AI from hype to reality. So first and foremost, the model reasoning capabilities. So these smarter foundation models with more advanced reasoning capabilities, are much better in planning and coordinating across different tools and APIs that are presented to them. The second is data and knowledge integration, so the ability to ground their insights in enterprise context and their proprietary data sets. The Third is the infrastructure and the agenttic protocols, so the scalable, secure foundations and protocols, you think model context protocol, the agent to agent communications to run and connect these agents safely. And finally we have the agent development tools with the wide variety of frameworks, sophisticated tools and patterns that are available right now. It's very easy to build build agents and deploy them. So we see excitement across the industries and functions as these uh enablers converge to unlock the practical AI agent uh implementations. So this slide illustrates the agentic AI maturity scale from the rules to reasoning. To set the stage, many of you are familiar with the rule-based automation where LLMs were called in a sequence in a pre-defined flow, uh, executing the task one after the other. Then in 2023, 2024 saw the boom of generative AI assistants with RAG capabilities and web search capabilities to answer questions from not only limited to their. Uh, to, to their, uh, uh, uh, pre-trained data, but you can access the more, uh, recent data data available out there as well. So I believe now we are in tier 3, that is the goal-driven, uh, AI agents that can independently execute, uh, tasks toward higher level objectives, uh, though they don't set the goals uh for, for themselves. There is a 4th tier of autonomy where. I don't believe we are there yet, but, but the agents can set the goals for themselves as well. However, with the recent, uh, we're seeing LLMs with substantially lower hallucination and more reasoning capabilities, so the future is bright, I would say. But in reality, most of our enterprise's customers are operating mostly between the 2nd 2nd and 3rd, where you have more rack-based web search-based services with more autonomous agents coming being put in the case. So a good question you can ask yourselves like where does your use case fit in this level of autonomy. So let's talk about some of the agentic uh fundamentals, uh, and the core uh agentic loop. So as I mentioned before, typically you can create an agent from an LLM by by attaching it with memory, uh, tools and goals and allowing it to reason. So the the flow of this reasoning can vary, but most prominently the dominant agentic reasoning pattern is called react reasoning, and then act. The loop starts with the user interaction. The agent receives the input and then decides what action to to take next, depending, depending if uh tools are needed to answer the question or this can be and and this loop can be single step or multi-step. So after each. Tool call or after each step, the LLM reasons about whether its previous action was enough to answer the user question or it needs further more tool calls before it can proceed or it should stop and directly answer the user's question. You can see the floor here. You can see that tools, memory, and goals feed into the LLM that have access to these external data sources, and it executes its steps under the observability and also the guards that are set by the developer. And then using all of this, it interacts with the user to answer their question. So I want to go a bit into the agentic reasoning strategies. So as I said before, the react is most prominent, where you have the the intertwined loops of thought, action, observation, and then so on. But there are other strategies that can be useful as well, and especially for financial applications and deep research applications. So RACT was introduced in around 2023. Combines reasoning and acting iteratively. Then you have the rev that is reasoning without observation, which generates complex execution plans up front before calling any tools and then executes that plan in parallel gathering data across these different tool calls and different APIs. For example, in financial research you have stock prices, earnings reports, market indicators simultaneously it can, it can execute those steps. And in a way it significantly speeds up the data gathering and this process in these comprehensive research workflows. Then finally you have the other pattern called reflexion that promotes self-reflection, iterative refinement, um, allowing the agents to evaluate the quality of their generated reports, uh, identify research gaps that are still there in their report, and then refine their conclusions. So in financial research or deep research agent implementations, all these three strategies beyond just the standard react are often often included. For example, uses parallel execution for efficient data gathering, reflections, self-reflection for quality assurance and maintaining the data brevity, and and also finally using the reacts adaptive reasoning for complex multi-step processes. So now that we have discussed the agentic reasoning patterns, uh, let's talk a bit about when single versus multi-agents are useful. So with today's LLMs, uh, single agents are capable, especially with the, with the latest models that we have, but the more, more we expand their scope, the more likely they are to make mistakes. So when you are providing an agent with too many tools, uh, this can lead to confusion between tool selection for the agent, right, especially when the tool definitions overlap. So not to mention that all of these tool definitions, their, their argument, uh, their, uh, dock strings and their API strings end up into the prompt that is sent to the agent, agent LLM, and this can end up in ballooning in size if you have too many tools. Further, in complex cases, in complex use cases, you will see improved performances when, uh, through agenttic specialization. That is like when you make the uh agents specialize in specific areas rather than being generalists. So this is the scenario where multi-agent especially helps where you can allow specific sub-agents to excel in their area of expertise, have a manageable tool set, and that leads to better overall performance. So when you do decide to go into a multi-agent, what are the different patterns that you have? So you are seeing the two most commonly used patterns here, that is the supervisor sub-agent pattern and the agent swamp. But there are other patterns as well, but the key difference between these two patents is in their control structure. So in the supervisor subagent, you have a supervisor that controls all the routing between your worker subagents, and they have the final decision making capability. So creating more of a hierarchical approach. A swamp pattern is more collective and collaborative, so that each agent in the swamp has awareness of the the message list of the other agents, and they can think about when to hand off the sessions to one of the collaborators. So this creates more of a distributed decision making process. So supervisor subagent tends to be more structured and predictable, but swamps offer some unique advantages as well. So the awareness of other agents in swamp can prevent repetitive tasks across the different agents if they have kind of similar tool sets or they are connected to similar data sources. So next, you, you can have hybrid patterns as well which can be useful. So for example, think of a supervisor handing off some research tasks to a swarm to a swarm of agents. So combining the structured hierarchical flow with the collaborative intelligence. So beyond these basic patterns, there are additional and more complex patterns you can use. I would direct the audience to the reference if you want to read up more on what are the different. Um, uh, different, uh, uh. Uh, multi-agent patterns out there. So in the financial research analysis that I will show in a few moments, uh, we'll use the example of the supervisor subagent paradigm for structured control while also maintaining, uh, while maintaining specialized focus across domains. So let's, let's think why, what are some of the crucial requirements of financial analysis assistance. So the deep research in financial analysis requires some capabilities that go beyond your simple Q&A. So let me outline some of the key challenges that make such a make it such a demanding use case for AI agents. So the first and foremost, deep research demands complex reasonings. Here, the financial analysis requires multi-step reasoning and hypothesis generation, data gathering, and combining all of them through validation and refinement cycles. Then you have the multimodal data integration challenges where typically an analyst requires to process text from reports such as like filings, news, structured data like financials and prices, visual content including the earning calls, plus audio and video from such calls and interviews. So this comes with hundreds from hundreds of global sources with varying update frequencies and reliability standards. Finally, there is a significant scale versus quality trade-off where with such a huge amount of data, it's very easy to miss some of the data points returned by the tools and these data sources. So the deep research requirements demand complete audit trail of such data sources and material information detection while maintaining consistent research quality. So with that, this is an example of a financial research architecture that addresses the complex requirements of the deep research agents. So we implemented a supervisor, as you can see, a supervisor subagent pattern where you have a supervisor interacting with the user, taking their query and responding to them, and if needed, handing it off to one of the subagents where they interact with different. Data sources. So in here first you can see an example of a structured database agent that is connected to your SQL databases, has the SQL executor as a tool. So it usually takes a query, creates a SQL, uh, takes a user's question that is handed off by the supervisor to it, creates a SQL query, executes it on the database, gathers the data, and creates some kind of an answer and returns the control back to the supervisor. Then you have the news agent and SEC filings agent. Those are connected to more unstructured data like news, news reportings, and then you have the SEC filings. All of these are, let's say, are connected to these agents are connected to the MCP server that is a knowledge base, a vector store, let's say. And uh it can take the query from the supervisor agent, run it through the MCP server and return the uh the report or the answer back to the supervisor. Then you will have maybe some of your internal APIs and your partner APIs that you can um that uh the uh the agent can interact with to if some of those questions require you to uh to interact with those uh APIs. So in this architecture, each agent operates independently while the supervisor coordinates the overall research workflow, ensuring the comprehensive analysis with tool calling with the different kinds of databases available, and then returns a comprehensive report summary back to the user. So with that, I'll hand over to Vidya who is going to talk about some of the Bedrock agent core capabilities. Uh, just wanna make sure, can you guys hear me? OK, good. Uh, uh, thanks a lot, Rahul, for introducing about, um, the agentic patterns as well as about, um, um, deep research agents, right? So before I go into the session, how many, uh, how many of you have implemented agentic solutions in production? I see Pretty few hands. How many of you have heard about Amazon Bedrock Agent Core? OK, good. At least I see a lot of folks. Yeah, uh, I think the specific, uh, reason I was, um, um, asking about all these questions was as part of this session we are going to cover specifically the challenges what we have seen our customers have been facing to put their solutions into production at scale. That's, that's what my session is going to cover because it's going to be a transition from theoretical architecture patterns, what you have seen. To the practical challenges what you will hear from uh Deepen, uh, what the expense of Yahoo Finance. So I, I in this session I would like to discuss about how to build deep research agents leveraging Amazon Bedrock agent core on AWS, right? And the primary thing is it's how to scale it for production at enterprise scale and what are the challenges, what we have seen and how we normally have solved it. As part of this, I will go over a couple of the services. And how an example pattern you're leveraging bedrock agent core so that at least you guys will get an idea how to build. This is not anything specific to deep research agents. This is specific for any multi-agent pattern what you have to build, what you want to build on AWS. These patterns will help you a lot. With that, um, let's, uh, uh, look at it, right? So in general, I think most of you who are enthusiastic about agent AKI, you normally build an agent, right? Building an agent is always the start, but putting the production, uh, the agent in the production scale is always a complexity. Yeah, I mean, even when all of us are, uh, specifically started using creating that specific my agent or whichever agent uh code which I have started to build, you will always start with a simple set of tools, right, and to demonstrate, hey, this is working great, but as you start thinking about putting at a production scale, that's where you hit the complexity. And what we have heard multiple times and what we have seen as a ABS was complexity will basically slows down the innovation. Right, Complexity around infrastructure management and everything will slow down the innovation. So as you can see in general, this is a standard pattern what we have seen. Uh, this is how what we, when we work with large enterprises, the challenges what we have, uh, observed when you're building within when you're doing a small my agent on your laptop with a simple set of tools, it's pretty easy. But as you want to put it in a production, as you scale it to multiple users right in your, in your team to use your agent. As well as when you expand to build to interact with multiple agents within, let's say within your org, uh, where there are shared set of tools which you want to use like GitHub, Atlassian, there are multiple other tools you want to integrate for and, and ultimately what we have seen is when you started putting these agents across BUs, there'll be thousands of agents interacting with thousands of agents either within or actually to organizations along with the thousands of tools as well. So this progression from individual tools to enterprise scale deployment presents significant challenges around either in the coordination between the agents or the resource management or security and governance. These scaling challenges are the primary concerns what we have seen for most of our customers failing to deploy their POCs in production, right, because when you're doing it, it works great for your small set, but when you scale it across your organization, that's where it fails. So that's why you hear a lot of challenges saying that, hey, we have been building but we're not able to scale, and that's one of the primary reasons for the failure. With that, I just want to highlight, right, always to scale this production, uh, these solutions requires an operational specifically excellence. So as you can see, a POC always starts with a lot of excitement and potential, but by the time it reaches. To get a meaningful business value from your agents, you typically, these are the five typical challenges what we have observed when a POC to realize onto a meaningful business value. So as you can see the performance right when you're scaling from either you or a couple of your teammates testing to multiple users. There will be challenges around the performance. That's one of the things. The other thing is scalability around dynamic resource management. What I mean by when you're testing with a couple of you folks, it might be working great. Most of you might have seen when you try to launch a chatbot or a workflow across thousands of users, you will start seeing. Bottlenecks around throughput and other issues, right? And the other main aspect was security, security for authentication and identity management. You always, one of the primary things we always keep on hearing was, hey, what is a guarantee of how can two agents interact in a secure way in a proper authentication mechanism. And the other aspect was context between your basically your users might be interacting with the chatbot today and I think if they come back after one month they would like to feel as if they're talking to another human who knows about them, who has entered context. So essentially preserving context across uh long term sessions is is a primary key component is another primary challenge and primarily when we talk about enterprise, it always comes down to. Can I audit? Can I do observability? Can I scale and compliance, right, because every company has their own compliance policies. These are the five critical things for all of your POCs to, to scale it to enterprise. These are the five critical challenges which you need to solve always. There's no way to get around these challenges. So what we have observed was, I mean, uh, just want to highlight was what does Amazon Bedrock agent core offers, right? Amazon Bedrock Agent Core is nothing but an agent tool chain for you that will specifically address all these production challenges what we talked about. So these are the primary challenges we at AWS have observed ourselves, and that's why, that's where this modular solution came in, as you can see, just want to highlight. You don't need to use all the components of Amazon Bedrock agent code. You might have already built your agents running somewhere. You just want observability. The solution can offer you if you, if you just want the memory feature for your existing framework, this platform can be essentially leveraged. So the three key value propositions the Bedrock Agent Corp specifically offers was, as I mentioned. It will help you to get the time to value for your business use cases, so you don't have to worry about infrastructure and operational overheads. You don't have to worry about throughput and auto scaling related issues. The other aspect I want to primarily highlight which I have primarily seen as a primary concern was you can build your agent with any on any framework or. Any model of your choice and you can bring it on to Agent Core to use that specific component. So this is not specific for 11 framework or model, so you are not really logged into any technology stack. That's that's the biggest advantage which I have seen a lot of our customers are able to leverage. And the last specific thing I just want again whenever we talk about enterprise, it's all about secure, scalable, and reliable reliability. So that's what this platform offers you to. Uh, deploy your agents at scale in a trusted, proven methodology for a long time. So I mean, just want to highlight how does a sample agent, whichever agent you have built, looks like on Agent Corre. Let's say you have an agent which you didn't even deploy anywhere, so Agent Corre offers runtime where you can essentially deploy your agent in a secure and more trusted way, specifically leveraging the Agent Corps runtime. And and all of us have tools, right? When I made my tools, either you might be hosting your own MCP servers or a lot of us have already have our solutions running with APIs. All those tools can be hosted on Agent Core gateway. You can, you can think of that as a single set where you can host all your tools which has a built-in features for Agent Core browser and Agent Core interpreter. And the other, other main thing I want to highlight was the security. The authentication and agent core identity, basically identity between uh identity can be performed by agent core identity where a secure access can be provided for all your agents. And The agent core memory. Agent core memory specifically offers you persistence across sessions as we discussed earlier, making sure to preserve the context across all the sessions is provided by Agent core memory. And the other last aspect was agent core observability. It will let you monitor. What what are the actions your agent is performing as well as performance metrics, latency, token, uh, token usage so that you can integrate into any open telemetry standard-based tools. Again, these cover only some certain set of tools as you are here at reinvent, you might be hearing a lot of raw launches around agent core policy which will which will provide a fine-grained access to what your agents can do and cannot do so that that can provide you the guarantee as well as the agent core evaluation, which we are not covering over here, but as I mentioned, you will be hearing a lot of announcements around Agent Corp as part of this reinvent. So, as, as I mentioned, it, what does Agent Corre offer? Just wanna make sure Agent Corre offers you a secure and scalable runtime for your agents. So essentially the runtime supports, I think, as, uh, Rahul mentioned, if. The DP requires the specific need for multimodality to do complex reasoning, so it provides a mechanisms for you to support large payloads for multimodal modalities, specifically for text, images, audio, basically audio and video scenarios. And it offers, you don't have to specifically configure anything. It offers auto scaling. So if you have varying users right in the beginning you might be supporting only 10 users or you have like Black Friday events for retail customers where you might have to scale the throughput and again bring it back. So auto scaling is inbuilt into agent core so that it can handle. Varying workloads, so the system also supports, I think you will have different workloads based on the patterns. Either you might have the real-time usage or batch where you might need to provide millions of inferences on a nightly basis on a weekly basis or a monthly basis. So it provides you an inference mechanisms both for synchronous and asynchronous approaches. The other main thing we have been talking about security, right, so the session isolation. In agent court provides you a way to easily provide you multi-tenancy for your users so that every user session is secure and private so that the session information, everything is preserved separately and there is no cross-pollination happening at any point in time. So the main thing I want to highlight was it's pretty simple. If you already have an agent, it's pretty simple to add the agent core, the runtime decorator, identity decorators, and observability decorators on top of your agent core code. It's a simple 10 line code which you can just add to any of your agents and just deploy onto agent core, and basically the agent core infrastructure will take it from there to provide you the infrastructure, you know, provide the inference, as well as in a secure enterprise scale. The, the main thing I want to talk about, right, so because as most of us um building enterprise level applications, one of the key components always will be observability. How will I debug? I mean, basically it's burning a lot of tokens, so to get all this specific information you don't have to really build another observability tool. You can essentially integrate agent core observability, which will provide the metrics in a very standard way similar to the other applications where you can have the observability audit logs and everything directly going in open telemetry standards back into. Your existing monitoring and observability tools. So with this you can you can clearly observe how is the agent performing, what are the actions it's taking for particular inputs, as well as making sure your latency, token usage, and everything are within what your expectations are. So again, I'll not go into this architecture because you heard from Rahul and uh you'll hear more from Depan specifically how how they solve the challenges, but the main point I want to highlight was how to leverage agent core for these multi-agent architectures, specifically the deep research agents, right? So as you can see here, you can host all these agents on agent core runtime. So all the subagents what you heard in multi-agent architectures, uh, specifically can be hosted on the agent core runtime. And all the tools, all the MCP servers or APIs will be hosted on Agent Core gateway, and the authentication and everything for that can be provided with Agent Core identity. And the memory, memory features specifically to to persist long term and short term memory will be enabled by agent core memory. And the last thing I just want to highlight was observability, right? So for all these across agents because you want to see when a request comes in. Which all subagents are called, which are tools are called, what is what is the token usage, where exactly is the latency, so you will get a granular information so that even if there's a problem, you can ideally identify where exactly is the problem. Is it within your agent, their counterpart agents upstream, downstream. So there's a clearer basically the patterns which you can observe with the observability feature. So with that, I just want to close the thought with what does the Asian Corp specifically offer in deep research context on this in the financial domain as well as in any domain for the multi-agent solutions. So as you heard, one of the primary things what we need for any deep research agent is persistent. Long running workflows, it offers right now an 8 hour run time, right, so that which will enable you to run any deep research cycles which will include multi-market analysis, comprehensive backtesting, as well as model refinement, right? So this will allow you to run for long periods and have a highly scalable solution. The other thing which we just discussed was the advanced memory management which offers the dual memory architecture. One is for the long-term memory. Specifically, if you have done enough research as humans, we'll have that context in our mind so that we can leverage when we are doing when a new update comes in, we'll just process the delta. That's exactly what the long term memory provides so that the agents can function in a similar way leveraging the historical correlations and the research context at the same time making sure the existing workflow information is preserved. And the last thing which I want to highlight, I mean just because for all enterprise systems to making sure that basically this Asian core offers a production level enterprise platform where you can deploy in a more secure and trusted way and automatically scale with all these required components. OK, so with that, I just want to make sure I'll hand over to Deeppan to specifically talk about their experiences in Yahoo Finance in general in building these deep research agents. Right. Give me a thumbs up if my voice is coming through, OK? Good. So yeah, thank you Vidya. That sets the stage perfectly for me. In the remainder of this session, I'm going to talk about how my team built the financial question answering system and what we discovered along the way. Uh, but before we dive in, uh, a little bit about Yahoo Finance, uh, our mission is to empower investors to make confident financial decisions, and we are the number one financial news and information website on the internet, reaching out to over 150 million users worldwide. And we are apart from the US, we are present in 10 priority global markets. You can, you can find Yahoo Finance on the web as well as mobile apps, both iOS and Android, where you can look up real-time market updates, insightful interviews, and comprehensive investor tools. Now, today, there's a widening gap in how information reaches different investors. On one side, you have retail investors who are working with limited time and scattered information. They jump between apps, news sites, social media, and the brokerage dashboards, and prices are in one place, fundamentals are in another place, news in the third place, sentiment comes from somewhere else. It's, it's a slow process. It's noisy, and it's often incomplete, and by the time they piece together their investment thesis, the opportunity might have already gone. And on the other side you have Wall Street professionals. Who are operating with a completely different toolkit. They have teams of analysts, they have premium data, and they have MLN AI models. Who can crunch all this data. And that Introduces a gap between these two cohorts of investors. And that's where we sit. Yahoo Finance addresses this gap to a certain extent by bringing together a lot of information at one place. But there is still, it doesn't solve everything. There's a structural structural advantage that professional investors have in the investing world. And if that gap was not challenging enough, the sheer scale of information in the investing world makes it even harder. There are thousands of companies listed. Look at the sheer number of patent applications filed in one year. Clinical trials. These public companies, every day they file 4700. SEC filings. 3000 news articles published in a single day from reputed publishers. 110,000 press releases. And that's just the static side of the world. There are 12,000 ETFs globally. 370,000 trades happening in equity markets, just stocks and options. And from all these traits, there are 1.5 billion data points generated. This is for the professional investors. This is fuel for the sophisticated system they've built over the years. But for retail investors, this is an ocean. And if they try to swim in this ocean, they're most likely going to drown in it. And this asymmetry is not just about access to this data, but it's also about who has the tools to make sense of all this data and derive insights out of it. So, with AI, uh, a lot becomes possible in finance. AI can now read across millions of data points, filings, articles, summarize them in seconds. It connects the dots that are impossible to link manually. So what else can it do? It can Optimize your investment strategies. You can process information from many different data sources, gain insights and present them in an easily digestible way for retail investors. It can discover investment opportunities, uncover hidden gems, explore themes, and find patterns before they become obvious. And it also on the personal finance side, they can help you manage your wealth, save wisely, spend wisely, and reduce your debt. So a lot can happen and a lot can be done with AI and finance, but when we started out this project, we set a specific goal. Our goal was to help the retail investors with clear and accurate answers to their financial questions so that they can do the investment research in a better way. And we narrowed down the scope further to focus on equity research using a variety of data sources. Fundamentals, price histories, news, SEC filings, etc. And we wanted to explore the AI agent-based approach to solve this problem. And while we were clear about what we wanted to do, we are also clear about what we don't want to do. We don't want to provide financial advice. We don't want to provide trading recommendations, no tax or legal guidance. So, let's jump into this specific problem. Now, before I dive into the architecture, I want to highlight some key challenges with financial question answering. The first, the data in the financial world is incredibly heterogeneous. A lot of the data is structured like price histories, financials, insider transactions, and a lot of the data is unstructured the SEC filings, news articles, earnings transcripts. And all this data comes from different sources, and the access mechanism is different. Some are accessible through API. Some are provided by feeds. And then there is temporal complexity. What do I mean by that? Many companies follow different fiscal year patterns that don't align with the calendar year. So when you're asking what was Apple's. Revenue growth, uh, what are the latest Apple's, uh, Apple's latest revenue growth? The answer can be different. You need to know about what is the fiscal year boundary for Apple, and the answer will be different the day before you ask when you ask the day before the earnings versus the day after the earnings. And then there is all mixed types of data sets, especially SEC filings where you have tables, that's a lot of structured data in that, and there's a lot of commentary about the management discussions and product launches and risks in the SEC filings. And all this data comes at a variety of rates. During typically during the earnings season, there is a lot of information that flows in, so your system needs to be able to intake all that information and make it available for your Q&A system. So with that, uh, let's start looking into how we built the multi-agent system. So we started out simple. We have a chat API. And uh the client can query that API. API has a database connected with it. And it has an equity research agent, which is accessing an LLM through a service. Very simple setup. Everything happens end to end synchronously. This is good for a prototype, but when you try taking it to production. The API layer starts becoming a bottleneck. Why? Because these queries typically take anywhere between 5 to 50 seconds. And we looked at web sockets, and they, they are stateful, and scaling that requires a special infrastructure. So what we did was We break that down. The query pattern, instead of just querying and waiting for the answer, the client would submit a question. And then pull for an answer. And what that allows us to do is decouple the API from the agent and take the agentic workflow into an asynchronous execution. And then when the agent is done doing its work, it can write the answer back to the database, and through polling, the client can retrieve that. Next, we introduced input guardrails. We don't want to answer all kinds of questions. So input gutters help us filter that out before it even reaches the agent. Now let's dive into the equity Research agent. It acts as a supervisor agent which is connected with several other agents. There's financial data agent, insider transaction agents, which are mostly tool calling agents, so they have access to internal and partner APIs. So depending on the question, they can query specific information from those APIs. So they don't have to rely on the web to find that answer, and we can be confident the answers are accurate. The SEC filing agent is a rag style agent which has access to a specialized knowledge base where we index all the SEC filings, and similar with the news agent, it also has access to its own specialized knowledge base. Interestingly, the knowledge base that we build holds the data that we license through our partners, and often that's not enough. So we also go out and do a restricted web search from the reputable sources and make that available to the news agent for answering any questions. And all these knowledge bases are kept up to date through workflows, which are event driven. As soon as any new SEC filing is published or a news article is published, it gets indexed into this knowledge base. The agent also has access to an observability solution so it can keep track of token counts, which agents are being invoked, how long they are taking, all the latencies and all that. We are also exploring output guard rails, so once the answer is formed before we write it to the database, we want to check that for any potential investment advice or any kind of future predictions which we don't want to do. Apart from that, we have some workflows which crunch a lot of data and build. Specialized data sets which we store in a database, and we are exploring this new pattern of text to SQL style agent which have a unified knowledge base attached with it and can answer using any kind of tabular data. And these are just our new explorations on, on the patterns that we've already solved, like tool calling through APIs and RAG. We are also extending them to more data sources like earnings transcripts, video transcripts, etc. Now Let's look at how you would build this system on AWS. What you're looking at is a completely cloud agnostic system architecture. It is cloud agnostic. It is framework agnostic. It is model agnostic. You can use any model, any framework, and any cloud to implement this architecture. But how would we do it on AWS? We picked lambda functions for the asynchronous execution. They are serverless. They provide resilience. If something fails, it can be retried without affecting other queries that are being served. The handover between the API and the agent takes place through an SQSQ. So API would write the request to an SQSQ and you can control how many agents, how many lambdas are concurrently executing that allows you to control the concurrency of the system. It also keeps the costs predictable. And the LLMs are abstracted out into LLM services so you can use any models. For your agent, you can use any leading frameworks. We used lank chain and for financial data and insider transaction agents, we used lank chain again for tool calling capabilities. For SEC filings and news agents, we use bedrock agents which use the bedrock knowledge base. For input and output guardrails, we use AWS uh bedrock guardrails, which I'll dive into a little bit more later, and the databases we use RDS which hold a lot of context, uh, history of the conversation, etc. And finally, the workflows are all implemented with lambdas. And the observability please, we use cloud watch matrix. Now as you can see, we are using all the scattered services from AWS and there's a lot of work that has to be done to operationally make it ready for production. What can we do with Agent Core? It is a single framework that allows you to do everything uh with that. It has agent core runtime, which takes care of the asynchronous execution part. It also has agent core identity, which allows you to uh uh have associate a particular role with your agent, so you can control what data a particular query can access and what data is restricted. The services and data stores can be abstracted behind Asian core gateway. The observability piece comes in built with Asian core, so you don't have to go look for other services. And agent core memory allows you to store the long-term and short-term conversations, so you don't have to have a separate database for that. So with that, let's look at some examples of this uh question answering system. We're comparing Nvidia's Capex to their peers, and it presents the information nicely in a table. It gives you key observations from that. Why is a stock down? That's a typical question that retail investors have, and this gives you a clear answer to that. And depending on when you ask the question, it will take the latest information and digest that in an easily consumable way. Have inside a sold metastock and you can be elaborate with that. You can ask it to present the information in a certain way, give me all this in a tabular format, and it can even derive certain columns such as transaction value, which is not information in the database. It is derived by the agent. And a couple of examples. Should I buy Apple? Where is the market headed this year? agent would refuse answering those questions because we have intentionally avoided that using guard rails. So with that, Let's talk about AWS guardrails. Uh, we, we use that as a primary mechanism of defense, it can. It can avoid any kind of prompt injections, and it allows you to configure policies which are very, very specific. You can detect particular topics in the query. We don't want to be answering any generic queries like what is a black hole, for example. So it helps you identify finance queries. It helps you set some word policies. It can identify credit card information, etc. and Any queries that contain that information never goes to our agents in the first place. And the whole deployment process of guard rails is really flexible and it allows terraform-based deployments, so that's pretty useful. A few words about how you can enhance the rag and knowledge bases. Uh, adopt adopt hierarchical chunking when you're working with large documents, especially SEC filings and news. These are pretty long and they have a structure, and hierarchical chunking helps with maintain that structure in your indexes. Uh, you should integrate metadata deeply, uh, include the, all the metadata, including dates, filing types, uh, publishers, etc. into the, uh, into the indexes so that retrieval, uh, can use that. We also experimented with query rewriting for some of the time sensitive queries. We also experimented with knowledge-based re-ranking so that the most relevant chunks are indexed first. And on the system prompt refinements, uh, especially in the finance domain, uh, we should rely on the LLMs to To identify the query intent and some of the named entity recognition as well. And provide just hints to the LLMs on what tools are available, and it does a wonderful job of identifying which tools to use for a given query. You want to present that information in a standard format, so just some simple instructions on how a fiscal quarter should be represented or how large numbers should be represented works very well. And for Tackling the temporal complexity, we tried various things, but what helped most was just providing a tool to query the fiscal quarter of a company, given a ticker. Now We've covered the architecture and the retrieval foundations. Now, let's talk about the evaluation because building a system for financial question answering is one thing, but proving that it works reliably, that's a completely different ballgame. So the main challenges in the evaluation are Finding the representative data set of user queries, you don't know what the users are going to ask until you go to the users with a feature like that. So what, what do you build for? And how do you evaluate that? You've got to build a golden data set of question answer pairs. Which is a time consuming process and it requires a deep domain expertise, because you've got to carefully validate all the answers for accuracy. And even if you spend the time and resources to build this data set, it quickly becomes outdated because many of such queries. The answers would change quickly. So, there are primarily two ways of tackling evaluations. You have human evaluations or you have automated evaluations using LLM as a judge. Human evaluations are highly trustworthy. They catch subtle things, subtle mistakes. They are flexible. It can handle a variety of different types of queries, but at the same time they are slow, costly, and they are not scalable at all. There's also teams of evaluators, so there is variability in how they are judging the answers. There is subjectivity that comes into the picture. Whereas the automated evaluations, they are stronger where the humans are weaker. They're fast, cheap, they can evaluate it in a consistent way, and However, they, they do have the trust gap. Would you trust a system that has been evaluated solely by AI, so answers are produced by AI. AI is evaluating it. How do you trust that system? So we take a hybrid approach, and I'm gonna walk you over how we did that. First of all, the data set creation, we forget about. Question answer pairs. We just look at the questions. What are the questions that we can generate? We use templates so that with a smaller set of templates, we can generate a large data set. We the evaluation rubric where we define common matrix and we provide clear instructions on how each matrix should be evaluated. The scoring scale is fixed and the same guidelines are applied to both. For humans, it becomes instructions and training. For AI judges, it becomes a part of their prompt. And we take a small data set, start with that, and while the humans are evaluating it, we run AI evaluations on that. We have the scores from the human and we Go in a loop to fine tune the prompt for the AI judge and Try to converge onto the same scores. And then we find the offset the gap between the scores of AI judges and human judges on the same data set, and then when we scale the evaluation on a larger data set, you can apply that offset and you can have a very good educated estimate on what your system is performing. So some results for the scale of the evaluation, we have about 150 question patterns and we expand them into about 450 questions for human evaluation. And once we have tuned our AI judge, we can scale that and run that AI judge on almost thousands of questions. For quality matrix, we have simply 3 matrix accuracy, which is from the ML domain, it's akin to precision. What we are presenting, is that accurate? Coverage refers to, are we presenting, uh, are we covering all the key data points in the answer, and how are we presenting it. Is it easily understandable, what is the structure of the response, etc. In terms of performance, we have the latencies ranging from 5 seconds to almost 30 to 40 seconds, depending on the query, how many calls are happening in that query. And we have tested a concurrency of 100 questions, but it can be linearly scalable to 1000 because you can just increase the number of lambdas in your account and scale that up. And if you are facing the scale requirements beyond that, you can always look at Fargate or any other dedicated compute. And in terms of costs, it's about 10,000 to 50,000 tokens per query, costing us about 2 to 5 cents. So with that, uh, That's where we conclude. Thank you very much for your time. Uh, I'll invite my co-hosts on the stage. Uh, I hope we have given you something today to take back to your teams, and, uh, hopefully you can, uh, adopt some of these techniques, uh, in your day to day work. We'll be available here for another 1015 minutes if you have any questions, you want to go deeper into any concepts, or you want to share similar projects that you have worked on, uh, feel free to, uh, approach us and we can spend some time talking about it. Thank you so much for your time again. Um, yeah. Thank you for attending. If you will, if you like some more uh deep dives, um, uh, uh, sites, you can go onto this to start learning more about uh AWS and its services. And specifically, if your focus is on Agentic AI, you can, uh, scan this QR code to learn all the AWS has to offer for Agent AI.