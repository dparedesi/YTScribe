---
video_id: BKgG8DSdNUo
video_url: https://www.youtube.com/watch?v=BKgG8DSdNUo
is_generated: False
is_translatable: True
---

Alright, welcome everybody. Can everyone hear me OK? Awesome, that's great. So this is a code talk, so I just wanna make sure everyone realizes this is not a dark room to hide and just look at slides. This is interactive. I'm gonna start off with some questions that we're gonna ask and we have a few slides to go through for sure, but the more questions you ask us, the more we'll get out of it, and I have a way to encourage that. I've got stickers, so ask a question, get a sticker. That's the way this is gonna go today. Um, so we wanna get through, we'll get through, uh, a little bit of a setup here to set up the conversation. We're gonna talk about performance, obviously. Um, but after that, uh, I really, we, we want you to ask us questions, right? That's the idea behind today. That's gonna be more of a console talk than a cod talk, but there you go, rename it, we'll get there, so. But before we dive into it, I just wanna get a sense of who's in the room today. Show of hands, have you ever looked at your application's performance, the, the metrics around your application performance and thought, you're not really sure why it's running as fast, it should be running faster than it is. Alright, I see a few hands up. For those of you that didn't raise your hands, I'm gonna say you're either incredibly lucky or optimistic, but we're gonna figure out which one today, so. OK, hands up again if you ever tried to optimize performance by throwing more computer at the problem, maybe a bigger instance, more cores, more memory, lots of hands up for that. Thank you. That's why the stock is so high. And then last question. Last question, how many of you have heard of terms like pneuminodes, cache lines, CPU affinity, but honestly weren't quite sure exactly how that impacts your application performance. Right, I see a few hands for that. OK. I think you've all come to the right place. I think based on the show of hands, you're in the right place here, but there's a persistent myth in our industry that cloud that that modern compilers and cloud infrastructure have abstracted away the need to really understand hardware. We just write clean code, deploy it to the cloud, and magic happens. But there's a, but here's the thing. And this is what we're gonna explore today. The difference between code that runs and code that flies, often comes down to understanding the hardware beneath these abstractions. So let me paint you a picture. Imagine you have two identical servers, running identical code, processing identical workloads, but one consistently performs 40% better than the other one. No visible errors, no obvious bottlenecks in your tools. What's going on? The answer might be as simple as which CPU cores your threads landed on, or how your data structures align with cache lines. So when you're pushing the limit of what's possible, when every millisecond of latency matters, when you're trying to squeeze every drop of performance out of your infrastructure budget, that's when understanding these low-level details transforms from interesting trivia into critical knowledge. Today we're gonna pull back the curtain and explore memory topology and explain why your RAM isn't just one big pool. We'll examine the art of hyperthreading, and why sometimes using fewer cores gives you better performance. We'll talk about, and more importantly, we'll talk about actually measure performance in a way that gives you more than just pretty pictures. And most importantly, we're gonna make it, learn how to make informed decisions on when these optimizations matter and when they don't. My name is Seth Fox. I'm joined by Arthur Petit-Pierre. Let's dive in. So, I wanna talk for a second about how to read the EC2 instance portfolio. Over the years we have lots of instances, lots of instance families to sort of dive into, but there is kind of a decoder ring, and that's the sticker you'll get. It's gonna look a lot like this, um, but sort of break, break this down for folks so that we understand what we're looking at. So we have, you know, I, I, I've highlighted three of our, I call our workhorses the CMs and R's, those are compute optimized, memory optimized, and general purpose instances. And what does that mean exactly? The, the, the, it is basically a VCPU to RAM ratio. And you can see the ratios up there, and as you sort of move through those, the VCPU to RAM ratio doubles as, as you go up that stack. Now this decoder, we're gonna decode a compute-based instance here, but it'll work across, across our instance families. So if we look at this, we've got a C8GN 2 extra large. So what does that mean? I've got a compute-based instance, right, because I know that I, I see the C. It's 8th generation, so you can see where it fits in the lineage of compute-based instances. And then there's a couple of options for this instance, G and N. The G tells us this is a graviton instance, so it's got a graviton processor in it. And N tells me this network, this, this instance has some difference in networking, some specialized networking. So now we get into the number, the 2 X large is gonna tell us how many VCPUs there are. So we, we, everything doubles. We start with 1 is medium, 2 is large, 4 is extra large. 2 extra, yeah, 4, yeah, and then 2 extra large becomes 8. So now I know I've got 8 CPUs and because of that, I've got 16 gigs of RAM because I know the ratio for those things. So when I break this down, this is an 8th generation compute-based instance with 8 VCPUs, 16 gigs of RAM, and the specialized networking is up to 50 gigs of networking. So while this is decoding at one compute basis instance, like I said, this will cover our entire instance portfolio as you're looking across that and you can use that to break down different pieces of the different pieces as you're making those instance choices. One last comment before you go to the next one, the, we, we mentioned in, uh, the, the table at the top left, um, that CMNR or the. Uh, with the different amount of memory. There's a myth that we encounter extremely often that dates back to 5th gen, which is that the processors on the seas might be faster than the other ones. The last time it was true was for Intel 5th gen um AWS instances. Starting from gen 6, we have the exact same processor on C, M and Rs. So, if you're using an M6I you have the exact same processor on a C6I and the exact same processor on an R6I. Makes your life as a, as someone analyzing performance much, much easier. It's also much, much easier when you're using things like Spot and you wanna divi diversify, uh, that's still the same processor. And as much as possible, we will try to uh keep that a real thing for the next generation, so that's definitely true for the 7th gen uh across the board. So, on the graviton side, the 7th gen Graviton-based instance, that's a Graviton 3 and that's the same one. For C7G, M7G and R 7G. That's the exact same thing on the AMD side, that's the same Genoa-based uh processor on AMD 7 G. and uh we've done that again with Gen 8 and we will do that again with Gen 9. I, I cannot promise that it's gonna be the same for gen 15, but, uh, for as long as we can do that, we will try to do it. And, and we tend also to make that broader, so, uh, with the eye instances, for example, we've been using the same uh processors as in CMNR. Yes, uh. I'm not sure whether that thing is on. Give it a try. Yes, it's on. Cool, um, so you have AMD, Intel, and Graviton, and even within Intel and AMD you have multiple families and multiple, you know, as I said, lineages, um. So Even within different CPU brands, they are still. The same performance. No, no, that would be too simple. That'll be, yeah, that'll be too simple. So, uh, the, the processor is exactly the same within the same, uh, brand of processor. So C8I, C, uh, sorry, C8I, M8I, and R8I. That's the same processor and the same performance profile within the generation. Within the generation. Now if you move from an Intel to an AMD they're gonna be super different. If you move from an AMD to a Gravidon, they will be super different. And, and I cannot tell you there's one that's gonna be better at everything compared to the other. In, I can give you guidance, uh, I can tell you that in general, uh, a graviton processor will be more cost effective, but in some cases, an AMD processor will be more expensive but will be faster. And there will be other combination with Intel. And there's never going, well, it's gonna be hard to have an easy answer to that question, cause they, they are optimized for different things, and so they behave differently. And, and you will have, and we will see that later, you will have cases where, oh, you will have plenty of cores in the same memory domain on a graviton-based instance, whereas AMD has done a totally different type of optimization and you have clusters of eight cores, it will have a different performance profile when you access the memory. It will have an impact on, on your applications. Um, so I know within like, uh, vendors, server chips, they have different SKUs and they have different TDP and clock speeds, and, but you're saying that these easy-wo VMs, like the underlying hardware is just a, a uniform SKU across the board, so there's no like. I know like the lower core count tends to have like higher boost frequency and and that sort of thing, but like going lower core count on these instance types will never yield to higher performance in like single threaded applications versus higher because the underlying hardware's uniform. Uh, as usual, it, it won't be a, a straightforward answer, um, but yes, a VM is a slice of a big server. So when you run on an easy to, um, instance, what you get is a slice of the big server. Internally, by the way, we call those big servers droplets because the cloud is made out of droplets. And, and so, that, that big droplet is gonna have as many cores as the biggest sizes in the family. So, if I take the example of a um CAG 48 XL, that's gonna be a two-socket server with two graviton 4 processors, each having 96 cores. Now if you take a CAG. large, that's a 2VCPU instance, that's one slice of that two-socket, 192 core server. Um, so, um, in a sense, no, it's not because you select small VMs that you will get a higher frequency, uh, because they're taken as slices of a bigger one, and those bigger ones, they have all the, the exact same underlying processor within a generation and for a specific vendor, those droplets are homogeneous across the entire. Yes. OK. Um, now, that being said, we do have, if you really care about, um, super high frequency, we do have special instance types like the uh um R7IZ. They're usually post fixed by Z, and those have been designed for high frequency. So above, usually above 4 gigahertz. Um, we used to have M7, uh, M5ZN. That was one such instance. We had Z1D. Uh, they tend to be instances that we designed for things like EDA, uh, where getting this, the, the highest possible single thread performance is very beneficial. Cool, we go the next slide. Um, yeah. Uh, so, talking about type families and processors, so we started talking a little bit about that, but basically the, what we call the general purpose, um, regroups three different types, uh, the tea, uh, so the tea there are the burstable instances, so. Uh, they are the only type with the flex type of instances in AWS to be overcommitted. Meaning that on those ones, we run more VMs and we sell more VCPUs than there are actual physical cores. Uh, great for cost, uh, great when you have an application that has a limited, um, performance requirement, but probably not good for anything where you really care about performance. And we mentioned it, so those ones are not over committed, um, when I mean not overcommitted, it means that every time you get, uh, let's say a two VCPU based instance, we've carved for you two VCPU on that instance. And, and we do it the right way, meaning that um those allocations are static. The, the VMs, they are not floating around. When you get allocated uh two cores, you stay on those two cores, so when the cash is warm, it's gonna stay warm for you. Um, we never allocate anything across the boundaries of a new node, so you will never end up on a On an AMD instance with 4 cores on one CCX 4 cores on another, it's always across the boundaries. Um, so, if you have uh uh a VM with 16 cores in 7 gens AMD that's gonna span over uh uh 2 Cs, and, and we'll uh uh dig that a little bit, uh. Later. Compute optimized, um, again, don't be, um, don't think that because the name says compute optimized, those processors are faster. It just means that if what you care most, the most about is having, um, compute power and you don't care much about having a lot of memory, that's what you need, 2 gigabytes of memory per uh VCPU. Memory optimized again, doesn't mean that the memory is faster. It just means that you get more. Uh, so on, on an R uh instance type, you get 8 gigabytes of memory per core. You, uh, that can be much more than that, X that's 16, Z, uh, well, there are a few Z instances, uh, but it happens that that Z at 8 gigabytes of memory. Um, accelerated compute, that's where you will get GPUs, FPGAs, and various other types of accelerators that we have introduced. Storage optimized, uh, means that you get local discs that are physically inside the server and not the virtual discs that you have things like, like EBS and HPC optimized, those are very special instance types that are made out of the same hardware but have uh um a special ring fencing mechanism. Um, for HPT application. Flex instances, uh, you will see that some of the, um, the M and DR instances exist as a flex version. Um, that's exactly the same hardware as the normal class, except we overcommit them and we load balance them so that most of the time, you will get uh 100% of the performance, but you can go down to 80% of the performance of the underlying hardware. Um, super convenient when you're not super, super critical on the performance side, they're cheaper. Um, but if you want perfect reproducibility on the performance side, then probably they're not for you. Any questions on this? There's a lot, there's a lot here. I think similar to the Ts, is this the way you used to use? Um, they're better than the Ts in the sense that you don't have that mechanism where you credit CPU cycles and then you consume it and then you reach a stage where, uh, you're out of credits and then we have that unlimited mode. Um, it's a more steady, uh, type of, um, utilization, so it's more predictable, let's say because I've been thinking about using, uh, synthesis for things like the staging environment. Develop an environment where there's not a totally makes sense. staging is a great staging is a great place. Now, if you really wanna evaluate the, the maximum throughput that you can get from an M, don't take the MFlex. Sometimes you will get 100%, sometimes you will get uh only 80%. But, but for, for staging environment where, when it's mostly functional tests that you're gonna do, do on them, doesn't change anything. And then when you, uh, um, and when you know that only getting 80% of the max is sufficient for you, that's also a great fit. And most of the time you will get more than 80%. And do they work the same interface as like normal on exactly. Nothing changes. It's just like, you name it, whatever dash Flex, and it's, and the biggest sizes are not available as Flex. Nothing back here, yep. So on a instance with um uh with unlimited credit, um, do I know when I'm being throttled and I'm not getting the full 100%? Is that visible in any way? Yes, it is, but to be honest nowadays don't use, don't use the, the, the, the T classes anymore like unless you really do things that are non-critical, use the, use the flex. They're much, much better proposition. Got it. Thank you. I guess kind of a similar question for the flex instances, is there a way to know kind of where in that 80 to 100% your workloads are getting? OK, so if you're, you have to trust us, if you're being bottom, right? There is a bottom. So what happens behind the scene is that we are load balancing those instances with transparent migration. So we have a pretty efficient mechanism to ensure that you're not all of a sudden. In a droplet that is so hot that you're down to 60%, uh, we just transparently migrate things and Got you. And transparent migration has some has been something that AWS has been shy to talk about. So we've been doing it since quite a long time, mostly for reliability purposes. It's now in the EC2 FAQ, but we were doing, doing it a long time before it was in the uh in the FAQ. Uh, we don't expose it in, to customers in the sense that you can't schedule the migration of one of your instances, uh, but we use it a lot behind the scene and it's a really reliable mechanism. I mean, the way to think about it as an instance is a completely virtual construct, right? It can, it doesn't live in a place, it lives in the cloud. OK, um, let's talk a little bit about what are the actual processors, uh, that are on those different instance type, and, and here I'm gonna mostly focus on CMNR and, and anyway, that's probably 90% of what you're using, um. So, for those of you who are more familiar with the code names of the different vendors, um, we often get the question of, OK, what is behind your 5th gen, 6th gen, whatever. Um, So, gen 5, and, and another important thing, when you're doing performance comparison, um, even though I would tell you, like, use the latest, cause that would probably be the, the, the right thing to do. If you do comparison with different uh with different um processor types, um, there's kind of a, uh, um. A generation gap, um, a, a Graviton Gen 6, for example, would be comparable to an Intel or AMD Gen 5. And, and it's true across the board, so Gen 6, that's comparable to a Graphvidon Gen 7, Gen 7 Intel and AMD that's comparable to a Grapheddon Gen 8, and, well, for the rest of the Gen 9 Gravidon is not yet there, but that will come at some point. Um. So Gen 5 on Intel and AMD Intel, we had Sky Lake and Cascade Lake. That was a weird thing that hopefully we won't do anymore. Uh, we introduced it with Sky Lake, and then we did a silent refresh with Cascade Lake, and it had almost the same performance profile, but in some cases, the Cascade Lake was a little faster. Uh, so that was a little weird for customers. Um, we've not done it again, uh, customer complained and we, we listened. Um, AMD gen 5 was ROM, uh, and on the graviton and on the graviton, uh, side, uh, Gen 6 was Graviton 2. Um, one recommendation I would have, don't use those any longer unless you're using them through Spot. Um, the price performance ratio of those instances is no longer worth it. Um, so switch to things that are more recent than that, and you will get better performance. Uh, Gen 6 Intel and AMD is priced exactly the same way as Gen 5 and it's faster, uh, not extremely faster, but faster. So, there's absolutely no good reason unless there's no capacity available to use Gen 5 on Intel or AMD or to use a gen, uh, uh, a Graviton 2 on the Graviton site. Um, Even Gen 6, compared to what we have released recently, I would say, they start to be old. Um, OK, they're cheaper than gen 7, but in terms of price performance ratio, you would be better off using a gen 7 or gen 8. So, on the Intel side, that's an ice lake, on the AMD side, that's a Milan, and on the Gravidon side, 7 gen is uh Graviton 3. Um, Now, more recent, uh, Gen 7 on Intel is a Safare Rapids, uh, Gen 7 on AMD is a Genoa, uh, great processor, and, um, on Gravidon, that's a Gravidon 4. and then the latest that we have released a couple of months ago, uh, we have a granite rapid on the Intel side and we have a terrain on the AMD side. And Gravenon, well, it's easy to guess and that will come. Uh, a couple of trends that we have seen, um, over the last years, the number of cores per socket is going up. And that's true across the board. Uh, you've, you've seen that on Intel, you've seen that on AMD, uh, you've seen that on Gravidon, um, and that won't stop, that's gonna be true for the next generation of, uh, the various instance type. The dollar per VCPU is also increasing. Um, the manufacturing technology, the memory technology, uh, all of that is more expensive. So the price on a per VCPU basis has gone up. That being said, the dollar per amount of performance has decreased because the performance has increased. So, even though on a per VCPU basis, it's more expensive, if your application actually benefits from the performance of the processor, you get a better value prop. The power consumption per unit of work has also decreased. So if you wanna, uh, be a little bit less intensive from a power consumption standpoint and be a better, uh, planet citizen, you'd better use the latest generation of instances that they're a little bit better in. Does anybody have sustainability goals in the organization? Show of hands. Yeah, there's graviton and taking care, taking advantage of the latestors will help with a lot of that. Um, another trend that we've seen and, and we'll see how that will evolve is that some of our processors or using sim simultaneous multi-threading, so the ability to use, uh, two or more thread of execution per physical core, some of us do not, um, and that has an impact on the performance and how you measure the performance and we'll, uh, we'll, we'll see that a little bit deeper, uh, later in the presentation. Um, so a couple of things, uh, oh well, we pause we get that questions. What, what questions? Where can we go next? Everyone's gonna be quiet today. It's the first date. Everyone should be excited. Not tired yet. Here we go down in front. OK, so a couple of things about the virtualization right here. Are all of the families of the that we've been discussing available in spot instances and where does the sapphire rapid name come from? Uh, so, uh, they're all available at as spot right from the beginning. So when we release spot, the spot market is enabled, but the, the, the question is not so much or the. Um, technically available as spot, as much as is the capacity sufficient for a healthy spot market. Cause when we release and uh some of those things are uh very successful start from the release and customers are massively moving over to the new generations, there might be very little capacity for spots. So, before you get a, a, a healthy spot market, usually takes a couple of weeks, more realistically, a couple of months. Uh, though I would say that in general, the very early adopters of new generations or the spot customers who have, um, co, usually the spot customers using Carpenter or the very early adopters. Because when you use an allocation mechanism with Carpenter saying, I want everything starting from Generation X, the day we release before any on-demand customer has started to use those instances, boom, you're using it. And the price is low and the performance is great. And then the on-demand customers start to switch, and then the spot market kind of disappears for a couple of weeks until we launch enough capacity that you start to have a healthy uh spot market. Now about your second question, the code names from Intel, uh, I think that most of the code names are coming from rivers and lakes in the Washington state. Uh, that used to be true for sure. Nowadays I don't know because I don't remember whether there's a fire rapids anywhere in Washington. And I, I moved like I, I, I was in Seattle until uh six months ago and I've never seen any safa rapids, so our processor name is a little bit more straightforward graviton 1234. It's pretty easy to track and you know where it came from. You have another question here? Real quick a reply to what you just said, but why is graviton 2 generation 5? Because there was no graviton when we released the first generation of EC2-based instances, and don't ask me why the first generation of graviton-based instances was called A1. Because that was our first arm processor and maybe at that time we had the idea that we would call the next 1 A2, uh, but we never did that and we had A1 which was a porting vehicle and then the next gen we had CM and Rs that were C 6G and M6G and R 6G. Like, honestly, I would not want to be the one within the EC2 product management team having the responsibility to name the instances because however you choose to do it, you'll, you'll screw up at some point. Uh, and then my other question was, so you said that. As generations increase, the price per VCPU goes up, but the price per performance goes down. How do you, is there any methodology for like evaluating whether you'll save money going to a new generation? Like if you're on an 8-core workload, you have to go down to 4 cores in order to decrease your price, right? Mm, it depends. If your application is really running on a single instance, yes, that would be a problem. And there's the trick where if the performance goes up, because usually you, you have multiple parameters. Uh, it's not only the performance of the CPU, but it's also, OK, I might really need that amount of memory for my workload and, and I also need that amount of network for my workload. Um, so if, if network is really not the bottleneck, potentially you can go to, you're on a C, you can go to an M with half the number of cores, and maybe that's the sweet spot for your application. But more often than that, uh, you might have a pool of servers, a pool of web servers that are serving a given workload and, and you were using, you have your uh um A certain number of requests that you have to serve coming from your customers and it's fluctuating and let's say that on average you're using 100 of those services. Well, if you have one that is faster now, maybe on average you will be using 80 of them. So, they're more expensive on a per instance basis, but you're using less of them. And, and it doesn't need to be, I need to be able to use half the size. And don't worry. The, the idea is not to necessarily worry about the CPU load on your machines. Worry about how much, what is the, is the API request per second. What is the business measurement of that instance, right? The throughput of the instance. And if that increase and then you test it, right, that goes up, then you're getting that price performance metric. Now if you have that single server, um, only one, and there's no way you, you can distribute that, then you're right, you get better performance, but you pay more. So I've got a question about uh spot markets and like the health of them. At what point do you guys start tearing old server racks out? I run a lot of bioinformatics workloads. They need boxes and memory, but they don't really care about speed. I've got everything going back to like M4s. Is there a point where like there's a deprecation, stop using these. We're gonna start tearing them out. So if you're running, if you have things running on an M4, it's probably no longer running on M4 hardware. It's now running on xenon nitro. Uh, where we emulate our previous generation of, um, virtualization system and we make you believe that you're still running on an M4 and it, and it works, it, it works and, uh, if, if you're in a company where you had to validate that software with that specific piece of hardware, it still worked the same way, uh, but on our side it's probably no longer an M4. Uh, so we try as much as possible not to retire in instances for customers who need them. So there are accounts, uh, where you can still launch an M2, but that's no longer a real M2. Uh, now, from a price performance standpoint, don't run on, on anything that old cause it makes absolutely no sense. Um, from a price performance standpoint, there are other reasons, uh, to do it, but Uh, otherwise, yeah, we, uh, um, I don't think we ever, well, we did, but for some specific instances like old GPU instances where we can't emulate that and we could no longer maintain the hardware, and then we have retired those instances, but in general, we try, we do our best to um make them available for as long as we can. Question back here. I have one question. Uh, you mentioned about the price for the latest, uh, instance type, the price would be lower, right? But one observation I made is when you're going with the reserved instances, I saw the older generation are much cheaper. Uh, so that's, uh, so I think what you mentioned is very, very true for RDS where they change their pricing model for DRIs. Um, I don't think I've seen that for EC2. I, my experience, EC2 instance, also I observed, OK, it's possible, and, and we've been trying also to shift customers away from our eyes and to saving plans. So what I mentioned in terms of pricing was applying specifically to on demand, uh, the way we've chosen to price those long term commitment can be a little different. So the second question, uh, all the instance types are like one CPU, like memory and CPU are like strictly restricted, right? Is there any plan for AWS to, uh, you know, change? I can select the memory. GPU would be like, let's say 2, memory could be 4 or 8, the same way as GCP does it, not that I would be aware. Thank you. But we offer more diversity also. It's like, it's the two ways to handle that problem. It's either you have a very small number and then you make that, uh you can go custom, or you offer a lot of different combinations, and I believe that we cover a very Uh, a broad number of possible combination. If there's something that is really, really missing, uh, please let your account team know we have a mechanism called PFRs where we signal, um, our teams that, well, there might be that thing that is missing, and if we feel like, uh, the demand for that is big enough, we might just do it. My understanding is they may be moving away from that, by the way, so keep that in mind. I don't, I can't confirm this. They may be moving away from that, that choose your own instance configuration, um, it's, it's a challenge to maintain for sure, right? But with 950/950 instances, by 100s you can find something that meets your needs within our, within our existing portfolio. At the end of the day, it's where we choose to expose the complexity. If we decide to expose, to expose that level of flexibility, it means that we add some complexity in the way we carve the instances on the physical hardware, cause the physical hardware, well, is fixed. So, um, it's, it's really where we put the complexity. So far, we have chosen to put the complexity on Uh, on, on you in some way, uh, where we offer fixed sizes, we offer plenty of them, and, and you choose among those. Thank you. Um, I have two questions. Uh, first one you mentioned, um, socket, uh, count is to the maximum. No, on a, on Intel-based X class instances, you have 4 sockets, and on UClass, uh, that are very specific, you can't get them on demand and they're mainly targeted for SAP. You can get up to 8 sockets. The other question is around hyper threading and then you can uh if if you're going to talk about it, you can wait when you get there uh but I've always wondered about, um. Intel with hyper threading turned on, for example, a simple example, 2 VCPU, uh, VCPU with Intel hyperthreading turned on. And AMD which doesn't have hyper reading turned on. So I'm, I'm gonna talk specifically about that and I will show you how to know. Thanks. Up down here. Hey. Without increasing the VCPUs and moving on from the previous generation to the new generation, how come the performance is getting better? How is the performance better going from one gen to the next one? Oh, that's because, um, for all those vendors, um, they're improving the way, uh, those CPUs are designed. So there are plenty of ways you can do that. Uh, gen over gen, there's usually bigger caches. And that's true at L1 has been pretty much uh fixed on the latest generations, but L2 has grown, L3 has grown, you, you have massive L3 cache on the latest generation of Intel and AMD. So that helps a lot when dealing with memory excesses and making your workloads usually faster. Another area that can be improved is um Use, um, changing the way the branch predictor is working, um, same thing, plenty of applications that are super sensitive to how the CPU is able to predict what is the next set of branches that, uh, your code is gonna go through. So it's a The combination. Um, the, the, the memory network has also improved, uh, becoming more scalable. Um, the memory latency has stayed pretty much at the same level, but the memory bandwidth has increased massively. Uh, on G 5 Intel, if I remember well, you had 4 memory channels, latest generation has 8 memory channels. Um, latest generation of AMD based instances as 12 memory channels. So by increasing those, by improving those different areas in the processor, you make them better and faster. Let's say if I have, uh, if my application has any performance issues, instead of just throwing more CPU into their. You want me to, uh, so it always helps or not, um, giving more CPU to an application is not always going to make it faster because to benefit from more VCPUs you need to have an application that has some level of parallelism, um, so if it is able to use multiple, uh, CPU that's gonna potentially make it faster. Up to a certain level, cause it's you never or almost never have 100% of your application that can be paralyzed, so it's probably part of that application that is still sequential and you won't be able to accelerate that to the infinite. One of the things that I've uh forgot to mention about how processors have improved, they've also improved in the way that we're, uh, in how many instructions they can execute in parallel. Many people have a mental model of the processor of at every cycle, you do one thing. Uh, but modern CPU they do plenty of things at every cycle. They do memory loads and stores, they do additions, they do multiplication, and you can, on some processors do up to 6 instructions per cycle. Um, and some applications are built and optimized in a way where they can only extract a couple of instructions per cycle from a given processor. And unless you Um, you significantly optimize them, they're actually under, under utilizing the, the, the CPU. So, it's not a, there's not a straightforward answer to, to your question. It requires more analysis and, and lower level optimization. Thank you. OK, let's move on. Um. So, um, 111 thing that, uh, well, now most of our instances are using the latest generation of our virtualization stack, but when we moved from 4th to 5th gen, we've changed the way we do virtualization. We moved from Zen to nitro. Um, this, this definitely has an impact on the performance cause we've uploaded a bunch of the virtualization primitives to, to dedicated hardware. Uh, so, it has lowered the noise of the virtualization system. So, even you, you could not really do the comparison because um we never exposed the two virtualization systems on top of the same processor, but there was a significant gain when moving to uh the Nitro virtualization stack. Um, it also reduced the noisy neighborhood effect that you could get from the previous system. There's almost no noisy neighborhood, uh, uh, problems with nitro, except at the memory bandwidth level. Uh, there's a, uh, there's another thing that nitro made possible, which is bare metal instances. Because we've moved all the virtualization primitives outside of the instance, so, uh, everything with regards to um network access to EBS and all of that is uploaded, so there's nothing uh that we really need to do at the, the hypervisor level from a security standpoint. So we can offer instances as a metal option where we remove the hypervisor and you get full control over the CPU. The, the drawback of that is we only offer them as full as full instances, um, so they do not come in small chunks, um, but in some cases, if you need to get full control over the processor, it's possible with the metal instances. From a performance standpoint, don't expect the metal instances for normal workloads to be faster. Um, the, the level of performance penalty introduced by the network, the, the nitro IP advisor is extremely limited. Um, there's a little bit of an impact, uh, for people having very, very strong requirement at the network level. So, people doing high frequency trading, for example, uh, you'll see a difference. Um, But for normal people doing normal web application databases, there's absolutely no point in going to medal. Unless you need for other reasons to uh run your own virtualization system, that, that becomes uh more of a functional requirement and not so much a performance requirement. Um, um, we talk about it here. It was the question before about hyper threading you were gonna answer it. Is this the slide to talk about it? Yeah, yeah, uh, oh well, the title of the slides, by the way, is wrong. Um, so you had a question about hyper threading, I think I forgot who asked, yeah, um, so that thing has changed quite a lot, um. So, before we introduced graviton, all our instances uh had hyper threading enabled. So, you had, for each physical core, you had two threads uh mapped to a physical core. Um, starting with graviton, we introduced stances that were uh single threaded, so each VCPU is mapped to a uh physical core. Um, Starting with AMDG 7, AMD is also without hyperthreading. And that has a bunch of consequences on, on the performance side. So I can show you a quick um demo on that. OK, so what I'm gonna do is, um, I have, uh, here I'm connected on a C7I, so 7 gens Intel-based instances. Uh, if we look at what we have in slaprox slash CPU info, that's a C7I large, so that one exposes two VCPUs. And those VCPUs are actually uh threads of the same physical core. So, I'm gonna launch a quick um OpenSSL speed test. Um Next I must have it here. And I'm gonna do exactly the same on a graviton-based instance. And that one also same size, so that's a large again but this time we have, we still have two VCPUs but each of those VCPUs is a uh physical core. Um, so we get, we got a certain level of, um, of performance with that one. So if we look at what we were able to do in terms of signature per second, we're roughly 39,000. Now I'm gonna do a second thing where I will do that, uh, but this time with two execution threads. So, um, OpenSSL has the ability to, uh, parallelize, uh, what it is doing, so it's. Actually using two processes to do that now, and, and we'll see what we will get from there. And so, we get a slightly higher performance uh at 42,000 signatures per second, but we don't double uh what we had. On the graviton side where we have one VCPU web map to one physical core, if we do exactly the same. So, uh, first what we can observe is that in terms of signature per second we were slightly slower with a single, uh, thread of execution and now if we do it, uh, on two thread of execution. We have almost doubled, not quite, but almost. Um, and, and I'm, and I don't wanna use that test to say Graviton 4 is better than um Intel in, in that case, what I wanna illustrate here is that um the, the SMT versus no SMT has an influence on performance. If you test your application at low levels of loads, Um, what you'll see is actually two different types of design. Intel has big beefy cores and potentially they will give you extremely good latency at low levels of utilization. On the graviton side, we have smaller cores, but we have more of them, allowing us to map one VCPU to one physical core. At low levels of loads, potentially we are not as fast. However, we scale higher. So, two different designs leading to two different um performance profile. I'm not saying use one versus the other, that's not the point. There are plenty of other metrics and there are cases where the Intel processors will be faster and there are cases where the graviton processors will be faster, but keep that in mind when you're evaluating the performance of uh those instances, and the same would be true of AMD processors. AMD processors, it's a little different. We have chosen because AMD had very high core count uh processor, we have chosen to expose them as non-SMT starting from gen 7. Um, they're more expensive on a per VCPU basis, but the performance you will extract if you're parallel enough and if you run them hard enough is gonna be extremely good. And I think that's a key point for this demo is this was paralyzed, right? Like the application can't be paralyzed, it's not gonna be able to take advantage and then to going back to Arthur's point earlier, if that's a key distinction if the application can take advantage of having those bigger, larger cores is important. Yes, so this information about. Is that I'm, I'm pretty sure if I, I'll help everyone else here yeah, um, so my question was, so this, uh, information about hyper threading, for example, and probably CPU instructions and other things, yeah, I can probably find it on the, on the internet, probably there's a blog post from AD West saying this. Is that exposed in any APIs that could, it's not exposed in APIs, but the processor tells you. But I, but then I'll have to launch an institute to check it. Yes, if I have, uh, there are plenty of things that the APIs are not exposing. The API is not exposing the amount of L1 and L2 and L3 cash that you get on any instance. The API is not exposing the puma topology. The, the API is not giving you, I don't know, the memory latency or the theoretical memory bandwidth. There's plenty of characteristics of a processor that the API is not giving you. Or um other examples, each of those processors have um options in their instruction set. So, uh, some generation of Intel processors have the ability to expose a 57 bit, uh, virtual address space. Not all of them, some of them. Um. It's not in the EC2 API. If you launch an EC2 instance, you can check whether the operating system has been able to detect that specific feature. But there's way more to a processor than what we can really realistically expose in the API, um, because one of the possible use cases would be, um, so I have quite a few EKS clusters, for example. So if I could expose, I mean I could create my own catalog, uh, I, I could, you know, do the homework and get exactly the instructions I care about and all that, all that kind of stuff, but I could create no pulls. That would expose those instructions as labels so I could schedule specific workloads onto a specific set of processor, right? So it's not, it's not like you're doing this indefinitely per processor per family per. Well, remember if it's a C8M8 R8, they're gonna have the same processor in them, right? So it's not that sure, but a 910. I have to just do well, and when we refresh, absolutely you need to and that's true of any time we have if we've launched something new, always test before you. I understand the request and I get it. It's, um, but there's a question of to which depth do we wanna go there. Um, so far we have chosen to limit ourselves to exposing the frequency, exposing the CPU type, and once you launch the instances you can get all those information because they were exposed to the operating system, but we don't make them available as an API an API. The, the, there's a, we do have an internal discussion on whether we wanna make, we make it part of the documentation which could be a way to do it yesterday, I think, right? The reality is that we have our team has that documentation for ourselves because we find it useful. So yes, we could expose that. What else? Who else's questions? Uh, can we go back to the presentation, please? Right here, right here, Arthur. Oh, sorry. Um, with your demonstration, it was a benchmarking on OpenSSL and you compared a. It's a terrible one. I don't have any conclusions from that. So you're comparing a 7th generation to an 8th generation uh graviton that was on purpose. If you tried the 7th generation graviton, I think you'd have probably lost significantly. Uh, the 2, the, the, the 2 VCU1, I would not. The 1 VCPU that would be even slower, but the, the, the 2 VCPU, no, I would still win that with a graviton 3. Once upon a time I tried to migrate a load balancer to um. But that was another problem. I know which one instructions you will be talking about. So the problem you, you probably have seen with an SSL load balancer was a graviton 2 problem where on graviton 2, we made a mistake, uh, that we, and, and we did not anticipate that it would have that kind of consequences. Uh, and the problem was that we did not anticipate that RSA would be, um, that much slower on Graviton compared, on Graviton 2 compared to Intel-based instances. The reason behind that was that RSA uses a lot of 64 bit integer multiplications. And we had a single 64 bit integer multiplier on the graviton two core. And when running a um uh SSL connection, a TLS connection, At the session establishment, Graviton 2 was much slower. The AES part of an SSL uh session was much faster on Graviton 2 compared to, uh, Intel-based instances. So, if you had long connections, Nothing was visible and Graviton was doing great. If you had short connections, like, um, let's say, uh, um, a gateway for a bunch of IOT devices or a gateway for an observability system where you re-establish an SSL connection every time you send metrics, Graviton 2 was terrible. We fixed that with Graviton 3. Adding one multiplication unit on the core, making Graviton 3 much faster. And then we also fixed partially, we mitigated that on Graviton 2 by optimizing RSA for Graviton 2. So, within AWSLC, which is a uh crypto library uh from AWS we have a much, much faster RSA implementation on Graviton 2. So that thing no longer exists and Graviton 4 is doing great on crypto, um. So we've closed that gap, but that gap definitely existed at the beginning of the life of Graviton 2. Great question. Who else? Where should we go next? We've got just under 5 minutes. OK, well, we have 5 minutes to talk about memory topology, Ben. Um, So, Earth isn't flat, um, and it's very visible on some AWS instances. Uh, so you know what that thing is? Well, actually, that's mentioned. And so that's the memory topology on a uh C7A. And what it exposes is that that processor is structured with groups of eight cores, uh, they were called CCX, uh, compute core complex. Um, and those eight cores were sharing a slice of L3 cache and a connection to, um, the, the memory die. So, when you run on an MD based instance of that generation, your application won't see a flat topology. If you have an application thread that runs on that first block and accesses something in memory, And that part of the memory will stay warm in the cache. If you try to access the same uh part of the memory from the next CCX, the information won't be on the cache. Um, and I've seen cases of customers moving from um 4 Excels, so, uh, that would be, oh sorry, 2 Xcels, so that would be one single slice to 4 Excels, now you are spanning across 2 slices and seeing their application being slower. Because now they were, they were across two memory domains, and if they were trying to access the memory from one domain where uh things have already been warmed up from the other domain, well, they had a cache miss, had to replenish the cash first and that wasn't as fast as what they expected. Um, and that one is on a 24 XL. On a 48 XL, you would have an other level of uh of non-uniformity because you would have a second socket, and you would have an even greater distance between the cores on the first socket and the cores on the first socket, and the memory associated to those two sockets. Um, so, when you deploy things on AWS, pay attention to that, uh, the maximum sizes of our instances tend to be two sockets. So, when going from, uh, let's say in that generation of instances from 24 to 48, there are cases that may not be faster. You will get more core, you will get more memory. Um, but if your application doesn't take into account the fact that the memory is no longer, uh, flat and uniform, you might have, uh, uh, weird surprises. I had a customer a couple of weeks ago who moved its database from a 24 XL to a 48XL and the P50 uh latency of its database doubled. Because now memory was spread across the two sockets, OK, it had more memory, more VCPU, but the database on average was slower. Some requests were just the same speed. The overall throughput was higher, but the latency was definitely higher. And this, this can also be an issue with, with Coopernity since someone was talking about using that before, right? If your containers land on different. On different domains right now, all of a sudden on different sockets now the, the connectivity between them is slower than you might expect, right? I see nodding heads. So, so that's something to definitely be aware of as you're looking at. I know Kubernetti's has got new features becoming Numa aware, right? Something to look at. As you're doing that, so just something that can become super important. These are some of those low level differences that start to make a big difference in. Yeah, my recommendation, unless you know exactly what you're doing, would be avoid the two socket sizes on your Kubernetes cluster. Um, well, if you know what you're doing, then go do it. Uh, but otherwise stay with the single sockets again. Well, we don't expose that in the API. But even with multiple sockets, if we, if you land on a multi-socket host, but you have a short or a small on demand. Yes, Alright, so does that advice apply as strongly to trying to keep it to a single, uh, complex size, a lot less because the difference in terms of latency and memory bandwidth between two C is much, much lower compared to between two sockets. And well, thank you everybody. We are out of time. Please fill out your surveys. And for those of you that want a sticker but didn't ask a question, come see me. I'm happy to give them out. Thank you.