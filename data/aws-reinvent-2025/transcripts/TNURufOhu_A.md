---
video_id: TNURufOhu_A
video_url: https://www.youtube.com/watch?v=TNURufOhu_A
is_generated: False
is_translatable: True
---

Um, firstly, I'll apologize for a bit of AV. We might have some challenges partway through, but, uh, thank you for your patience. So hi folks, welcome to Dat 442. If you're not here to learn about Aurora and resilience, you're in the wrong place. Uh, my name's Tim. Shortly, excuse me, I'll be joined by Mark, and we're gonna talk today about, um, Aurora and how to bring resilience to your applications using Aurora. So who here, show of hands please, runs a database in production? Most people call, who here, cos you're still, you're at Reinvent, you're not at work right now, who here feels a little bit uneasy about running a database in production? Uh, some, lots of hands actually, right, um, so you got a pager in your pocket for that kind of thing, so, um, I'm, I'm, I'm happy that, that you're here, we're gonna, we'll learn today. There's a whole bunch of choices we can make about making, uh, any system more resilient, and Aurora is no different. Alright, so what we're gonna do, first we're gonna define resilience so we know we're talking about the same thing. Right, so resilience is technical definition, the ability of a workload to recover from disruption. Um, or from for infrastructure to recover from disruption, to dynamically acquire resources, to meet some demand, and to mitigate disruptions caused by misconfiguration or something like that, right? So it's not just about when something crashes, it's not just about when a server catches fire, it's got performance and a whole bunch of other things in it too. And it's really got two pillars underneath it, it has availability and disaster recovery. So availability, that's the proportion of time that a workload is available for use. So this is usually a historical time measure. We might say it's been 5 9s availability for the past 1 year or something like that. That's when we talk about availability. The other one is disaster recovery or DR. So normally when we talk about DR, this is the techniques you use to recover your workload when something goes wrong. Maybe there was a volcano or something like that, or some human threat. Um, here we use recovery time objective, RTO as one of the measures that we talk about. So that's the maximum amount of time between one single event and the resumption of service, that's your return to operations time. And then there's the recovery point objective, the RPO, which is the maximum amount of data, normally measured as an amount of time, that we can afford to lose, maybe due to asynchronous replication or something like that. And you might want your RPO to be zero, that's a valid value as well. OK, so that's the level set, so now we're just a bit of an agenda. Um, next we're gonna talk about Aurora Posgris, which I'll call APG and Aurora MySQL, AMS. I have to say that word about 10,000 times, so apologies for the uh acronyms there. Now they're both very similar, so a lot of the features that they support are similar, so I will talk about them together with respect to resiliency. And then about halfway through, Mark will pick up and he will talk about the same set of questions that we're gonna ask ourselves about DR and HA. He's gonna answer them again, but from the point of view of Aurora D sequel. So keep your bearings, first part Aurora my sequel and post Chris, second part de-sequel. OK, so first, if you don't know anything about Aurora, uh APG and AMS what are these things? So they're relational database services that are that combine the speed and availability of a high-end commercial database with the simplicity and cost effectiveness of the cloud and open source. So APG and AMS are fully compatible with Postgress and MySQL open source engines, and that allows your existing applications and tools to run without modification. So today we're gonna get into some questions, like 5 questions we're gonna ask ourselves, but if you wanna ask your own questions, there's a chalk talk. 424 on Thursday as well that you can, you're more than welcome to come along to and hit me up. Alright, so question one we're gonna ask ourselves about HA and DR. What if my database node fails? So if you're in self-managed, this is self-managed, little tag up in the corner there we can see, um, so if you're just starting out with self-managed, you might have a system that looks like this, you've got one node, it's got your database engine, it's got your storage inside the same node and there's only one of them, right? So that means that um all of your storage and execution work happens in there. You might back that up with some third party tool, you might send your backups to uh S3. When you do the backup, we might need to do some kind of quiesce thing there, so that's gonna slow down your performance, maybe give you some bumps in availability, that's not great. Because that's really difficult, we probably don't do that back-up very often, maybe we do it once a day. So that means that your RPO is something like 24 hours. That's not so great. So then if that node goes down, it means that your storage and your engine has gone down, right, so that's not great for your availability and it's maybe not great for your durability either. So your RTO could be many hours, it's not a great situation. So let's look inside Aurora and look at the durability story first, the RPO part. So you can start with a single database cluster over there on the left-hand side there and and one availability zone, single instance, and we're gonna use this empty box at the bottom here, we'll fill it in as we go through today. This is Aurora storage, Grover is its code name. This is where all the magic happens with Aurora for AMS and APG. So Aurora storage is a multi-tenant storage fleet. We've got storage nodes, they're the big pink boxes, they're spread across 3 AZs. They're multi-tenants, so they've got some other customers' data inside there as well. They're the little colorful boxes. Sorry about the bad graphics. Alright. And so what we're doing is we're gonna, every right that you make through from your engine, we're gonna make 6 copies of that right across 3 availability zones. You're only gonna pay for one of those copies. What that means is that we can lose a whole availability zone worth of storage nodes, and 1 more of them, and we still don't have any durability problems, right? So it's a really strong durability story out of the box with Aurora. You did nothing to configure this as you're getting 6 copies across 3 AZ's. Alright, so your RPO is 0, there's no asynchronous replication going on here. So just by using Aurora, MySQL and PostScripts out of the box you get this. So we'll dig a little bit deeper in this area. So what is this data that we're storing in here? So a traditional database, um, writes a log record of what it's doing, then it writes the pages, and then it might come back and checkpoint them. It has to do this in case it gets interrupted, right? What Aurora's doing is it's writing just those log records. It's not writing pages, never writes pages to disk, it only ever writes log records. The storage understands how to unders to apply those log records and turn them into database pages again. Which we need when we need to uh save some space or we need to satisfy some reads. So if you think about it, what that means is Aurora has this whole timeline of what's happened inside the database. Every single commit that's happened into the database, we have it there in sequence. So that's how we can implement some of these features we'll talk about. Alright, so first one of those features is back-up and restore. So on the left there you've got your production cluster, your database is is writing down, so I've got some colored lines there of those boxes. They are the different database pages and their log record lineage, right? So what we're doing is the storage engine, not the head node, is continuously backing up all that data to S3. So the first key point there is your head node's not doing it, so it can't possibly impact performance of your head node, there can't be any quiescing or anything, this is continuous. It's happening all the time. So what this allows you to do then is to do a point in time restore from anywhere from the earliest restorable time, which can be up to 35 days, configurable in your retention window. Up to the latest restorable time, which is about 5 minutes ago for a busy database. End point in time there we can choose. So let's do one of those restores. Let's choose time equals 7 because I'm unimaginative, to do this restore. What's gonna happen is, those storage servers are gonna pull that storage uh data back from S3. We're gonna apply our internal log records to catch you up to exactly that point in time. And then we're gonna start up an engine and the engine's going to do its normal transaction recovery logic, so it's it's logical level, um, To handle interrupted transactions and things like that, right? So again to answer the question, the second part of your job's really easy because all of this comes out of the box, you didn't have to do anything to support PITR Grover Grover's doing for you underneath. Alright, so that's data, but what about database instance availability? So in your most basic configuration can look like this, you've got one instance. If it fails, then we can make another one, but it'll take 10 or 15 minutes. So what you can do is you can bring up another um instance here, we'll call it a standby. Aurora will keep um keep it in sync as well, so remember they're sharing the same storage and we're, we're every transaction that's happening, we're telling the other guy about it as well. So now you've got a replica, so whenever something needs to fail over. We'll make them the same size, whenever something needs to fail over, then the other one could take over really quickly within just a few seconds, and because they're the same size, they can they can shoulder the same amount of load, right? This one becomes a primary. So this gives us an RPO of 0 because they're shared storage. They're not replicating storage, right? We're not worried about asynchronous commits or anything like that. When you fail over, your RTOs typically around 30 seconds. So by doing this, this is one of those patterns to get what we call a multi AZ availability pattern. Really increase your availability. OK, so what about performance is the next question. So you can performance can fluctuate, right? Your application can push harder on you, um, then it can push softer on you and you might be wasting money instead, so we wanna be resilient to these performance level changes too. So open source, you've broadly got two ways to deal with this, right? So you can scale up an instance, probably that means, you know, buying some hardware and doing some uh migrations, that's a bit hard. Or we can do some scale out, which means in the application layer, you would have to be aware and divide your database into pieces and the application has to know which piece to go and talk to. That's really difficult for consistency, you have to deal with it as well. And noisy neighbors is tricky to deal with too in this situation. So back to AMS and APG, what can we do about this? Well, we can add that replica that we had before, we don't just need to add it for um, availability purposes, we can add it for performance purposes too, cos we can just send reads to that other replica. Right, it's, it's gonna see stale data up to that, that, uh, asynchronous lag, 100 milliseconds or something, maybe like that, right, but it's completely offloaded, it's not gonna have any impact on performance on the writer. Now if you've got another workload, maybe like some kind of analytics job or something. Then you can run that on another replica, then, then all the resources on that replica are isolated for that application. It's buffer pool being the most important one for performance. So this is really important for uh resiliency against performance changes when you can separate out your noisy neighbors. So you can add up to about 15 of these replicas, so you can be quite fine-grained here. Each of them, if you're really big, they could be 48 X largest, that is 1.5 terabytes of memory, that's a big system, 15 of them, right? Now, I haven't talked about anything to do with storage scaling yet, and that's because you don't have to worry about it. So the storage is, is scaling in capacity automatically and it's also scaling in performance automatically. There's no there's no performance provisioning in AMS or APG at all. OK, now instance scaling, so we have this instance off to the side there, it's an RAG, latest Graviton 4. It's a 16 X large. Pretty big box, not the biggest one we make, right? Um. So we can add the same sized ones, good for failover like we talked about, we can add different sized ones, 8 XLs, 4 XLs, we can add this DB.servius thing too, which we'll talk about some more. That's, that's an, an elastic instance that grows and shrinks all the way from 0 all the way up to uh 256 aurora capacity units. Um, when we do have a lot of instances like this, we have to decide which order we want to fail over to them. There's no good me trying to fail over to one of those 8 XLs because it'll be too small to deal with the load that I have. So I need to define these, uh, failover tiers, we call them the tier with the lowest number, tier 0 in this case, that's where we'll fail over to first. That servius uh instance, we'll talk about a little bit more, can expand and contract so you're resilient to going up when you have a spike in workload and then you're not gonna get stuck there paying a bunch of money, it's gonna scale back down again. OK, so with all these instances, that's all good, but my application has to connect to them somehow, right, and that's can be a bit pain to know which one is the writer, I need to um talk to that one if I need to send transactions that have rights in them, like updates. How do I do it? And I don't wanna have so many of these connections either, I don't want to have lots of idle connections, that's not great for performance, not great for cost. So, I don't know which connection's gonna go where, and it's gonna have high overhead even if I can work it out, so that's probably not a great situation for self-managed. So I can use two techniques here, pooling and limits. So we can limit connection count per DB instance, the big one should get more connections than the small one maybe, uh, per type. I can avoid reconnections, this is all pretty standard, there's no special aurora here. If you've done databases for a while, this should look pretty familiar. Some of this stuff is provided for you by the driver frameworks that you already use, if you're using Spring maybe and JDBC and some other things, right? Um, but they tend to fall down with clustered databases like what we're using here, cos they don't understand what's going on. So the first thing we're gonna talk about is endpoints. With Aurora, we have two endpoints, generally speaking, you have a writer endpoint which will always point to the node that is the writer right now. And when that changes, we will point it somewhere else. So you can always just send your rights to the writer. Then we have a reader endpoint, which points to the pool of, remember, cause there could be 15 of them, the pool of instances that can accept reads using DNS round robin. Uh, so because it's pulled that lets you, uh, just do some coarse load balancing there as well. So when we need to fail over, then we just repoint the writer endpoint. Um, that can happen really quickly just with a DNS look-up and a reconnection. Your application already has to handle this reconnection anyway for other errors. So this way we can turn over in basically what the DNS, um, time is, 35 seconds or something like that. OK? For further flexibility, you can create a custom endpoint, if you wanna just group together a few endpoints and call them the analytics endpoint or something, you can do that too, it's up to you. Right, so further to this though, we have this thing called AWS Advanced wrapper drivers, right? So this is when we talk about availability, not just performance of that connection kind of question. So each of your applications, um, pieces of code has to have specific reconnection code inside it to deal with these errors. Right, it has to recognize that this is a retrial error, it has to deal with that. Uh, it has to do this fast enough cause this is availability time, it's a bit of a problem, right? So Aurora gives us two extra tools here. We've got rapid drivers and we've got RDS proxy. Um, so the rapid drivers, they've got intimate knowledge of what's going on inside a cluster. They know which ones are the readers and the writers at this point in time. Um, and so they can handle failover much more quickly. They're built by wrapping around your existing platform driver like JDBC in this case, but we have them for ODBC, no JS Python, a bunch of other ones, right? So if you've already qualified your platform driver, we're not gonna change it, we're just gonna wrap it with some plugins. One of those plugins is called enhanced fail failover. Another one's rewrite, splitting, blue-green, which we'll talk about, and a, and a few more. So if you're not using these drivers, we really encourage you to go and do that. Great for a whole bunch of reasons. What this will do, this will reduce your failover time down to about 6 seconds as well. OK, so if you're interested, um, there's a QR code for the uh for the GitHub project there where all of these are open source, we're not doing anything kind of secret squirrel here that you can, um, Not do yourself, we're just doing it for you, right. So that was all in one region, but what about multi-regions, so what about if a whole region fails, right? Um, well, that's a problem, maybe if my business needs to continue, um, when that region's down, or if even if that region didn't fail, but if it's cut-off from the network, which is maybe even a, a more likely failure scenario. So what we've got here, what we'd like to have is a copy in another region, and we'd like to be as up to date as possible, we'd like it to not cost us very much money, and we'd like to be able to use that copy for other things. If you're self-managing, this is pretty hard. So with Aurora, we have Aurora global database for AMS and APG. So this is where you can add up to 10, but I can only draw one here cause my slide's not big enough, um, 10 secondary regions. So on the left region A, we have the same picture we've been talking about for 15 minutes now, right, some replicas, some storage. When we add another region into this thing. We internally in Aurora, we spin up a replication server replication agent, we handle this asynchronous physical replication between them. So notice how, so you're um, Head nodes are not involved in this at all, this is not logical replication, you're not having to pay for the performance tax of Bino and MySQL and things like that, we're doing it all underneath, OK? So that means your application doesn't even know that this is happening, so you can have up to these 10 global uh secondaries without even knowing about it, right? So we write some of those log records, those colorful boxes I'm so good at drawing, they also get replicated over to the other side on on the storage in region B. You notice how there's no head nodes in Region B at the moment in my picture, there's only storage, so in this particular configuration right now, this is good for keeping your data available, um, durable, but it's not really good for availability, right? We haven't got our workload able to run over there. Your RPO, the lag, because this is asynchronous between the region A and region B, it's typically about 1 2nd, depends on your choice of region pairs. Um, I can't do anything about the speed of light, I'm afraid, so, uh, if it's they're far away, it's gonna be a little bit slower. Alright. Um, and then if you wanted to, what we could do is we could add some other instances over to the, uh, secondary region as well. Now I've drawn a symmetrical configuration here, but it does not need to be symmetric at all. You can have as many uh different instances there if you'd like. And they're read only nodes, they're replicas just like we said before, so they can do low latency local reads. Alright, so if you don't test it, um, then you don't know it's gonna work. So what you gotta do, first, we have this global endpoint thing, right, so the same question we had before about how do you know which one is the writer that you've gotta point at. Well if we zoom that out to the next level up, the, uh, the global level, how do you know which region is the primary region at that point in time? Well, that's where this thing called a global endpoint comes in. So if there's some issue in region A, we've got that global endpoint name there, and we're gonna, you're gonna trigger a failover first, we'll talk about that some more. Um, that means the writer is gonna move over to region B, this is a managed operation. You see that allow data loss flag down in the bottom corner there. That's to say that, remember to try and remind us all that this is asynchronous replication, so anything that was inside that asynchronous replication window has a chance of being lost when we do a force failover. OK, so when that's happened, the writer is moved over to the uh other region, we've got a new primary region, now Reg B, and the global endpoint is going to be pointed back at that using route 53 data planes, so that's happens as fast as we can make it happen. Alright, so this gives you really high right availability across multiple regions. So what if something hasn't failed in global database, we just wanna do what's called a switchover. So this is what you do when region A and region B are both healthy, not when one of them's gone bad. Um, so what we wanna do is we want to, uh, tell region A, you should stop being the primary, and region B you should start being the primary at this point in time. Now this is pretty interesting the way we built this to to allow it to happen so quickly, so I want to go into it a little bit. So we have this log record stream, right, so we insert this special log record into that stream, a marker log record, and that flows through, just like we described before, and the other side sees it, so when the other side sees it, it says, OK, at this instant in log time, not wall clog time, I should become the primary right now. And Region A knows that he should stop being the primary at that time. So there's a nice clean handoff we can get, zero RPO, really low RTO. And um I just think it was pretty funky, I wanted to tell you how it worked, right? OK, so, and, and then we also know that that'll give us this RTO of about 30 seconds, an RPO of 0. Right. So the final question we're gonna ask ourselves is what about maintenance? So like version upgrades. So remember we're fully open source compatible, so we have a version, we might have postgress 16, and we need to get to postgressive 17, well how do we do that? We might just have a patch or an OS patch that we need to apply, how do we do these things? They're all upgrades, we call them. So if you're self-managing, then you'd have to take downtime window, right, you'd have to go and talk to your customers, work out when they would like to have some downtime, um, you would need to do some take some snapshots so that you can do some rollback, that might be difficult if you don't have the cool storage like we do, uh, you'd have to coordinate with everybody. You'd have to do compatibility and testing and lots of it between your application, between the old database versions, all these other things. Something people often overlook, you know, is performance, is that when you do an upgrade, probably even if you just do any old restart, your performance is probably gonna drop because you've just restarted your engine, lost your buffer pool, right? So performance testing's gotta be part of your upgrade compatibility as well, if you're self-managing. So whether you're in AP APG or AMS, Uh, because we're fully open source compatible, we can't ignore this, right, we must, um, maintain a strong security posture. So we provide a fully managed and automated, if you like, um, minor version and patch upgrade experience. So we can see here in the console, uh, we got some maintenance actions there, there's um upgrade available for this, this database cluster. Um, so we do rolling operating system upgrades now that we didn't used to do that, that means we have an increased availability. You define a maintenance window, if you don't want to define it, we'll define it for you, that says that this system will receive any automated maintenance within this window of time, if you like, OK? You'll get maintenance here and in the AWS health dashboard, and this is all able to be automated if you like, it's opt-in, um, we of course strongly recommend that you opt in because you don't have to think about it anymore and we'll deal with it. Alright, now, a new thing for us here, AWS organisation's upgrade roll-out policy, bit of a mouthful, but what that means is, we had a good story for a while about how to automate the life cycle of one DB cluster. But I hope that most of you probably don't have one DB cluster, you've probably got tons of them, you've got a fleet of them, and you wanna, you want to manage them as development and QA and prod or some other mechanism that you've got, but we never really gave you a way to deal with that before. This is where organizations upgrade while their policy comes in. So on the left we have this a GUI screenshot, on the right we have the JSON version of the same thing. So what this means is that you set a policy based on tags. So, here we have a tag called EV, and we say that any resource inside my AWS organization, that might be an account or a DB cluster, um, that has, An end flag value of prod will be upgraded at last. And you can see some other ones there, if it has an end value of dev, it'll have to be upgraded first, go into this some more. And anybody who doesn't have a tag that I recognize, they'll get upgraded second, that's the default at the bottom there. All right. So how does this happen when we actually roll it out? So we we built that policy, now we have to assign it to the resources in our organization, maybe it's an account or all of the things in the organization. And then what'll happen is we, you can, you can see this, you can see the upgrade order is determined to be 2nd for that resource there, that's really helpful. So first we'll notify you, then we'll wait for the maintenance window of the resources in the 1st wave, and then we'll upgrade them. And then we'll let them sit there and cook for a little while until the maintenance window for the resources in the 2nd wave comes along, and then we'll upgrade them, let them sit for a while in the final wave. Right, so we're not actually blocking from the first one to the 2nd 1, but we're waiting, and if you've found some issue in that time in there, you'll know how long that window is, it's different for different resources, then, You'll be able to remove the tag or turn off AMVU at that point in time to stop the progression of upgrades. Alright, in-place upgrades, if we're doing major upgrades, they're incompatible by definition with um with open source. So we can upgrade in place, we can do this first by creating a clone of the database cluster, which happens really fast and doesn't cost anything. We don't charge you for the storage other than for when you make changes to the storage. Then we can practice on the, on the clone, we can upgrade the clone, we can test the clone with an application. Then when we're happy with that, we said, great, OK, throw the clone away. An upgrade in production, that's one way to do an in-place major version upgrade. Pretty straightforward. Alright, a more flexible way, if we can do it, is to use what we call blue-green. So here I've got my production situation here, I've got a couple of replicas, an endpoint, and I um, I'm gonna use blue-green to do a major upgrade. I don't have to do just for a major upgrade, I can do schema change, static parameter change, anything like that. Blue-green means I'm gonna call my old environment blue. I'm gonna make a new environment called green, and I'm gonna keep them in sync with logical replication. You're not doing any of this, I'm doing this. We do the upgrade, still in sync with logical replication. You test it. When you're happy, you you trigger blue-green switchover. In switchover, I promote the pri the uh old one to the green one to be the production. The old resource still there, all the resource names are renamed, so that you don't have to go through any of your scripts afterwards and things like that, it's after the Blue greens happened, it's as if it never happened. OK, very flexible, and this this switchover can complete in as little as a minute. Alright, so in case you missed it, this same thing with blue-green, we just announced for global database as well. So here we have our blue uh blue configuration, our green configuration across two regions in this case. Everything I just described to you for blue-green, exactly the same, except multiple regions. Alright, so so far we've learned how APG and AMS out of the box give you protection, just that storage at the bottom there, against one whole AZ unavailability plus one. And then we saw how we can add some replicas in here to increase our availability and and allow failover to happen, and we can use AWS advanced rapid drivers to uh smooth that out. Then, we learned how global database can be used to provide durability and availability across up to 10 secondary regions, supporting low latency local region reads and simple cluster endpoint management with that global endpoint. And finally, we learned some of the patterns to follow to keep your database cluster up to-date, um, with managed and automated cluster-wide upgrades, fast clones, and blue-green. So now I'm gonna hand over to my buddy Mark when he comes and maybe change some slides I think, um, to tackle those same five questions again, but from the point of view of DSEel. Thank you. OK. Hi, everyone. My name is Mark. Sorry about the uh unplanned downtime there. Um, for the rest of this talk, we're gonna be looking at building resilient applications using Aurora DSQL, which is the newest engine in the Aurora family. Aurora's been around for about 10 years. This is only 6 months old. And DSQL comes from Our customers asking us to give them the best of relational. So this is the ability to run complex queries, joins, evolve your schema. The best of distributed database architectures like Dynamo DB where there isn't a single point of failure, where we scale out rather than up. And if that's not all, also the best of serverless services like Lambda, where you're only paying for what you use. Um, we're gonna go through all 5 questions again, starting with what if my database node fails. But before we do that, let's take a crash course into the DSQL architecture. On screen, we have self-managed posttress that you've downloaded from the internet, you're running on an EC2 instance. Our application is connected in, we're exchanging some credentials. And the server is gonna fork itself, creating a dedicated Unix process for our connection known as a backend process. We can connect a few times, a bunch more processes. And each of these uh sessions can run queries that are gonna, uh, pause, plan and execute your SQL against local storage. Of course, we can run updates to, sending those to the writerhead log, which is being used to keep storage up to date. Keep in mind these three components because they're gonna make a comeback right now. Here's DSQL, our applications connected in, and it's connecting to a component that we call the query processor. This is just like that back-end session. We get 1 of these per connections. And just like we saw in Postgress, we can send uh SQL select statements that are gonna be turned into read queries, sent over the network to DSQL storage engine. Meanwhile, writes work a little bit differently. If you run an insert statement, putting a new row in the system, that row is gonna be buffered in memory on the query processor. If we run an update statement, the QP is gonna take that update statement, turn it into a read against local storage to fetch the current version of the row. Uh, process that statement to produce a new version of the row, and then again, keep it in memory. And you can keep doing this as you build up your transaction, insert, update, delete until we call commit. When you call Commit, that entire bundle of everything you've asked the database to do is gonna be sent over the network to a service called the Adjudicator, which is responsible for concurrency control. So this is the part of the DSQL architecture where we're checking for if there are any conflicts happening with other transactions running in the servers. For transactions that commit, they're sent to a service called the Journal, which is a backbone service of key AWS services like S3. And the the Journal is responsible for durability. It will replicate our new transaction to at least 2 availability zones. And finally, just like what we saw in, in traditional postsgress, we're gonna be using that journal to keep our storage up to date. If we open another connection, we have another QP that QP can do reads and writes. Um, but there's nothing in this architecture that says that that QP needs to be running on the same host, or even in the same availability zone. DSQL is an active active service, which means that any connection can read or write any data at any point in time. So let's talk about availability. What happens if my database node fails? We're connected to that flashing QP in AZ 2, and we're doing some, we're doing some fancy joins. And uh it turns out our storage node that we're talking to fails. It explodes. Oh no, what do we do? And the answer is you as a customer don't. have to do anything because the QP is gonna detect that failure. It's gonna start to shift traffic over to a healthy replica. Now this replica may be in zone, it may be out of zone. I showed that here on slide. And we're gonna continue with only a small increase in latency for any in-flight operations that had to be retried. Meanwhile, DSQL is continuously backing up your data, and will detect the loss of this node. DSQL is gonna go and take a recent snapshot of your data, create a replacement node in the same zone. That node will come online, it will connect to the journal, it will catch itself up. And then a query processor is gonna start to shift traffic back over to its local replica, which is gonna give us the best performance because the intro AZ latency is simply better. But what about that, what about rights? What about that adjudicator sitting over there in AZ1? Well, the reason we have a single adjudicator is because that's gonna allow us to do this conflict, uh, resolution protocol really quickly without coordinating over the network. But we don't want a single point of failure. And so what DSQL does is it's gonna keep standby adjudicators in the The zones. Notice the arrow from the journal, right. These standbys are simply following along with everything that the that the leader adjudicator is doing. And if anything fails, then within just a few milliseconds, these, these standby adjudicators are gonna run a leadership um protocol. One of them is gonna become the leader. Hey, it turns out it's R1 and AZ2. And the QP is gonna retry any in-flight commits against the, against the leader. There's nothing for your application to do. It just works out of the box. Now, there's a lot more going on with the DSQL architecture than I can show you on the slide. But the good news is that you don't have to know about how any of this works. Um, DSQL is an architecture that's been designed for high availability. There are no, there's no single point of failure in DSQL. If you're, if you're only spending a few pennies per month on DSQL, or running entirely on the free tier, you're getting active, active out of the box. But of course, DSQL is designed to scale too. And so let's dig into how we do that. First, let's talk a little bit more about that concurrency uh control protocol. DSQL uses a protocol called optimistic concurrency control, which makes the assumption that transactions seldomly conflict with each other. But what happens when they do? Here we have two clients that are trying to update the same row, see where ID equals 1 to a different value. It doesn't actually matter what the value is, but they're gonna prepare their transactions, and they're gonna race to the adjudicator. It turns out that the transaction on the top gets there first. The adjudicator is gonna record that transaction in the journal, and our other client is to be, is gonna be given a post gross serialization failure error, and it's gonna be told to retry. And so when you're building applications on DSQL, this is something that you need to take care of on your application. When you call commit, you need to take care of availability issues, you need to take care of optimistic and currency control issues, and then just simply retry. This, this is just a few lines of code. It's a wild true loop. Um, and what we recommend that you do is you build, build little helper functions that can, uh, that you can uh use throughout your application. Or if you're using some kind of framework, then put that into the lowest layer possible. Now, when you design your applications, you want to avoid creating unnecessary conflicts that we don't need to retry. And if you do that, uh, then we're gonna be able to commit these two transactions at the same time because notice that they're updating different rows. And as you do this more and more and more, you may start to increase the load on the adjudicator, the load on the journal. And so what happens then? Well before you exhaust the adjudicator or the journal's capacity, DSQL is going to notice this increase in utilization and start to shard itself internally. And so if we're updating some records, the QP will automatically know about this internal sharding strategy and direct your commits to the right stack. But of course, we can run transactions that sometimes touch rows for multiple shards. Uh, DSQL will handle this automatically for you. We have an optimized implementation of two-phase commit that will ensure your transactions are atomically committed even across multiple shards. Something similar is happening on the storage side, which, which we won't have time to get into today. But I do wanna talk about rescaling. So imagine we're opening a bunch of connections. These connections are all running their selects, they're slamming our single storage host, load is going up, latency might go up. But again, well before this becomes a problem, DSQL is gonna detect this. And using the same trick that we saw in the availability slide, DSQL will use a recent copy of your data to bring A new node online, that node will catch itself up from the journal, and the query processes will start to automatically load balance across this replica. And we can keep scaling this out without limit. DSQL is doing this automatically on your behalf. And a key reason we're able to do this is because in DSQL, replicas are always strongly consistent. So, in order to understand this, let's make a new DSQL cluster. It's completely empty, it doesn't have a single row in it. We've made a table my table, and we're just about to insert our first row into that table. Here it's, here we go. It's in the journal. It's been committed to at least 2 availability zones. That means the journal says, I got your data, and we're gonna send that commit acknowledgement all the way back to our client. And now our expectation is we can start a new transaction. We can run select staff from my table, and we should see our row, right? Because otherwise, we'll have an eventually consistent system. Eventually, consistent systems are very difficult to reason about. They're very difficult to write correct code against. And that is the reason for our small technical interruption because we had old slides. OK. Um, so what, what we do in DSQL is use a time-based synchronization protocol. And we combined two techniques here. The first technique is we use the EC2 time sync service, which gives us a microsecond accurate uh timestamp using GPS clocks. The second technique is that we use a library that AWS developed called Clockbound, which is gonna allow for any errors in that measurement. And when we combine these techniques, what we get is a value that is linearaizable, that is, it is a timestamp that is guaranteed to be after any commit timestamp that the service has previously given out. And now that we have this transaction start time, this start, we include it in our API request to storage. And uh the, the job of storage is to, is to identify that it's running behind, to wait to see the update come off the journal, and then return R rows. So how do I manage connections? Here we have 6 connections to DSQL. And how do we get them? Well, between your application and the query processor is a service known as the session routing layer. And its job is to make sure that connections are always available, that you can get connections quickly, even in the event that many hundreds of thousands of connections flap uh due to a networking event. This is a very deep and complex service, but at a high level, what's happening is that there is a massive warm pool of these query processes. Again, each of these boxes represents a single process. So our team is maintaining a large fleet of VC2 instances. We're taking on the responsibility of capacity management, and on these instances, we're packing them full of QPs that are ready, they're running, and they're just waiting for you to connect. When you ask for a connection, uh, DSQL will simply pop one of these QPs out of the pool, and your application uh can start to run transactions. Now, let's look at failure of connections. We've got 6 connections open, one of them has failed. Ideally, what's happened here from an RTO point of view is that we've simply experienced an error on 1/6 of our connections. The other 5 connections are totally healthy, right? This is a system with no single point of failure. And what your application needs to do to handle this is to detect that failure, to retry. This is where that retry handler from the, from the optimistic control uh uh section is gonna come in handy. Add a retry on connection errors, and you'll get another connection, and, and you're back to 100% availability. Now, unlike, um, unlike failures of storage and failures of the adjudicator, uh, we can't have your application automatically reconnect, right? This is something that you need to do. And so in order to take advantage of DSQL, you need to build the ability to detect, detect failures, to retry on new connections into your application. We were actually quite worried that customers wouldn't realize this because, you know, we, you know, see page 43 in the manual. And so what we did instead is we said that connections have a 1 hour maximum lifetime in DSQL. And then if you simply use the best practice of a client site connection pooling library, this is the, the, the Java C-3PO library, and configure it to do health checks, configure it to have a max. maximum connection age, then right out of the, right out of the box, your application is gonna be uh following our best practices. Now, while we do recommend using client-site connection pooling, in DSQL, there's no need to use server-side connection pooling. You don't need PG Bouncer, you don't need RDS Proxy. And that's because the QPlayer is a separate service, it's horizontally scalable. And so we've been able to build the concept of connection pooling right into the service. So if you want to run PG Bouncer, feel free to do so. But that's just gonna be something that you have to manage, patch, monitor for availability. It's not gonna provide you any benefits. Now, throughout this talk, I've been showing your application talking to QPs in the same zone. Um, and the reason we do that is to lower latency, right? Postress, um, transactions that are interactive, you're going back and forth between your clients and the application all the time. Uh, each of these trips, uh, you know, it's gonna take some time. And so by talking to a QP in the same zone and to a storage host in the same zone, we're gonna give, give you the best performance. Now, in the, in the event that your application is somehow available, and yet DSQL is completely down in a region, don't worry about that. DSQL from the top to the bottom is able to automatically fade over to a healthy zone. And so if an entire zone becomes unavailable, just reconnect and you'll get a healthy QP. If you'd like to learn more about this, about this part of the service, um, please join me tomorrow at 1 p.m. for 439, where I'm gonna be doing a deep dive into how we built this. Let's talk about regional failures. DSQL allows you to set up uh what we call a multi-region cluster. And the way you do this is you create a cluster in region A, a cluster in region B, and then you designate a third region as the witness region. Now, you'll notice the witness region doesn't have any storage boxes. It doesn't, it doesn't have any QPs in it, it doesn't have your application in it. The job of the witness region is simply to have a copy of the journal for reasons that we'll get to in a moment. Once you do this, DSQL is going to pair these journals together using a quorum-based protocol. And then our application can can continue to be active, active even across regions. So here you can see we have our application in region A, AZ 2, our application in region B, AZ 2. And they're both able to do rights. So what happens if region B fails? Well, imagine for a moment that we did not have the witness region. It would be very difficult to disambiguate between region B having failed completely versus region A and region B simply not being able to talk to each other. This is where the witness region is gonna come in handy, because it's gonna vote with, with alongside region A to kick B out of the quorum. And DSQL is gonna be able to detect this automatically. Um, it's gonna reconfigure the journal so that only A and C talk to each other. And if you were somehow able to connect to the cluster in region B, it would be completely unavailable for reads and writes. Um, this happens automatically again, without any intervention. It happens very quickly. And by doing this, you can continue to use your cluster in region A. There is never any data loss. And if you continue to write data into region A, DSQL upholds its guarantee that transactions are replicated to at least one other AWS region before acknowledging the commit. Meanwhile, later, Region B is gonna come back online. DSQL will detect that. It will catch it up to date. And then it will uh undo that, that configuration option, and we'll be back to, to 2 zones. So how do you take advantage advantage of this? The pattern we recommend is the same one that we've been uh using throughout this talk, one of active redundancy. And so you should deploy your application alongside the cluster in region A and alongside the cluster in region B. There's a couple of reasons for this. One reason is that you're gonna get the best performance, right? Because again, you have this low latency connection to your cluster. But the second reason is this is gonna allow you to continuously validate that your application works in both regions. On top of this setup, you should create some kind of global endpoint. You can do this with something like a Route 53 DNS record. And then you'd automatically direct your customers' traffic to the closest available healthy region. So what happens if something goes wrong, Region B goes offline. You're not gonna lose any data, and DNS is gonna start to shift your customers over to the healthy region. And you know that this region is going to be healthy. Why? Because you've been exercising in all of the time. So this is my favorite, uh, favorite slide I had to make here, uh, throughout Reinvent, because there's not a lot on it. Um, and the reason for that is with DSQL, there's simply no maintenance for you to worry about, uh, as a customer in the same way that there's no maintenance to worry about as an S3 customer. DSQL is a fully managed service. Our active architecture is allowing us to do much more maintenance on your behalf. So think about those query processes, right? They're only running for an hour. After an hour, either you hang up or we hang up, we throw them away, we make a new one. And that new one is gonna have the latest operating system, the the latest security patches, the latest performance, performance improvements, the latest features. And so there's just really nothing for you to do as a customer. OK, so what are the takeaways for today? Aurora has 3 engines. We have Aurora for Postgress for MySQL and our newest engine DSQL. Out of the box, all 3 of these engines are gonna give you 3 AZ durability. Both of these, uh, out of the box, all three of these engines are gonna give you the ability to scale your workloads. With APG and AMS you're gonna scale primarily by going up for rights and out for reads. With DSQL, we offer hands-free, automatic scaling, but you need to make sure your application scheme are designed to work best with the DSQL architecture. If you've outgrown a single region, consider going multi-region. With APG and AMS you can test your failover practices without losing any data. With DSQL, you can take advantage of active active across regions. And finally, all three engines offer easy maintenance with automated security updates and minor version upgrades. Thank you so much for attending our talk. Uh, Tim and I will be hanging around afterwards for questions, so please do, please do come and talk to us. And, uh, thank you. Have a great reinvent. Uh please complete the session survey in the mobile app.