---
video_id: lwRdG35pA2g
video_url: https://www.youtube.com/watch?v=lwRdG35pA2g
is_generated: False
is_translatable: True
---

Hello everyone, thank you for joining our session today. Um, we are going to talk about a very interesting topic. Um, my name is Yon, and I'm senior engineering manager, and here with me, Kevin McGee, a principal engineer, and we are both from the Elastica team. So today, I'm going to start with the introduction to Elasticash and Valy, and later we're going to dive deep together on some advanced use cases and data modeling that I hope that you're going to benefit from building your application. So for those of you who are not familiar with Amazon Elasticash, Elasticash is a fully managed service that makes it easy to deploy, operate, and scale the in-memory data store on the cloud. Elasticash improves significantly the performance of your application by providing a microsecond response time. We support 3 open source engines. We started with Redis Open Source, MemkechD, and Valy. Valky is open source high performance key value data store. It was forged from Reddi open source last year when Reddi changed the license, and last year we also announced that Elastickesh and MemoryDB support the Valky engine. So today all the examples will be based on the VALKy engine and to make it even more fun, we decided to build together with you an application so we can understand together how all this data modeling can help us to build a highly scalable application. So Kevin and I chose to build together with you a massive multiplayer online game. We definitely need to have highly scalable technology to support that. We need to make sure that we are providing a very low latency and high throughput to our customers so they can enjoy the game. So let's start from caching because caching is the fundamental to make your application to run much faster. But I want to start with some high level concepts. So let's assume that we start to build our application on EC2 and we have the persistent data stored on some relational database. I'm using RDS. So as long as our data is going on a steady state, this architecture can work well, but When we start to see more workloads kicking in with read and write throughput, we have multiple options. One of them is to scale up our RDS or to add more replicas so we can scale out and benefit from reading throughput using the replication itself. Now RDS itself has the caching layer, but it keeps the caching on pages and blocks that does not contain the result itself. So, from time to time, you need to fetch the data from the disk itself, and there is some uh uh additional latency that is involved in that. For that, we can choose the Amazon Elastic cache that explicitly store the data in memory, and once we start to populate the result into the cache, immediately we can start to read the data from the cache itself, and to improve the query response time and actually to receive a microsecond response read time. We can also relieve some pressure from the back end and from the database itself, and we can optimize our cost by allowing the back end to scale down. Now, let's let's move forward and see more advanced use cases. I want to, start to talk about lazy loading, uh, which is one of the strategies to handle the cache, and I want to show you how I'm managing that together with invalidation to the cache itself so we can make sure the cache stays fresh and without a stale data. So I'm using Amazon Elastic cash, and I'm starting to read the data directly from the cache. If we receive the data, if the data is there, we have cash it. All is good, we get, get that in a microsecond response time. If the data is not there, we have a cache miss, then we need to go to the persistent uh uh layer, the storage itself. Here I'm using Amazon Dynamo DB. And right after that, I'm updating the data into the cache. So this is the way I'm lazily populate the data, warming the data upon every read. Now, let's assume that one of the item now has been up to date and what we are doing here. I want to make sure the cash stays valid and have the no state delta, and for that, I'm going to use the trigger function that I have on the Amazon DynamoDB and to trigger a lambda. The lambda is responsible to invalidate the specific item on the cache so we can make sure that next time on the lazy loading on the same path we're going to update that data. Now sometimes we have a very hot data in our cache. And we also use time to live because this is the best way to manage our cache memory, and we want to make sure that the data is also evicted from time to time. So we're going to use expiry. But if we have a very hot item on the cache, we still want to make sure that we have the way to to update the time to leave before it's expired so we can benefit from keep reading it from the cache itself. And for that we can allow some background tasks to update the data before it's going to expire. In my example here, I'm using Valy multi-command. And the multi-command actually queued multiple different commands and executed atomically on the server side. What you see here, I'm using the get first to receive the item itself, and then I'm checking what is the expiry time that's left for this item. Now in different examples, as you can see here, we have multiple clients that get slowly listening to the expiry until I'm going to get the last client that crossed the 5 2nd threshold and is the one that's going to update the data on the cache to make sure that we have more expiry period for this hot item. Now, as a connector to that, there is the tendering head problem that we want to avoid. This problem occurs when multiple processes or threads attempt to proceed simultaneously, often resulting in hypertension and wrist strain or a bottlenecks, and this is something that we want to avoid. So let's assume that I have a very hot item on my cash that's about to expire. Now, if all my microservices will try to fetch the item from the cache now, they all will receive a cash miss. What eventually will happen, they all automatically, all together will go to their database and will put much more pressure on my RDS. And eventually we're going to impact our latency, and this is something that we want to avoid. And for that we can build a kind of barrier, a synchronization mechanism that allows only one of the clients to request the data from the database itself and only him going to update and to populate it into the cache. So once he's finished and when all the others are waiting for him, we can read the data and receive a cash hit. So how do we do that? How do we build that? Let's for that example, I took only 2 microservices, and I will go step by step and follow you, how we can build that together. So the first micro microservice tried to fetch the item from the cache, but the item is not there, so we're going to receive a nail. The 2nd microservice now is a bit slower. Try to fetch the same item. It will save a meal as well because the item is not there. Now, the first microservice will try to acquire a lock. I'm using here the NI and the EI arguments, which means that NI, if the lock already exists, I should fail. And X means that I'm going to expire the lock after 5 seconds. Its successfully, because the lock is is not there, I was successful, able to acquire them, and then I'm starting to query the data from the database. Now, my 2nd microservice tried to acquire the lock, and remember, because I'm using the Annex, uh, I'm going to fail, because the lock is already there. Now what I'm trying, what I'm going to do, I'm going to wait until the lock will be freed. So I'm waiting periodically until the lock is freed by the first microservice. In the meanwhile, the first microservice finished to fetch the item from the database, is going to populate it into the cache and free the lock. After that, my 2nd microservice will be able to continue and to fetch the item from the cache itself. Another advanced approach is the client citation. That means that if I'm even more sensitive to latency, I can use my local cash to serve my on-demand workloads, which means I can benefit from very low latency capability. So for that we have multiple ways to achieve that. First, the idea is very simple. We are fetching the item from the remote cache. Once we receive it, we store it on the local cache, and from that point, I don't need to talk with the remote cache because I can serve the items and requests for my local cache. But there is a problem, I need to manage now the uh the the data, the freshness of the data on my local cache. One approach is to use the time to leave, the expiry, that I can put on my local cache. The 2nd 1 is to subscribe to the remote cache and to receive notification upon every changes. So let's see how it works. The first and the default approach of Valy is to remember to store all the data, the mapping between the clients and the keys that they accessed, and every time we change one of the keys, I have the data stored in my VALKy and I can update the specific clients. This is very efficient, but on the other end, we need to have additional memory to store on my server side. The second way is to subscribe to a prefix. In my example here, I can subscribe to a user as a prefix. So every time any microservice that's going to update a key with the same prefix, I'm going to receive a notification on that. So, how I'm, how I'm going to implement that on my client side? First, um, we recommend as a key principle to use uh a connection pool, a long-lived connection. The reason for that is because creating a TCP connection is an expensive operation, specifically, if you compare to uh uh Valy get and set, which is in order of magnitude faster than creating a connection. Additionally, using a client connection pool with a finite side reduces the overhead of connection management, and it also bound the concurrent incoming connection from the client application. So what I'm doing here, I'm splitting my connection pool into two types. I'm going to have connection zero as the invalidation connection, the control connection. And all the rest from 1 to 9 going to serve as the data connections. Connection Zero will subscribe to the invalidation channel. And all the other connection will perform the tracking on redirect on connection zero, which means that the connection zero will encapsulate all the invalidation to all the rest of the connection. So let's assume that I'm updating one of the keys. Immediately Connection Zero will receive that update on the invalidation connection with the specific item, and we'll, we'll make sure to update the data on my local cache. Now, sometimes we are thinking how to store the data on our VLKI engine. So we have, it's very rich. We have a lot of ways to store the data there. I want to talk about two types because in my example, I want to store some session information. I will start with hash data structure, which is a very common data structure to store session data. It's very simple because it's mapped between key and value pairs, and we have the key that that map that directly access to the cache itself. So in order to create the hash itself, we're going to use the HS command and we're going to provide them the hash name and the list of pairs of key and values. We can also benefit from a very fast random access in a constant complexity to receive a specific item from the cache itself. Let's go through a simple code to see how easy it is to use that. So here I'm using a Valky glide to connect to the uh uh uh Valky server. And I'm generating a session, and I'm using the, the uh native uh uh HashMap that I have on my Python to use the HashMap. Then I'm generating a UU ID that will be serve as the session ID. Later, I'm going to use the HS HS command, the VAI command, to serialize the edge, uh the hash data and using the session ID itself. To receive the data from VALKY, I just need to use the session ID that I generated, and I'm going to use also the educate all command, and later I can serialize it to my local objects. Very simple. A different data structure that we also recommend to use is a JSON, very popular. So JSON is a data structure that consists of two main objects, uh, um array and key key value pairs. In this example, I'm creating the JSON document using JSON.set. And I can simply use the adjacent path to retrieve uh specific items from the Jason. I can use it to retrieve it from the array itself, or from the upper level of the Jason. I also can use the arraya pen to add additional items into the existing Jason very simply. Let's go through a simple code uh to see how it works. So I'm connecting here to my uh Valky server and I'm creating uh the JSON document at the beginning. Then later, I'm receiving the uh uh uh the email of one of my users, very simply, and same I'm doing that for to retrieve the age, and I can also use the increment number to increment one of the indexes in my genson itself. So what is the main difference between them? Jason usually used for a nested document, while Hah is a key value pair for more simple objects like a session store, while with Jason we can use it for a more complex ones. Now Jason supported Jason Pass to filter on the server side, while with the hash, we need to do it and manipulate it on the client side, but both of them we can modify the items on the server side. So we announced a few weeks ago the support of semantic caching on elastic cash, which can also retrieve one of the best performance for vector search in a high ratio of recall of 95 recall. So to understand the semantic caching, we first need to understand the vector embedding. You can think about translating unstructured data like documents, videos, audios, images to represent representation of your data. Now, the vector embedding captures the semantic relationship between the different semantic elements. For example, with text, the semantic awareness of a world like a book can be sometimes when we're reading a book, and sometimes when we are reserving something. So to process the creative vector, we involve ingesting data from the source, splitting it into chunks, and then converting it to vectors. So let's assume that we have a chatbot or some other application in our game, and we want to get a query prompt from the user. So we will use the same example, like I mentioned before, like a book, but I'm going to add the furniture and finance here. And we will again visualize it with only 3 dimensions, only for the simplicity. So let's assume that we already have a question on our vector that says how long do I have to wait for the next quest. And then I'm receiving a new query that said when does the next quest start? So, uh, the, the cool thing here is that the semantic search can actually understand that this have the same meaning and returning us the results. So let's see how it works. Let's assume that we have the generate the generative AI application, and our user is now query, uh what is the next quest to start? When, when it's supposed to start? So what we are going to do, we are going to query the foundation model and to ask them when the next quest is going to start. This is a bit costly and expensive from both cost and performance perspective, but once we receive the data, we can return it to the client itself. Now let's assume that I'm adding elastic cash into the architecture. I'm using the vector search here. And the user now tried to generate the same question for my application, but before that, I'm going to use the Amazon Titan to generate for me the embedding vector. Once I have the vector itself, I can fetch the item, the data from my elasticcache vector search. Because this is the first time, I don't have the data yet on my cache, I'm going to receive a cache miss. And now I need to go again to my foundation model, ask the question, and get back the response. But this time I store the item on my cache for a woman for the next time I'm going to read it. But let's let's make it even more cool, more complex, and now I'm receiving similar query, not the exact same query, and. We can understand that traditional caching will not work in that way. We cannot, we cannot just uh store the data on the cache and expect it to be as a cache in the next iteration. And for that we have the vector search. So let's go through the flow, we go to the generative AI, then we create the embedding vector. Once we generate the embedding vector now with a different question, we are asking the elastic cash vector search, but this time we're going to receive a cash hit because, because the contextually it understands that they have, they have the same context and with that way we are saving the need to go to foundation model which is expensive both from cost and and performance. Let's go through the API itself and see how, how it works. So I'm using here the FTre to create uh uh the index itself. I'm going to use Hah. I can use also JSTO to store the data but here I'm show an example of Hash. We are using the HS HSNW as the algorithm because it works well in high dimensional space while very optimized for performance. I'm going to use here uh dimensions of 128. Uh in my previous example I showed you only 3, but here I'm going to use more dimensions, and I'm going to use the distance metric, the cosine algorithm. Now, to search the item, I'm using the closest neighbors I retrieved, only 10, in that example, from my vector search. Now I have another option that I can filter out some of the results on the fly on the server side, and for that I'm using the index. Here in that example, I'm using filtering only for per country. So in this code that I'm going to show you, we populate the vector store in the text data that builds the foundation of the semantic search body. So first, we're going to, to, to load the text data and to split it into chunks. And we're going to use the for that we're going to use the log chain library. Next, uh, we're going to use the chunk that converted into embedding, using the embedding model, and we, we are using the vector tan to use the embedded embedded vectors. And to query the data, we're going to use the FT Create that we use for FT Create. We're going to store it first on the vector search. So to fetch the item, to fetch the data, we're going to use the FT search, and we use the same here, we're going to use the closest closest neighbors. Here we're using only 3, and once we have it, we can search the item on the vector search itself. Now I want to call to Kevin to continue with more advanced use cases. Thank you. Cool. Let's talk about some, uh, additional data structures that you can do, uh, in Valy, starting with, uh, one of the, uh, functionalities of publish and subscribing. So Valy, as we talked about before, is a rich, uh, data structure server, and, uh, with our MMORPG, we wanna be able to answer, uh, questions about, um, an in-game chat. We wanna build a chat functionality with, uh, for our. Uh, for our game where players can join and participate in an in-game chat that's, uh, ephemeral. So let's say here we have a Shadow Dragons Guild and a bunch of users that wanna participate and communicate with each other for that we can use this pub subs, uh, feature within Valy. So if you're not familiar with the pub sub pattern, uh, it's a messaging pattern where there are uh producers, publishers of messages that are decoupled from the subscribers. So, uh, subscriber, uh, comes in and, and subscribes to a named topic. So let's say our topic here is just shadow dragons, and a user can come in and say, I wanna listen to messages on this topic. Another user can come and publish messages to those topics, and so I can come say hello, and then anyone that subscribed to that topic will get those messages. So how does this work inside Valy? So, the data here is ephemeral. ValLKy is basically just used as a messaging bus, uh, so that the data is relayed from producers to consumers directly. It's not stored in, in memory other than for the transition between the producer and consumer. It allows at most once delivery. So if you're not listening to the topic, then you'll miss the message. However, there is, however, another data structure that we're not gonna go into today called streams that allows you to store, uh, messages inside Valy in, in, in memory for, uh, long term storage and has a number of other rich features, uh, such as consumer groups to be able to resume where you left off, uh, but PubHub is off, uh, our, our choice for this, uh, since the chat message is, uh, just ephemeral here. So there's two different types of pub sub in Valy. One is what I call classic pub sub, and one is sharded pub sub, which is a relatively new. Uh, we recommend using sharded PubSub because it has better scalability properties. Uh, so in classic PubSub, all of the messages, uh, that you send are forwarded to all of the nodes in the cluster, so you kind of have this, this large fan out, and that means any subscriber can go to any node and get all, all of the messages on a topic, uh, but of course that doesn't scale as well as you have high, very high volume, uh, pub sub traffic, whereas with sharded pub sub, uh, just like with. sharded key space in cluster mode enabled, it goes to a particular shard, and all of the producers and consumers know which shard to go to. And so the fan out is much smaller. The main downside of using sharded PubSub is it doesn't support these wildcard, wildcard identifiers. You can't subscribe to a large number of topics. You have to do a specific name topic so that you get hashed to the right shard. So how does this work in practice? How do you actually do it? Let's say we have 2 users here that like we had before of of Alice and Bob, and they want to join this, um, and participate in this chat room. So for shattered PubHub we'll use this S subscribe command for Alice to uh to subscribe and we'll we'll prefix our our key name here with chat uh and then shadow dragons being the name. It'll return back uh the, the identifier of the, the topic as well as the one saying it's successfully subscribed. Now Alice can publish, uh, it doesn't have to be subscribed, but she can publish, uh, to this message, uh, and we're gonna use, uh, we can do any sort of string, uh, to publish to a topic, but here we'll, we'll format it in JSON so it has enough metadata to render the chat in the, uh, the application. Uh, so we'll do S publish, uh, we'll get back to one saying it's been successfully done, and then immediately after that we would get this S message because Alice is also subscribed to that topic, uh, and the message has in it the the topic that, uh, it was published to as well as the actual message that the, the, uh, publisher sent. Meanwhile, um, Bob can come along and subscribe to it similar to how Alice did. And then uh do a publish just before saying Bob here uh and then that message shows up on both of them because they're both subscribed at the same time. So you can build pretty rich uh applications this way, it doesn't require a whole lot of memory since these messages are just used uh stored transiently and relayed to the the subscribers. Next, let's look at a couple of different probabilistic data structures, uh, that we'll talk about in VALKy. Valy, of course, is an in-memory data structure server, so storage matters. You don't wanna be able to, you don't wanna have to store a, a large amount of, of data if, uh, if you don't have to, uh, and there are some applications in, in our MORPG where accuracy doesn't really, uh, is, isn't paramount, and so we can get away with, um, answers that are, are, uh, perhaps off by a bit. Uh, so one such example is a unique user count. So say we want to keep track of how many daily active users, unique daily active users are playing our game. Of course we can do a counter and, and get, uh, you know, non-unique, uh, users, but we wanna do something that is, uh, a little bit more advanced. And so, uh, normally let's say we, we want to do it on a day by day basis. We have 4 different. Users here that are playing our game and traditionally if you were to use a set data structure it would take over and space we would store all of the different user names or user IDs that are associated with that counter and then we could just take the set and that way we would ensure uniqueness but within Valy there is a data structure called a hyper log log and this can approximate set cardinality so it can answer the question of how many unique users you might see. Uh, the advantage here is it's, uh, uh, doesn't take linear space. It is bounded to 12 kilobytes in, uh, in Valky, and it has a less than 1% standard error rate, uh, which is, is pretty remarkable. Uh, so how exactly does it, uh, and, and it allows constant time operations because it is, is small, so it doesn't require, um, uh, you know, even, uh, space or, uh, login, um, uh, access. So what is the intuition behind how hyper log log works? So say these are some poorly drawn, uh, coins that we're flipping. Uh, if I tell you I'm flipping a bunch of coins but I get, uh, 5 heads in a row, uh, it's, it's likely that I flipped a bunch of coins in order to see a sequence of 5 heads in a row. And this is sort of the intuition behind how the algorithm works is it takes uh uh these uh values, observes them, and then uh it accounts uh the repeating patterns that are less and less likely over time. So what does that exactly mean? Um, the algorithm, uh, of hyperlog log takes your user or whatever string that you wanna be able to, uh, to count, and it runs it through this murmur hash which, uh, produces a 64 bit uniformly distributed binary string. So it's just a uh uh a what seemingly random uh value of of zeros and ones. And in Valy it takes the 1st 14 bytes or bits of that, uh, that binary value and uh computes a hash bucket out of it. So there's 16,000 buckets that can be uh uh computed within the hyperlog log. And then the remaining digits, uh, it takes and accounts the number of leading zeros and this is the the analogy of coin flips, uh, the larger number of leading zeros that are happening in this, uh this uh uniformly distributed binary uh binary string, the, the, you know, more values you might, you may have seen in order to, uh, observe, uh, that pattern. And what it does is it takes the bucket and then the the number of leading zeros and it stores the maximum number of leading zeros per bucket that it has uh observed in the uh in the sequence of strings being added to it uh and then it takes the harmonic mean of this overall so it's a mathematical equation to basically uh identify the uh the total number of observations. Luckily you don't have to actually deal with any of that math when you're using the hyper log log in Baly. It has a pretty simple API to interact with it, uh, which is just these, uh, PF prefix, uh, and, and PF add will add a number of, of, uh, items into a particular hyper log log. So say we want to name it daily active users with the date, we can add 3 to it. We'll get back a 1 saying it's been successful, and if we issue a count command, uh, then we'll get back 3 in this case. And if we were to try and add uh the same uh user it will come up with the same hash it shouldn't modify it uh we will get back to 0 saying it has been unsuccessful. And the account is unchanged in this case. There's some other interesting properties here too. So say we wanna be able to subdivide it. We now wanna, instead of just doing daily active users, uh, over the entire MORPG, we wanna kind of, uh, partition it into different games so we can add 3, the 3 users to game one and maybe just Alice and Doris to game 2 here, uh, so they're, they're, they're different hyper log logs. Uh, hyper log logs are composable, so we can use this command called PF merge. Uh, where we can take the two of them and it basically goes through in that that data structure we talked about and just um updates the max number of leading zeros, uh, based off of the, the number of, of in, in each bucket, uh, and it allows you to then get a total sum of of unique users across multiple hyper log logs. So in this case we have only 4 unique users even though we've had 5 different observations. So we'll get back a 4 when we count that aggregated hyper log log. So I ran a quick, uh, little simulation of uh the memory usage uh and uh error rate of, of a set and a hyper log log. So in adding 10,000 numbers to a set, uh, you'd get back 10,000 exactly, uh, because it's storing every single number in the string and then calculating that that cardinality, uh, but the memory usage of the set as calculated by Valy is about 420 kilobytes. Whereas if doing the same thing with the hyper log log, I got back 9,987, so 0.13% error. But the memory usage here is 12 kilobytes, uh, so you can see that, uh, that 12 kilobytes will not expand, uh, beyond that. It's the maximum size of, of the, the hyper log log, but the set usage, uh, would increase with the number of, of unique users that get added. Next, let's answer a slightly different question of whether two players know each other. So say you want to keep track in your games, whether you've observed or interacted with a player before, basically forms this graph, uh, that you'd have to keep track of the different users that have engaged with each other, uh, which you could store in Valky with sets again. Uh, we have the different interactions for each individual player. Uh, the downside, of course, is that this is also ON, uh, storage space. So can we do better than that? Can we, uh, not have to store all these, uh, sort of, uh, combinations? And with that we can use a bloom filter, which is a relatively recent data structure added, uh, to, to Valky. And this, as, as answers a different question. It, it answers the membership testing question. So whether a given, uh, user has, uh, interacted with, uh, the, the particular, uh, user that you're, you're querying for. This can result in over 90% memory usage savings over a set depending on your configuration. Unlike the hyper log log, this has some configuration options. It has this tunable false positive rate, uh, that you're able to, uh, to tune. The, the lower the false positive rate, uh, the higher the, the memory usage of that data structure, uh, but by default it's stored, it's a 1% false positive rate. So that the, the error that you can get is that you, uh, would have seen interacted with someone when in reality you hadn't, uh, done that. Uh, this is, uh, also a constant time operation. It, it, it uses a number of different hash functions which we'll see in a minute, uh, in order to, to test and to add, um, members to this, uh, this set, uh, but it's typically a relatively small, uh, constant time. So how does this work under the cover? So the intuition is that you have this fixed size bit array, um, that has a number of different hash functions that operate on it. So say for this example we have 1010 bits here. And we wanna add Bob to it. So let's say we just have two hash functions. We'll calculate the hash, uh, #1 of Bob, and then module it by the number of bits, uh, and we get, uh, bit number 5. We'll go and update that in our, our bit array, and we'll do the same thing with the second hash function here, uh, which is, uh, another, uh, another unique hash function. We'll get, uh, uh, the value of 9, and we'll update those too. And we can do the same thing with with Alice with everyone that we add, uh, goes to the same function as, as, uh, uses this, uh, this murmur hash as well. Uh, we can update Alice here. There's conflict. You get, uh, back 5 and 5 is already 1. We don't do anything with it and let's say #2 of Alice, uh, is the, the, the first position, and so we'll go and update that. Um The, uh, let's go back one second. So it, the actual, um, uh, when you go and test membership, it goes through the same uh exact process too. So say we want to check to see if someone else is, uh, present, we will go through and um uh observe uh each whether the bits that are hashed are 1 or 0. And this is why because the hash function is, is, uh, non-unique, you can have, uh, false positives. uh, so in this case we, we can see that that Alice was not previously added that that first bit is 0, but some, uh, if for example you get back, uh, position a bit 5 and bit 9 for another, uh, user, then, uh, you might have a false positive, uh, where it conflicted with Bob. So if they're all 0 that uh or you know at least 10 then it was definitely never added to the set, but if all of the values that we look at are 1, then it was maybe added to the set previously. So we don't have to, don't have to actually store the underlying strings, just basically some representation of the hashed value. So the memory usage savings are pretty significant here. So if you look at a 1% false positive rate, you can get about 112 million items with 128 megabytes or scaling linearly 448 million items with 512. And if you were to drop the the false positive rate to 0.1%, then you can um get uh uh using the same memory usage you have still a relatively large capacity 74 million with 128 megs or uh 298 million with 512. And of course you can kind of tune this uh as your needs fit. How do you actually use this in Valy? So again, it has a relatively simple API where you can just interact with it as you would a set. So there's this BF.add command which allows you to add, uh, strings into a given bloom filter. We get back to one here saying Bob has been added. We can similarly add multiple users, uh, simultaneously atomically to the interactions here. And then this exists as the membership testing command where you can just test if a given user was there we'll get back a 0. However, uh, and Bob of course would would return back uh 1 saying that uh it does exist, it has been added, but what can happen is that you get back a false positive here. So we're testing whether Frank has been added to the interactions with Alice, but we get, uh, it will conflict in this case with either Bob, Charlie, or Doris's hash values, and we get back a 1. So let's move on to uh another uh data structure within uh VAKI that can keep track of geospatial data. So let's say we are managing our MMRPG and we have this, you know, virtual rich world where users are in different locations and we want to see which users and, and, uh, points of interest are located near a given user at a at a point in time and for this we can use the Valy's geospatial commands. So what is, uh, geospatial? So how, how does it work, uh, so in, uh, in Valy, uh, you would map first with a latitude and longitude, uh, basically you would take your virtual world and, and divide it into, um, the normal, uh, uh, grid here. Uh, that latitude longitude that you, you store as input is taken into a, uh, geohah algorithm. And stored as um uh a traditional geohash which it works by sort of subdividing the world into smaller and smaller grids uh and it generates this uh this normal geohash is is a string of of characters we would take it into uh uh sort of this 52 bit integer, uh, and the properties of the integer are interesting in that the, the closer these, these numbers are, the, the closer they are in, uh, location to each other because of the way that it interleaves the latitude and longitude. Uh, and this has the, uh, the nice property that you can kind of use it as a score, so things that are, are close to each other and, and numerically are close to each other in latitude and longitude. And so this is, this score is then fed into another data structure within Valy called a sorted set. Sorted sets are often used for things like leaderboards where you just have high scores. You wanna be. Able to to keep track of like the top end uh in this case the sorted set is actually used to query things that are nearby a given uh uh latitude and longitude. However, it has some convenience functions on top of it. So this, um, allows you to do sort of bounding box and radius querying within o of logn time. So how would you actually use it in practice? So let's see how it works. You'd have these geo add commands, so it has these convenience functions on top of the sorted set. You can do a geoad with different latitude and longitude. Here we're adding uh Alice as well as a point of interest of the the beach down here on the bottom. We'll get back to one saying it's been successfully added. And then let's say we have another player here that that uh is wants to query for uh the the points of interest around them. So you use this geo radius command and you query the map. Um, it has a slightly, uh, different latitude and longitude so we can do different radii here. So let's say we do a 3 kilometer radius, uh, we would get back both, um, items that we've added here, Alice as well as, uh, the, the beach at the bottom. However, if we were to do a smaller radius here, just a 1 kilometer radius among along that point, uh, we would just get back Alice, um. So you can use it to, to, uh, query things around it. You can then use it to kind of filter further so that you can query the distance between any two points in that as well and get it back in a number of different units. So say we wanna see the distance between Alice and Beach one, in meters and get back that it's about 1600 m between them. So you can use it to further filter the results of the radius if you want to do things like bounding boxes. Now let's look at um a uh usage of rate limiting in our application. So you might wanna use rate limiting in an MMORPG to enforce game mechanics here. So let's say we wanna answer the question, does a player have enough stamina to cast a given spell at a point in time? Um, so as Bob is trying to access a resource in this case the spell casting and can only do so many spells in a certain time period before that quota resets. So we can do this simply in BALLKI using a uh a counter effectively uh and the string data type here can be used to to also support numerical operations. So you have uh some increment and decrement commands in VALKI to manipulate uh numbers. So if I were to increment, uh, just a given counter here, um, it'll create it and set it to one. Uh, and I can further manipulate that, and you know if I increment it again it'll go to 2, and this is of course a constant time operation that you're performing, so we can use this to build a, uh, a rate limiter for, for VALK for our for our application. So let's say we want to be able to start our counter here at 0 and allow 3 requests uh until a given um time delay is up, uh, so you just have 33 requests let's say in a like a 5 2nd period. So what we can do is we can start our counter and um use a Lewis script uh to do this atomic logic to be able to um count return back a value and then make sure it expires at the end of that time period. So we'll use a TTL basically to reset our rate limiter at the end of the uh the duration. So just looking at this numerically, users trying to offer requests we'll count up to 3 here. And then once it gets to 3, no more requests are allowed until the TTL fires. As soon as the TTL fires, that item is basically removed from the uh the cash, and then it can that same process can restart again as if the user has sufficient credits. So let's walk through that Lewis script, uh, to see how it works. So, uh, as I said, uh, Valky supports some simple scripting operations where you can do compound operations atomically, uh, in a Lewis script. Uh, Lu is a language that you might not be familiar with, but it's relatively straightforward. It can interact with, uh, the server command to do simple logic on top of it. So let's say we have a couple different, uh, globals here in our, uh, script. We wanna set our limit to 3, and our expire time, uh, would be 4 seconds in, in this case. We'll start by fetching the the key that we are operating on in this case the the given user's uh stamina, uh, and we'll fetch it from the the Valky server that we are running on. If it doesn't exist, meaning that the, the it hasn't, uh, either has been evicted or it's starting from scratch, then we'll set it to zero and set the expiry time to 4 seconds, uh, and if it does exist, then we, we have already previously fetched the value, and then we just check if we're within our limit. If we are, then we'll be able to increment the the key and return 1, saying that the user is allowed to cast the spell, otherwise we'll return 0. So this is sufficient to build a simple rate limiter that just allows a specific number, uh, but let's say we wanna have something that's, uh, a little bit more, uh, let's see how you actually use it too, uh, so we, we can load the script first, uh, by just, uh, inserting it into balky. We can do script load, uh, and put that entire string that you just saw before, uh, into the server. Uh, which then gives you back this, uh, Sha ID. This is the unique identifier, uh, hash value of the script that you can then use in, uh, subsequent requests to invoke that script. So then, uh, you can do eval Sha, uh, the script ID, and then pass it the arguments here like we saw before. We wanna give it the key name, the number of spells that Bob is, is, uh, has placed in this particular example. And uh it will then return back a 1 or 0 whether that is an allowed spell or a disallowed spell. This is relatively simple, but let's say we wanna have spells that have, uh, different types of, uh, different costs, so they can do a simple spell or a more complex spell. So our previous example wouldn't have worked with that because it just allows a simple counter, um, and with this we can do a, uh, we can do a token bucket, uh, which is a. More advanced rate limiting algorithm where you have a given capacity and refill rate of that bucket and if the bucket has enough credits for your given spell we'll we'll subtract from it, otherwise it will be disallowed so we keep track of a little bit more metadata associated with a given user's spell count here. We can do this in Valy by using the hash data structure that Yon talked about earlier, and for this we need to keep track of two different pieces of information. One is the number of of tokens that are currently in the bucket, and two is the last time we updated or touched that bucket. And, and you need the time stamp because, uh, uh, tokens accrue over time and so you can calculate the difference there. So let's break down what the script looks like in this scenario. Still using Lewis script, but it's a little bit more complex, uh, so we can look at some advanced Lewis scripting here. So rather than having hard coded values like global values in the script before, uh, we're gonna take in these values as arguments here. So we have the bucket size and the refill rate as arguments to the script, uh, uh, as well as the, the number of, uh, tokens for the, the given request, and we still get the key out. So the first thing we need to do is, is get the, the value of the hash which may or may not exist if we haven't run this before, uh, so we will, we, we'll fetch uh using HMG, uh, as well as the current time which is a, a value of a command within BLKI. The first step is to refill the bucket, so we, we check to see if there was an update time, uh, and if, uh, that, that exists, then we'll go ahead and, and, uh, calculate the difference in terms of credits. We'll refill the bucket is the first step, and this means you can, you don't have to be running this, um, periodically. You can just run it any time that you need, um, to, to query it, and it will refill the bucket up to the certain, uh, to the max size. If the buckets already at the max size then um or if it's beyond that, then we'll just go and and reset it to the uh to the the max capacity. And then we need to determine if the request is allowed. So if we have enough tokens, uh, if our current token count is, is more than the number of requested tokens, then uh we will subtract and allow the request. Otherwise it will be disallowed and the number of tokens are, are not updated. And then we'll go and update the the data structure uh to uh represent the current number of tokens and the last update time. And lastly we'll do some optimization here. We could keep this hash in the data structure forever and it will eventually get updated, uh, but you know, say the user goes away, we want to be able to reap these, uh, at some point in time and so we can again use the TTL to expire this hash data structure at the point where basically it would have, uh, reached the, uh, the, the ceiling of the, the, the bucket capacity anyway and so we'll take the bucket size by the refill rate and just expired at that point in time. And with that I want to, to thank you for for coming and uh we'll be available off the side here to answer any questions that you may have about BALKI or data structure modeling. Thank you so much.