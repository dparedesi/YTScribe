---
video_id: vvdjlAHojY8
video_url: https://www.youtube.com/watch?v=vvdjlAHojY8
is_generated: False
is_translatable: True
---

Imagine this, it's Friday morning, you've had an easy week and you expect to sail through the day. And then you get this email. Your AWS budget has gone off, and it's gone to you, it's gone to your manager, it's gone to your FinOs team, and everyone's asking, what happened? You panic. You start looking through your data, trying to figure out, did something go wrong, did something spike the spend? Was it something you weren't anticipating? As you go through the data, you realize nothing was wrong. It's just that you've been building and building and your spend has been going up and it's hit that budget much earlier than you anticipated. You have to now manually go through all of your infrastructure and try and find ways to optimize. Your cool Friday has disappeared. Now I never want you to get this email. And hopefully after today's session, you won't. Just to introduce myself. My name is Steph Gooch. I'm a senior essay advocate at AWS and joining me, do you want to introduce yourself? So it's Kenneth. I am an enterprise architect at Batson. And also A Hero and they also manage the user group in Malta. So today we're going to be looking at 3 things. Firstly, we're gonna be looking at cost optimization opportunities that you can take home and implement into your businesses. Some are gonna be simple, some are gonna be more complex, and they're gonna be from Kenneth. The idea is that he has lived this and gone through and optimized his business's architecture so you can take this with you. We're also going to utilize some AI. Of course we have to in this day and age. The goal with this is to help you speed up saving time and saving money and we'll sprinkle this throughout the session. Finally, we want you to prevent future waste. We want you to learn these things but also have mechanisms that make sure that you never get that bill in the first place. I'm gonna hand it over to Kenneth to introduce your company and start with some simple optimisations to kind of get the ball rolling. Thank you, Steph. So I work for Betson. Betson is one of the leading gaming companies, betting and gaming companies that, that, that is around, and we offer multiple services varying from game of chance, sports book, poker, horse racing, and as you can see, we're going to operate in multiple regions. So. We operate in, as you can see, quite a lot in different markets. In reality, we operate in 24 markets. Similarly to AWS, we try to vary our markets, and within AWS is the regions. Now, having a lot of regions and a lot of markets, there is a lot of costs. So our flagship brand is Batson, and You know what we're going to talk about now. How we're going to optimize costs for Batson? No, let's start. So we're going to start with the simple optimizations, the classic ones. So for some of you they might be. Easy. Some of you might like, OK, we know this. You know, but at the end of the day they are going to put more and more costs. We're going to start with this nice gauge over here. Yes, you see it, right? It's 1000, and slowly, slowly with the cost findings we're going to start moving to our target. So let's start with the first one. So the first one that we're going to talk about is cloud trail. So Cloudril is an AWS service which is used to capture. Logs for API calls. It is free, but if you have multiple trails or using cloud trail lake, you're going to have costs. So in our case, we're not using cloud trail lake, but we had multiple cloud trails, which obviously that puts more costs to it. According to best practices, there is the organization trail, and we opted for that. So to achieve the the cost savings, all we had to do was go onto each account and delete the extra cloud trails. The second one that we did for in terms of cost optimization was related to volumes. So for 20 million reasons, we had a lot of EPS volumes which were not attached. But ABS, if it's, if you have a volume, you're going to have costs. So we did a very simple thing. The outcome was delete everything. In some cases we took some backups because the volumes were needed for compliance reasons or for whatever, so we took backups and removed everything. The next one is networking. So when it comes to networking, we had a lot of elastics. And the majority of these elastic ALBs were public facing, which means that we are paying both for the public IPs and also for the elastic load balancers, which means double the cost or extra cost for nothing. Apart from the elastic load balancers, we also had net gateways. We're going to talk also net gateways and one of the deep dives, uh, how to optimize net gateway costs, and also we had some extra transit gateways. So basically clean up everything and the cost was saved. The next one is right sizing, and as you have seen, the right sizing, there was quite a lot of impact, so. The right sizing was one of my favorites for a very simple reason. I had to use a lot of tools which Steph is going to cover in the next slide, and it gave the opportunity to optimize a lot of our infrastructure. And this leads to the next two items that we are going to discuss. So we had the storage. The storage was both for S3 and for EBS volumes. I'm going to start with the S3. DS3, we had extra data that was not there. We didn't need it, so we got rid of it. There was data from S3 that we needed, and we moved it to Glacier, and we also implemented the intelligent teering. Now, Coming back to the right sizing thing. We're going to start with Graviton. So Graviton, it's uh an arm-based uh CPU that AW has developed and it's very cost-efficient, both from a cost and also from a performance perspective. So when you compare the price versus the performance, graviton is much, much, much better. Also, graviton can be used for a number of services, not just E2s, and this leads to the next one, which is the database part. When it comes to RDS, we had a lot of post grass. We had some MySQL, MariaDB, and these were like running in Intel. Everything worked, but they were not cost efficient. So basically, With the changing to graviton, we reduce the cost. It offers better performance and We managed to save costs also. So now I'm going to pass it on to Steph. And you can explain one of the tools used for the modern, the modern the right sizing. Thank you. Yes, cost optimization hub. Hopefully some of you have heard of this tool, but if not, do not worry. It is a free service that can allow you to see one pane of glass for your optimization opportunities. The reason this is important is some of the ones that Kenneth mentioned, deleting resources, right sizing, moving to graviton. You can't do all of those at the same time. But often when we see cost optimization, people just bucket them all together and you have this inflated cost. This is a more realistic view of what you can actually achieve. What we're gonna do is in a lot of my sections we're gonna go through demos from the console and we're gonna see in this one how you find cost optimization hub, how you use it and find some of the data that backs these recommendations. So from the home screen, the fastest way to actually get there is from the bottom, there's a savings opportunity that you can see. Now diving into there, it will take us to the hub or you can get there on the left hand side of the billing console. You're greeted with this lovely doughnut graph which shows you your recommended actions and how much you can save. This is great, but you can also change it to account, region, resource type, whatever is gonna help you on your optimization journey. To dive into these a little further, down the bottom we go into view opportunities. You can now see a list of all of the resources that you can save money on. And if, if we choose one, we can see we have migrate to graviton, but we could change that to something like right sizing. By changing this, it will adjust the information at the bottom. The stuff at the bottom is going to be other metrics and data behind it, things like usage, things like how your commitments will make an effect on these savings. But to get to the real data, we wanna go to compute optimizer. This is what powers all of these recommendations. And in here we can see the recommendation or others, ones that might be more suited to your applications or your commitment types and the different savings they have. Data drives decision and so it is crucial that you look into where this comes from. But Kenneth also mentioned idle resources, which we can do on the left hand side. These list resources that could be classified as idle, and they give you the criteria of why these are idle. Why do we say these can be deleted and what you should do about it. These are updating all the time. There was actually a new one announced at Reinvent, so I encourage you to go check and see what's appeared in your consoles. But this is a great tool just to get started with cost optimization. But I promised you AI. Let's get into some tools which can help you with these journeys, and this is going to be Amazon Q and Quiro. I know a lot of developers don't love spending time in the console, so for the rest of the session I will be focusing on things in your terminal or in your IDE. Specifically Amazon Q developer, so the plug-in chat and Quiro CLI. These are your AI assistants that can help you optimize your code across different reasons. The thing we'll be focusing on the first thing is MCPs. These you can set up in your whichever one you're using and they'll allow you to have these preconfigured usages to connect to tools such as cost optimization Hub to speed up your optimization journey. We're gonna do another demo. This time we're gonna be looking at Quiro and we're gonna actually connect to cost optimization hub and I'm going to be using my MCPs and I'm gonna show you what they look like and show you how you can use them to not only just see your savings but actually start to put them in place. So we kick off with my MCP setup, this is just my one I've got, so I've got all the ones that come with cost, pricing, cost explorer, cloud formation, cos that's what I'm building in. And also my billing and cost management one. This is the one that connects to tools like cost optimization hub and Compute optimizer. OK, let's kick off Kro CLI. Just type in Kiro CLI, and what this will do immediately is engage your MCPs. This takes a little bit of time and so to make sure that it's loaded, you can actually do slash MCP to see if they've been set up correctly. Let's ask a simple question. Can you find me all of the compute optimization recommendations for this specific account, so I'm already logged in. What it will do is look at my MCPs, connect, I'll allow it to go and do whatever I want it to do, and it will come back and it will say, yeah, sure, here are your recommendations, I'll list them out for you. It's got some RDS, auto scaling groups, EC2, exactly what you just saw, no surprises here. This is all well and good, but what if I wanna do the recommendation? What if I choose one of these, I wanna go and look at maybe my workshop web server. And I wanna go and make that change. But I have a big code base, I don't want to look through it manually. I wanna be able to copy the link to this and give it to Kiro and say go do for me. So here's my code. I'm gonna give it the folder so it knows where it is. If you're doing it in a project, this will already be in the kind of account and code you're in. Here's my code, can you go and find, and I'm trying to do good prompt engineering guys. So I'm gonna give it explicit resources and things I want it to look for. So can you go get my workshop web server? And can you make the recommendation change I've asked for, which is gonna be moving to this different resource type. Once it's done that, I don't know about you guys, but I hate writing the CLI commands for uploading cloud formation, updating it, so I don't wanna do that. So I'm saying, hey, can you give me the CLI too? This is just local, it's no CICD pipeline, it's just me deploying and testing. So can you just give this to me? Once that's done, it's gonna do what you think it's gonna do. It's gonna go through all of the code, it's gonna look for something that matches Workshop web server. It's gonna look for the change that I've suggested it makes. Because I have that cloud formation MCP in there, it knows the syntax. Now I know this is a very simple change, we're just doing a bit of a swap in swap out. But the more complex your changes get, the more likely you'll need something like that cloud formation MCP. Once it's done that, it'll give me that CLI command. Now I'm lazy, I don't wanna go run this myself, I'll just say, hey Kiera, can you run this for me? One of the best things about these two demos is they're very similar in time. So the first one, the one where I logged in and I saw all of my savings and cost optimization hub, is a very similar time to me using Hero, finding the recommendation and making the change. What that hopefully is gonna do is inspire you to start using these AI tools to do exactly what we said, speed up your optimization recommendations and get saving faster. With that we've done some simple ones, but I want kind to take you through some more advanced optimization, so it's in a list through some of them like this before, but it's in a deep dive into the last three to give you a real in-depth optimization suggestions. Over a do. Thank you. So we're going to start with the advanced optimisations. So currently we're 50 km halfway through. So we're going to start more for the advanced ones. So we're going to start with CloudWatch. So CloudWatch is used for logging. And and obviously you're going to have quite a lot of logs in your systems, and that is going to generate quite a lot of cost. So how can we optimize cloudwatch? First of all, any extra logs that you don't need data, whatever, you, we can, we used to, uh, we got rid of them. We didn't left log groups which never expires. We set proper thresholds and also by this we reduced the amount of logs that we also produce. So by optimizing code, by trying to limit certain application generating, so we did some optimization from there, but. There was also another one which was quite interesting, which is the infrequent infrequent access. So by changing the logs to infrequent access, we managed to reduce quite a lot of costs. The second one that I want to talk to you about is AWS config. So config per se, it's, it's a very useful tool where it continuously records events that are happening on your account. So if I'm changing a resource or something, an event is generated on config. So specifically, I'm going to talk on EKS. So everyone who works with Kubernites knows that if you're going to have a load, the EKS will start doing auto scaling. So guess what? We had configene Bonics. There are 2 ways to capture. Conflict changes, one of them is daily and one of continuous. Guess what? DKS was configured for continuous. So in busy periods. When there's a good game, A start doing a lot of scaling, so it is regarding a lot of changes. So basically by changing from continuous to daily, we managed to reduce quite a lot of logs from there. The next one is data transfer. So there's a very simple rule, if you're going to generate traffic on AWS, you're gonna get charged. So There are different types of data transfers. One of them that impacted us badly was traffic between AZ. So instead of like for example, we had not gateways in some accounts we had in some VPCs we had only one NT gateway. So the AC2s or the computes or even Kubernis that wanted to to talk to the internet, it has to do cross easy traffic, reach the net gateway, and then gene and then access the internet, but this generated. traffic. And we had the data transfer charges. A second one that we had was Traffic to S3. So we use a lot of S3 and S3 in general. By default, it's going to resolve to public IP space in the Amazon network or the AWS network. So that means that if I'm from a private subnet, I need to go to the net gateway, to the internet gateway, and reach DS3 endpoint and go back, which means extra data transfers for nothing. There's a good free feature that AWS offers. Yes, you heard it right, it's free. Which are the gateway endpoints. This also applies to dynamo. Modi, so we implemented S3 gateway and points for every VPC, so any S3 traffic that we wanted to. Access instead of going through the net gateways, etc. it goes directly with the S3 endpoints. So that saved quite a lot. We also tweaked a bit Kubernis to try to keep the traffic as much as possible in the same AC. So the next one is lambda. So one of the things that we did with lambda was to change to graviton. So based on that, we can save, we saved quite a lot of costs by changing the architecture for lambda to, to graviton. Also, one of the things that with this change we managed to get was to reduce the compute requirements that was needed for the lambda function. To, to execute. Also, through code optimization, we managed to reduce the complexity and that also reduced the amount of time that the lambda was being executed. So that's where the cost savings related to lambda. Now the next ones. Are going to be the deep ones, the deep dive ones. So we're going to start with the net gateway. There's, we're going to explain how to do it. There is and there is data migration service. So let's start with the gateway. So So the net gateway is needed for private subnets to access the internet. So this would be a typical setup. You have a normal VPC. You have the internet gateway. And you're going to have 3 AC, as I mentioned before. We changed from one net gateway to 3 net gateways, one per availability zone. So till now, nothing complex, 3 net gateways and the IGW. So. We're going to have 3 net gateways running, so there are the hourly charges, and there is the data processing now. We're going to change a bit the setup. So same VPC as before. We're going to have AZ1 And we're going to have the S3 VC gateway endpoints. So all the air traffic that I mentioned before now is going direct. The second optimization that we did was the network ACLs. So any traffic that is required to be blocked, we block it at source. We block it at subnet level, and traffic is not going to pass across multiple AWS networking networking items such as transit gateway. So that is going to reduce the traffic. So this setup for the 3 AZ is the same now. Where is the net gateway? Does it run away? No, it's coming. So we have a centralized egress VPC. So what is, what this egress VPC is going to do? So we're going to have They are not gateways. And HJZ. And then we're going to interconnect the Transit gateway with The VPCs, so we're going to talk. How this makes sense in terms of cost. It's not a networking session, although I'd love to do a networking session. It's a cost session, so we're going to talk how we're going to save costs. So when you calculate the cost for 3 net gateways in terms of hours and assuming that you're going to process 2 terabytes, the cost is going to be $190. So With the centralized set up, that cost is going away because we don't have any net gateways. So till now, perfect, $190 out of the window, so we did some savings. We'll Next one We're going to reduce the traffic, at least in our case, the average was 385 to 45% because we are using the S3 endpoints and the network ACL. So less traffic. Now we introduce more costs. Why? Because we have the transit gateways. The transit gateways need their attachment. And we had the 3 central net gateways and the attachment attachment processing. So for the fixed cost we're going to have $218. So the variable cost, which is the traffic being processed, that adds to another 104. So wait, you said cost savings. 192 322. Do you know how to calculate maths? That is the cost savings, but It doesn't apply for small setups and it's going to lead up to this nice graph. So the breakeven point, it's at 10 VCs. If you're going to go with less, at least in our scenario. It's not worth it. If you start increasing the number of VPCs, you're going to have better savings, going up to 8%. So with this. You can reduce using AWS components to reduce the net gateway costs. So that is the first one when it comes to the net gateway. The second one that we are going to talk about is Saving cost by choosing the right region. So again, networking, again, it's not only networking, it's going to be cost. So this is the AWS network map. So why do, why are you putting this into place? We wanted to transfer data from Europe to Sao Paulo. So as you see those nice. Purple lines between Europe and the US and then to Sao Paulo. In order to reach Traffic from Frankfurt to Ireland or whatever in Europe to Sao Paulo, we have to pass via the US. So What's going to happen? Let's say we're going to have our data data migration service instance, which is a 665 4 X large for this. For this part. If you go into Frankfurt, which is the EU central one, that is going to cost 2,573 per month. If we go to Ireland, the price is reduced to 2300. If you go to US East 1, it's even cheaper. It's 1,619. But if you go to Sao Paulo, guess what's going to happen. The cost is going to increase quite considerably. So Choosing the right region is going to save us money. Obviously, US East one is the best place in this case. So by. Building the instance in US East one. Instead of doing it in Sao Paulo, we would save $2,438 per month. So, although this is example is on data database migration service, there are other services that you can use the same logic to save costs. If you have tools, the prices between regions vary also. So if you have an environment we have multiple regions. Then choose wisely. The next one is web application firewall. So how we're going to lower the cost for what? So. is used for securing or protecting endpoints. So Everyone uses WF or hopefully everyone uses WF because security is important. So there are two things with WF. So there are the standard features which are some of them are listed now, and there are the premium features. So let's start with the standard features. So if we're going to do geo-blocking rules, That is a standard feature. So it's included in the base price. If we're going to use many AWS rules, some of them, they are still within the standard standard list. So what we're saying here is use as much as possible the standard features like geo-broing rules, like default rules, like core core rule sites, rate limiting rules. A loveless blockless IP reputation. 2 Minimize the traffic that is going to hit the premium features. As the premium features are going to be much more costly. So as you're going to see now, these are the premium features. So we have bot control, account creation, fraud prevention, and account takeover prevention. So if we're going to hit the These rules with all the traffic that was supposed to be blocked by the standard rules, you're going to have a nice bill at the end of the month. So, it's important to use custom rules, to use labels, and to scope down. To minimize the hits that you're going to get for the premium features. The next one is the challenge part, so. Everyone knows what we have with captchas, those nice dreaded puzzles that you try to get when you're trying to access a website. But there is also the challenge. So what's the difference between challenge and captcha? So captcha, you have the nice puzzle. Challenge, it's a silent check that is done in the background. So If you really need to have capcha, go for it, but watch out for the price because it's $4 per 10K. While if you're going to use the challenge, the price is 40 cents per million response. So unless you need to have capcha. Do it, but make sure that you are a bit cautious. The next one is the web capacity units, so by default. The standard prices to cater for 1500. You can go up to 5000. That's the limit that currently allows us. And for every for every extra 500 that you're going to have, you're going to be charged 20 cents per million request. So let's assume that I have 2500. So is there a way to optimize this? Yes, there is a way. So if you go to implement. Normally implement Cloudfront, which is the content delivery network offered by AWS. So, Cloudfront, guess what? It supports SWA. So you can associate awa with Cloudfront, so you get 1500 units. cloud front then needs to redirect your traffic pointing to your prem to the your infrastructure. So in this case, We're going to have an origin. That origin usually it could be an API gateway, it could be an application load balancer. All these things support SWA. So by using the two combinations, cloud front. Wealth and the origin in this case ALB API gateway with wealth, you're going to save. Money. So now I'm going to pass it on again to Steph. So she's going to show us more interesting stuff. Thank you. Some of those examples I never even heard of before Kenneth and I started working together on this presentation, and hopefully they've shown you to think a little bit outside the box when it comes to optimizing, especially with your networking. But when it comes to AI we can start to utilize this to help us do this, because sometimes you want to take into account our whole architecture, not just those individual resources. So what we're gonna do in my next demo is we're gonna look at my very large piece of code, which is just my big cloud formation template and see how we can use an inbuilt feature, an Amazon Q developer, to optimize this. So in my ID I have this whole file, maybe I inherited it, it's not really mine, and I've been told to have a look and see if I could save money. Rather than go through this manually, if I select all, right click Amazon Queue and hit optimize. It does it for me. So what it will do is send it to that chat feature and it will start to do just that, it will start to look through and find ways in which you could optimize the infrastructure. And it'll also make me a new version of this file that has savings. What's really cool is it looks through some of the stuff we've talked about, so it's gonna show things like networking, it's gonna look at things like storage, it's gonna look at things like compute and be able to see all these savings, and the savings it finds in the end are huge. This is a great example of, it can at least give you the art of the possible, you don't have to do all of these things, but you can start to understand what is good. What it also does, apart from just give me cost savings, is it also looks into other areas of optimization. So if we look up it has architecture and it has security. So this inbuilt feature can just give you a first kind of go over your entire architecture. So this is a very simple one that you can just have a go with at home. But yeah, hopefully you'll find some savings in there that maybe you didn't even know and to anticipate. This is all well and good, so we found some simple optimisations and then some complex optimisations, but what about saving for the future? What about preventing waste? How do we make sure that we're not gonna do any kind of mistakes or get a risk of that budget alert in the future? Service control policies. Now some people might not like this answer, it's a bit tentative, but they are a really good way of preventing potential waste before you even deploy into the cloud. We're gonna go through what these things are, a bit of a recap, and then we're gonna show you how we can use them with AI. So a bit of a recap with them is they're policies set at the organization level, so either the OU or the accounts level, and they are a set of guardrails on your account. They cannot set permissions, but what they can do is set an overarching permission for an account, so what is allowed in there despite the policies of your roles and users. And the great saving money What we're gonna do is look at the logic of an example, again just to recap for the folks who haven't come across these before. Well, hands up, who has heard of SEPs before? OK, about half the audience. Hands up but still if you use them in your accounts. OK, hopefully more of you after today's session will take advantage of these. Let's have a look at the logic. So for example, here we have one where we don't want to use a certain instance type. So the idea is if you have in this account or this OU, you cannot deploy, as in start or run an EC2 instance that follows the criteria that it has a 24 XL or a metal. I've chosen this example because not many of you require a 24 XL or a meta for your day to day usage for those development accounts, for those sandbox accounts. You don't need these. So don't accidentally deploy them or someone else accidentally deploy them and then you have to foot the bill. You might be like, Steph, I need that, OK, so you do have a bypass roll you can access so that you are allowed to deploy it. But this kind of case is accidental spend. I see this a lot with customers, they have a massive spike in their bill because someone did something wrong. Not you guys though cos you're here, but. Possibly other people in your organization, and so that's who we want to protect against. So what are some other cases that we have for this? Well, we've been through some of them, so things like using GP3. GP 3 is 20% cheaper than GP2 and up to 1 terabyte is exactly the same performance. So often we suggest, hey, have this by default for your resources. We also have denying of those NAT gateways. Like Kenneth said, if you move to that example, you don't want people spinning up NAT gateways in random accounts, you want to control what they can do. Tagging. I know people, we're not going to talk about tagging, don't worry. But if you have certain tags for scheduling, for automation, for security, for compliance, you need to make sure that they are in there. And before deploying a resource, if you have them as a preventative measure by having these service control policies, you'll be able to stop people from missing them. Standardizing those types, for example, not using those big instances, but also what should we use? Graviton, managed services. Always the best example is using graviton for managed services, and that is gonna save you 10% depending on your resource by default. Finally deploying in the regions, as you saw from Kenneth, it makes a massive difference where you deploy. And sometimes we deploy there by accident. When I was younger, when I first started building in the cloud, I would come in, my console, deploy some resources, come back the next day, and they wouldn't be there. Where'd they go? Turns out I was in Oregon. Who hasn't had that happen, it just turns into a different region. And someone's just like me. So that happens all the time. Again, I'm not saying it's gonna be people in this room, but people in your organization, junior members of staff, new employees who don't know what the best practice is for your organization yet, might just deploy a different resource in different regions. So you wanna make sure you prevent that kind of thing beforehand. Now we've got some ideas for these SEPs. Let's make a service control policy and let's use AI to do it. I'm also gonna slip in here a cool little feature I found when using Kiro that you can take away and have a go to. So in this example we're gonna take the SEPs we spoke about and get Kiiro to make me a service control policy. So I'm gonna say, hey, can you make me this service control policy? I've copied and pasted the text from the last slide into here and it's gonna think and say sure Steph, I'll make this for you. I haven't done the best prompt engineering because I haven't said what region is good, but it's taken a guess. It knows what I'm trying to get at and it's given me this as a service control policy. Great. Pause, what if I need to leave this for a bit? What if I need to go away, I have other meetings, I have other workloads, I have other things to do. Well, what I can actually do is I can save this conversation to come back to it later. So if I do slash save and the name of a file, like a SEPTXT. It's gonna save that somewhere in the project. Then kind of imagine later on, I'll clear this scenario, clear this chat. I'm now back, I'm ready to do some service control policy work. I do slash load and the name of my file, and it will remember the conversation we had. So if I say, hey, can you recap the SEPs that we went through before, it will be able to ingest that and have it in its memory context. This is just a cool little feature I wanted to say. Now I have my SEPs loaded again. Let's have a look at some code. I'm gonna say, hey, I have this big file, can you check and see if I'm gonna come up against any problems with a service control policy in this code? And let me know what those would be, because it's better to do it now than when I'm trying to deploy into my Amazon account. When I'm gonna get hit up against other more efficient roadblocks. So it's gonna look and say, Steph, you've got a lot of issues. You've got a lot of things you've got having up against this SUP. What I'll ask you to do is go and make that change for me. What I think is good about this is we've actually got the benefit of a service control policy without deploying a service control policy. So if service control policies scare you and your organization, you could have them locally on your computer and allow them to have some kind of tests. You can also obviously do this in part of your more complex CICD pipelines. I acknowledge that. But just as you're trying things out, if you wanna check them before you put them up into your accounts or organization, this concept of having them saved and tested is great because when you have confidence, like yeah, it's not blocking me cos I don't want them to block you working. I want them to prevent any issues. So once you've tested it, now you can move it into have it into your organization and be able to use it. We're going into the last thing before we finish today's session. So we've done some good work, we've done simple opttimisations, complex optimisations, preventative ways. But as we move into a world where AI is going to build building our infrastructure and our code. How do we make sure it also knows the best practices for optimization? Context. We have to give it some context of what we like. This allows you to store information about you, your projects, and the way you like to work to improve the relationship you have with your AI tools. This works both with Amazon Q IDE and developer as well as Quiro. If you take anything from this session, I would take this context element because what it has allowed me to do is work a lot more efficiently with my AI assistants to give them information about how I like to work so I don't have to keep repeating myself for conversations that we have about how I like to build. What we're gonna do in the next demo is I'm gonna ask you to build me some code. And I'm gonna give it very minimal prompts and we're gonna see if it remembers some or thinks about any cost optimization opportunities for us. Some of the stuff that Kenneth mentioned, which was cloud watch logs and lambda functions on graviton. We're also gonna do a bit of a before and after for both Amazon Que and for Kiiro. So let's start in Amazon Queue. So I'm gonna say here, can you make me a simple test cloud formation for lambda? That is it. And it's nice and easy, it's like, yeah, sure, I'll do that. And it would just make me a little file with a test lambda function, nothing too crazy here. What we want to see is, does it know about optimization? No. There's no graviton and there is no cloud watch log group created by default. To fix this in here we use rules. So we give it a rule, create a name, I'll call this YAML cos I'm just deploying, and I'm gonna tell it what I like. I like my lambda functions to be optimized. I want them to use graviton to be IA to always have a log group and set a retention policy. Let's take the same prompt and do exactly the same thing, but it now it knows my rules. And what we should see is that it's going to take this into consideration when it builds. So we're gonna have our test lambda function is now going to have some extra lines in it, and it's in a scroll down and we're gonna see hey yeah there's graviton. That's how you put it into a lambda function. And yep, here's my log group, following my rules. Great. Let's move into Kiro and see how to do it in there. So for Kiro it's actually called context, so we can do context show. Queue and Kiro kind of overlap a little bit at the moment, so what it means is it will show I actually have the file that I just made, so that can be connected in there, so just be mindful of that. But if I wanna add one myself, I could do context add and then give a location to a file. So this file is exactly the same as my other one, and I'll do add. And then again if I do context show, just to validate, yes it's in there. We'll take the exactly same prompt that we did before, drop it in there, and now again it will listen to what I said and what I like and it will build this lambda function with those small little snippets in mind. Again, what is really good about these things is if you ever find yourself repeating conversations with AI, these are really handy to have. For example, if you really like to code in one language and not the other, then just say this, set it up there and allow you to have some more information about things like your projects. As we can see in this one, yes, it did a good job, it has remembered what I like when it develops. Use context to speed up time to deployment by making sure that it understands what you like before you do it. Let's bring us to the end of today's session. So remember this from the start. I never want you to get this budget alert. I want you guys to be optimized as you move forward and to take some of the considerations we have into how you build. What I want is this, I want you to succeed. I want you to save all this money and I want you to reinvest it. I want you to reinvest it into new staff, into new certificates, into new resources, new features of your applications. Whatever is gonna make your business better. To do this you do need to do things like cost tracking and see what you do and justify what you have saved and put it into new and exciting places and make sure that you're using those AI tools to speed this up for you. Let's go with some quick call to actions for you guys to take home. Work smarter not harder. Use those AI tools to help you speed up this kind of process. Optimize that low hanging fruit forever, do the savings and then make sure that you don't have to go back two weeks later and do the exact same work again. Save your time with SEPs or using those contexts. And finally, save time with your comms. Might be thinking, what's that? Context, optimize MCPs and save. I know we have gone through a lot of different ways to optimize and a lot of things today, so what I ask is you guys take one thing. Choose one thing from today's session that you found interesting and try and implement it into your organization and just see what happens. You can let us know. Couple of resources before we finish. If you scan this QR code, you're gonna be met with a bunch of useful resources about topics we've spoken about today, including a YouTube channel which is dedicated to more in-depth cost optimization and also things about AI. So have a little look in there. You can also find Kenneth and I's LinkedIn on there, scan that. If you are also around tomorrow and you would like to come to another session on cost, just throw this in there. We're doing an advanced uh code-talk, uh, tomorrow at the Wynn, if anyone's interested. But with that, thank you so much for joining. I can't wait to hear about the optimization that you make. Thank you so much.