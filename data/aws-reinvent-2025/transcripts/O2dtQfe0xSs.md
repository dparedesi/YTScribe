---
video_id: O2dtQfe0xSs
video_url: https://www.youtube.com/watch?v=O2dtQfe0xSs
is_generated: False
is_translatable: True
---

Mic is hot. Beautiful. Well, thank you all for coming. I know we're right here at the, uh, towards the end of the day. Expo Hall is closed. Well, it'll be closed in 30 minutes, so I appreciate you all spinning the end of this with me. Um, my name is Alex Dragg. I work on the, the product and go to market strategy side here at Kong, and you'll notice a second name up on the screen, Michael Field. You'll not see a second speaker. It's because Michael Field is actually an agent. He's going to help me give this presentation. Just kidding. He's just a co-worker who slacked off and, and wasn't able to make it. Um, I was hoping I'd get a laugh, cause, you know, when the, when the crowd laughs, I feel so much more comfortable here on stage. I was able to just, just give me a fake one on 3, it just makes me feel so much better. Well, you're so that was OK. I wasn't even on 3. That was excellent. Awesome. Thank you all. So anyways, um, agents, agents, agents, agents, I'm sure that you all have heard a lot about them at this event this year. Today I'm gonna be talking about. Building state of the art gente infrastructure. Right? Cause this is where we find ourselves. Look, we're not just talking about agents because it's, you know, yes, there's a lot of hype, there's a lot of agent washing, there's no doubt about that. But there is a lot of promise here, right? There's a reason we are accelerating into this era of AI agents, right? We've gone across lots of eras, cloud, developer platform, mobile, whatever, and now we are finding ourselves hurtling towards the era of AI agents, and not for fun. For these reasons, Yes, we can have lots of technical conversations about the AI agent. But they do real things for the business. They make internal business processes, business operations a whole lot more efficient, or at least they can. They actually, you know, sneak peek to a few slides from now. They actually aren't really doing that right now for most folks, but they can, and the promise is there. We do believe they'll be able to reduce costs for the business, boost productivity of every single individual who leverages an agent or teams of people who are leveraging an agent. We think there are actual agent products that will be able to be built and generate top line revenue or more top line revenue and of course increase customer attention, right? There's look, the, the promise is depending on how deep you wanna go looking at the business, looking at business value, looking at the tech, looking at technical value and process and operations, there's the, the, the potential really is. Really, quite honestly unknown, right? We just, we just don't know how good this could get. However, it's not where we find ourselves. We think about the promises of these things. There's 4 numbers up here on the screen, you can read them all. But there's one that I really want to focus on. It's that 6% number. When we think about automation, we think about more effective process, we think about shipping new, uh, net new generating products that will drive net new revenue. Look, it all comes back to margins for the business. Alright, everyone in here who's ever had to make a case to buy buy technology, build something, look, it comes down to a business case at the end of the day, and at the end we're looking at margins for the business, right? Agents and AI programs today are decimating these. So you're 80, it's 84% of companies see at least a 6% erosion of net business margins, not AI program margins, not this one project, not this one product, gross business margins, and that's just 84%, and that's just 6%, which is a lot. A quarter of companies, and this, I'm not making this up by the way, there's a real report that, that, uh, that showcases this. We have a blog about it if you wanna go read it on the website, but a quarter of them see greater than 16%. I mean we're talking about something that's decimating the fundamentals of business success and now when I talk about this quite often. People say, well Alex, um, look, we're, that's fine, like we're investing, we're willing to lose some money upfront, but the reality is like that, that's not why this is happening. Right, that's not why this is happening. It'd be one thing if the number one challenge for cost was, well, we're just, you know, we're investing a lot of tooling, we're investing a lot of people, investing in a lot of training, but that's actually not the case. Right, the case there is it's actually fragmented visibility across models, model providers, the infrastructure providers, cloud environments, local environments, hybrid environments. Nobody knows who's using what, building what, consuming what, exposing what, to whom. It's only gonna get worse as agents proliferate and sprawl and you know there was a lot of numbers on or were a lot of numbers on the previous slide around security we'll talk about that today too for sure and lo and behold it's also a fragmentation problem. The vast majority of organizations because look, there's a lot of talk about it at this event. AI governance, majority of organizations, they simply don't have it. And even if they do, it oftentimes is just a bunch of memos, right? It's a bunch of policies that we say we're gonna follow, we say we're gonna implement, it's just on a document somewhere, right, not actually implemented, enforced in the real world at one time, etc. etc. and what is the result of that? Well, there's a reason why 20% of all incidents now are categorized as shadow AI incidents. 8 86% of organizations are completely blind to how data flows around, right? Data moves around, these AI data flows. And look, I'm not saying that anyone's doing anything necessarily wrong. It's hard. There's a lot we have to do. It's more than just two things, right? It's gonna get a lot more complex in a second, I promise. But it is this, just this part is hard, just knowing the right model for the job is hard, right? OK, now we bring the right model or models. What about security? What about guard rails? What about res uh uh quality and accuracy? What about scrubbing sensitive data, PII once again, especially as we continue to expand the amount of models that we're using, amount of clients and consumers who need to leverage these things, amount of data that we're training these models on. OK, now we gotta be able to measure stuff, remember cost. Well, can we see how many tokens are being consumed everywhere and not just on the corporate cards, but all those engineers that go and start consuming on their individual personal cars, then expense it later, right? Can we see all this? Can we implement things like controls on consumption, usage of tokens, right, can we rate limit these things? What about not just token rate limiting, but something like semantic cash and caching responses to make the actual cost of every transaction on average go down. Look, I can keep talking about all the things that we need to do. Right, there's a lot, it's very hard. And organizations right now have so many things that the market, that vendors, guilty as charged, are throwing at them, they're having to think about, right? And that's just the LLM. The reality is a lot more than that. When we think About the agent We have to think about what we refer to here at Kong as the entire agentic data path. Normally I have a nice little build for the slide, but I just couldn't get the animation to work. But normally what I would show is that, yes, you know, people are thinking a lot about this agent. So I start with just the agent, right? So OK, what does the agent need? The agent needs a brain, needs some reasoning faculty. LLMs, excellent. Yeah, let's go slap something in front of the LLMs. You hear about LLM gateways, AI gateways, there's a reason those are, those are popular, right? They can implement some of those controls I just mentioned on the previous slide, rate, limiting, caching, etc. OK, the LLM is only trained on so much data. What is it, if, if, what does the brain need to make decisions? It needs context. It needs extra information from time to time. So, here comes MCP to the rescue. No, actually not, right? MCP is really good at enabling context. They can fetch context, mutate context, that's fine. Where does the context come from? What provides the context? Well, we're talking about static data, static information, this is gonna be an API call most of the time, at least, we think this is going to be the best pattern so far. But it's not just static data. Agents will need to make decisions in real time with the most up to-date information that they can. So now we're talking about real-time data, now we're thinking about event streams, event data, getting a little bit closer to the direct data. So it's not just L LLMs. The speed issues, the cost issues, the risk issues. So all those little squares in the slide before this, there's a lot to think about for the LLM. There's a whole lot more to think about across this data path. How are we going to actually make these things worse, work? This gets even more complicated when we're thinking about all the other agents. We start thinking about multi-agent architectures, agent to agent or agent to agent to agent to agent, right, each with their own data path to govern, to make cost efficient, to make secure, make resilient, right? And to prove my point, let's just talk about that cost thing again. If you look at the unexpected AI costs, which are what are driving these these cost issues so far, hopefully you can see it's probably a little small for those of you at least in the back. LLM token and API costs, that is the, if I count correctly, the 5th challenge, right, for these AI programs. The 5th, uh, the 5th, uh, greatest unexpected AI cost. So once again, it's a lot more than just what's going on with the LLM. All of the context and connectivity matters. Once again, LLMs are our brain, they need context, MCP. Mutate, fetch, enable at the end of the day, enable context for more APIs and events. Remember what is the challenge, what is the thing that's driving these problems, Fragmented visibility. Not only in consumption of resources. Visibility into how data moves across this data path, and it's because we've ended up here. Right, this is the, I, this is not me saying everyone, anyone's done anything, you know, horribly wrong, we've all been experimenting. We've all been experimenting. And that's good. We have to experiment. It's really, really good. But, look, The the the problem is fragmentation. We must unify these ecosystems. Right, we've been thinking in, in sort of pockets right in the API land, which is where Kon has spent most of our time, right, API management, API gateways. We've been trying to, you know, talk about this unification of the API ecosystem, right, but often they look, they just, the migration can be painful. It seems scary, right? But now we have a real reason to do that. Now we have a real reason to think, at least not a real reason, but more compelling, more, uh, immediate reason given all these AI use cases and AI initiatives to unify everything from the API to the event to the LLM to MCP to agent to agent, right? We must unify then this this connectivity. But it is not just about implementing the proper technology or getting rid of other stuff, that's not my point, I'm not here to pitch at all. You won't hear me talk about a Kong solution on the stage. We can talk after if you want, but not up here. But remember, fragmentation is the killer, so how do we go about solving this? We think it starts by asking the right questions. Now, I speak to folks all over the place in the org, CTOs, you know, Chief Data Officer, the, the VPs of engineering, to the technical, the platform teams, the data teams, the individual app devs, and what I oftentimes hear. So we're asking this question a lot. Right, what applications are we gonna build? What agents are we gonna use, what frameworks are we gonna use, right? What are these agents actually going to do? And that's a, that's a good question. I'm not saying don't ask that question. They ask this question. I'll also ask these depending on who you're talking to, what's going to power these things? Right, will it be the APIs? Will it just be MCP? You know, there's been a whole lot of conversation around there around whether or not MCP is going to, uh, eliminate the need for APIs, right? I, I don't think so, um, but, you know, maybe they're asking this question. Direct data access, real-time data, who knows, right? There's a lot of questions around this that are being asked. But our feeling is that you don't wanna start there, right? The reality is like all this is, this is changing rapidly. Every week, there's some new thing someone is telling you to do, right? As it pertains to AI and AI and agentic application development, right? And so our feeling is that look, no matter what kinds of applications you'll need to, to use AI to build, no matter what's going to fuel them. The relationship will be there, something to solve. These things are gonna change a lot on the right and left. There is a consistent problem for the organization to solve, and it is this thing in the middle. Is this connectivity layer in the middle, what we call the connectivity layer of AI and we think we've seen in practice with with customers that when we start here, we start building a flexible, secure, reliable, resilient layer that can adapt to the changes over here on the right and left, this is when the organization can truly proceed into this genic era with confidence. OK, well that sounds fine. Maybe you believe me, maybe you don't. If you don't, let's just talk after. I'm gonna try and convince you. But if you do, the question should be, what do I do? OK, what, what is this AI connectivity layer? How do I approach this? OK, let's think about what an AI connectivity platform could look like. Right, I think about it in terms of 5 pillars. First pillar, build pillar. All these resources, all the fuel, it's going to power these applications. We still have to go fast, right? How do we, how do we enable the builders to quickly design, test, debug all these resources, and the integration patterns and connections between them or between these AI applications and these resources. Take something like MCP. How do we, how do we test the consistency, the reliability, the accuracy of these transactions? How do we test the relationship between the server and the API, the server and the event? How do we test these things? How do we build these things? How do we ship these things quickly? But once again, remember, security's a problem. Speed, cost, risk. These are the three things that I talk about all the time. How do we make sure that we mitigate risk properly? This means how do we centrally and implement standards around exposure of these resources and access to these resources for the agents, for these AI applications, right? Third pillar, discover. We've built the thing, now we're running the thing, it doesn't mean that whatever needs to consume it can actually find it. Right, we've still not done a good job for the human consumers, oftentimes. Right now we have this new kind of consumer, this non-human consumer, this agentic consumer. Human consumers will still matter for sure. Both need to be able to find these things. For two reasons. One, for the sake of building on top of it. So think about for the, the consumption use case, right? How do I package up MCP server, how do I package up, um, or agents even, how do I package up the APIs and event data as proper self-service products with all the documentation, all the self-service registration, the ability to try it, the ability to have a credentials provision to start using, once again, all in a self-service, seamless manner. But that's just the first reason why discovery matters. The second reason. Brings us into our 3rd pillar or 4th pillar rather, is govern. We have to be able to see everything, discover what we have across the data path. See, is it secure? Does it match up with all of, are you doing the things that match up with all those uh bullet points on those governance memos? Right, are we doing security the way that we've agreed upon? Are we doing resilience the way that we've agreed upon? Dirty little secret of MCP is that right now it's actually not that much of a secret based on the conversations I've had. We've got a lot of individual engineers running around spinning up their own MCP servers on their local machines. Do we have to do that, we have to go through that phase. Don't get me wrong. Once again, we're experimenting, we're figuring this stuff out, but what about standards for MCP server generation, server development? We have to be able to find these things. Measure these things, measure specifically their compliance with our standards, and then automate best practices and guardrails once again across the entirety of the data path. This brings us To the 5th pillar. I'm realizing I'm blowing through this, you all will not be here for an hour unless everyone, anyone has a question, but monetize, there's plenty to say about this. Remember that margin's erosion number. When people here, especially API monetization, they think revenue. So some folks will think chargeback for sure. Both matter, right? Both of these things matter. We must, if we're gonna fix the margin problem, first start with proper cost governance. Yes, this can be charged back and cost sharing internally, but it starts with being able to see consumption. Define entitlements for these various resources. Remember all the resources LLMs, the agent, LLMMCP API event. Define entitlements, meter in real time, I cannot. Emphasize enough how much that real-time qualification matters. We can't wait a week, right, to see how many tokens have been consumed, how many APIs have been called, how many times. Real-time metering against those entitlements. And then be able to pair that with enforcement and control. Must be able to see it, measure it, enforce controls in relation to accordance with what you see and what you measure. But that's just the cost control. Remember, margins has that other part which is top line revenue gen, right? It's our belief that there's a massive monetization potential here. I think that every single white line on that data path slide earlier is potential monetization, depending on of course the space, the use case, the potential consumer. I was talking to, uh, I was out in Australia, uh, for about 2.5 weeks, about 2, 3-ish weeks ago, speaking to a lot of organizations about what they're doing with, with AI and one of them, it was really interesting. They're horrified about the cost implications of what right now are these external agents. So there are agents once again external to their company that right now are, you know, they're, they're scraping to go get the information that this particular, uh, company can provide or its travel data for the most part. They know that they wanna expose this stuff in a more standard manner. They don't wanna do the whole scrapey eye thing, right? They want to expose it as proper APIs, proper MCPs, well, really a BMCP service for the agents, right? But they're horrified as to how much this will cost. So once again, they don't control those external agents, right? They don't, they don't own those. They don't build them, they don't see how they're built, right? They're so, they're so horrified horrified about how much this costs and controlling the cost of this, this resource consumption that didn't even take a second to ask them, could we actually make money from this. Clearly these agents that are being built, they need this data. I could probably charge for that, right? Maybe, not all the time, but maybe. And we talked through this and it turns out, yes, there actually might be a very interesting use case here for them. So let's approach margins from both ends, and it's not just the MCP server. Once again you think about that. If you are an organization that has a nice unified governed API program and those APIs are sitting there exposing a bunch of business logic and data. Any consumer might need an agent or whatever else, or MCP server, monetization potential. So once again, when you're building your platform, all five matter. Build, run, discover, govern, monetize, right? Oh, supposed to be an animation. Oh well. The thing we're supposed to say, but it must be unified, it must be unified. Remember the statistics from earlier margins erosion, cost issues, risk issues. It's a fragmentation problem. We can continue experimenting with a bunch of different tools, but at the end of the day, the banker will call at some point, right? We have to clean this up. We have to think about how we take a unified approach to all five of these pillars. I have some perspectives as to to how organizations can do this. Once again, like I said, I'm not gonna pitch, we can talk after or we can talk about it any questions at the end because hopefully you'll have some. But I'm going to leave you with this. Once again, speed, we want to go fast. I agree. Uh, the business that figures this stuff out first, they stand to gain a lot. I sort of talk in terms of what I call this agentic AI innovation flywheel, right, where if you are the organization that can get your stuff out to market first but also not have to roll a bunch of stuff back or pay a bunch of fines and fees due to compliance issues and breaches. And or spend all the money that it takes, right, to go fix something after it's been rolled out into production, especially some big glaring issue. So if you go fast, balance with cost, balance with risk. But your competitor doesn't do that and they just go fast and they have to do all those rollbacks and pay all those fines, right? The organization that gets there right first, well, they get out, they stay out, they gobble up more market share, which means better top line, better margins, more investment into the speed cost risk, more investment in the speed cost risk, you see what happens on the opposite side. I call it the agentic innovation death spiral where you go out, you don't prioritize these things, right? You have to roll back, spend all this money, you lose market share, which means that, well, now we have less money to put into these programs and so on and so forth, right? So speed alone, just a fast way to fail. Speed, cost, risk across five pillars, across the data path. Defragment We must defragment, at least in some capacity. I'm not saying it's easy, but the early research says that this is where we have to go. How do we do this, 2 things unify visibility, yes. A lot of people and when I show that data path slide, they'll say, OK, well we have monitoring. It's like, OK, that's great. Do monitoring, do observability, that's awesome. But you must unify visibility of these things and enforcement of controls, best practices, guardrails. And I'll actually leave you with this. This is a quote, not for me, oops, but from that report, the 2025 state of AI cost governance report where I got all these numbers on margins erosion and what's causing it. This is the whole flywheel and death spiral concept in a quote, right? Stakes couldn't be higher. AI genic stuff is going from nice to have to must have, at least it will. The folks that master these things will protect the margins while competitors watch profits disappear. I'm not trying to be a doomer. I might sound like one. But I'm not. Look, This is a massive opportunity. This is not just talking about how bad things are. This means that everyone's struggling with this. Everyone's having a hard time. Once again, the opportunity is for the organizations that can get it right, right? They're the ones that, quite frankly, I, I think stand to dominate the space in the specific industry and verbal vertical that they're in. Which is something that is, especially a lot of times technical teams who are trying to figure out how to get internal buy-in, right, get budget, uh, get influence to go build these things. What's something we can take to the top, right? This is something that could define the next decade of the businesses, the companies that you all work at, you all work at's success. That's all I got.