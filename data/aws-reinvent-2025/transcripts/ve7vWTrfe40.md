---
video_id: ve7vWTrfe40
video_url: https://www.youtube.com/watch?v=ve7vWTrfe40
is_generated: False
is_translatable: True
---

Hey, good morning. My name's George Lewis and I'm here with Wandana, and today we're gonna talk to you about Amazon S3's release pipeline. What I want you to walk away with today is some of S3's best deployment safety practices. We're going to dive into our testing infrastructure, how we actually do it at scale, how we allow our global teams to scale when we're talking about testing and validating before we do deployment. We're gonna talk about multi-partition deployments. How do you go from commercial to China? How do you go from China to US gov cloud, European sovereign clouds, and then ultimately into Amazon dedicated clouds that have different partitions, different considerations. Then we're going to talk about application feature controls such as feature flags. We're going to talk about and on cords and being able to do AB style deployments. And finally we're going to end up with stateful deployments and pipelines talking about how we maintain state and durability for S3. Before we get on, I, I did want to say that we have a couple of assumptions for this crew today. Uh, we already expect that you guys are experienced developers and operators coming into here. We're not gonna be covering some of the 100 and 200 level stuff beyond a couple of slides to give us some context, and then we're gonna move on pretty fast past that. We expect that you and your teams are able to understand and write unit and component tests. You guys have integrated IDEs and everything along those lines, um. We're gonna assume that you already have deployment pipelines, that your hardware is infrastructure is code, that you're not trying to manually do a bunch of this stuff, um, and that you've already progressed to that point. Although this presentation is keyed for, uh, larger organizations and more complex software systems, uh, smaller teams and, uh, teams that are kind of on the path to that large one should be able to get quite a bit out of here and be able to take some of these lessons home. All right, let's start with some of the context around S3. Uh, we have millions of servers worldwide. Uh, that's 39 regions, each with 3 AZs, and we're growing year over year. Uh, this includes Amazon dedicated clouds and, uh, China partitions. Um, our fleet includes a large variety of different types of services, but we're mostly gonna talk about three major components inside S3. The first is the S3 web server, which is our stateless API front end. Standard web server sort of architecture. Then we're going to dive into index and storage fleets, which are the key stateful architected systems and which, as their names imply, actually capture the metadata, the indices of the storage, and actually durably store our information. Alright, let's get into deployment safety. Deployment safety really starts with testing. Let's first look at, uh, um, how we, uh, uh, some of our testing systems here. As we said, we're not spending much time on unit tests and basics. Instead, I'm gonna focus on these three systems noodles, HiFi, and performance testing that allows us to kind of horizontally scale, um, across those dimensions. Uh, first is Noodles. Noodles is a test platform for writing functional behavior driven tests against S3's public APIs, and we built this specifically for S3's, uh, distributed teams. On the surface this looks like any other behavior driven test system. Uh, you're, you are, you have the given when, then sort of, uh, structure along those lines along with scenarios and everything along those lines, um, and most of you are probably pretty familiar with this. Here is a little bit of a code snippet of how this works. The top part is where you actually define the feature and the scenario, such as put returns of 200 for a bucket owner, given that Bob owns the bucket and uploads the the item. After actually defining kind of the user base feature uh along those lines, then we actually uh link that uh feature to the uh to the step that actually connects it and allows it to actually run. Um, here we can see that um the uploads PNG to the image buckets translates to the put object response method that we previously defined. Now in our case here, most of these, um, like put object steps right there is defined by the infrastructure team that put noodles together. So when we talk about our distributed software teams right there, they're not spending a whole lot of time on actually doing the step definitions along those lines. They're defining the test and just doing the link to previously defined. Uh, uh, methods along those lines that way they don't have to think about what's actually working under the hood right there, uh, for folks that are starting this from scratch, the initial step definitions are where you're gonna benefit from spending the most time, um, that's gonna allow other folks to move a lot faster and focus on the more behavior driven components that, uh, that separate from, uh, implementation. Where Noodles really becomes powerful for us at scale is that it abstracts and uses pooled accounts and resources, and what I mean by resources is buckets, cloud watch metrics, the actual accounts themselves, Dynamo DB tables, all the logs and metrics are centrally owned by a central infrastructure along those lines. And so individual service teams don't have to worry about trying to set up any of this component right there. They essentially write the the test and it goes into this infrastructure uh on there. The last big win here for us with Noodles is it is a right once run everywhere product, and I do mean everywhere using definitions provided by the developers along with the abstracted and shared infrastructure. Noodle converts those to be on-demand tests that the developer can manually run from their IDE, from their, uh, integration environment, from our, our web server integration environment, as well as our non-prod region, uh, validators. Noodles then can take those same very uh those same tests, translate them into actions for continuous carrot canaries that run in production against every single production region. So what this ends up being is you write once and you run absolutely everywhere. Moving on from functional testing into more of our science-based testing. Uh, HiFi is, uh, a, um. Is our, uh, sorry. Uh, functional tests are the bed and rudder of software development. However, they always suffer from the problem that they test the known, known issues. And even when you don't intend to, we don't always cover the possibilities of how our APIs are being interacted with. If you've owned even the simplest of APIs for more than a week in public, you know that your customers have used them in some way that you did not expect at all, um. This is where automated reasoning and high fidelity test uh model based testing comes into play. The basics of model-based testing is that we are starting with the specification or model of what we want the system to do, not necessarily how the system goes about doing it when the, uh, the test, the system to validate the system implements the specification rather than testing the implementation. While this sounds simple when considering a small number of APIs and implementations, where this becomes critical is when you start increasing the number of dimensions and options per API and when you have multiple implementations that you expect to be kept in concert. Take an S3 put request for example. It has dozens of possible headers, content type, cache control, content encoding, server side encryption options, ACle settings, meta metadata headers, and blahy blahy. Model-based testing doesn't just test each one of these headers individually, kind of like what you would do in your functional test. It tests every combinatory uh possibility. What happens when you have conflicting encryption headers? What happens when the, what is the error precedence for uh multiple headers that are malformed? Do you throw back a 400, 401, right? Which error gets returned first? How do and then in S3's case, how do you compare S3 general purpose buckets to say S3 Express one zone directory buckets which has a separate implementation of the exact same API? Same thing with S3 on outposts. Now while there are some API differences between those three implementations, the ones that are different, that's fine, but the APIs that stay the same, customers are going to expect that they operate the same every single implementation that you have. Now let's get into what this looks like from like a a builder perspective. Our HiFi system has 3 major components. The first is the actual model or specification, and this isn't just, you know, a, a science document right here. This is actual executable model or specification. When building the model, precision is the key and it's where you're gonna probably spend most of your time doing the construction. Not only are you going to be looking at public documentation and your own written specifications, but you're going to want to spend some time validating that your customers are actually operating against that. You're gonna wanna do some log dives. You're gonna wanna do some research to see how customers are actually interacting with you today and actually see those those permutations along those lines and not just rely on your specification if it's in your logs and customers are doing it today, it is part of your specification even if it's not written down someplace, um. Uh, after the specification work, honestly, the, the model is not nearly as complex as a lot of people wanna make it out to be. Uh, you can think about this as little more than a key value, uh, uh, store, um, that's in memory. You can see like some sample code right there for, uh, uh, one of the, uh, very basic model right there. It's a basically key value pair. Um, and the big thing is just making it executable, meaning that you can actually provide requests to it and it provides the response in accordance to that model. Uh, the next component here is the, uh, test generator which is where the specification, the automated reasoning, and customer behavior kinda come together. The generator service continuously queries the API specification and is generating the requests. Remember our example earlier of all the permutations that were possible for just the S3 put object API? Well, here is where all of those uh combinations actually come together, and you get that expressed that true systematic coverage of all APIs. We then utilize real customer behavior to shape those tests into workflows and combinations that customers actually use. So do all the math combinatory along those lines, but make sure that like anything your customers are actually doing, those are key uh uh workflows and now not just headers we're talking about they do a put. They do a get. They go out there, they head the object. They change the les. They those standard workflows that are across multiple APIs. Make sure you're testing those out as well, not just uh the individual, uh, API right there. Uh, finally, we also sample some arbitrary, uh, uh, requests in, uh, in production and rerun those in there so that we're continuously looking at new customer behavior that comes in. All right. The last component of HiFi is the uh validator uh service. The service validator uh feeds its test from, uh, it's fed tests from the generator. And then executes against service endpoints or implementations under test. It, it runs the same test against executable specification. And uh ultimately reports deviations from uh the model or specification. You get a kind of a a a nifty little divergences found and that you can deep dive in here now. You don't just want speci uh uh a nifty little science report. You also wanna be able to hook this up to your cloud watch alarms and set this up as a a a blocker for your pipelines. If you don't match the specification, you need to stop right then before customers go out there and end up using it wrong. All right, in High Five we've also taken the right once run everywhere perspective that we discussed earlier. Currently we have our validator service running against our, uh, integration gamma, our regional non-production, uh, validators, and a couple select production regions. We're actually gonna be expanding this to hit every single region, uh, worldwide to actually run all of our, uh, model-based tests as well. You might ask why do I care about this if, if I've I've blocked any model uh deviations from there. This is particularly important for things like web server where you're integrating with off systems that are outside of your pipeline. Um, you, you get into a new partition that A system has a new bit of code that. Uh, Instead of returning a 401 not authorized, they return a 400 bad request, and suddenly your system starts doing a 400 where previously you would always do a 401, right? How your system reacts to some of those, that's why we want to check our, our models in production as well. All right, the last part of testing is, uh, performance testing. Um, some of you might be thinking how does performance really do with, uh, deployment safety? Sure, I care about it. I want my, my system to, uh, operate fast and everything along those lines, um, but performance, particularly when we start talking about performance as the peak amount of traffic your service can, uh, take on any individual hardware instance type, it becomes a different thing, right? Uh, Ultimately, performance testing gives you when taken together with the forecast of your predicted customer traffic, um, your minimum capacity plan and your safe capacity plan and uh on there of course then we go into AZ redundancy S3 is a regional service and you can end up with the exact amount of capacity that you feel comfortable with in every single AZ and be able to handle an AZ down event. So this is why we care about it. We have 3 phases of uh performance testing implementation inside S3. First is our uh software feature performance. This is done by the um uh the the micro. This is the isolated environment for the team and it's micro level performance. Next is instance performance. This is where software will meet the representative hardware types, uh, but still be in an isolated environment away from, uh, production. Finally, we actually produce per region ratings um on there and we'll dive into each one of these here in a second. OK, first, software feature performance, um. First, this is micro performance. Uh, it's, it's best reasoned about by the individual, uh, service teams that are actually putting out the feature for some organizations they try to consolidate and have one central performance team that kinda owns the whole world. They struggle in reasoning about how individual API performs take something like um a compression API. Along those lines, let's say you were doing compression. The amount of compressibility or entropy inside the object is going to change how the compression and decompression algorithms actually work. A lot of times as a centralized performance team isn't going to understand how the implementation can change between this Z standard or whatever standard of compression that you're using. So it's really important that. The teams own this up front. Now individual service teams generally don't have the exact representative hardware when you have a large fleet of a million servers and. A bunch of different types out there. So what you can do is just give them standard hosts along those lines. You want to get them as close as possible, but what we do is we run at least two iterations that swap hardware con uh configurations and isolate software performance from any sort of hardware, uh, variant. This cross comparison eliminates any sort of hardware bias. If software B um consistently performs better um regardless of what host it runs on, it's a real software improvement or a real software regression. You don't have to worry about, well, was it this, was it that, uh, or anything like that. Similarly, we also compare across each dimension in combination and API separately. We found that it's very, very easy for, um, micro individual API, uh, performance issues to go completely unnoticed in larger scale testing, uh, uh, environments, right? They basically get averaged out everything, you know, 80% of our requests are get and so it takes a lot for a performance degradation to actually. Spike up in some of the the tests along those lines, so it's really important that you spend some time and get each one of these dimensions along those lines. We look at um all of our different encryption types. We do different object sizes and we compare those in the AB directly to themselves and so you end up with a big long list of all the different dimensions and how their performance compares from uh software A to software B and then that single. Uh, red line out there that says suddenly the performance regressed right here will give you the indication of where you need to dive into. All right, the next part is, uh, production qualifications, um, uh, the. So why do we do production qualifications? You guys have operating services and it sounds a lot like I'm testing at in prod right now and we sort of are, uh, the first lesson we learned is that when, uh, we have highly varied regional traffic profiles. Some of this is based on the age of the uh of the region and the well established workflows. Uh, US East 1 has some of our, our, uh, oldest workflows that have some of the largest uh object requests that we have inside S3. And that is actually our largest technical difference from region to region inside S3, uh, with each new launch region average is somewhere around 250 kilobytes per request, while US East 1, the average request size will actually be over 1 megabyte. What this ends up getting is that as requests get smaller, individual hosts get higher TPS, and that higher TPS then stress CPU much higher than it does bandwidth. When you start getting the requests going higher and you're spending more time streaming, it starts to put pressure more on the bandwidth in your neck, um, and you have lower TPS along those lines. And how you scale even your, your microservices that are off your main request path. A lot of times it depends on TPS or bandwidth along those lines, so what this means is that. No, your same software, same hardware will work completely different in different regions just because customers are utilizing you completely different, right? Obviously capturing this deviation for capacity planning is essential to make sure you have the right amount of capacity in the right region. Um, and the right type of capacity, um, with this sort of knowledge you can understand, hey, what type of instance may work great in US East One and be the most efficient from a cost, uh, perspective versus let's say we go to, uh, um, one of our European regions and we can actually, um, we need to go up with CPU and a smaller neck along those lines. Um, this is why we built S3 rise. Uh, our systems that allows us to continuously qualify, um, our production fleet, uh, safely. Uh, so how do, how does S3 rise work? S3 rise is a step function orchestrator that integrates with S3's capacity control system. S3 capacity control system, uh, integrates directly with our, our DNS settings, right? And what this allows is S3 to change the DNS weight for the systems that are under test. Uh, as customers go and recycle their DNS and of course you're having high performance systems, so they're doing that all the time, hopefully, um, they will start putting more and more uh uh traffic on that individual instance even if you're at your normal uh load for the day uh along those lines. Um, and we'll continue to ramp this up until the, the, uh, host actually begins to see, uh, resource exhaustion, right, not to the point where we start breaching KPIs and latency or, uh, availability or anything like that, but once you start hitting, say, 92% CPU, you start to see Iran. Being where you want it to be, um, your, your bandwidth is, is at the right point, you say, OK, we sustain that for a bit, and it's from that that um that we say, OK, that is the appropriate rating for this software hardware combined with the customer profile in there. And we save that inside our S3 rating store and we do this in every single region continuously from S3 rating store this produces a uh uh goes into our capacity forecasting and our our forecasting team can actually look at what sort of instances and everything along those lines, and it's continuously updated and customers do change. We've seen it quite a bit. Customers will migrate from one region to another for uh for business reasons, and that can actually with large customers shift your overall traffic profile and what's best for your uh your reason at that time. And of course, um, if you haven't caught it yet run once right once run everywhere we run this in every single, uh, region on every single major uh instance type right there. Now S3 is a pretty heterogeneous fleet. Um, we have a lot of those, but we always make sure we go with our worst, our best, and any major components inside the, the, uh, the fleet right there. If you have onesies and twosies, you probably don't have to do those all the time continuously, um, but everything else you need to do constantly. All right, and that brings us back to the uh the initial performance slide about how we drive capacity management along those lines. OK, um, this rolls directly into blast radius containment. Now I'm not gonna spend a whole lot of time on here. I, I'm sure that most of you understand blast radius containment from a kind of a stateless architecture, and, uh, Juanana will get, is gonna get into blast radius reduction a lot more when we start talking about staple systems here in a few minutes, um, but like, ultimately with a web server, uh, we try to keep the changes to contained to the smallest, uh, fault unit, uh, possible, um. Uh, here's a, a quick look at the, uh, uh, S3 web server pipeline. This is obviously a really rough draft right here. Uh, we start with pre-production, uh, uh, testing. This covers everything that we discussed with the in the first portion of it. Um, and then it goes into what we call these validators. Now I've said validators, uh, uh, a number of times in here, but to be specific of what these are, these are production hosts in in each production region that does not take production traffic. It uses canary traffic only. Now what we found, particularly in Amazon dedicated cloud environments where you have different partitions and different, uh, micro instances of uh services that you're integrating with. That just being able to load up the software and it connect to all of its dependencies before you roll into the first prod region and taking is very, very beneficial. In most pipelines you're going to find that you're not going to your Amazon dedicated clouds until after you do most of your commercial capabilities right there. And so, um, if you up front make it sure that at least the, uh, the software loads, it can connect to all the dependencies, you'll cut off a whole bunch of different errors where, uh, they've they fat fingered or copied in the commercial, uh, endpoint and, and stuff into configuration, uh, on there, um, after we go through, uh, validators we do, uh, a first, uh, region. I'm not gonna call it a sacrificial region. I get in trouble when I do that. Um, but it is one region we go to one box, we spend a lot more time baking between the one box and each one of these AZs in this first region because it's the first time it takes production traffic and then we roll into US East One. Now US East One obviously is our largest region. And it has the uh the largest variation in different workloads and traffic patterns uh for us and so by going in there what we're basically doing is we are exposing the software to these varied traffic patterns early before it gets into 39 other regions right there and we can make sure that that this passes as a quality gate. Immediately after US East 1 we start going in, we start our exponential fan. Now 3 or 4 years ago we used to have like 6 or 7 stages of fanning. We've actually collapsed this with all of our shift left kind of testing along those lines. Now we really only have 2 waves right here. We do 4 regions after US East 1 and then immediately after those 4. Uh, which one of them includes a gov cloud region, includes one of our China regions, as well as two commercial regions. Then it goes to the rest of the world all in parallel. Now this doesn't mean that we're not doing the AZ by AZ. We are going to all these regions, but each region will only have software patching or deployments to a single AZ at a time for our web server. Now, uh, what are we doing while we're doing this fanning out? Uh, we're gonna start monitoring our deployments. I, I'm not gonna spend a whole lot of time on this. Um, you probably already have cloudWatch set up. You understand some of these things. Here is my list of core metrics and essential alarms that you guys should have. Um, depending on what your service is right there, uh, this will be available afterwards if you want to dive into there. There's also the, uh, Amazon, uh, uh, AWS well architected if you want to dig into some of, uh, some of the very specific alarms along those lines. Um, these are very easy to set up. So here is my obligatory AI, uh, uh, with cloud Watch metrics and actually start iterating very quickly through it. It actually, you know, all joking aside, it, it does actually, it's really good for, um, um, using Amazon Que to kind of. Audit your alarms and everything along those lines and making sure from a deployment standpoint that they're all linked up with your deployment zones. So cloud watch events you're if you're using like Code deploy, it's a lot easier. CodeDeploy will allow you to meet these up. Meet up the deployment metrics along with your deployment zones and it recognizes when it needs to roll back a deployment automatically and kind of puts those together right there if you're not using CodeDeploy or something along those lines, use something like Amazon Que to go through, look at your metrics and alarms, and make sure it's actually tied to your deployments so it knows how to stop a deployment when something goes bad. That is crazy, all right. And it just doesn't like the AI site. That's fantastic. All right, now, uh, monitoring your, your application service are only part of the equation, uh, application alarms that are looking at customer traffic struggle to detect issues that happened prior to your load balance or prior to your application, or when you have something that just takes your application completely out and you're not actually reporting anything. Although you can and should have alarms on customer traffic when it bottoms out, many times it doesn't bottom out exactly like this. The, the drop in traffic is hidden inside your normal ups and downs throughout your day, and you don't even know that there is some sort of silent failure going on. This is where canaries come in view. We've talked about them a couple of times throughout here. Uh, but what a canary is, is it's essentially you spin up your own customer client, right, that you own that you run, that is gonna push synthetic traffic to your public APIs from where you actually expect your customers to be, um, uh, asking for. If you're com uh, if your customers are mostly coming outside of AWS, you need to make sure you have an instance that's outside of AWS coming in. There so that you can actually detect along those lines. This prevents that silent failure, uh, category. Those, these are the worst operational events that I've ever been in when somebody goes, I, I don't know what happened. I just had a bunch of customers call me and say that they're having a bad day and they're not able to get through those canaries are gonna actually begin seeing the 500 spikes just like your customer and give you a a solid spot to start. Um, for, for S3 we talked about noodles earlier and we talked quickly about those canaries, uh, again we, uh, uh, we automatically build our canaries based on the unit task right there so you basically get all of the tests that you guys are doing from integration all the way through as canaries as an automatic component. Highly recommend that for your uh service teams. Um, now we built a lot of this ourselves, um, but you can get something very, very similar out of cloud, uh, cloud watch, uh, synthetics. We don't use this in S3 simply because of, um, uh, dependency, uh, management and when we come up inside building a region, so S3 doesn't depend on, uh, on cloudWatch synthetics, so we can actually launch because CloudWatch needs S3. Um, uh, it has global coverage. It has direct integration with the rest of the cloud watch alarms and metrics. It'll automatically correlate canary and customer traffic, uh, alarms, um, and it one big thing is cloudWatch synthetics will also give you the opportunity to test UIs. The example I have up here, which is one of our, uh, public examples that you can download, is a, a very quick sample of, of, um, testing, um, Amazon.com. Uh, and actually go into the web page right there. Alright, we're gonna go to our last section which is, uh, before I hand it off which is application, uh, feature controls. Um, let's start with feature flags, uh, since this is the first day of reinvent, this is the day that all the feature flags are flipping for AWS, right? uh, turning everything on before tomorrow's keynote, um, basically, uh, um, uh. Basically engineers code the feature, deploy it when the feature is hidden, and then they gradually make the feature available by flipping the flag, uh, from true to false. Um, in S3 we utilize app config, uh, for all of our feature flags, which is, uh, and our, uh, dynamic configuration capabilities. Um, it worked like this. First you define the, the feature fags and any validations you have. Pretty simple along those lines. And then you're gonna write uh clients uh code on your uh application site to dynamically pull these in. You can set this up to either be event driven, meaning any time you change this, um, but I also recommend, and in this example we actually show uh a regular pulling of that just to make sure that all the events actually get through, uh, on there. Um, now let's talk about allow list and deny list. These are usually used in conjunction, uh, with feature flags in order to launch a, uh, a subset of users. This is where you see beta tests, alpha launches, internal launches, and everything like that. Particularly in S3 we use allow list to uh for our own dog fooding mechanisms, right? So if we're gonna launch a feature, the first thing we do is we allow list ourselves. We have first start with S3's, uh, uh, own accounts, then we'd move to AWS internal accounts, then we'll do big Amazon, and all of that will, uh, operate, um, uh, along those features before we ever send it to, to customers, and then we can actually use the feature flag to roll it out during something like reinvent. Uh, the last tunnel I wanna talk to you about is, uh, using shadow modes. Uh, in a shadow mode deployment, uh, real world production traffic is copied and sent to a new version of the service, right? The, uh, this is the shadow service in parallel with, uh, your existing production service, um. The shadow service processes the the the request, but its responses are are never sent back to the uh to the to the customer. Instead they're logged or metriced or along those lines so that you can then compare results between your primary system and your your shadow system. The implementation is, is pretty straightforward. Uh, right here I give you a, uh, a load balancer example. API gateway also allows you to do, uh, this kind of right out of the box where you can split it here. This is fifty-fifty along those lines. Now, personally I don't like using this for features as much, right? Uh, I prefer to use allow list and feature flags to, to roll those out. What I use this for is internal infrastructure improvements and, uh. Just generic software upgrades if you're gonna change your web server engine, right, and at the end of the day you expect your uh customer traffic to be the same today as it is tomorrow, right? This is a really good way to make sure that everything is staying the same, same, uh, you're updating your JDK and sure it's supposed to work, but this will actually give you evidence that yes it did actually work the same way and it's, you're not getting something weird coming back to your customers. Um, We've been talking about the web server. Obviously the lack of shared state gives us a lot of options with the failure area. One web server goes down, it's not going to affect that many customers or that many requests. And once you take it out of the service, it's not going to affect any other requests that are going into any other service. This is only uh good and well if you don't have to worry about uh shared state and you're operating a web server, but what happens when you do have shared state? Say you're a durable storage service like S3. Well, for this, I'm gonna hand this off to uh Juan Donna who's gonna come in and talk to us about the stateful deployments of S3. So stateful pipelines are those that deploy software to hosts that persist data, and we have to persist this data and keep it available and durable throughout the entire deployment process and beyond. So I'm going to walk through what data preservation means, the specific risks to data during deployments, how we mitigate those risks, and dive into the details of how S3 performs maintenance activities on millions of hosts every month. So let's start with why stateful deployments are different. Everything that George just talked about for stateless pipelines still applies to stateful pipelines. In addition, to preserve the data on these hosts requires careful planning and maintenance and understanding of where the data lives and how it's organized. So we are talking about our index hosts. These are the ones that manage and store S3 metadata, and that is used to find the data. On our storage nodes and our storage nodes where we process the actual data. Stateless hosts don't persist information, every request is independent, so they can be restarted any time. Full host, though we can't randomly restart these servers in parallel. We have to understand exactly what data each host is responsible for and what the redundancy of that data is, the mapping of the redundancy, and ensure data preservation when we take that host offline. So let's look at the key dimensions for data preservation. Integrity is about making sure that the data stays accurate and trustworthy over time. Think about it like a bank that guarantees that your transactions records are not tampered with or altered. Consistency means everyone sees the same information at the same time. Like when you're shopping on your Amazon.com, you put something in the cart on your phone, go to your laptop, it's still there. Everything is the same. That's consistency. Resiliency is the ability to recover from loss. It's similar to when you keep your photos in multiple places on your phone, backed up on your local storage, stored in the cloud. You're making your data resilient against failure. The more copies you have in different places, the better the tolerance to failure. And finally, data durability is the ability to protect data from loss or corruption over time, ensuring it remains intact and consistent even in the face of failure. It's measured in terms of probability of loss, and of course, as you must have heard multiple times, Amazon S3 is engineered for 11 lines of durability. Data durability considerations are the continuous process from design and implementation to deployment. Every time we build a new feature or a new service, we do durability reviews. This helps promote a durability culture among teams, implement mechanisms to keep our customers' data durable, and provide real-time visibility into factors that threaten data durability. One of the areas of focus during these reviews is deployments and having clear paths for both successful and failed deployments, including the ability to roll back, roll forward without impacting. Any data and really the goal is that the data must stay available before, during and after deployments. So how are, what are some of the specific methods we use to preserve our data for integrity, we use end to end checksums. Think of these like digital fingerprints that travel with your data everywhere it goes, from the moment you upload it through all our processing, storage, and yes, even through deployments. They help detect bit flips in memory, bit lot on disk. And background processes can use them to validate the accuracy of the data. Consistency is when you upload or delete objects using S3 APIs, the operation is atomic, so it's either successful or it fails. There's no partial upload. And then S3 has strong read after right consistency, which means you have a unimodal experience. When you read your data, you always get the latest version. And This consistency is maintained even when we are updating hosts in the background. For resiliency, we are constantly monitoring system health and data integrity. If we detect any compromised data, we automatically kickstart recovery processes and start repair to restore resiliency. The goal is that your data again stays available through all the processes, normal operations during deployments, even post deployment. And finally, redundancy is key to hitting those 11 lines of durability. We store data across 3 availability zones using a combination of the application and Asia coding. And then we've used these durability simulation models that factor in real hardware, the AFRs of the annual failure rates of various components. We have lots of data about that in S3, and we also take into account the repair time, the mean time to recovery, to figure out exactly how much redundancy we need. So while we build for correctness, we still need to monitor for threats to our data's integrity, availability, and durability. And for that, we channel the methods used by security experts where they create security threat models to strengthen their systems. And so in a similar fashion, we In a similar fashion, we create durability threat models to help us identify every possible way that data could be compromised, and then we plan actions to reduce the impact of these threats. One of these threats is hardware failure, storage drive wears out or power supply fails, or maybe bit flips escape memory error correction. So we can partially mitigate with redundant hardware to replace the failed units. But if a drive or the host fails during the deployment process, the data associated with that host will need to be rebuilt. Another threat is software bugs, and these are bugs that escape all the testing, all the stuff that George talked about, but they're typically edge cases or unlikely conditions because there's some changed behavior or some timing change when you did a new deployment. and bugs like incorrect error handling could impact data, or version incompatibility can compromise data preservation operations. So here our impact reduction strategy is using data validation tools to catch the problem and trigger recovery processes. Operator errors, of course, human errors can result in an accidental deletion or maintenance requests being scheduled on too many hosts. So to reduce the impact of human errors, we continue to automate our processes. And Impact mitigation really is a continuous learning and development process to reduce the probability of failure, but there are not really P 100 solutions. So S3 deployments are across millions of hosts, and let's just assume we have a 99.95% success rate. For 1 million hosts, that means 500 deployments will fail. That can translate to 500 times that we have a poor customer experience or there's a potential threat to data. Now that is a problem. If you have 10,000 hosts, that can still mean 5 bad customer experiences, still a problem. So, I'll go back for a second to this often quoted Murphy's Law. Anything that can go wrong will go wrong, which comes from aerospace engineer Edward Murphy Jr. when a technician made a costly error during a safety critical project. Murphy's Law, though, isn't about pessimism, it's about being prepared. It's a mindset that drives thoroughness and proactive problem solving to prevent disasters before they happen. This philosophy really shapes our threat mitigation strategy. It's a ploy. We start with the assumption that every deployment will fail, not just the few that statistics tell us, but every deployment. So this becomes a P-100 solution because now we are prepared to handle the impact of that failure. For stateful pipeline deployments, this translates into the assumption that the data on that host will not be available after the deployment, and we need the ability to restore the resiliency of that data and having enough redundancy in place before we start the deployment to rebuild and restore the content of the host is being updated that is being updated. Now this would be very simple if you could update one host at a time, but obviously that's impractical. Instead, we serialize deployments by availability zone and create carefully constructed groups for parallel upgrades. This grouping process isn't random. We need to consider the initial data placement policies that determine where information lives, the physical physicalact location of hosts, and how these locations relate to other hosts already in the reservation group. For example, if we have 3 replicated copies, of course we have lots more. That are on host 259, then these servers will not be part of the same reservation group. So that brings us to our actual deployment workflow. S3 manages maintenance operations through a reservation-based model coordinated by a host reservation system. Think of it as an air traffic control for a storage infrastructure. No maintenance happens without permission. The host reservation system controls deployment access to all stateful hosts by requiring both humans and automated systems to obtain exclusive reservations before performing any maintenance. This includes all planned activities like patching, firmware and software updates and hardware maintenance. It also includes the automated remediation workflows that respond to detected issues and ad hoc operator work during emergency situations. So it doesn't matter whether it's scheduled or the critical response. Every operation has to go through the same reservation process, and this is why. We can make sure that we never exceed safe maintenance thresholds across our fleet, and it's part of S3's operational model for disciplined, coordinated access to our infrastructure. When the reservation system receives a maintenance request, it performs a series of safety checks. First, it examines the current reservation state of the target host because multiple actors may simultaneously have requested maintenance on the same host. Next, it validates there's sufficient offline capacity to sustain taking this host out of service without impacting performance. And it also ensures that approving this request won't push us beyond our fleetwide maintenance limits. After all of these checks pass, the reservation system checks if it's actually safe to deploy, but since it operates at the physical infrastructure layer, it lacks visibility into the data mapping on the host. So who has the actual data placement information? The service itself, right? Both the index, the metadata storage service, and the data storage service, they know the current health of the host, the projected impact from maintenance based on the data that has persisted on that host. So the host reservation system checks with the service whether it's safe to proceed, that the data's resiliency will not be compromised if the host fails to come back up. S3 has developed custom software specifically designed for this decision making process. Every service comprehensively gathers intelligence about each host, its current health status, allocated resources, and the specific data it persists. The service performs additional. Safety checks. It verifies no conflicting deployments or maintenance activities are in progress, maps the host's physical rack location like I mentioned within the data center and availability zone, and then correlates this information with the other hosts reserved for deployment. The service, with its internal knowledge of data placement and redundancy patterns, actually is the one who authorizes whether hosts can be reserved for deployments in a resilient way. So once all safety checks have passed and we've confirmed it's safe to deploy, the system reserves the host and masks it as available for maintenance to begin. But what if the checks fail? This request has to be retried. And so the fleet updated component in our deployment workflow is responsible for handling both cases, either initiating the re logic or starting the actual deployment when the reservation succeeds. Fleet Up Data is a regional scheduling service that orchestrates all maintenance for stateful hosts, patching, firmware, software deployments, and hardware maintenance. It can intelligently combine these activities to minimize downtime. Its built-in safety consoles, velocity controls, and casualty tracking help prevent widespread issues. Plus it allows us to pull the hand-on cord that George talked about at any time to halt deployments, switch hosts to read-only mode, or handle multiple failures simultaneously. So is this all theory, or do these techniques actually yield results? So I'll share a bit of real S3 data from storage hosts in one AZ and the results of two of our risk mitigation strategies. The first was transitioning from being in a reactive mode to building durability threat models to being proactive in building guard rails. And the second was adding automated safety checks to place manual interventions, so. Operator escalations during deployments in that particular AZ went from 478 in 2022 to less than 10 in 2025. So, that brings us to the end. So some final thoughts to wrap wrap up. To ensure deployment safety, testing strategies and containing blast radius is of course primary, so it can help you to reexamine processes you have in place to check if there are additional testing strategies like science-based testing that can provably show the correctness of your software. And it can improve your customer experience, or if there are additional actions you can take to reduce the blast radius in how you group your hosts for deployment. Not all regions are equal, so be aware that capacity, performance, networking can all vary between regions, A, so take that into account for your pipelines. Application consoles like and-on cords and featured flags are safety nets that help mini crises from becoming major, major disasters. So if your pipelines don't already have mechanisms that allow you to stop deployments or roll back changes at a moment's notice, it's worth adding those. When deployments to deploying to state full hosts, additional considerations around data preservation is necessary. Assuming failure can be powerful as a P100 solution. And failure simulations assist in understanding the resiliency and redundancy of the data. So the techniques we've shared today represent years of learning from both successes and failures, and hopefully some of these techniques can be applied or adapted to your environments. So thank you for choosing to spend this time with us. Um, please take a moment to fill out the session survey in the mobile app and um George and I will be at the back if you guys want to come talk to us, have any questions. Thank you again.