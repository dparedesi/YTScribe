---
video_id: 2pfSB4yxg-E
video_url: https://www.youtube.com/watch?v=2pfSB4yxg-E
is_generated: False
is_translatable: True
---

Um, my co-speaker, Fernal could not be here for personal reasons. Hence, today I'm privileged to be a solo speaker. Welcome to our session ISV 404 Build resilience as Multi-account resilient Testing patterns. I'm delighted you decided to join and appreciate you investing your valuable time with us. How many of you have experienced that panic inducing 3M call about your multi-tenant SAS environment failures? Any volunteers? 123 Great. What if you could proactively validate your multi-account resilient architecture? Against controlled experiments in terms of injecting faults to ensure that you know you have your workloads. Tested within the tenant boundaries as well as mitigating any other failures. Today we'll explore how the leading ISPs use AWS fault injection service to ensure that the SAS architectural designs are validated and tested. Without impacting your SA's architectural critical components. And usually these failures are injected to ensure that you mitigate any of your end customer availability issues in terms of services or SLAs that you're committed to your end customers. I'm Kanaya Vijay, a senior solution architect aligned with the UK ISPSA team. I'm with AWS for 5 years. I'm based out of Dublin. So today, first we'll begin with understanding some of the core principles of SAS architectures and how every successful SAS solution underpins how a multi-tenanant solution has to be operated. And then we will really understand how SAS providers must adopt a comprehensive resilience framework. Again, we talk about some of the fundamentals of how AWS shares our experience in truly building resilient applications to our customers. And then we will talk about a real world user journey with an AWS reference architecture of a multi-tenant control plane and an application plane architectures where we have set up um two of the SAS solutions to showcase resilience testing patterns. Again, the core of today's talk is to showcase you multiple resilience testing patterns on how do you induce various falls following the AWS best practices and some of the SASS fundamentals. And then finally you'll walk away with key learnings on how to adopt multi-account resilience experiments for your own SAS architectures. And irrespective of your current maturity level. And finally, you'll open up the floor for any question and answer sessions. So to build a truly resilient SAS architectures, SAS providers must adopt 6 critical pillars. Robust tenant isolation to prevent security breaches and data breaches. Noisy neighbor mitigation to maintain consistent user experience, as well as the overall performance of your multi-sentence solution. Comprehensive identity and access management to ensure that you have secure tenant boundaries. Tenant aware absorbability to ensure that you can instrument various tenant usage resources and service throttling limits and various other absorbability related metrics so that you can withstand any sort of disruption in terms of failures. Strategic tiring to serve your diversified customer segments. And last but not the least, the cost of tracking mechanism to ensure that you will be able to charge back to your end customers. And all these fundamentals remain the core building block for you to build a truly resilient solution on AWS. And one of the best practices as part of AW's well architecture reliability pillar is a failure management where we talk about test reliability. And the idea of correlating SAS fundamentals with the AW's well architecture reliability pillar is to ensure that you need to consider reliability as core pillar to validate and test your workloads against any disruptions, and this will ensure that your workload is ready to withstand both. For partial failures or the grave failures against your functional and non-functional requirements. And again, the objective here is to have a controlled experiments which you induce as part of your multi-account SAS architectures so that you would be able to either completely avoid minimal impact or completely no disruptions without impacting your critical workloads. And if you really want to deep dive, feel free to grab the QR code. Now, how do you approach a structured methodology and a continuous iterative process for you to build and manage your SARS architectures on AWS? So this is where to address this challenge, AWS developed a resilience life cycle framework in sharing resilience, learnings and some of the best practices that we have learned based on working with hundreds and thousands of customers to truly build resilient systems. Now, in terms of resilience life cycle, we have 5 critical stages where you can adopt various best practices, AWS services and strategies to mitigate your resiliency objectives. To start with, you will set the objectives, and at this stage, your key objective would be to really understand what level of resiliency is needed for your workloads. This goes back to purely a business conversation, not a technology conversation. In aligning your objectives in terms of RTO RPO or availability or any SLAs that you have committed as a business to your end customers. With those objectives in mind, you work towards the 2nd phase of design and implement. So this is where you would anticipate various failure modes to ensure that your workload is rightly designed with the right technical architecture and accordingly adopt the right tools. So once you design and implement based on the objectives that you have set in the previous stage, you move towards the 3rd stage of evaluate and test, and the primary focus would be to perform resilience testing. And here again, we are not trying to bring in chaos here. It's about inducing falls with the controlled experiments to ensure that you know the limits or the boundaries to reduce any blast radius, and there are two phases to evaluate and test where. The first stage would be to do a pre-deployment activities related to your software development life cycle that you traditionally do, and the post-deployment activities where you focus on resilience assessments, disaster recovery testing, or even resilience testing using AWS fault injection servers, depending on the nature of your SAS architectural workloads distributed either single account or multiple AWS account. And then you, once you're happy with your test results, you move on towards operate. This is where you look at your observability tools, logs, metrics, and various other best practices to instrument various disruptions causing your availability issues or maybe various other system dependency issues. And once you have the right instrumentation in terms of observability, you move on towards respond and learn where based on your various incidents that your operational team manages in terms of infrastructure failure, availability, or application level failures, you would really have a mitigation strategy on run books, playbook. Books and operating procedures and all other uh you know, incident management related uh best practices and then you continually learn on how you are mitigating those failures and then you funnel those learnings into your set objectives and various other stages as part of the overall fly flywheel and I think the key goal here is that you can apply these flywheel for your existing workload or even the new workload. Feel free to grab the QR code if you're interested. Uh, we have a very great detailed white paper on this. Now, this brings us to our next topic of phase of resilience experiments. So for every experiment that you want to um induce for for your specific workloads, you eventually get started with steady state. Steady state is Pretty much what a normal looks like for your workload behavior depending on your user traffic patterns or depending on your um you know your your service that you're offering to your end customers. So you need to measure your steady state behavior to know based on metrics or logs that how your system is behaving. So once you know the baseline version of your steady state, you move towards the hypothesis. Hypothesis is nothing but a scenario where you want to validate your assumptions or weaknesses of your workload, which is more of unknown. And how do you know these unknown into unknown factor is defining the hypothesis with with a clear business outcomes that what is the weaknesses of your workload and what you want to validate with the specific objectives. And once you define the hypothesis, you proceed towards the third phase of running an experiment. So this is where based on the hypothesis that you are defined that. Say I want to induce fault A to the component A, B, and C, and then I, I'm expecting that the system behavior will be either a fault error rate increasing 10% or maybe an infrastructure failure or any other failures that you're planning to induce. So for example, if you have an EKS spot, then you want to induce an EKSpo CPU stress, or maybe if you have an RDS database, you want to fail over to another region. Uh, depending on the hypothesis that you want to define. So once you run the experiments using AWSs fault injection servers, a multi-account setup, You would really evaluate now, OK, now your system has behaved and you want to go back and verify what was the outcome and what kind of metrics that you observed to verify those systems and then accordingly you move towards improvement of your architecture or run book or playbook or any sort of uh you know learnings that you had as part of this hypothesis and this flywheel has to adopt for every resilience experiments that you you uh defined. If you don't do this, then eventually it becomes a confusion for anyone involved as part of this phases of resilience experiments. So be sure that you create this and apply as part of the resilience life cycle, which is more of an inner flywheel within the resilience framework. And now this brings to a situation where now this all sounds great, but how do I get started on AWS? I have tens or hundreds of AWS accounts, uh, you know, where my SAS workloads are deployed. How do I induce faults, not in one single account but multiple accounts. So this is where AWS fault injection service helps you to do that. So the way it works is. Again, it's a fully managed service for you to eliminate any heavy lifting that you have to do in terms of building custom scripts or any other you know, implementation that you have to perform to automate or induce these faults. So the way it works is where FIS integrates with IAEM where you have to define an orchestrated role as a single pane of class to induce various faults with other accounts. And then you can access FAS through management console CLI or APIs or any means that you want to integrate or get started. The very first step is to configure an experiment template, and this experiment template has three critical components. First is an action. Action is nothing but the activity that you want to perform in terms of the faults, depending on the nature of your AWS service adoption. And then once you define the action, so it could be a sequential action or it could be parallel action you want to take as part of the faults and then you would go ahead and define the targets. In this case, the target would be the AWS resources, nature of your architectural patterns, whether you're using um serverless architectural pattern or maybe traditional EC2 based deployment models. You decide on those specific targeted resources and accordingly the action will be induced with those specific targeted resources and the third critical element is FAS safeguard. So again, as I mentioned earlier, resilience testing is about a controlled experiment to reduce the blast radius. So this is where you will use. Safeguards to define a stop condition for these experiments to say that I have certain thresholds set for my experiment. When those thresholds are met, the experiment can be stopped by creating an alarm so that way you're not kind of creating a chaos with your upstream or downstream systems. And then once you define all these three components, that's where it goes as an experiment template. You can also use scenario libraries which we have pre-built as an AWS opinionated recommendation, which we think is a common set of experiments that customers are interested in where you can get started much faster. So once you configure the experiment templates, you eventually you can get start, start the experiment or stop the experiment in an ad hoc basis as well. Another great thing about FAS is that it enables you to have your third party absorbability tools out of AWS to consume or you know, consume certain events from those tools via Amazon Event Bridge where you can start the experiment or start the experiment. It creates sort of an openness platform for you to have a single pane of class in terms of their first experiments against your AWS resources. And again, there's a great case study from BMW um group who have actually leveraged FAS to identify their weaknesses of some of their critical components, and they are actually adopting this as a mental model to evaluate various critical components. So now We want to talk about a real world scenario of a SARS workload architecture. So let's take an example of a persona who are interacting with the SAS architecture. So for today's session we have two critical personas who are trying to interact with the overall SAS architecture. So one is the SAS provider who are a software company or an organization trying to design, build, and deploy and manage the SAS offering to their end customers. And as you know, SAS providers typically have to manage multiple tenants. It's a multi-tenant solution we're talking about. So as per AW's best practices, we recommend. As a SAS provider to break your overall SAS workload architecture into control plane and an application plane. So as in terms of the control plane operations, these are what you see on the screen like tenant management, admit management, billing metrics, and authentication are the core common modules as a global services serving across multiple tenants. These SAS providers eventually have to operate their workload at scale to ensure that tenants are happy in terms of the service offering, and they are eventually paying for what they are consuming. And the second key persona is the tenant who are actually the consumer of the SAS application um offered by the SAS provider. And for today's session, we have, I've set up two different SAS solution offering. One is a SAS e-commerce solution. Providing a tenant-specific product management in terms of product catalog where tenants can configure their product catalog. Similarly, tenants can also configure their order management in terms of order processing workflow based on the product that they have configured. And these tenants can also interact with the second solution offering about the SAS retrieval augmented generation where tenants can onboard their organizational related data and that data would be indexed, you know, in a vectored store and then the vector based on the tenants specific context along with the user query from the tenants would be passed to generative LLMs to generate the response and accordingly the response is sent back. In terms of both of these solutions, 10 tenant isolation, noisy neighbor, in terms of the SAS fundamentals that we talked about are super critical to ensure that you maintain those boundaries, and we will see in practical sense on some of the patterns that how do you verify the tenant isolation is working as expected based on your authorization strategy that you would have implemented as part of your SAS architecture. Now what we have seen earlier is just a kind of a workflow or a very logical construct of how tenants interact, right? So beyond SAS provider and tenant tenants you could have multiple other personas as well depending on your nature of business and the interactions. So what you see is an uh AWS SAS reference architecture for today's setup. So we have the topmost layer of user access layer where tenants are interacting with the SAS application, um, and again they're also interacting with the RAG application. Based on the integration with Cognito for authentication and authorization. And the user Access your the the tenant, the SAS provider specific control plane related components. You remember I talked about the control plane components in the previous slides. So all of these functionalities are managed by the SAS provider in terms of the control plane, and the control plane talks to the application plane through an Amazon event bridge as an event driven pattern. And the the first interface of the control plane is a core application plane offered through a kind of tenant work workflow authorization in terms of managing the tenant onboarding process as well as the other integration elements. We also have an EKS application plane hosted as a pool model offering uh the tenant specific name spaces in terms of order and the product uh related microservices having their respective data stored in terms of the dynamo DB. And we also have a siloed deployment model of a surveillance application plane as a surveillance architecture where tenants are specific on their deployment model where they can adopt the surveillance architecture as well in terms of the order processing workflow and then we have a generative application providing the retrieval augmented generation. With a shared service model of where tenants are sharing the same infrastructure in spite of isolating the tenants in terms of their data, in terms of their interaction, authorization, and all other SAS fundamentals that we talked about. Now, let's talk about resilience testing patterns, right? So I've been talking about resilience testing all the time, right? So resilience testing is about how do you induce purposeful falls into your workload to validate your assumptions with the controlled uh bounded context to ensure that doesn't impact your critical workloads or in customer experience. So this is more around how do you define those hypotheses and induce the fault and improve the overall resiliency posture. Now before we get into the patents, I just want to quickly talk about the multi-account structure and the setup that we have done for today's patents where you can see we have picked two regions, US East One and US West 2, and there are multiple personas interact with this solution. So we have a phase administrator who could be an SRE admin or lead or DevOps SRE or a lead who's trying to actually perform this faults and then tenants interacting with the solution, and then we have SAS provider. So we have uh AWS organization as a route under which we have a test OU. And these tests for you have 4 different accounts performing a specific role in the overall multi-account setup. So where we have account 1 representing the single pane of glass for AWSFIS experiments with the AWSFIS experiment orchestration. And then we have account to representing the control plane and the e-commerce application plane where we have System Manager to induce custom faults based on the System Manager documents along with AWSCSTS to perform cross-account authorization with the FAS experiment target and then we have again account 3 for the rack solution and then we have account for where we want to actually observe various system behaviors using Amazon Cloudwatch with the cross account absorbability. We also have some bucket policies configured for one of the patents to replicate the data from your primary region to the secondary region. So This is how you would set up for your workload in terms of multi-account architecture for your fault injection service. So again, if you see that account 2 and account 3 is an example for today's stock, but you could have 10 or 20 or hundreds of accounts depending on the complexity of your business where your workloads are running, you want to induce the falls. Account 1 and account 4 could be your single pane of glass from your orchestration perspective and absorbability perspective, and account 2 and account 3 can replicate across multiple accounts. So you need to carefully design how your art structure is, how the account structure is, how you want to create the hypothesis to ensure that you see the real value out of this. So, let's get started with pattern one. So pattern one is a multi-tenant noisy neighbor. So how many of you have experienced multi-tenant noisy neighbor scenario for your workloads? Cool, so it's a scenario where one tenant's activity adversely affecting the overall service performance of other tenants, and to mitigate this, SAS providers typically adopt a workload management and resource allocation strategy. And one of the common methods is to have a throttling mechanism to ensure that tenants are accessing or stays within their quota limits configured per tenant. Now what you see is an AWS reference architecture where You know, as I mentioned earlier, in terms of the resilience testing phases, we start with the steady state in terms of Amazon Cloudwatch to observe how the system is behaving, and then we move on to defining an hypothesis and then we go ahead and run the experiment, we come back and verify, and then we improve in terms of our learnings. So in terms of the the tenant interaction, so where we have two tenants for patent one, tenant 1 is a security ISV deals with threat data, and we have tenant 2 is an HR tech ISV deals with the employee award data, and both of these tenants performing their respective rack query against API gateway. And the API gateway has been used, configured with the usage plan to limit per tenant request in terms of 10,000 tokens and 500 input tokens and 500 output tokens with 100 requests per day. And the APA gateway is attached with the lambda authorizer which actually extracts the JWT token, understands the tenants specific token limits, and also validates the uh the role along with the dynamically generates the tenant's coped credentials which are passed towards uh lambda rack service and it also the lamb the the the uh sorry, the lambda authorizer also checks against the dynamo DB to ensure that the tenant stays within the limit. And once the response is received from the lambda authorizer, then the API gateway forwards the request towards lambda rack service, and its job is to invoke Amazon bedrock knowledge base to specifically retrieve the tenant context from Amazon Open Search vector store, which are already vectorized based on the data ingestion workload, which is not shown here. But the context is retrieved along with the user query. Both of those information is passed to Amazon Bedrock LLM to generate the response, and accordingly the response is sent back. So this covers the overall flow of, um, you know, the, the retrieval augmented generation. And on the top you see is an Amazon event bridge where. Every one minute, um, the Eventbridge rule invokes the Amazon lambda in terms of the token usage, and then its only job is to analyze the cloud watch logs, and then it updates the Dynamo DB table, which will be used by the lambda authorizer. Now what we want to do in terms of the hypothesis is to induce two kinds of faults. One is to actually disable the even bridge rule, and second fault is to actually delete the cloud watch lock streams. And after inducing this fault, we want to validate what will happen to our system behavior in terms of whether tenants can actually bypass those throttting limits or they stay within the limits to simulate or understand the noisy neighbor scenario. So let's quickly jump on to the AWS console. So I hope you are able to see my screen. So what you see on the screen is um the 4 accounts that we have set up, where as an administrator, I can fed into these accounts. Now, in terms of the court set up. Let me quickly connect. So in terms of the code setup, so you can see that we have a tenant, so this is a project for the rack. We have a CDK under which we have tenant template. Tenant template has multiple code repositories in terms of the Python modules. So you can see the Bedrock customs services have individual modules related to the Python. And then we also have multiple other TypeScript CDK, which actually performs various activities in terms of creating roles, deploying the AWS resources, and trying to bundle the overall Python modules into lambda and deploy that. And the correlation of these Python modules, you can see, for example, we have Bedrock custom, we have a Python module which is associated with a specific type cri CDK where you can see that The type script is invoking or reference towards the. Bedrock custom Python module with the specific Bedrock log Python with the handler and the handler with the specific lambda functions and you can also see that based on the TypeScript CDK it generates the log group along with the role ERN which is passed as an environmental variable. Towards the type script where the CDK creates the cloud formation script, it deploys the necessary resources, and it also uploads or packages the overall Python code into a lambda function. And it uploads into S3 and the cloud formation actually deploys as a lambda. So this is just one example for the bedrock related functionality in terms of the tenant token usage. Similarly, we have the services under which you can see the rack service. We have the core rack service in terms of the lambda, which has a similar functionality of Showcasing, uh, you know, like, let me go back to the respective type script here. So we have service.TS, which is again a type script specific to the rack service, and you can see here we have also referenced the authorizer lambda along with the specific combined attribute-based access control which we are using for the authorization strategy. And you can see that we have the authorization service referenced again with the specific Python module with the lambda handler in terms of the function and the necessary environment variables are passed, uh, which are again correlated with your TypeScript along with your Python module packages. So the way the project is set up where we are using infrastructure as a code along with the Python to bundle it, deploy it to ensure that any changes that we do as part of the SAS workload architecture are deployed in a simplified way and easy to maintain, and we can mitigate any sort of failures. Now what I've done is In terms of, so I have two terminals here, Terminal 1 and Terminal 2. Just to save some time, I've already executed a script here. So just to show you the, the tenant related information, so let me just go back and execute. So here, like, as I mentioned earlier, we have two tenants configured, tenant 1 and tenant 2. So tenant 1 have their respective registration ID along with the email address with the tenant config, uh, reference to input token, output token, API gateway. Similarly, we have tenant 2 with their specific registration ID, email address, um, and then the tenant-specific configuration as well. Now, to understand the steady-state behavior for this patent one, I've actually executed a script with an invoke API towards the API gateway with the specific uh input, the tenant name, and I'm trying to ask a question here. What are the threads related to gather host information? I'm sending 13 requests, and as you can see. Each of the requests I'm receiving HTTP status code uh 200 sounds great until request number 7 and then from request 8 onwards I've received HTTP status code 403, which is access denied. And this is because tenant one has already overshooted in terms of their token limits, um, considering the noisy neighbor scenario. And then we have tenant 2. Who are actually, I'm performing the same query, but in this case, the query is different because the tenant context is different. So who are the employees received award for the teamwork, and I'm sending 4 requests just to differentiate, and all of the requests have succeeded. Now this validates that, OK, now we have the throttling strategy in place and now with this, we want to just quickly jump onto the AWS console where I've configured a dashboard for pattern one where we have various absorbability metrics related to the AWS services to understand how um tenants are using our services. So we have a dynamo DB latency and throttling, we have dynamo DB token usage, table operation, bedrock innovation metrics, and all that. And we also have the respective authorization. Uh, in terms of, you know, the lambda attached to the API gateway, uh, to analyze, and I can just show you one of the error, why you have received 403, and the reason is because tenant token limit exceeded. Now, I want to go ahead and induce the file that I talked about. So, what you see is an AWS console where you have FAS service uh aligned with the resilience testing, and then we have resilience management related to resilience Hub. So, as I mentioned earlier, the first thing is you'll, you'll configure the experiment templates uh where we are already configured here, and you also have scenario libraries, experiments based on the various experiments that you run in terms of the overall status, and you have spotlights to talk about the various blocks and all that. So, once you understand the steady state, you come to this experiment template where I've configured noisy neighbor fault, disable event bridge rule. So let me click there and then start the experiment. So I've started the experiment. Now, what we are expecting is to do is FAS would induce those, both of the faults into our multi-tenant uh account, and you can see that we have this experiment ID along with the account targeting is a multi-account. And we have IAM role configured, currently it's in the running state, and you can export the experiment into reports as well. And we have two actions based on what I explained as the architectural difference. So one is to disturb the um event bridge rule using AWS SSMN command, which is a custom fault. And again, some of the AWS services have native integration of FAS where FAS can induce the faults into uh AWS services. But in this case, because it's a SAS workload, I, I have the flexibility to configure my own custom faults. So that is why I'm using AWS uh SS. Send send command, which eventually uses system manager documents. So document is a custom set of steps or scripts that you want to induce in your SAS workload architectures. So both of these actions are completed. Similarly, you have another action or activity. Uh, using again AWSS Simpson command where I want to delete the cloudwatch lock streams. And then you have the targets. So in this case, since you're using an SSM system, um, System Manager, you would eventually use an EC2 instance, which will use SSM agent to perform those actions. And then we have a target account where um we have deployed the Iraq solution. We have tax timeline and log events. Now this experiment is completed. Now with this, I want to ensure that you know uh in terms of the observability, let me go back and show you the alarm here. So let me refresh. So I have an alarm created as I induce the fault. And you can see that we have delete this has deleted one of the lock stream action, which ensures that we have fault induced. Now, can anyone guess that whether if I send the same request query again, would that succeed? Because already tenant one is overshoot on their limits. Any idea? Both injections are. Yeah, the falls have been induced. verdict. again. Exactly, so it's going to fail because the lambda authorize the, the um. The lambda that was attached to the event bridge rule has already captured by analyzing the cloud watch rock and it updated the dynamo DB. Now any request going in, the lambda authorizer would actually go back and check in the Dynamo DB and then say that, OK, now this, this tenant is already overshooted on the limits in spite of inducing the fault, right? So for today's demo. I'm going to reset the Dynamo DB table so that at least we'll see what will be the behavior based on the faults that we have induced. So what you, you see on the screen is a dynamo DB table related to the token usage. So I'm just going to go ahead and Reset this But in real world production system, you, you won't be able to do this and you don't need to do it, right, because this is an example of the throttling limit has already been set and we have a tracking mechanism using dynamo DB table because of which you won't be able to do it. So now I've reset the Dynamo DB table, now I go back and find the same set of experiments to see what happens. So I go back to the Visual Studio code, and I send the same request now. So here we are validating our, we actually have executed the hypothesis we run the experiment and now we are we're trying to verify. So I'm going to send the same request here, 13 requests for tenant one, and then similarly, I'm going to send this time I'm going to send 14 requests or 13 requests for tenant two just to see how both of these tenants perform in terms of the throttling limits. So this is going to take some time um to complete, probably a couple of minutes. Let me see if this request is proceeding to ensure that after we reset the Dynamo DB table, um, Yes, so we are receiving the response. So let it go through. So meanwhile, let me quickly jump on to The experiment templates to showcase you how this looks in the real world. So again, what I've showed you is an ABliss console version of visualizing how the experiment has behaved. You can also configure adjason and upload that into you can actually import it into FAS and then the experiment can be configured. So here I'm just going to quickly show you the experiment template how it looks. It's a similar version of the UI, but here it's pretty much clear that first you have to configure the action. In this case we have two actions configured with the action ID of SSM send command you can see that the document ERN. So this is super critical if you want to induce a custom fault. And this is a system manager document with the document ERN, and the duration is one minute and the target is our instance because it's a SSM send command. So whereas if you induce a lambda fault or maybe EKS pod actions, then you would see that the action ID would be respective AW services. So we have two actions configured and you can see that the second action has again a different ERN and then we have the description you can export into report. You can also do a log configuration and the targets are configured with the respective EC2 instances. You can see that both of these resource ERNs are associated with an EC2 instance in the same account. And now if I go back and show you the SSM documents as well, so this is how the SSM documents looks, right? So you have a schema version with the parameters passed as an input in terms of the log stream delete, so you are passing which lock stream to be deleted with the run command with the log group and set of, uh, you know, steps against the AWS services. Similarly, you have for the event bridge rule as well. Um, so this is how you will configure an experiment tied with the specific action commands, uh, using the System Manager. So we are just waiting for this script to complete. So meanwhile, let me quickly jump on to, sorry. So let's quickly jump on to pattern 2. So patent to about tenant isolation, right? So one of the critical principles of any multi-tenant solution. How do you ensure that your tenants are not able to access other tenants' data or the resources, right? So what kind of strategy you would use? Any volunteers? What is a common method of tenant isolation strategy? Sorry? Different accounts, that could be one strategy, but still, how do you control, you know, your, your principles are not able to bypass accessing other tenants' resources or even the data in terms of data isolation. Yeah, that's one strategy. Sorry? That's uh the, uh, yeah, that's more of to enforce certain policies that you want to uh bring in as part of your multi-account strategy. So, to have a, a straw bust tenant isolation mechanism. Customers typically adopt 3 authorization strategies. Number 1 is role-based access control, where every tenant will have their specified role. And that role will say that with specific AWS resources that they can use. For example, you know, you have an EC-2 with S3 and RDS, so you can have a role defined per tenant, and based on that particular authorization you can restrict those specific AWS resources. The second mechanism is to dynamically generate an IEM policy for each tenant. Based on IM identities, it could be IM user or role. The third common strategy is to use IAM role with attribute-based access control, um, where you define an attribute-based access control policy and attach that to a tag, and that tag will decide which specific IM principle in terms of the role can access which resource. Now for the rack solution, we have adopted the third strategy of using IEM role with attribute-based access control. So what you see is very similar to pattern one. The only difference or the point that I want to highlight is. Whenever a tenant sends their query, lambda authorizer dynamically, you know, dynamically generates an attribute-based access control policy. Our tenants scoped credentials as part of the request. And then it passes that to the API gateway, and the API gateway passes those credentials based on the principle in terms of the IEM role. Attached to that tag and that tag will be eventually passed on to the lambda, that is rack service, to ensure that when it invokes, for example, the knowledge base or any other resources, it will try to match the tag. If both of those principal tags and the resource tag matches, then the request would be authorized. If not, they would get access denied data, right? So this is the common strategy using attribute-based access control. Now in terms of the hypothesis, what we want to do is we want to induce a microservice bug. This could be a human error or any other, uh, you know, coding level issues could happen that someone mistakenly hard coded the, uh, knowledge base ID of tenant 2 into tenant 1, right, which may not happen, but anything could happen in terms of the failures. So if that's the case, whether tenant 1 can bypass and access the tenant 2 data is what something that we will validate. And Let me quickly go back and see. So what do you see on the screen for the pattern one? You can see that Compared to the previous iteration, all of those tenant requests have succeeded. So as we induce the fault, so tenant one can completely execute 13 requests, and tenant two can also proceed to execute 13 requests. So based on the fault that we've induced and after we reset the dynamo DB table, tenants can actually overshoot on their limits. Which is a sort of a violation from a throttling mechanism standpoint, right? So it means tenants can actually create more number of volume of requests which could actually jeopardize the overall SAS solution in terms of the account level or regional level limits. So some of the key learnings for patent one is that how do you enforce an IEM lease privilege mechanism to to restrict any of your IEM identities within your organization or accounts. Can go ahead and delete such, uh, you know, IM rules or logs or anything that would actually restrict your tenant behavior. The second strategy is. If such fall happens, how do you ensure that overall the legitimate tenants who are still within the limits because of the noisy neighbor of multiple tenants trying to create more number of requests would actually put other tenants to receive a throttling error because of your account level or regional level quotas that you have already pre-provisioned. So how do you mitigate that? So this is where you have to look at the broader scope of the tenant and the noisy neighbor scenario of mitigating the throttling mechanism. And last but not the least, is to have a more Notification and alerting strategy for the SAS provider to know if there are certain tenants who are actually bypassing those throttling limits and accordingly have some sort of automation or remediation to actually go back and increase those quota limits either at the account level or regional level. Now, in terms of the other patterns. So have pattern 3 for serverless applications. How many of you have adopted serverless patterns for your workloads? OK. So again, serverless is one of the um you know, modern architecture that most of the customers try to adopt because of, you know, reliability and high availability as we manage, uh, you know, in terms of infrastructure and you try to focus on your specific application related components. Now, one of the common challenges with the surveillance application is time modes, right, because you have so many, uh, in so many serverless architectural components where you would have a circular dependency between components and maybe upstream or downsea systems as well. So to overcome this challenge, how do you induce a purposeful fault within your serverless architecture to identify those system dependencies or the timeout scenarios? So one of the common patterns is where you could actually induce AWS uh lambda faults again using FAS, which is a native capability that we support, where you can induce the lambda falls in terms of adding a latency into your lambda and identify how the user behavior and the various other system components like API gateway error rates increasing or maybe the user error rates increasing where users have a bad experience. Now again, you can leverage a phase to do this, and the common pattern is again, no, you will adopt or go through the same phases of implementing it. And the fourth pattern is the EKS application high availability. So where EKS, um, you know, managing workloads within the EKS is a complex task, right, because of so many, uh, you know, dependencies within the EKS of configuring pods, node clusters, and various other, you know, internal dependencies of your, um, Qanities. So one of the common patterns where customer adopt affairs is to actually induce EKS pod actions, and we have so many pod actions that we support natively in terms of terminating your uh instances uh in terms of the node group servers and also inducing the CPU stress. IO stress, memory, and latencies. So all of those faults are natively supported as part of FIS where you can induce the CPU stress, for example, against multiple tenants where tenants could be isolated based on the Name spaces and in this case the tenants are trying to create a request towards tenant one and tenant 2 and both of those tenants where you can actually label the faults based on the application uh selector based on the application selector as your criteria you can induce the faults. And some of the dependencies are prerequisites for you to induce the falls. You have to configure Cubanity's service accounts. The Cubanity service account is a role-based mechanism where you provide certain permissions to FAS to actually create an FIS pod. Which will sit along with your existing um pods in your clusters to induce those falls to ensure that the EKS failures in terms of the pod actions can be mitigated. Now, I have another pattern about cross-region data replication on how to use uh S3 post replication, you know, if you're dealing with multi-regional architectures where especially the data injection workflow have complex steps, and if you're relying upon S3 to uh replicate the data using cross-region, then this could be one of the patterns that you can adopt to induce falls across multiple regions as well. If anyone is interested, please let me know to discuss. Now, in terms of the key takeaways, The first and fundamental element is to understand your workload architecture, right? So what are the system dependencies that you want to validate and are you interested to identify cross-regional failures or maybe are you trying to validate how one of the components would fail, will have a circular dependency on other components. So understanding your end to end workload architecture is super critical for resilience testing. And then once you understand your architecture, proceed towards identifying the key objective of resilience testing. So again, you know, if you don't know what specific hypothesis that you want to create in terms of resilience testing, it is going to be very difficult for you to justify what was the business outcome and how do you justify the outcomes in align with the improvement of your workload architecture. And that is where you define your hypothesis with a real consideration of the weaknesses and what validation that you want to do and then accordingly run the experiments. This is where you can use a phase experiments based on the various supports that we offered and then finally. Once you run the experiment, once you verify, the real benefit would be based on the outcomes. The outcomes is the learning that you have identified and then accordingly improvise your resiliency posture to ensure that you can withstand those failures or disruptions based on your testing. And then, as you mature towards your resilience testing phases, ensure that you would be able to integrate resilience testing as part of your CICD as a continuous iterative process. And again, you know, resilience testing is a continuous iterative process, just not a one-time activity, because your system would evolve, team would change, and you would have various disruptions to handle. So ensure that you perform the resilience testing in a continuous iterative process. Feel free to grab the QR code if you're really interested to deep dive on various other best practices and guidance from AWS. And again, if you're interested to have 1 to 1 demos or even opportunity to grab some of the swags, meet us at the kiosk in the AWS village. And last but not the least, if you really want to upskill your AWS knowledge, feel free to register on AWS Skill Builder, where you have thousands of on-demand trainings as well as hands-on experience as well.