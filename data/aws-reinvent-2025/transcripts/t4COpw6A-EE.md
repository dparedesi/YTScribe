---
video_id: t4COpw6A-EE
video_url: https://www.youtube.com/watch?v=t4COpw6A-EE
is_generated: False
is_translatable: True
---

Hello, everyone. Welcome to DAT 410. This is Post-Grec SQL Performance Rew workload tuning. Uh, let me ask you this. Have you ever received alarms at 3:00.m. in the morning due to degrade of database performance? It could be high CPU utilization or high query execution times, our plans are switching, then yeah, we're going to see, today we're going to see a few common performance challenges which most of us face and fix together. My name is Bai Sheikh, and I'm a senior database engineer for RDS and Aurora Post-Classical databases, and I have a co-speaker. Hi everyone, I'm Vlad Vlas Chana. I'm the tech leader for databases here at AWS. Um, I'm here to help Baji, so. If you have any questions, please raise your hands, and I'll come with the mic to you so you can ask the question so everybody can hear it and then we'll try to answer your questions to the best of our knowledge. Thanks, Lynn. So, let's start with it. Before starting the content, I just want to start a load generator. I have a slide to talk about this load generator, I just started the load generator. So, yeah, meet, meet Mr. John. He's a senior database engineer at any company. Uh, he's responsible for PostScris SQL databases in his company. He did a pretty good job in installing, setting up Posdtre SQL databases. He tuned it very well and he set up all monitoring, all the alarms. But eventually, data grows, he started seeing alarms at 3 a.m. in the morning. Due to query execution times are high. And CPU utilization is also high. Does that sound familiar? Then let's fix the post-reel performance issues of John together. Before that, let's look at the key areas to focus for performance tuning. I would start with CPU utilization. It could be due to suboptimal queries which are going to, uh, full table scans, which is high CPU intensive, and your application, application workload is undersized, for instance, uh, where you, you see high CPU utilization. And by default, Potre SQL uses parallel queries. So if you have a large number of parallel queries, you'll see more number of connections using CPU. So that's where you, you see high high CPU utilization. Next thing to look at is memory. Again, suboptimal queries, memory intensive queries, and then Postgra SQL uses process-based architecture. So each connection is a process which consumes some memory. So if you have a high number of connections, you'll see high memory utilization. And Postress SQL has some memory-related parameters based on which you control the memory. But if those parameters are overconfigured, that's where you see high memory utilization. Next thing to look at is stories and IOPs. So Posre SQL uses multi-version concurrency control mechanism. So every modification to the database will have two versions of Rs, old and new. The old version of Rose called as low, which will be cleaned up by maintenance activities like vacuum. Otherwise, you'll see high storage and IOPs utilization. And if you have more indexes which are unused or duplicate, every modification to the database will lead to update those indexes unnecessarily, which is where you see storage and I obstacleization. And Postress SQL uses work memory to control the query operations such as sorting. But if that memory is insufficient, that that's where it creates temporary files on disk, which is high IOS activity. And next is application pattern. So if you have a large number of queries which are blocking each other, that's where you see slow database performance, not due to query executions, but those queries are conflicting each other. But if you have long running transaction, are idle in transactions for a longer period of time, which will block the maintenance so that eventually the database will slow down. So if you have more number of ideal connections which are doing nothing, they still consume resources, which is where a connection puller would help to optimize the connections. So these are a few key areas to focus. Next we can look at query tuning methodology, which is a step by step process where you would Start with looking at active session summary in database insights, performance insights, or you can look at page stat activity view inside the database, and you can look at top sequels and top weight events consuming those resources, and you will generate, explain, analyze plan with buffers option where you can see shared buffers information as well. And investigate, investigate from that. So it's a step by step and iterative process. So you need to find out the top sequels and start fixing from there. Uh, but how does an explained analyze plan look like? So this is a simple plan. So every arrow mark represents the plan node, and the top row is the consolidation of all the plan nodes. So it has, if you observe first row and every plan node, there are two sections to it. So first part is estimations and second part is actual. So let's look at the number. So first number is estimated startup cost. And then, and then total total estimated cost, and then total estimated rows, and then, you know, uh, width of each average width of the rows. And the next section is actual where it starts with actual startup time in milliseconds, and this is total next is total time in milliseconds, and the total number of rows uh for that particular query. Every plan node has these two sections. So if you look at The second part of the, uh, each plan node, so that's the execution time of that particular plan node, and the consolidation is the top row. So the top row has the number of rows executed with that particular query. And next is the loops, number of loops. If you have complex query, you'll see more number of loops. So that's the kind of basic for explained analyze plan, but What problems to look at in explain and analyze plan. So in the demo, we have several problems to look at in detail, but at high level, we'll start with bad estimates. So if planner planner is dependent on statistics, if the statistics are not up to date, you'll see bad plans and execution times are high. And you can look at sequential scans, full table scans, which are CPU intensive, where an index can help. But if index is already there and a strategic index can help to improve the performance, and you can look at buffer rates with buffers option in explain analyze plan. And as I said, you need to look at each plan node and pick up the slow operation in that plan node and start investigation from there. Uh, this is an e-commerce. We have a simple e-commerce app, that's a load generator I've started with. This app has simple e-commerce tables like orders, uh, products, users, and we have the load generator. That's what I started. So it Opens the connection to the database, more than 1150 connections and it has balanced reads and rights, and it provides summary every 30 minutes on how many queries run. We can see that, see that in the demo. These are the 5 critical performance issues we are going to cover. So we'll cover in detail in demo, but at a high level. First one is how rewriting a query could help, and the second one is it's going for a different plan, but other plan can help. How do you switch between the plans and You 3rd 1 is you already have an index. How can a res index can help to improve the performance? And 4th 1 is heap only to pull updates. How can that help to improve the performance? And we have some lightweight locks we can see in the demo, and what are those locks and how can we fix those locks. Uh, this is the content. We can start with the demo. I'll just switch back to demo. Is this OK? It's feasible, or you want foreign sales to increase? Hello. So this is the load generator I've started. It has 8 queries that are running, and it gives you information about how many times each query run and the aggregation of the queries. For that particular thirty-second interval, as we fix the queries, we can see improvement in these, uh, number of queries. Um, and switching back to the console. This is the instance you can go to performance insights. Of that instance, let's select the last 10 minutes, and you can see different weight events with CPU being the most top weight event, you can see more than 60, 70 sessions are consuming CPU, but we have a lot of weight events. I'll go to Database Insights, which is an extended version of Performance Insights, and continue monitoring from there. And if you select last, maybe 10 minutes. You'll see the same. Uh, weight events where CPU being the most. And I'm switching back to terminal. And let's connect to database, and I was talking about PGSTA activity as well, you can just. Uh, query, PG stat activity view, uh, substring of query for maybe 20 characters from PG stat activity view. Where that name is, database name is e-commerce. So you can see all connections to the database, more than 150 connections. So these all are running against a database, and if you see all our inserts, uh, selects, updates, all sorts of queries. So it has balanced read and writes to the database. So let's go back to the console and select the CPU weight event, which is being the most. If I scroll down, you'll see the top queries that are consuming the CPU. So, today we're going to fix top 5 queries one by one. Uh, another thing to monitor is if you go to database telemetry section, you have individual metrics on CPU, memory, IO, and all sorts of monitoring. If you Weave metrics and cloud watch. You can create your own dashboards to monitor these metrics. I have a simple dashboard created with uh metrics like CPU, read ops, right ops, all those basic metrics. We have commit latency and all those. And if you see, The CPU The last 15 minutes. Which is touching 100%. So now, let's go back to database insights and pick up those top queries and fix one by one. I'll just select last 10 minutes, and then the CPU weight event. If you scroll down, you can see the query. So this is a select function call which is consuming, uh, more CPU resources. If I check that query, this is the query that's running against this database. So, let's see the DDL of this query, uh, this function. And see what's inside it. So it, it has only a single select query with a predicate to a function call. So if you execute this function, So sale movement type is coming to this uh query and the overall function gets the count of all, uh, sales movement type. Uh, but if you execute, explained, analysis on this query, it's gonna take time. So a few minutes back, I have executed, explained analysis on this function just to save some time it takes more than one minute. Uh, so if you explain, analyze buffers, uh, for the function call, you don't see much information on explain analyze directly on the function. Uh, for this, you can use PL profiler extension, which gives you the internal query details of the function, or you can check the function and manually execute the, explain, analyze for the queries inside it. So, as we have only one query, I just did explain, analyze on that query. So we're gonna spend uh a couple of minutes to, uh, deeply look at this, explain, analyze plan, and then for other queries, we can just spend less time. So this is the explained analyze plan. The arrow mark that I talked about, this is the plan node, one plan node, as we have only one query, uh, one table, one query, single. It's a straight query. That's why we have one plan node, and the top line is the aggregation of all the plan nodes. So now, uh, first thing to do is which plan node is slow. So as I have only one plan node, if I look at the plan node. The cost estimated cost is over 3 million and rows are nearly 2 million, and actual total time is around 60 seconds, and the rows returned by this plan mode is 1 million, only 1 million. So now, another couple of things to look at is rows removed by filter are 10 million rows. But if you observe this plan node is returning only 1 million rows, but planner thinks. 10 million rows should be removed from what it has done. This is because if you look at the number of rows in the same table, the 11 million rows, and if you group the movement type for sale, this is 1 million. So this sale movement type is what it's writtening here. So, and if you exclude that sale, that's 10 million rows, which is where it's removed by the filter. So now what's happening is for each row this function call gets executed. That's why it gets executed for all 11 million rows. And then planner realizes that sale movement type is only 1 million row, so I need to remove all the rows that I. that work done for 10 million rows, it should be removed. So if you observe this is unnecessary work for a planner to go to 11 million rows to execute that function. And if you look at shared hit, these are, these are in blocks. So 87,008 kg blocks are fetched from the memory. So that's high if you multiply 87 by 8 kB, it's around 800 megabytes or something, so that much data has been read. Uh Now, if you look at this planned node, it's going for a sequential scan, which is a full table scan. But if I look at this table DDL, With backslash the option, you can see. Well, I'll connect back. Once again and then Look at the DDL for this table. This is the movement type index. So there is an index on the movement type column, but it goes to a sequential scan for this query. This is because planner doesn't know the value that's coming from this. If you execute this function with a select call, then it gives you sale, but planner doesn't know what value it's coming there. So that's why for each row it's it's executing that function. So to make the optimizer aware of that sale movement type, if I change the query. To maybe I'll take this query. From here And explain, analyze buffers. And if I use select call for that function, if I go back to the function name, So, if I add a select call to that function and see what happens, it should pick up that index because it knows that sale comes to that filter, uh, filter column. So, first time it's gonna take time because if you see, uh, it's going for index scan, that's what we wanted. So now select call gives index hint that you should go for index scan and If you observe IO timings, this is for the first time index has to be loaded into memory. That's why you see IO timings there. But if I run it again, as it's already in memory, it took 10 seconds earlier and now it took 800 milliseconds, but initially it was nearly 1 minute. So we brought down the execution time from 1 minute to 800 milliseconds. Not only that, you don't see that rows removed by filter here. So planner exactly goes to the sail movement type rows here. And if you see the shade buffer hits, it was 87,0008 KB blocks. Now it's just 2,0008 KB blocks, which is very, very less work for a planner. That's why execution time is slow. But if you look at all of the metrics. The cost was 3 million earlier. It's 4 43,000, so everything, uh, came down. So, to fix this query, we just need to Rewrite this query to use the Select call. I'll just do that. I'm just modifying that function. To use that select call. And now if you see the definition, detail of that function, you can see that select call has been added. So every connection that comes after this modification should go to index scan. If I go back to my load generator, so function calls for every 30 seconds are only just 9 queries. If you keep monitoring, if you wait for the next summary to be displayed, then you can see the improvement in that function calls as well. So now it improved to 12 queries. If you wait for a few minutes, you'll see the improvement, actual improvement. Uh, another thing, I'm switching back to console to look at other stats. If I go to, uh, dashboard, if I scroll down to, maybe I'll pick up commit latency. We win cock blood watch matrix. If I select for the last 15 minutes. So it's slowly coming down. It was around 23 and now it's coming to 21. So commit latency for those function calls has been reduced and it gives more resources to other queries as well. So you can see little improvement of the number of queries for other queries as well, because resources are consumed by this function call are less now. So, and we can see other metrics like queries finished. Maybe I'll go for this metric. If you go to cloud watch. And deselect all metrics and select only queries finished. So this should also get improved over the time. So you can see it's improving the number of queries. If you see the total query time, This should get reduced. So it started with 5 million and now it's at 525 because other queries are also running, it created resources for other queries. So, but it eventually, uh, comes down. So that's how we can rewrite the queries to improve the performance. This is an example to rewrite the query. So if you're using function calls to predicates, make sure you use select call, otherwise optimizer will get confused and it Takes a sequential full table scan instead of index scans. If I go back to database insights, oh, so Is there any reason you would, you would want to, like, why would we not always do this? Or is this just like, why would you ever not prefix it with a select? Uh, to call a function directly. So if I understand the question, so why would we need to add that select call? Why would you ever not do that? Like I didn't even know this was a thing and like I'm thinking about all the queries I've written and like, man, I should, I should be doing this everywhere. Is there any reason we wouldn't wanna do this? Yeah. Let me just to paraphrase, she's asking why the, the database engine would not automatically do that. Going that white parking line and it's like a yeah, understood. Yes, yeah. Thanks, Cla. Uh, so yeah, why database engine itself, uh, picks up that as a select call? That's the question, right? Yeah, it's, it's, it's a kind of, so, but yeah, that's how optimizer works. It only looks at uh the function call. If it doesn't have, it has to execute that call and get the value for that filter type. But if you have that select call, it already execute that call uh first because it's a subquery now. It's not the actual filter, it's a subquery. So, it executes the subquery first and then gives the value to the further rows. Thank you. Uh, and if we observe the next queries, I'll fix this third query first. Any questions? Yeah, yeah, sorry, I had a, I had a few questions on this example too, just before you get too, too much into this. Um, did earlier when you were running those explained plans, did you, um. I, I thought the first time you ran it, you said that you weren't getting very much, uh, very many details. Did you run a second one before you made that, tested that fix? Uh, this is because of the IO timings. I mean, the first time it has to load the index into memory, so all the extra lines are related to IO, and he fetches this IO timing line. So that's where it's. Did you have to add uh a um parameter or something? Did they explain? To get, to have it come out with more. Details, or is this just the regular explained? It's just a regular. It's just it feels like it didn't happen and the everything. to function in a way. To. Oh, OK, that was the difference. OK, thank you. That, that was what I was asking, uh, that's, that's what I wanted to know. Thanks. And then 11 other thing, all, all of these like, uh, post grass like command line things you're doing, uh, such as the explained plan or like the, the, the, the slash D, uh, function, uh, are those something that you could or would want to do in the AWS console? Not in the AWS console, but any client tool like PGAdmin or other client tools. You can use those client tools to see the details of the, uh, tables and what indexes, what functions, everything can be done through client tools. Just one more, uh, just a very quick follow up. Is there a significant difference between what things you've shown here and if you're doing an Aurora postgrad sequel? Oh, this is Aurora postgrad sequel. OK, thank you. I'm so sorry I haven't mentioned that. If you go back to the console, this is my Aurora instance that I started looking at. But they explained as a standard for that so you can use it for any. Post Chris flavor database. Yeah, yeah, there is nothing special if it's RDS Aurora community post Chris. So, next query that I want to pick is the 3rd 1. If you look at the query, it's getting average price for the products. For a certain category, only for the active products. If I select this query and generate and explain, analyze plan, Perfect. This goes to index scan, uh Bitmap index scan. The difference between normal index scan and Bitmap index scan is for index scan, it takes one row and get fetched from the index file. So for one row, one iteration, but Bitmap index scan, it takes a group of rows and it goes to fetch all the rows at once, so it reduces the number of iterations to the index files. So that's why Bitmap index is more. Efficient than normal index in some cases. Uh, so, perfect. It's going for a bitmap index scan, it's going for an index scan, but the two, things that we looked at the last query, we are again looking at here. So you can see 30,000 rows were removed by this filter and 13,0008 KB blocks are fetched. So if I look at the query, Uh, it's getting average price for the certain category, which is active. Only for the active products, it's getting the information. But if I, uh, look at only active inactive products from the products table, It's only 33% of the products are active. So if I know that I always select the active products, And not the inactive products, then I can clearly create a partial index on the same category column with only for active products. Let's try to create that index. I'm using concurrently option because all the queries are running in parallel. Uh, so IDX for category and only active on products table on category column. Where is active equal to true? So this is the partial index I want to create. So let's create this index and execute the explained plan and see the difference. So now, it goes for Bitmap index scan for that new index, and if you observe these two metrics, and rows remote filters are only 900 now, where they were 30,000. And the execution time is 360 milliseconds and 23 seconds, 23 milliseconds now. And he, uh, buffers are only 10,000, there were 13,000. So, some less work to do. So now, as we have created the index, I'll just go back to the load generator. And if you observe, uh, The function calls were 9, 12 before we fixed the first query, and now eventually you can see 418 queries are running for that function calls. So it's a lot of improvement due to that index scan for the first query. Now, let's look at the status index. The number of queries are 1200, 1200. And now, after, after creating that index, you can see 1200 to 2000, almost 100% improvement. So, aggregation of the queries are 20,000 now after we fixed the couple of queries, but initially when we started, it was 15,000, so we have 5000 more queries running for the same workload. Same instance, I haven't changed anything at instance level. Just fixing the queries improved 15,000 to 20,000. So now let's go back here. And go to the next quarry to fix. So, can we not use a combined index and of getting index on every single column? Can we not have a cavity and atmospheres of the index? Would that not have helped? Instead of using like active, right, and you have a combined multi-column index, OK, if I understand the question, so why can't we have a composite index on category and is active because we can't be creating like there so many indexes for different statuses, right? That's right. But again, if you create a composite index, is active has inactive products as well. So we want to filter only active products specifically because I know that I always search for active products. So if you go for a composite index, it still has a large number of rows removed by filter because that category have any number of inactive products as well. So a composite index might not be the right choice here. A partial index would work well. So that's why we created specifically on the active products. Any other questions? Oh, hi. Couple of questions, right? So, first one, we are thought for long that the index scanning happens from the leftmost column on the predicate. Right now, we are picking something in the middle. Is that something we're supposed to benchmark before we roll out? Uh, this happens because of the data grows as well, so. Let's say you only have active products uh for 90% of the products table initially, and your category column index works well. Now, eventually data grows. Some of your products become active and inactive. Now, inactive amount has 66% of the table, so that's where you see. But if you benchmark initially, you would have gotten the best performance. Got it, got it. So the other question, uh, you know, building on top of the question the gentleman raised over there. The more indexes we create, I mean, so that essentially indicates that we need to periodically monitor, observe, and optimize. I think that's probably the whole plan, because otherwise what happens is you end up creating in thesesis on every single one of those columns, right, if you plan it ahead of time, which you cannot, so. No, I understand. Yeah, yeah. So the more indexes, you'll get less performance for modifications to the table, right? So yeah, yeah, if you, you are, you are creating that partial index, you can keep observing the first index of category, and that is not being used other than this index. You'll keep monitoring the index scans for that particular table, that particular index. And if you observe that that index is not being used, you can get rid of that index. We have uh Uh, we have how to get those annual indexes as well, uh, going forward, but yeah, that's a good question. So yeah, we can create index and and single columns. Thank you. Thanks. And maybe I missed it. Uh, when, when you, that last example, what was the indication that there was something wrong with that, uh, with, with that query, and then how did you know that an index would fix that? Oh, because I looked at, uh, the inactive products, active products inside the, uh, table, so I know that only 30% of my table is active. What was the what was the initial, um, like red flag that this was this, this, these two. So rows removed by filter are high. So it actually planner worked on inactive products as well. And then it realizes and removed all those inactive rows. So you want a low number of rows removed by filter because it's doing more work. Like that's every time, correct. That's a hint there, yeah. Oh, thank you. Thank you. So, let's go to this query, uh. If I select this query, if you observe this query, I'm not sure if it's visible. I'll just paste it here with an explain, analyze plan. But first and then credit. And this goes to index scan. So let's observe the query. So it has a hint, like we use in Oracle. This can be done through the extension called PG hint plan. I already created a PG hint plan, and at the initial days of my application, I provided hint to go to this particular index because that index performs well for my query. Uh, so now, as I hinted for this particular index, now you can see index scan using that particular index. So, all good. But again, the red flags, same rows removed by filter are 400,000 and the buffers fetched are 400,0008 KB blocks. Uh. But why, why, uh, Optimizer went to that bad index because you hinted. Uh, if you observe the predicates, you have created it in total amount, and this is where a composite index can help. So, but if you look at the detail of the table, orders table, I already have that composite index. So if you see this index, I already have that index, but it still goes to the single column index and created a column because I hinted for that. The simple solution is to Just remove. That hint, so optimizer is smart enough to pick the right index. So let's remove the hint and see what happens. So it should pick up that composite index. Great. So it went to create that amount composite index, and if you look at rows removed, there is no rows removed by filter, so it exactly knows how many rows to look at. And shade buffer hits are only 37,000 from 400,000, so it is a significant reduce. But yeah, now we know the solution. The simple solution is to remove that hint, and it works well, but it needs an application change. That query will be at different places and removing that hint is not a short term solution. Maybe you can plan for a long term solution to remove it. But as a short-term solution, how do you make this query to use the different index without changing the query? So this is where query plan management extension from Aurora helps. So using this extension, you can capture the right plan and switch the plans between the queries. So this can be done using APG plan management extension. So if I create this extension, it will give you, uh, DBA plans table where it capture all the plans that hit the database. So if I create this, Plan and If I run That's query. I'll just drop the extension and recreate it. Yeah, I have a question for you about the extensions. So right now the Aurora product team has not guaranteed that extensions will be, uh, forward compatible. So is that gonna change on the roadmap? So for example, if we begin to rely on this extension and then as the product moves forward and it's no longer supported, it could put us in a little bit of a difficult situation, so. Are you guys endorsing that or how are you helping work with the product team to make sure that any extension that we pick will be forward compatible? I'm sorry, do you want to take that question? I just want to restart my. Yeah, I mean. Um, honestly, we are working with, uh, with the service team to make sure that some of these capabilities exist and they become more compatible, but we're not yet at a point where we can just guarantee it yet, so, um, I think you generally can expect that it's just we can't make that as a guarantee just yet. In time, I think you know as. More of these extensions get, uh, let's say a level of maturity that we're comfortable with we we would be able to make that type of a guarantee, but right now we're not quite there yet. But still, it's a tool, it's a release, it's supported right now by AWS so if you need to use it, by all means go ahead and do that. I had to reboot the instance just because I created an extension and it dropped some time back, so that needs a restart of the database. So I just restarted it. Yeah, go ahead. Is this only available in Aurora or RDS also extension? This is specifically for Aurora, OK, yeah. So now, if you look at DBA plans table, it captured plans for all the queries that are running. So all inserts, updates, selects, everything. So if I only specifically, uh, query for Our query that we are working on. which has created it. Is rebooting things. Yeah, yeah. If you create and drop, uh, extension, it needs a reboot. Uh, yes, this plan has been captured here, but just for, uh, you know, visibility, uh, clear visibility, I have a query. That just use JSON function to bring the index that is being used for that particular query, so it just It's, it's on DPA plans, but it's a simple query. If I execute this query, You can see that this query's plan has been approved in status, and this is the plan hash and this is the SQL hash. So, plan for this query is already captured. That goes to create an index. Now, let's remove that hint and run the query to capture the plan again. So now if we run this query. In the note, it says an approved plan was taken because that has, but instead of minimum cost plan. So this query plan has been captured already, but Adora knows that there is an approved plan. I should go for that. So it still went to created an index plan, the old plan, but it captured the new plan. So what I need to do is. To look at this DBA plan's table, and there is another plan which goes to created at amount table, the composite index, uh, uh, go to composite index, and this is unapproved at So if I unapprove this, reject this, approve this, it should go to the new plan. So that's how we can switch back the plans. Uh, before that, we have Aurora stat plans function which will show you uh plans being used for the live queries. So if you are running queries and you are confused which plan is taking, you can use Auroraat plan function to see which plan exactly it's taking. So, I'm just going for my queries tab, and This is a simple query on Aurora starts plan function with filtering that particular query. If I execute this. So, index can hint query. This goes for orders, uh, created at single column index still because I haven't approved the new plan. If you look at the number of calls, 1800, if I run the same query again, it is 2000 now. This is because These queries are coming in and it's taking the old plan still, so that's why you see the number of calls being increased for this particular plan. So now, how do you switch these plans? So Other plan management comes with a function called set plan status. So using this set plan status function, you can reject or approve a particular plan. You just needs SQL hash and plan as inputs. So I already built two statements on this function with SQL hash of new and old plans. If I just execute. To approve the new plan. Sorry to switch back between the windows, but if I Approve and reject plans, and if I go back to this, now, the plan with composite index created at an amount is approved now and old plan is rejected. Now the assumption is it should take the new plan with composite index. How do you check that? So again, you can execute Aurora Star Plan's view for that particular query, and now if you see there are two rows. This is for old index created at, there are 2,493 calls. With the new index, Uh, composite index, there are 1200 calls now. If I run again. So this is constant because it's not taking that old plan now. This is, this will keep increasing. So now we fixed that query. If we go back here. Planned instability queries are around 255, 260. So now the plan's stability queries are 800, from 255 to 800, and the total number of queries increased to 24,000, and previously they were 16,000 and around 20,000 it hit 20,000 with two queries fixed. And now the queries count increased by 24,000. So we looked at how to rewrite the queries. The first one. Second is we have the index, but we can create a strategic index, partial index to improve the performance. Third is you have the right index already created, but how do you switch back between the plans to use the right index? Uh, next thing to look at is, if I go back to Claudia's metrics. The update query is consuming more resources, so if you look at this update query, Update products set on notes column. So if I look at the TDL of this products table, And indexes for that table, there is no index on notes column, if you see. So now PostPress has a feature called heap only triples. If you are updating in non-indexed columns within a data block, if there is some space inside it, then instead of updating the index, because this is a non-indexed column, updating, so instead of updating the indexes, it will just create pointers inside that data block. So it doesn't need to update the indexes. So that iteration of that work has been reduced. So this is called a heap-only tuples. But you need empty space inside the data block. So this is defined by fill factor in Postgress SQL. So if you look at the fill factor for this table, This is 100%, so I'm saying. Right to the entire block, don't leave any space. So that's the meaning of hill factor 100%. Now heart to pulse needs some space. Either those are created by a vacuum for the empty spaces, or you specifically need that percentage of. Uh, need free. Yeah, yeah, sorry, I have a quick question. So is the query plan is gonna be the same for the writer and reader instance in the same cluster? If it's not the same, like where do you see it's gonna be different to be optimized for each of them? Um, sorry, did you understand the question a lot? So what you're showing there on the screen with the query plan, the the question is if you have a cluster of multiple readers or writers and readers. Do you need to make that change only once, or do you have to make that change on every one of those instances? Oh, it has to make the change on the writer instance, so reader instance will be updated. So it's propagating from the writer to make that change there. They'll propagate to the plan changes. It's gonna be Read-only workload. Uh, are there cases when, you know, they have to, where you can't, you need to have them being different between readers and writers? I am not aware of any cases. I haven't seen any, but. If it's the same query, then it has to be the same index. Because that gives you better performance. Yeah, I mean, the results of a query are going to be deterministic no matter where you're running it. Subject, obviously your replication lack. But if there is a slight change in query, then SQL hash will change, and then Right, that will not work for the query, yeah. Yeah. Yeah, uh, sorry, quick question. So what you mentioned is what query changes optimizations we do on the writer instance is automatically propagated to the reader instance, right, right, but it's not bidirectional, right? So what I optimize on reader instance will not because oftentimes we run into reader senses. You know that a lot of reeds going to the downstream systems like data arrows or whatnot, right? So re instance is always resource intense in nature, so we end up optimizing a lot on the rear instance. What kind of optimization and specifically for reader instances? What about query plans, right? So similar stuff we did on writer instance, right, but it's not those queries basically hitting the reader instance. Yeah, that's the same question. It's not bidirectional in nature, right? So. But if you want a different plan on the reader instance, yeah, it's the same question. So yeah, so it has to be if it's the same query, it has to be the same plan, right? So it is, I'm not aware of the cases for the same query. I need two different plans. One will work on writer and one will work on reader. Uh, it is, I'm not aware of those cases. Yeah. All right, thank you. I mean structurally the data is physically laid out in storage the same way it's the same storage so there's no reason you would have the same query needing different performance optimizations on different instances. The question is that you might only run that query on the reader instances because it's a. It's feeding downstream systems and it doesn't run on the rider, but that's not gonna impact the performance of the rider. But the optimization is still valuable intrinsically because it's again that query is deterministic in its response. It's always going to produce the same response given the same input data. OK. Coming back to the hot updates. So I need fill factor to be reduced to maintain some free space in each block for those index pointers. So that can be done by changing the fill factor for that particular table, uh, using this command, set fill factor to 80%. Now, I changed, uh, fill factor, but that's not, uh, immediate effect because my all blocks are filled now. I need to reorganize the blocks to leave that empty space for every block. That can be done by vacuum full. Uh, vacuumful is acquires access exclusive logs, so I need to, uh, get rid of the connections to improve the vacuum full time. So I'll just Kill the connection and load generator is smart enough to restart when you kill the query, so. PGN backend using PGST activity where database name is e-commerce and PAD not in the PAD that I'm currently working on. If that PAD gets killed, then you cannot move forward with the session. So I'll execute, vacuum full analyze um products table. So it's gonna take a few seconds to execute that, uh, vacuum fill, maybe 5 or 6 seconds. So after that vacuum fill, now we will have space for 20% space for those hot updates. If I Go back to the load generator. Let's check if it's changed. And the table DDL now fill factor is 80%. Uh, you can check whether updates are going for hot or not using PGSat user tables. Uh, there is a column in PG Stat. Use tables. N hard update. So this column shows how many hard updates for that particular table. Let's get the percentage. I'm just going to my query. Uh, this is hot updates. So I'm selecting update, how many updates, how many hard updates, and the percentage for that particular table. If I execute this simple select here, Then there are 70% triples going to hot updates. If you keep observing, then you'll see performance, the ratio will get increased, so 72, 72.1, and it will keep increasing. So now there are more heart updates to the table. If you go back to load generator, And hot updates are here. There are 2300. Previously also they were. 3800, but we restarted the instance, so it takes time to pick up. So we will eventually see the more number of queries for hot updates. Uh, so that's where how updates help. If you have updates on non-indexed column, it can create some free space on each data block to have those pointer instead of updating the indexes. Now, you can see 3900, almost 4000 quarries. Previously they were around 2 to 3000, so a lot of improvement. Uh, if I go back to this, and the 5th query we have is this function call. If I select this, uh, and this, And this is my function, so. We originally started by saying that there's no index on that column, so it was going to basically update the space in the table and then we said the way to improve it is to reduce fill factor from 100 to 80. What is better long term? Would an index on that column or reduce the fill factor? It's not because there is no index on the column. It's because how Post, Postgrace needs to update all the indexes for those DMLs irrespective if you're updating non-index column or not, but those indexes should be updated. Imagine if I don't have index, I don't need index on that column, but still, I am making progress to update all those indexes unnecessarily. So, but those indexes should be aware of these updates. So how can we do that? So that's where this feature helps. Instead of updating indexes, create pointers, you can at least reduce the time of updating. So those pointers should be available in the same block. It can't be in the new block. So that's why we need space in that block to update those pointers. Thank you. Another disadvantage is if you have that free space in that block, your inserts need more blocks. So asking your operating system to give you a block to update the data is again overhead. So if you have more updates on the table, less inserts, then you can go for that factor to leave that space. Otherwise, your inserts need more and more blocks because you're leaving that space in each block. So if you look at this function definition, This is a select on order items column. This is a simple select, but if you look at order items column table, this is a partition table with a daily partition. It's an each day partition. You can see 731 partitions. So now, We know that if we have a weight class on partition key, it goes to partition pruning, but before that, optimism needs to get a lock on each partition first, and then it prunes the exact partition for the data. But still, it needs to lock all the partitions. Now, Postgrace by default allows in postgrace, there is a non-fast path and fast path locking. Fast path locking reduces the overhead of acquiring the locks. So by default, only 16 fast path locks can be acquired for each query. So now if you have a partition table with 700 partitions, so 700 different independent tables, so now it has to acquire locks, 700 locks on that table. So, obviously, it goes to non-fast path locking, which is some overhead. So that's why you see, particularly lock manager. Loch Manager events for this query. So that lock manager event occurs when your query has more number of non-fast path locking. So, just to check, if I begin and execute this function. Select star from Function and let's check the number of Row a number of logs for that particular, I'll just reconnect to the database. Select count of locks from PG locks. So it acquired around 13,000 1300 logs, but if If I Remove the. Fast path locks, there are still 800 non-fast path locks. This is because it acquired a lock on each partition. But I have a new table. For this other items, order items underscore new, which comes with monthly partition, not daily partition. This has the same amount of data as partition this order items partition table, but it has less number of partitions. If I update this function, just to check, update this function to use the partition table, which has less number of partitions instead. And if you select this function in a transaction, if I go back here. And check the non-fast path logs, those are significantly reduced. So all those difference of locks overhead has been reduced. And also if you keep observing, you don't see those lock manager events or see very less amount of lock manager events here. So this will this will affect the query performance because of that locking overhead. We've fixed. 5 queries and one last thing I want to show is about indexes. So if you have unused or duplicate indexes, unnecessarily, you know, database needs to update or insert data into those indexes, which is, again, heavy work. Uh This is, this is a query to check unused indexes. This is not a query built by me. This, you can find it in Wiki. So if I execute this query to find unused indexes in my database, I'll get rid of this transaction. And you can see a lot of unused indexes are there and duplicate indexes as well. Sometimes in development instances, we keep checking indexes usage and we create more and more indexes, and sometimes we forget to drop the duplicate indexes. We create multiple indexes and we forget. So that's where the duplicate indexes comes up. If you check the duplicate indexes. There are 14 duplicate indexes. Not only that, these indexes are around 5 to 6 gigabytes in size. It's unnecessary storage as well. Uh Now, I can drop those indexes and see the performance. I'm getting rid of the connections because dropping indexes will take some time with that connections, but if I execute. This drop index command, it gives me a set of drop index commands. For this as well. For duplicate indexes as well. There are 14 duplicate indexes. Now, if I go back, To This query where I'm terminating all the connections. And now run the duplicate command. You just need to execute, Z-execute so that all those drop commands will get executed. And same for Duplicate indexes as well. You just need to execute, execute. So now all those indexes were dropped, all unused and duplicate indexes. We can keep observing. Our load generator has restarted all the queries, so we can keep observing these queries and you can see the improvement. So, we started with somewhere around 14,000, 12,000 queries, and after fixing our 5 queries, we are at 24,000 queries. So, more than 100% improvement, uh, with this tunic. So as it keeps, it takes some time to uh generate the report, but meanwhile, if you have any questions, I'm happy to take. done. Hi, um, I just wanted to confirm when you did the, the hot updates you made that change. You, you didn't change the code in any way, correct? It was just because you've no, no, had some spare space. I just changed, uh, tables fill factor. OK, thanks. The Second question, are you aware of any, uh, development underway for post-gress or for Aurora Pogress? To Consolidate query plans. For example, if you have a, a, a query and it's got a certain comment, but the rest of the text is identical, it produces a different hash or if you have spaces or carriage returns, same query, same number of parameters, but spacing's a little different. It's a different query hash. Are you aware of any efforts to consolidate those queries that are actually the same query functionally in into one hash or have some kind of mapping? Honestly, I'm not, uh, but if you have hints, it will remove the hints to generate the scale hash for that particular query, or if you have explained whatever before that, select, it removes those hints to generate a scale hash. But if you have spaces or enters, then, you know, it's a different SQL hash. But I'm not aware of anything that's being developed to uh enhance that. Oh, we can capture this as a feature request for the service team. See If they can work on that. I So the I mean, the entire extension is available only for our it's our extension, so we should be able to do it. It's just a matter of prioritizing the work based on relative to customer needs. So we can just, we can definitely. Um, let the service team know about this request. You would, or, you know, now that we know about it, we can put in this feature request for the service team. All right. All right, thanks everyone thanks Baji that was wonderful. I thank thank you thanks for joining us.