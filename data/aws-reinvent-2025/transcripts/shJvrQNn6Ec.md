---
video_id: shJvrQNn6Ec
video_url: https://www.youtube.com/watch?v=shJvrQNn6Ec
is_generated: False
is_translatable: True
summary: |
  Steph Gooch and Andy Brown deliver a live FinOps coding session focused on getting more value and control out of the AWS Cost and Usage Report (CUR) 2.0. After a short quiz covering CUR basics (use the 2.0 spec, data lives in S3 for Athena, cost allocation tags are opt-in, and account names can be added), they frame the hour around three themes: optimization, cost allocation, and securing access.
  Andy investigates an anomaly email about rising NAT Gateway spend. Using Athena on CUR 2.0, he pivots September versus October costs by resource to identify new NAT gateways and inter-AZ data transfer. Because CUR alone canâ€™t show traffic paths, he layers in VPC Flow Logs (deployed to S3 with a provided CloudFormation template) and queries them in Athena, converting bytes to gigabytes and mapping IPs with Amazon Q to see which instances and subnets talk to which NATs. The analysis reveals a public subnet routing through a NAT Gateway in another AZ, driving both unnecessary NAT processing and regional data-transfer charges. By multiplying gigabytes by published rates inside the query, he pinpoints the cost impact and the misconfiguration root cause, showing how blending CUR with another dataset surfaces actionable optimizations.
  Steph then demonstrates building a business-facing KPI for an AI-backed web app (API Gateway, Lambda, Bedrock, and caching). By tagging the workload and combining CUR line items with application usage counts, she calculates total invocations and cost per click. She compares months before and after optimizations such as switching to Graviton, right-sizing, and selecting cheaper model options, showing per-click costs drop to roughly 25% of the original. The takeaway: capture meaningful usage metrics (from CUR or CloudWatch), tag services, and report in units the business understands so optimization work is visible and fundable.
  Finally, they show how to share CUR data securely with application teams via AWS Lake Formation row-level security without breaking existing dashboards. Steps include registering the CUR S3 bucket in Lake Formation (hybrid mode), creating data filters that expose only selected columns (e.g., unblended cost) and rows (accounts matching an app-team naming pattern), and granting access to a dedicated IAM role. They create an Athena workgroup that uses Athena-managed query-result storage to prevent leakage through shared S3 buckets, and attach an IAM policy restricting the role to that workgroup and the filtered CUR table. In the demo, the app-team role can query only its two accounts and only permitted cost columns.
  They close by urging teams to enrich CUR with other telemetry (VPC Flow Logs, CloudWatch, S3 inventories), enforce targeted tagging where it adds business context, and enable Lake Formation-based row-level security so more teams can self-serve cost insights safely. All queries and examples are provided via their linked GitHub resources for replay.
keywords: AWS CUR 2.0, Athena, VPC Flow Logs, Lake Formation, FinOps
---

The AWS cost and usage report, one of the most powerful datasets we have to understand our cloud footprint. It was actually released 11 years ago, and can you believe what it was like before that existed? And we've come so far. We have Athena integration, we have CR 2.0, we have Focus, we have the Kirk query library. We have grown so much since then. However, some of the challenges customers have still are the same from 11 years ago. That is why we've broken down our session into 3 topics. Optimization. Who doesn't want to save money in the cloud? This is a classic reason why we have the cost and use report, to be able to find where we're spending money and to save it. But it can be quite nuanced where those opportunities are, and they can even be improved by having other data sets. Cost allocation, one of the biggest conversations I have with customers every single day is about being able to understand and see your spent. Securing our costs, not something we typically discuss in the world of fin ops and optimization, but we hopefully all in this room enjoy using the cost and use report and we wanna allow others to do that, but we wanna be able to control the access so that they can't see data they shouldn't. In today's session, we are gonna cover these and give you new ways to access your cost and usage report and solve these problems. Just to introduce myself, my name is Steph Gooch. I'm a senior essay advocate at AWS and joining me is Andy. Hi everyone, I'm Andy Brown. I'm a senior manager specializing in Finops. Before we get into the coding and querying of today's session, we wanted to warm up with a little game. It's early I know, but we thought we'd get the energy flowing. So if I can get everyone to stand up if you can and would like to participate. The way this game is gonna work is gonna be a question on the screen, and you're gonna have two options. If you think it's the first answer, you'll put your left hand up, if you think it's the second answer, you'll put your right hand up. It's an honesty game, if you get it wrong, you sit down. There's only 4 questions, OK? The last one standing will get a very special Kerr related sticker at the end of the session you can come and claim. The idea is to just recap on some Kerr basics and make sure everyone's on the same level. First question. What version of the Adabis costs and usage report has the newest features, Kerr Legacy or Kerr 2.0? 321. C 2.0, did anyone get that wrong? Oh, well done, folks, well done, well done. OK, so this is the newest C, this is the one that has all the features. If you have not used it, you need to start deploying it, and this is what we'll be using in today's session. Where is the data for the curse stored? Is it S3 or RDS? 321. Yes, S3 is the right answer. Basically any data you can get into S3 is available for you to be able to query in Athena, and that's what we're gonna be showing you as well later today. OK, you're all doing really well. I need to have a lot of stickers. Uh, are tags automatically added to the car by default? Yes? No. 321, get those hands up. No. OK, there's got a few of you. The key phrase there is by default tags, you have to have cost allocation tags enabled, so you don't have all your organisation's hundreds of tags. We'll show some tagging there. You can be very smug if you've got that one right, I can feel people's energy. Alright, last query. Can you add account names into Kerr 2.0? Yes or no? 321. Yes, giving it away at the front, yes. Yes. This is one of the new features available incur 2.0 if you're doing it. And we're gonna be using that as one of our ways of securing the curve further. Brilliant. Alright, you guys go get a sticker, please sit down and come to us at the end and you can grab one of our curve special stickers. That was a little bit of fun, get everyone standing and moving before our session because we know it is nice and early, and thanks again for joining us. Let's get coding. So this is a live coding session, like I said, we'll be querying. It is live, so we'll be doing it and we wanna have some help, so we'll be asking questions to you guys, that's the interactive part and you can shout them out. If you have questions about the session, we'll be taking those outside at the end so we can get through everything today. But warning. I am dyslexic. There will be spelling errors. There might be errors. What we have done is we have a nice little tally over here. So in our sections, whoever gets the most errors will have to do some kind of forfeit at Reinvent. We do this every year. Make sure that you get our LinkedIn's at the end to see what we do at the end of our session. We will make sure to share a GitHub repo in our resources at the end that have all of our queries so you can take them home, so don't feel that you have to make any kind of specific notes. We've also pre-recorded a session that's gonna be on a YouTube channel so you can watch this back later and follow along. But with that, I'm gonna hand over to Andy for our first query. Great, thank you, Steph. So who's been in this situation before? You're at your desk, doing your day job, and you get an email from AWS. Your custom anomaly detections detected, your nat gateway spend has increased. So you decide to go into AWS Cost Explorer, you filter by the service you see to other, and you see, yes, not only has your net gateway spend started to increase, we're also seeing some net gateway bytes, and we're also seeing some data transfer of reasonable bytes. So audience, shout out if you know what data transfer regional bytes are. Inter AZ? I think I heard an In AZ, yes. So data transfer of regional bytes is inter availability zone data transfer. So for example, an EC2 instance in USD 1A talking to a USC uh EC2 instance in US East 1B. And to be honest, I don't really remember all the different types of data transfer and how they pair as usage types in Cossets flow and occur. And so when I saw this, I just typed it into Amazon Que and it told me. But Steph If you saw this and you got this email, what would you do? Oh, I would just push it to the developers and run away, probably which is what you're meant to do, right? Yes. One approach, but probably not the AWBRS recommended way. So what I'm gonna be showing you today is I'm gonna be going into the cost and usage report. We're gonna filter and have a look at the costs, but then we'll realize that we need more information. So we're gonna be integrating a new data source, and then hopefully we'll find that root cause and optimize this spend. So with that, let's go into the console. Cool, here we go. So everything we're doing today we're doing incur 2.0 and we already have it set up to go into Athena. I have this query to begin with, which is they're designed to mimic what we're seeing in Cos Explorer, so let me run this. It's gonna start coming back with the usage types coming through, you'll see we've got a variety of different nat gateways and we've got this bend. But from this view it's still not very clear on what is causing that increase we saw in Cost Explorer. And so the first thing we're gonna be doing is we're gonna be creating like a quasi pivot table. I'm gonna take the September and October costs and split them out into their own separate columns. So let me scroll up, I'm gonna just navigate this out of the way to make it bigger. So within the sum statement, I'm gonna be putting a case. Think about case as being an if statement. So case when billing period equals and we'll do September 1st. Then return the cost Else return nothing. So When this runs Only September costs will start appearing this and then so I can rename the column September. And then I just need to do exactly the same for October, so let me copy. Put in the comma, I'm gonna change this to 10 for October. And change it to October. Cool. Now I can change my select statement, so I no longer need billing period because I have my worst statement to narrow it down to only September and October. I have it split out into two separate columns, so I can delete that. I'm also gonna delete usage type. Right now I don't care if it's data transfer or Nat gateway related. I just want to know which resources are causing this spike to work out what to do next, so let me delete that. I can change my group bias cos we're now only grouping 11 resource, which is the resource ID. And I'm gonna change my order by because I want to see what's the most expensive resources in December because that's where uh in October cos that's where we're seeing the increase. So 3. Fingers crossed, so everything, right, so let me run this now. While that's running, if you do see us about to make an error, you can shout out and stop us, because often it's easier for people, when you're looking at someone else's query to find an error, so feel free to shout out if you see us about to make an error. And my common one is commas. I always forget about the commas or add too many commas. So as we see here, we've got some NAt gateway spends coming through, and we've got some new spends happening in October. So we've got this one here, it's pretty much the same, so this must have been around in our environment before, but we've got two new resources. I could have tagging on this Nat gateway, which will probably tell me who owns this or what part of application with it. But the issue we have with that gateway and also with the mode of data transfer is. We don't know why. Like, for those who don't know what that gateway is, there's something you deploy within normally private uh VPCs which will allow your resources to have secure access out to the internet. It's just the middleman in the process, it doesn't actually generate information itself. So in this situation, am I reaching out to the application team because maybe they changed the application deployment and something's changed? Or am I reaching out to the networking team because maybe they changed something in the configuration which is causing these two net gateways to spin up and the data transfer going through. And this is where we hit the limitation of the cost of news report, and we have to try and work out what's next. I am 100% not a networking specialist. Really, I'm not. And so, as always, I use Q. Q is our best friend. And so if I open up the panel here, hopefully it will show you the query we ran earlier. Here we go. And what I did was, Q helped me cost optimize. Mynat gateways, and the things it tells me, it tells me exactly some of the reasons and why your Nat gateway Ben might be going across, and then it also tells me like where do you want to go for extra information. Here, Q is recommending that we use VPC flow logs. So VPC flow logs is something you enable in each of your VPCs and what it does is it tracks all the data flows within your resources so you can work out what's going on and who's talking to who. When you deploy it, data starts appearing within minutes, so it's not something like the C you have to wait for maybe up to 24 hours for the data to start flowing through. And also you get two choices of information of where it can go out to. It can either go to CloudWatch, so it's with the rest of your application logs, or you can actually have the data flow into S3. If you choose S3, VPC flow logs also give you a cloud formation template. And the cloud formation template will do the full Athena setup for you. And so we enabled VPC flow logs in our area, we ran that cloud formation template, and this is what we have set up within Athena today. So if I expand out the database on the left-hand side again, I can change now, you see I've got my BPC flow logs analysis table. And I have the table here. Whenever I get a new data source, I normally expand it out and I start having a little bit of a nosy round cos I want to get my feel of the data before I start using it. So let me preview. Enemy IQ, don't need them anymore. OK, and so as the data cuts coming back, we're starting getting some of the information available in BPC flow logs. We have the account ID it's all sitting in, we have the source and destination address. We then have some port information, and then we have some bite information, so for example this row here is representing 44 bytes of information going between the two resources. We have the start and end time. This is all in Unix epoch time. And so if you wanted to translate this out into something a little bit more human readable, there is a function within Athena to do that for you. But in our case, we just, the daily granularity is enough, and the VPC flow logs is automatically partitioned by day and so I don't need to worry about converting this. So let me change this into something a little bit more, just the columns of information I want. So I want the source address. and the destination address. And then I'm gonna do a sum of the bits. Can I type today? There we go. So my bits. Oh, I'm gonna add, I need to add in some filters now here, so I need to do a where statement because I'm gonna do where the month is in September. October, so you just narrow it down to the months where we saw that increase in spike. I need to add in a group by 1 and 2 for my source and destination address. I am also gonna order by the who's talking the most, so I'm gonna add in my order by, and I'm gonna order by 3 descending because I want to have those bigger speakers. And then keep the limit 10, because there's a lot of resources in my account. I actually want to know who are the biggest ones to work out what's going on. I could just run this right now, and this will return some information, but the byte numbers are gonna be really large because we've got a lot of transfer going in. And so really what I want to do is I want to convert the bytes to gigabytes. Because that's how AWRS charges for us stuff, so we can align it to what's happening in the car. And so audience shout out if you know this. If I take bytes and I divide it by 1,024, what do I get? Yeah, and here's some kilobytes. And another 1,024 megabytes, and then the final 1,024 gigabytes, yep. So I'm doing that so I can then compare it to my cost and user report. So let me run this and fingers crossed we'll get some information back. Here we go. OK, we'll get information back, but all the numbers are returning 0 bikes. And this is due to the way Athena handles data. So if we look at the definition of the table here on the left hand side, you'll see that bytes are stored as a big integer. And so this is the number without any decimal places. And so at above, when I'm dividing by 1,024, Athena continues to ignore decimal places. So you remember that row where we had that 44 bytes of transfer? You divide that by 1,024, you're gonna get zero if you round it down. And so there is a way to force Athena to think about those bikes and include them within it. And the easiest way to do this is to put a 0.0 at each of the ends of these numbers. Cause we're gonna say divide everything. But I care about the decimal places. So now when I run this, fingers crossed, I'm gonna get some real data coming back. And here we go. We've got the top 4 rows coming through. They're the biggest ones, everything else is under like 1 gigabyte, so they're small in the whole scheme of things. But like anyone know these IP addresses, like no, like, like if you do know IP addresses at the top of your head then well done. But I don't, I don't know these IP addresses, they don't really mean anything for me. And so once again we're back to our favorite friend Q. And so this tab here represents the account where the issue's happening. This is where we deployed our VPC and I've gone ahead and asked you to explain one of the IP addresses. And you see it's, it's able to pick up exactly which instance ID it's got the servers, we stopped it. It tells me what instance type, and all the relevant information. And so what we actually did was I chucked all these IP addresses into queue and told me to tell me what they are and also group them together. We have a quite large environment, we've got auto scaling up and down. And this, which one is it? Here we go, here we go. And this is what Q returned. It's using a light statement because like I mentioned we've got auto-scaling groups, so the IP addresses of the instances are changing over time. And it's categorized it into all the different subnets we have. We then have the two NA gateways, we've got running environment, both their public and their private IP addresses. And then we have this general group by. When I ask you about some of the IP addresses, it just didn't recognize them, it couldn't find them within our environment. And so it made a guess. The guess was either it's like an internet, maybe there's a file we're downloading or getting from, or maybe it's an AWS service, maybe it's S3 or RDS or something like that it's reaching out to. And in our situation, because we just carrying what we can control, we're just going to group it under here. And then this will repeat it again for the destination location, so let me take this. I'm gonna copy you. And let me go back to my query, here we go. And so I'm gonna just replace source and destination address with those two queries. So now when I run it, fingers crossed we'll get a little bit more useful information. here we go, and it's started to categorize. So I have a challenge for you. Can anyone recognize which row number is the issue? Oh, Did I hear? So 34, no 3, I'm pretty sure 3. Yes, 3 is the issue. There's 2 reasons why I know this. One, I ask you. But two, and I'll show you in a moment, I went back to what our fundamentals in the costs are. But I'll tell you why this is an issue, because we're not networking, this is not a networking session, this is advanced analysis for the curve. Two reasons. First of all, I mentioned that Nat gateways are designed to provide private resources, access to the internet. We have a public subnet using this Nat gateway. And a public resource already has a public IP address, it doesn't need to go in that way way, it can just go to the internet itself. And so why it's routing that. The second issue, we have this public subnet in availability zone 2 talking to a net gateway in availability zone 1. And that's causing that inter AZ data transfer. So Like I said, that that was purposely designed to challenge you. I wasn't expecting people to be able to identify this. And so how do we actually identify what the issue is and go with? And it's down to the cost, it's all about the money at the end. And so what I'm gonna do next is I'm gonna work out what the net gateway and data transfer charges are for these entries within this table. I'm gonna be starting with using a with statement. So with statement is where you create a sub query with Athena. So we're saying run everything here and we're gonna use it again later. So I'm gonna do with, I'm gonna call it VPC as, open the bracket and then go all the way to the end of the query we were working before. I want to take the semicolnel away and close the packet. So that whole thing we've been playing around with is now called BPC. So I can do select star. From BPC And now I'm gonna add in my two columns for nat gateway and data transfer charge. I'm gonna use my old friend case. So case when, source, location is like nat. Or destination locations like that, so anything which she's using in that gateway. Then I'm going to take the sum of the gigabytes. And I'm gonna times it by 0.045. The reason for 0.045 is this is how much AWRS charges for Nat Gateway data processing within USD 1A. Q told me this when in my earlier query, so I, that's how I get it from. And then everything else will be 0. And, and I call this as gnat. Data USD. Next statement is then for the into a validity zone data transfer. So this is going to be a case when the source location is like AZ1, and we want an and statement because we want it where AZ1 is talking to availability zone 2. So destination location is like AZ2. Then this is gonna be the sum of gigabytes. And this time we're gonna be times it by 0.02 because that's how much we charge for inter availability zone data transfer. And then I need the same again for this one, so let me, let me copy this. Yeah, this, and we're gonna do the other way, so we've got availability zone 2, talking of two availability zone 1. And then L 0, and then this can be then my data transfer USD. Fingers crossed this is all good, let me run this. It's running, always a good sign, here we go. And as you can see. It is row free. All the charges, remember if you go back to that original costs explorer view, the Nat gateway data processing started up, the data transfer started up, and we can see it's all row free. What this tells me now is I can reach out to my networking team to investigate. We got maybe a configuration change, maybe they did something for the root tables for that public subnet which is now forcing it to go in that gateway. And it's just that extra information to go to them to provide it. Can you switch it back. And so, what I want you to take away from this is think about what other data sources. I do love Decur. I wouldn't be on this stage if I didn't love the cur, but it's not the answer for everything. Maybe it is data transfer you're investigating and that's where the VPC flow logs come through. Or maybe you're investigating extended support charges and why they started introduced, and you want to bring in an inventory report. As long as you can get it into S3, you can get to Athena. Finally, if you don't know what to do, use Amazon Que. The example I'm showing you today is a genuine customer problem I helped with. And I like I really had no clue what to do. I use Que for everything, I chucked all the information, all the results into queue. And it told me exactly what to do, it told me where to send people, who to go look into, because Q knows your AWS account. Everything I showed you today is from the free tier of the queue as well, so it didn't cost me anything to run it. So this part of the session was very much focusing on a particular service, we're looking at that gateway. But what happens if you wanted to work out if your whole application is running efficiently and if you have the right KPIs to track that? With that I'm gonna hand that over to Steph to talk a little about our favorite friend, Mr. AI. Thanks Andy, yes. Cost allocation. Like I said earlier, one of the big challenges we see with customers, which has been made trickier by things like Amazon Bedrock when it comes to allocating that spend. So we're going to focus on just diving into some cost allocation for bedrock usage and then looking at a bigger picture. But there are two big reasons why we see customers not being able to cost allocate bedrock accurately. The first is billing entities, so when you choose your models, these have an effect on the entities and these show up differently in the costs of news report. So we have first party models, these are ones that are trained, sold and developed by AWS. So think of things like Nova and Titan. Next we have second party models. These are ones that are developed and trained by someone else, but they're sold on AWS. Think of things like Lama. Finally, we have third party models. These are ones developed, trained and sold by someone else, but to get them onto AWS you need to use Marketplace. This is why they show up slightly differently, and it can't be as simple as select or wear Bedrock. The second challenge we have is when you use a model, the resource ID just shows up as the model name. That's all the information you get when you do it by default. So it's unlike an EC2 or like we saw with the Nat gateways, when you get a specific ID that you can go and find in your accounts, you can't find it, it's just a model. But there is a way around that, and that is inference profiles. Inference profiles are a way to preconfigugure how you interact with Bedrock. And they allow you to choose some customization, so what model do you wanna use, what temperature do you wanna have, and you can tag it. Of course tags are gonna come up. So you can tag this resource and then see who is using that model and where are they using it. You can do this in things like cloud formation, this is an example I have, and this is also in the GitHub repo we'll share at the end. And so you can reuse this in your applications, or you can use it in an SDK, this is Python, and you can have that. What we'll be doing is we will be cost allocating some usage based on the department tag you can see on there to see who is spending money from what department in our accounts. Let's get into it. Just switch it. So I already have a bit of a pre-done query, uh, so it has some simple stuff in it, things like product name, usage, spend, resource, classic information you use. But I wanna draw your attention down here to the where statement. This is where we cover those 1st, 2nd, 3rd parties. Model information. So where we have product code like Bedrock or product, product name like Bedrock, that is how we cover everything. For example, when you use lauded, it comes up with clawed brackets, Amazon Bedrock, so we just wanna check that one line. The same with things like tokens, when it comes to 1 and 2 parties, you have line item usage type like token lowercase, but you also have line item usage type like. Token uppercase. Notice no there's no S, because sometimes it's token, sometimes it's tokens. Don't know why I didn't make it, but we wanna cover everything, so we've just got these where statements in there. I've already run this query and we can start to see some of the usage. So we see our spend, yeah, yeah, but this is where we start to see information like that resource ID. So I want to show you an example. These two rows are what it looks like when you have no inference profile. You just have usage of a model, right, if you wanted to cost allocate this, it's a bit useless. So we can see some people are starting to use inference profiles and see some tags. So we see things like the dev teams, they're using things like Nova Micro, we have security teams, uh, using Pro, and we even have some sales teams who are using Cloud. So we can see this difference from that first line, but we can see who the tags are. What we're gonna do first is we can see like here. Some people haven't tagged their spend, naughty people. And we wanna make sure that we can go and find those people and say, hey guys, you've already got the inference profile, you've already done the work. Let's just tag it, let's add those couple of lines. So we're gonna do a query just to find those people to have a bit of a hit list. So we're also gonna use a weird statement. Tokens as bracket everything up. And select from. Hm, 2 ticks, tokens. Right, so what information do we need to get? Well, we're probably gonna wanna grab those resource IDs so we can see what account and what the inference profile hash is. Then we're gonna get some usage because we wanna know who are the heavy hitters for this. We don't just want people who are doing hardly any work and like POCs, we want some bigger maybe production workloads. Uh, so we'll give that a name. And then we'll also get our spend, because as Andy said, it all comes back to the money. Because we have spend, we have to have some group buys. So a group buyer one in this case, and we'll do a ware filter. So the ware filter is going to be where the resource ID. Is like if we look down here we can see this application inference profile, that's the kind of calling card that we're using one. So we can drop that in there. And I don't want ones that are already tagged, so we can just filter those out. So wear my user department tag that I've extracted. Is no. Oh, go on, Aaron. Guys, and you can put a tally up the first one. What have I done wrong? Oh, what is it? No, it's just like, isn't it? Well, there had to be some drama in the session. I could say I planned that. Uh, OK, so here we have my inverse profiles, a nice little list that we can go and say, hey, you teams, you're in this account, we can see some usage, can you just go and tag this resource? Because we need to be able to cost allocate our applications relevantly. But we want to cost allocate the ones that have tagged it. So a small change we need to do for this is we need changes to is not null. Get rid of that application because it's kind of mute because of that, and rather than resource ID we can use news department. What we now see is our user departments, there's some usage and some unblended spend. It's as simple as that. We've got some cost allocation. But one thing I want to draw your attention to is the usage amount. So what we can see is the sales teams, they're the big spenders that we have in our organization. They have, 0 $2. But we look at things like the dev team, and they have a third of the spend, but almost the same usage. Does anyone know why that is? Almost, someone said free tier. It's because of the model selection. So earlier you saw the dev teams and the security teams, they're using Amazon Nova, which is a fraction of the cost of things like Cloud. Maybe the sales team need to use Cloud for whatever they're doing, but if you're also looking to optimize your bedrock spend, having some decisions about what models you're using will impact your cost. So if you start to see a team, a project, a department spend really spike because they're using Bedrock, you could go and have a conversation with this data and say, hey, your model selection is really impacting it. Is that the right model? If it is fine. If not, maybe POC's developing, try some cheaper models like the Amazon Native ones. So With that What happens when Bedrock is part of a wider project? So not just specific to its own little tokens in and out. What if we wanna think bigger? What if it's part of a full story? And what if we wanna track optimization work? See if we have moved to a cheaper model or if we delete things like idle net gateways that we don't need or incorrectly set up. This is where something like unit cost comes in. So the unit cost is the expenditure to sell one unit of something or one click. And the story in this image is someone's cost is going up, like we just talked about, the spend is growing and growing, the project we hope is doing well. And the question is, is the spend going up because more people are using our application, more people are kind of logging in. More people were demanding information from our applications. Or is it because someone left something running, like in that gateway, or someone chose an expensive model? How do we work that out? And that is where unit cost comes in. So we take the cost and divide it by things like invocations or usage of an application. What I've done is I've made us a little application. That we're gonna show, just a simple little web app, which covers, which is using resources like EC2, Aurora, Lambda, Bedrock. And the question is how much does that one click cost? That is the question we wanna answer. So what we need to do to do that is we need to grab all of the spend of our usage and then divide it by some kind of unit, some kind of invocation. And we can do that because the driving force for this bedrock usage is lambda. So every time I click that button, a lambda function gets triggered. And we can actually grab that data from the cur, so that can be our usage amount and then divide it by the cost. This is something that I didn't know before we got into the session and we started playing around with ideas that you can actually extract this from the cost and use report. So we're gonna do that now. Does anyone know? How I grab the usage of lambda in the Costa news report. What column it is No? That's OK, it's usage. Unsurprisingly, it's actually usage amount. We can actually steal this from my previous query. So usage amount, if we drop that in here and we sum it, and I filtered based on lambda, we filtered based on the application and the account ID and the month. If we drop that in there, we'll start to see how many times that lambda function was hit. How many times in a couple of months people have hit that button. Great, now we have a base. Now we need to go and get some spend. So we can group that up. It's Invocations. As So we're just gonna grab our spend for this part for this specific application, so we'll grab it from the car, so all we're gonna need is. Some of our spend, we can grab that from there. From our other query. Always safer to copy and paste. As cost We're also gonna see what products we have in here, we want to make sure that we're capturing all the right information. And finally we need billing period because we want to check when we're doing this, we're just looking over a certain couple of months. Cause we have a group buy, we've got to do that sum, sorry, sum we have a group buy, group by 1 and 3. And we need a aware statement. Luckily I've done a great job of tagging in this application, and so we've got a little tag we can use, so our tag. Oh, it's because we're on the wrong data set. If you ever have across that situation where suddenly you can't auto create information, it's because it's not on the right table, so you know. Uh, my one is called. Oh, wrong brackets. It's a user and it's my FinOps tag. Equals AI. Very simple, just selecting our spend for this. Oh, tell me. Oh You get an extra special sticker because he saved me. That's what friends do. We're not friends apparently. Alright, good bye. And this, oh, we also need another uh we statement, so we're just grabbing this month's. Thing. Am I safe? No. We're by 1 and 3. Right, you can put a tally on. I think I know who's going to be doing the challenge. catch. Yeah, the more fun you guys can see how we can fix errors. It's a more interesting show. Uh, so now we can see the resources we have and the cost as well. What is interesting is some of these are gonna be quite static, so for the September, my resource wasn't on the whole time, but if it was on 24/7, the same amount, it's gonna be very similar. Cost. Things like Bedrock really differentiate because of how many tokens come in and out. When you, when I run that little web app, sometimes it gives me a massive amount, sometimes it gives me a small amount of information back. Those things shift, so those costs are quite variable. So now we have all of this information, we can do another one with uh spend. Grab all of this. I don't need my product kind of breakout anymore so I can get rid of that. Change that to 2. And now we're gonna combine this data. So we're going to collect select from spend. We're going to do it in a join to connect these two things together. In a join Invocations, we're going to make a little shorthand to make the combination of these a little bit easier, and we're going to select the information, so selects.co. We're gonna do I. total invocations. And then the very simple arithmetic of the 1st 1 divided by the 2nd 1. We're also gonna grab that billing period, because this is how we're gonna join these, so we're gonna join on. I. billing equals S billing. Any issues? Am I safe? Oh no. Oh I every time. Guys, I thought we were friends. Where's the support? Now we can see these things, oh, I've gotta do a forfeit. Now we can see our cost per click. So we have those 89 clicks, we have similar numbers because they were about the similar amount of spend, similar things. OK. So what? What if we optimize? What if we got rid of idle resources we don't need, or what if we change model? What if we moved to graviton? What if we right sized? What's gonna happen to that per click cost? So what we can do is we're gonna add in another month's worth of spend. Where I actually went and did my role, I went and optimized. And now we'll see, hopefully, that the spend per click has dropped down. So it's now, what is that? 25% of the cost by optimizing. What I hopefully want to inspire you with this is to show you how to get this data, yes, but if you are doing optimization work in your organizations, try and find some KPI you can do. You can also do things like compute, gigabytes or you can do, Any kind of usage you can gather, people often try and focus on grabbing data that is part of their application, usage maybe in the application. That can be quite tricky, especially if you work centrally or you don't own that specific application, to get that is typical. Whereas if you do it like this and you have it in the car, you've already got access to all of that information. So with that, tag your usage. You don't always have to tag everything in your organization, but there are some situations like this where the nuance needs to be added so you can get more detailed information for your applications. Also try getting different metadata or data for your unit cost. Even things like API usage, lambda like we said, or there's so many other parts that you can grab data from Cloud Watch that you can drop in and use as a data set to combine. With that, I'll hand it over to Andy for our last query. Thanks, Steph. So who loves data? I am 100% 1 of them. There is a time and a place for a dashboard, but I am most happiest when I can dive into those numbers, dig and find and find those optimization savings. And I think there's plenty of people like us, like our application teams. On AWS there's a few different ways you can share cost data. Within the console for Cost Explorer and budgets, you can use billing views, which is a way to group accounts together and then share that group of information so they can go to one place. In the world of dashboards, Amazon Quicksite has native support for low level security, and our cloud intelligence dashboards has it as part of our solution. However, what about the cur? What happens if you want to give the cur to your application team, but you don't want to show everything to them? And that's what we're gonna be covering today. We're going to be starting off with Iris' recommended way for deploying the car, and this is what we've been using today. We're gonna be using Athena like normal, we're not gonna be introducing anything else. The only thing we're gonna be changing for the security is we're going to introduce a new service into the mix. And this service is A debris Lake Formation. Lake formation is a way for you to create a data lake with fine-grained security controls, so you can say app team, you can only see this. There's gonna be 4 steps we're gonna be following for this. First, in late formation, we need to tell it, we need help managing securities, and this is where the data for this is stored. And so in the Kerr world, we'll give it the S3 bucket. We then need to create our groups, so a group of accounts or groups of certain resources, and that's what they call a data filter. We're gonna be then reviewing our FINA configuration to make sure that our app team can go ahead and actually run the queries. And then finally, everything in AWRS is all about permissions and security. So we'll just be going through that and making sure they can do everything. So with that, let's switch over to the console. So what we have here, let me go up to my query here, I have a very simple query just to show you what we have in our Ke table today. So filtering by line item from an account name, and this should return 8 accounts we have currently in our Aid address organization. What I want to do today is I want to give access to my app team to their dev and Prod account. But they don't want him to see anything else. In this whole world we're using the new multi-session feature, and so this tab over here is my app team. So I'm still in my FinOps account, but I'm now in my app team role where everything before today or before this session, we've been using admin. Here you'll see they, they don't have any access to Athena. They're getting errors, they can't see any tables and there's nothing you can run. Let's go back to our admin. And we'll come back to them later. So like I mentioned, step one is telling lake formation I need help managing access to accessory buckets, so let me go to lake formation. And this is what lake formation looks like when you first start. By default, lateformation will take in all the information already stored within the ADoris Glue data catalog. So everything you already created in Athena for example, so your cur tables, any views, all that information will still be already available for you in late formation. Down on the left hand side, I need to go to my data lake locations, and I'm gonna register a new location. I need to give it the S3 bucket name, so let me browse. I know it's in CID. It's my data exports. We use one bucket for everything, so I'm gonna select that. We then need to provide an IEM role. This IM role is gonna be them working on behalf of the app team to actually go ahead with the data, so this IM role needs access to everything. AWS does have a service role for you to use this, and that's what we're going to be using today, however. Um, maybe you should speak to your security teams first to see if you want something a little bit more restrictive. And then the last choice you have here is for permission mode. You have a choice between hybrid access mode or late formation. Late formation basically will say that all access to this S3 bucket is now will be only maintained in late formation. So any permissions already granted elsewhere through IM for example, will then be ignored. In our world, we're going to be using hybrid access mode. Hybrid access mode is gonna be take the permissions with defining in late formation, but also use the permissions in IM so they work together in a hybrid way. We're good at naming sometimes. We're doing this because we already have the cost intelligence, cloud intelligence dashboard set up. We have our finance teams using the curve today and we don't want to break anything. But maybe in the future we want to move to Laker formation mode so we only access permissions in one place. So step one's done, we're told late formation, help me manage access to this S3 bucket. Now if I go to the data emissions tab, we'll see what's already been set up. Like I said, it's already taken everything we've got in Athena, so if I filter by table equals C2. You'll see that our admin role, the one we've been using all today, has access to the table and it has access to all the columns in, and then this is what row represents like the IEM permissions. So I now need to create a new permission from my app team and define what that means. I do this by going to data filters up here on the left. I'm going to create a new data filter. I give it a name I'm going to choose, OK, what access they're gonna have, they're going to have access to. My Data explore and then my C 2 table, and you can restrict both column and row level. In my case for my app team, I want them to only access the unblended costs. They don't need to see any of the other cost columns that might confuse them or they might make some wrong decisions. So I'm gonna do exclude columns, I'm gonna filter for the cost. I'm gonna select them all, they can have access to cost category, and like I said, they can have access to unblended cost, but everything else they'll get excluded. And then to restrict that they can only see their account information, we're gonna do the row level access. I'm gonna filter rows. And what you need to do here is you need to complete this statement here. Select star from C to where. And so in our situation, we can do line item, usage, account, name is like app, cos we have that consistent naming convention. You can put anything in it. If you want to filtered by particular products or particular resource IDs, you can put that, it's just a normal word statement you were writing in Athena. One thing you can't easily do is filter by tag. And this is due to the way that CR 2 works with tags. With CR 2, tags are now stored in a map function called resource tags. And so Athena and Glue can't see the values of the tag, and so this will fail when you try and validate it. There is a workaround. What you can do in Athena is create a view where you take the relevant tags into its own separate column and you do everything we're showing you today based on that view, not the table. And so that's the workaround, but for us, we ought to do it by account name. And so this should be sufficient for us. So create the data filter. So what I've done is I created the filter, but I haven't said who should have access to the filter. So let me tick the box, I'm gonna do grant permissions. And I'm going to choose the IM role for my app team. You choose what they've already picked up all the things cos I did it from the data permissions tab. And you choose what access you want, so I'm going to allow them to select and describe. I'm not allowing them to drop, I'm not going to allow them to grant any more permissions, just select and be able to run it. And then you've got this last tick box. In our case we're gonna tick it. Basically this tick box says, do you want to enable this right now? And so You might not, maybe there's lots of other configurations, maybe there's other data source or other teams you want to do, and you're not ready for them to go live. This is the only thing we're doing, so we're ready to go live, so we're gonna take it. If you don't tick this box, you can just go to the hybrid access mode here on the left and then you just go ahead and you've set it up there. But our case, we're ready to go. So that's the late confirmation confirmation done. Step 1 and 2. Step 3 is all about Athena. For those who set up Athena before, you know you need to provide Athena with an S3 bucket. If you give your app team access to this S3 bucket for the Athena results, they might actually see other people's results. And so get access to data they want. And if they're anyone like me, I'm sneaky, I'm very good at digging around and trying to get access to data in slightly different ways. And so this is not great, we want to try and avoid this situation. And so what we're gonna do in Athena is we're gonna create a new workgroup just for this app team. And workgroup, think about it, it's just a new configuration setting. So let me go back to Athena. I'm gonna open up the menu here and go to work groups. I'm gonna create my work group, give it a name. I'm gonna leave everything the same here, and then it's when we go to query result configuration. In the past, you'd have the customer manage and you would provide that S3 bucket you might have created. In our case, we're gonna be using a new feature called Athena managed storage. Here, Athena's gonna manage the S3 bucket for you. No one in your account is gonna have access to this. This is also available free of charge, so when things are free in aid of rest, take advantage of it. But it's free of charge and it'll keep the data for 24 hours. And so perfect for this situation, because it means no one can go and see other people's results. So I'm gonna create my wet brick. We're now in the final stage. The final stage is setting up the IEM permissions. And I'm gonna go to IN identity and Access Manager. I need to go to my app team role. And you'll see, I already have one policy for them, this app team have already actually given access to the billing view, so they've been used to seeing a group of accounts within Cost Explorer. And this is where they got like that appetite to see more and they wanted access to the cur. And so I need to grant new additional access to them. I've actually already written a policy for this, mainly because policies, as you know, always can be quite difficult because you need to try and work out what's going on and make sure you get the right thing, so I created one for you. As a reminder, everything we're showing you today is available on our GitHub, including this policy, so you'll be able to take it from there and reuse it. Then me photo for my car, go. But let me take you through what's in this policy. First, this section is the most important one. This is the thing which gives the team access to the information in late formation. So this would allow them to go and pick up that permission we signed for them. Everything else within the policy is our standard setup. So we have all the normal Athena things, they can run queries, get results. But you can see I defined the workgroup to be that one we created up so they can only run things in that workgroup and access that configuration. We then give them glue data access so they can see the table, but once again I only give them access to that CER2 table so they can't see anything else. So let me go ahead, let me tick the box, let me add the permissions. And we are ready to go, and this is where, this is where I'm gonna get some marks. Let's see what I messed up. So, I'm gonna go back to Athena. First thing I'm going to check is I haven't messed up my own access and I haven't taken anything away. So let me rerun this query here. We run it and you can see we're still getting 8 accounts, so good, my access is still stable, things are working. Now, let me go to this tab here. Oh no. Oh no, Andy gets an error. Do you mind, we should need Steph to lock me back in. Let me, I swapped over. OK, uh, you can keep talking. Yeah, so we're gonna be going back here and fingers crossed, once Steph's logged us back in, we will be able to see that they now have access to the cur table. All right, let's have a look. This is what happens when you prepare too early for a talk. Here you go. I don't know. Oh. Interesting. You need to go to the reinventing, yeah, maybe. Well let's see. But that's like, OK, once Steph's got this back up and running, we'll show you exactly, and fingers crossed it is working, and what will happen is within the FinOps account, this app team will only see their two accounts, and so they can now run queries. And so what we really want you to take away from this is everything we've shown you with late formation is not actually curve-specific. It could be anything you've done in Athena. We, yes, ta da. OK, so here we go. So we're in the Finns account. I'm in my app team role. I don't think that's, yeah, you can, you like, yeah, OK, yeah. So you can see here on my left hand side my app team can now see the C2 roll. So if I expand it out and I filter for those cost columns, yes, they can only see those cost columns I give them access to, they can't see anything else. And now if I go back, let me see what we've got here. Oh no. Which one, here we go, OK, a few, OK. I'm gonna take the query we had before. I'm gonna paste it in here, and now this is the one with the massive fingers crossed. Let's see if it works. It should now return back only the two accounts. There we go, so they have access, so they like I say, they now have access to run queries and do things themselves but only see their data. switch. So let me. Everything I've shown you today is not curve specific. So think about what other teams are reaching out to you and asking for data. Maybe you could repeat the same process. As long as you can get the data into S3, you can get an Athena. You can get to an Athena, then you can register it in late formation and manage it all day. So think about maybe you're downloading cost optimization hub data exports, maybe some inventory or other like reports you're producing, and you can use this solution for that. And so today we covered three main topics. We started off with uh looking at how to optimize data transfer by introducing BPC flow logs and using that as an additional data source. We then talked about AI and work out how do you know if your AI workload is efficient. And then finally we talked about how can we share this data with more people, but share it in a secure way. I'm gonna hand it over to Steph to finish us off. We've made it to our call to actions, a couple of things for you guys to take away. Discover other data sources. We showed you VPC flow logs, but there are cloud watches, S3, there's loads of other services that you can plug into Athena and start to connect this data. Tag your usage, like we said, where relevant. I always advise an account kind of organizational structure, but if there's specific use cases for tagging, do enforce those. Enable RLS give other people the power of the cur. Give them the chance to go in and dive into this data as well. Now I know that that is a lot of stuff that we have covered today, but I want you to have a go. I want you to pick one thing from today's session. And have a go in your accounts. That is it. Gonna get back after reinvent, see if it works for you. Here's the QR code you've all been waiting for with the GitHub repo in it. It's a Linktree, there's a bunch of different resources in here, including a home recording of this session. This one is recorded and will come out later, but we have a home recording in here that you can just pop in and see the queries. There's also our LinkedIn, so you can see whatever 4th I have to do. We've got another session, we do have another session. And then we will also have some resources like inference profiles in there. And with that, thank you so much for joining us today. I can't wait to see what you do with the Kosta news report. Thank you so much.
