---
video_id: 203JsHlad2U
video_url: https://www.youtube.com/watch?v=203JsHlad2U
is_generated: False
is_translatable: True
summary: AWS SageMaker Universe Studio represents a paradigm shift in simplifying data engineering and machine learning workflows by consolidating multiple powerful services into a unified, streamlined platform designed to eliminate complexity and accelerate productivity for data professionals. The presentation introduces Maya, a data engineer facing the common challenge of navigating AWS's extensive suite of purpose-built services, which, while powerful, often creates decision paralysis and integration overhead that slows down project initiation and collaboration. SageMaker Universe Studio addresses these pain points by bringing together SQL analytics, data processing, machine learning, and generative AI services under one umbrella with curated tools that customers already love and trust. The November 21st launch introduced revolutionary one-click onboarding that reduces setup time from days to minutes, allowing users to provision resources, configure tools, and access a revamped builder portal through a single IAM role that seamlessly carries forward all data and resource permissions across the platform. The new serverless SageMaker notebooks, built from the ground up, provide a polyglot environment supporting SQL, Python, and Spark code with exceptional inline visualizations, interactive charting powered by DuckDB, and the flexibility to scale compute resources including memory, CPU, and GPU as needed without leaving the notebook interface. Integration with Spark Connect and Amazon Athena delivers superior interactivity with spark backends that start and scale in seconds, enabling seamless transitions from gigabyte to terabyte-scale data processing without cluster configuration or code rewrites, eliminating idle compute costs through pay-per-use pricing. The SageMaker Data Agent serves as an intelligent productivity booster that understands the complete context of workspaces and notebooks, generating boilerplate code, fixing errors with AI-powered debugging, creating complex workflow plans across data ingestion, transformation, feature engineering, hyperparameter tuning, and model deployment, all while maintaining awareness of data semantics and relationships to produce optimal code and help users debug semantic errors that traditionally prove difficult to resolve. The demo showcases real-world scenarios where Maya explores taxi cab data, performs data quality checks, removes negative values, adds derived columns, persists data to S3 tables and Iceberg format, and leverages the Data Agent to generate sophisticated visualizations and geospatial analysis breaking down trip patterns by New York City borough, all without writing extensive manual code. The transformation journey progresses from a collection of isolated services requiring manual integration to Universe Studio's unified access point, culminating in the latest releases that deliver always-on serverless notebooks with instant onboarding, seamless permission management, superior Spark interactivity, and AI assistance that collectively reduce onboarding from hours to minutes, accelerate code development, eliminate tool-switching overhead, remove idle cluster costs, enable seamless gigabyte-to-terabyte scaling, and compress overall data pipeline development from days to hours, fundamentally allowing creativity and innovation to take center stage by removing technical complexity and operational friction that previously hindered data professionals from focusing on delivering business value through their analytical and machine learning initiatives.
keywords: SageMaker Universe Studio, serverless notebooks, Spark Connect, data engineering, AI-powered development
---

Sorry, let me start again. Um, I'm Diana Rangerda. I, uh, lead the engineering team behind, uh, SAMcA Universe Studio, and I have with me Brian Lyles. Uh, Brian is a senior, uh, principal engineer, and he provides, uh, technical leadership to both, uh, Universe Studio as well as, uh, uh, data processing suite of services that includes EMR, Glue, Athena, among others. OK. So, uh, every year, when we meet to discuss Uh, the roadmap, we always talk about, uh, what new innovations we want to deliver to customers. However, this year, we asked a different question. What if simplicity itself is the innovation? It was a shift in how we thought about innovation. Remove complexity so that everything works, just works. In today's talk, we're going to talk about uh uh SageMaker notebooks that we recently launched, as well as all the innovations that we delivered using the same principles. So today's uh uh hero of the story is, uh, Maya. Maya is a data engineer. Of a growing enterprise. And she has very simple goals. She's tasked with building data pipelines, ATL pipelines to process customer data. And uh She's uh expected to collaborate with her colleagues who are part of her team. And then she knows that she's going to work with AWS Services. So she wants to be super productive. However, The path seems rougher than she expected, so she cannot get started right away. So it's the paradox of uh choices that she encounters. She has access to a lot of powerful AWS services, but uh she cannot easily get started yet. So the choice means when she opens her laptop, she looks at a lot of AWS services that are purpose-built. There is So powerful, it is going to do a specific job really, really well. However, at the same time, it means Maya has to select. The services that works for a task. She has to understand how to configure, how to integrate the services. Most of the time, single service is not good enough. Further, she needs to learn and understand all about the services. So this slows her down. Further, she can't easily collaborate with her uh teammates. She needs to prepare the data, probably and expect uh uh the data scientist in her team to pick it up. From a place that, they manage uh the data transition or handoff uh in a manual fashion. So this is the problem that we set out to solve for Maya. So we built Sagemaker Universe Studio. So how many of you have heard about StageMaker Studio? Can I get a show of hands? Oh, not bad, cool. So SageMaker Universe Studio tries to Bring SQL analytics. Data processing, machine learning, and geneer services all under one umbrella. We hand-selected, curated set of tools. That customers loved, and we integrated them to SageMaker Universe Studio. So this changes everything for Maya. She just has one place to log in and get access to all the services. And she just has one place to collaborate with her teammates. And even her teammates are going, going to log into the same uh project that Maya is working with. It becomes very easy for Maya to collaborate. So this is Really good for Maya. I mean, she's very happy. It solves a lot of her problems, but still, many problems remain. And Maya started wishing for more. So, Universe Studio had lots of uh powerful tools, all in one place. But Maya started thinking, It takes multiple days, sometimes even weeks, depending on the complexity of the setup. To set up the studio and get started to work. So she wished, what if? Setup was instantaneous. Further, all the data and the resources that she, that she had already access to, they didn't carry forward seamlessly. So Maya again started wishing for What if all my data and permissions, permissions to all the data and resources carried forward seamlessly? So let's see how we address this. On Sep September November 21st, we launched one click onboarding. In Sagemaker Universe studio to help Maya to go from days to minutes. So there are multiple entry points for Maya from AWS console, from S3, from Athena, from Sagemaker and Redshift. And with one click, by supplying the IAM role that Maya already has got access to for her data. The studio is set up. It provisions all the resources under the hood, sets up the tools, and takes Mayer directly to the buildup portal. The build-up portal is completely revised, revamped, streamlined experience. And we have uh completely revamped the information architecture as well. This was great for Maya. But The problem for Maya was seamless data access. Let's see how we solve or address that problem. I talked about the role when Maya set up the uh studio, one, during one click setup. So the role had access to all Maya's data and resources. So when we provision studio resources, or all the tools, the tools are provisioned using the role that Maya supplies, so that uh she can access all the data and resources from uh all the tools. Within Universe Studio. Maya also has got access to all the third-party data sources. She can query and access and load them right in. Not just that You can connect to any data sources or computer sources if you have got access credentials by creating connections and using connections from Unify Studio. And we have Data Explorer that provides consistent view of all the resources that Maya has got. So she can access the catalog. S3 files All at one place, including connections, doesn't matter which tool you go, you have a consistent view from all the places. So Maya's wishes that didn't end, she continued to think. What if I had the most modern. And lightweight notebook that feels native. Maya is a poor user of notebook. She's a Spark expert because she's a data engineer, and she knows that she's going to spend a lot of her time on notebooks. So she wishes for a lightweight notebook that provides amazing visualization and feels a lot more native. Let's see what we did to address this Visa Maya. We delivered Sagemaker notebook. November 21st. SageMaker notebook is a surveless notebook that is built from the ground up. And it is available on Universe Studio. It is server-less, like I said, and it is also polyglot. It means it supports multiple languages. Customers can write SQL queries, Python code, or Spark code, and also interact with charts, all from the notebook. And we provide great inline visualizations. Maya can query the data and get the visualization of the data right in the notebook. And she can also do interactive charting. Once she loads the data, we optimally store the data in the back end. And Maya can interactively create charts. In the back end, we convert the interactive charting requests to blazing fast queries. By the way, in the back end, we use Duck DB for doing that. And if Maya wants more powerful instances, she can switch to more powerful instances. She can provision more memory, more CPU or even GPU for that matter, no problems. This basically changed the way Maya could actually do the data engineering and write the Spark code or Python code. So I will hand it over to Brian, who is going to provide demo. One click uh on boarding and uh seamless data access in the notebook. Brian, take it away. All right, you can hold the mic. I'll just take the all right, so, um, hello, I'm Brian and um we tried to get Maya to come do this presentation, um, but there's a couple of things. One, Maya is only two dimensional and, um, Maya's not real, so, um, you get me instead. And what we wanna talk about here is um the first introduction that Maya would have seen of using the notebook inside of S StageMaker Unified studio and because um you all can see that I'm 6'2 and the laptop is way down here, I pre-recorded the video because um I wanna make this as a great an experience as I can. So let's go ahead and get started. So what you're gonna notice here is, um, this is one of the landing screens for for the um StageMaker Unified studio, but I actually wanna talk about what we're going to do here. Are there any data engineers in this room? Or are we all developers? All right, there's a few data engineers. So when we get a new piece of juicy data, what's the first thing that we do? Well, first thing we're going to do is we're going to explore the data. And this is different than what developers do, you know, developers just dive in. Um, data engineers look at their data, they hold their data, they explore their data, they get comfortable with their data. So that's what we're going to do here in, in, um, Sagemaker Unified Studio. So when we get started, um, we're gonna choose a product and because Maya's new, we're just gonna choose the taxi product and that leads you to a home page and from here we can now select the tool that we're going to use and in this case, we're gonna use notebooks and you'll notice that notebooks have samples or pre-made notebooks and because Maya's new, um, one of her teammates created her a pre-made notebook. So we will go ahead and select this notebook. Now, inside of this notebook are a few different things. There's 3 main areas, and what we have is metadata, cells, and the data agent. So, let me, um, pause it here so I can actually talk about this. Um, we understand that everyone here has seen notebooks before, and we understand that notebooks are sometimes a lot of information. So what we did is we broke down the notebooks into. Sections where you can actually see data that you have access to the work that you're doing, or maybe you need to talk to someone else and we'll talk about that later. So let's go ahead and get started with what's on the left side and on the left side is all the things that you can use in conjunction with your notebook. So we have shared and local files that could be local to the project or an S3. Um, we have data in catalogs, and in this data, in this demo we're gonna use S3 tables because I like S3 tables and then also, um, things that we're going to see is, um, I can also see all the connections I have to S3 data. Um, there's something called variables, which we will talk about later. I find this pretty neat. We can talk about the local computes that we can use with our notebook, and because it's Python, guess what? Um, we can configure our Python. So let's hop right into our exploration. So for those of you who aren't data engineers, the first thing we're gonna do is check our schema and our metadata. And one thing you're gonna notice here is, um, Dia mentioned that this is a polyglot notebook. And because we're polyglot and it's 2025 and I'm an anti-Python person. I started with SQL, so you're gonna see a lot of SQL up front and then see some Python at the end, and there's gonna be very good reasons for both of those. So we're just gonna start by querying our notebook and we're just gonna look at the schema and the metadata and notice some of the rich output. You get some distributions for the data that's coming out, um, and notice I'm not doing this in real time. I ran this beforehand, but, um, I assure you this is not a very big file and it's in S3 tables and it's actually not very slow. So we're just trying to get familiar with what the data looks like here and. Wait a second, hold on, um, as in any data, it's not clean. So we have to go through and figure out well what does that actually mean? So we're gonna look at the, we're gonna look at some columns and we're gonna see what the distribution of the data is over those columns. Um, some things we might find, and I'll show this in a second, is, um, we might find that hey this is in parque and notice instead of having just a table output here, I actually did a chart output and why do I show this is because, you know what, sometimes you need different modalities for different types of data. So we should use those modalities whenever uh they're applicable. So, and you'll notice that this is not a very big piece of data. It's only about 1.2 gigabytes, so it's only in about 8 parquet files, and we are just gonna continue looking at this. And now we're gonna look at the, the distribution of data. And I know a lot of you who are data engineers are familiar with the taxicab data set. Um, I was not, and what I found out is, wait a second, this data is not clean. There's negative numbers in here. So we as a data engineer, we wanna go validate and notice that I haven't left StageMaker Unified Studio notebooks to be able to do any of this, but I can actually see looking at my data that, you know, some of this is not correct. So what I'm going to do here is I'm gonna do a few more queries just to, you know, really dive into what the data looks like and, you know, make the decisions on what I plan to do next. And because I'm so confident in my typing, I'm actually gonna move over here as well and one thing that we're gonna notice is that, hey, um, wait a second, you can't have total negative amounts. I mean, I would love that world where I could pay a taxi cab a negative amount, so I wrote this big piece of sequel to actually figure out what that all looked like and you know, look through the mean standard deviation and look at the quanttiles, but you notice that that line, that that sequel statement. With 68 lines and you know something, this is where I'm talking about where we might wanna move back to Python. I was able to do the same thing in about 3 lines of Python, but you know what? that did not matter with StageMaker Unified Studio in the notebooks because I can actually, depending on the cell, move to where I need to be. It's all the same compute. It uses the same connections and um I think that it'll be very helpful for people who are trying to explore data in multiple ways. Um, some paradigms are only available in. Um, Python, like if you wanna do plotting and things like that, but you know what, everybody knows how to do SQL and they can get started. So what this does is it puts us at a place now where Maya knows that, hey, I have this data, it has 42 million rows in it, um, some of it's not correct. So in our next demo, which I will do in a, in a second, we're gonna move further onto that. Back to you, Daya. Literally hands full. Thank you, Brian. That was awesome. I mean, Uh, it was so simple for Maya to set up, uh, her studio as well as access her notebooks and start writing the code instantly. She didn't have to do anything right. So it is just the beginning, right? Now, real work happens when this needs to work in the production, where the data will scale and Maya has to analyze bigger data. So in this segment of the talk, we're going to talk about all the innovations that we delivered to help Maya scale. So Mayer's wishes continues. So, Maya is a spark expert, like I said. And, uh, she wishes for better connectivity with the Spark packet. She has access to an outdated way of connecting to Spark. Now, she wishes for a more modern way to interact with the Spark. And when she wants to scale from 1 gigabyte to 1 terabyte, she wants everything to happen seamlessly for her. She doesn't want to configure or tune the clusters. So, let's take a look how we address Maya's wishes. I want to give you all a brief intro about the spark interactivity. And I'm going to start by telling a story of making coffee. Because Running spark jobs and accessing spark cluster is a lot like making coffee. In earlier days, accessing spark means you had to get closer to the spark cluster or spark backend. It's like you're going to the kitchen yourself, grinding the coffee beans, brewing the coffee, and making it on your own. Of course, he had full control, but it is tedious and it is messy. Then came Livy. Livy is a broker that sits between the Spark client and the Spark backend. It's much better. It's like Maya getting an opportunity to call a coffee shop and place the order. Of course there was somebody there to receive the phone call. But if Maya wanted to make some changes to her order, then she had to call again. And if Maya wanted to get the status of the order, she had to call again. So the communication was not seamless. It happened in chunks. Then came Spark Connect. Spark Connect made it all very seamless. It is like a mobile app. Modern mobile app where you can place order. And you can track the status of the order, make changes to the order, and when the coffee is ready, you can pick up. So All customers, including Maya, they deserve better Spark interactivity like Spark Connect. Let me explain what we did. We took Amazon Athena, which is already serviceless, already performing. And made an integrated spark connector. I made it even better. Amazon Athena Spark, it starts in seconds, it scales in seconds, and customers pay only for what they use. There are no idle clusters. And it scales seamlessly. And with Spark Connect, customers like Mayano. can scale seamlessly from 1 gigabyte to 1 terabyte without configuring the cluster. Or without rewriting any of the code. So it's fun when All three things, that is notebook, Spark Connect, and uh Amazon Athena come together. So now the notebook. Comes integrated with Spark Connect. And it can access a thinner Spark packend using Spark Connect. Spark Connect is smart. So it enables customers to mix and match Python and SQL code along with Spark code. Python and SQL code, they execute on notebook compute locally. Whereas Spark code, sorry, Spark plan is shipped to the Spark back end. And we spark correct. The interactivity is very, very seamless. So this changes everything for Maya. She gets amazing spark connectivity. And she can scale seamlessly. She doesn't have to deal with the clusters. She gets great interactive spark experience all from the notebook. So Maya's wishes do not end. She continues to ask for more. When she's Maya, when Maya is working with notebook. She needs to write a lot of boilerplate code. And she needs to write code optimally, although she's aware of Spark. She's a Spark expert. She wants code assistance. Further, when things go wrong, Maya has to go understand what went wrong and figure out how to fix the error. And if Maya is working in adjacent domains or with new libraries, she needs to go look up and understand new ways to write code. Of course, all of us are in this situation wherein we go look up the internet. To write the code. Next When Maya wants to. Solve a complex problem. Need to design a workflow consisting of different steps. And then write code for each step. And then keep all the steps and track in, in the head. So it's pretty arduous task for Maya to keep track of all the steps of the workflow. And when she's working in the context of a notebook in a workspace, she needs to understand the semantics of the data, how data is integrated, how they are connected. In different fields of the data. To write optimal and effective code and debug code when things go wrong. Syntax error is fine. You can get away with it, semantics error, hard to fix. So she wishes for. Productivity booster. So how did we solve the problem? So we launched SageMaker Data Agent. Sage make a data agent. Sits along with the notebook and it is always ready to go. And it understands the complete context of Maya's world. All the workspaces, the notebook shelves, everything. It has the knowledge of all the information. And Ma can generate code, including boilerplate code using the context information that the agent has. If there are any errors, no problems. She gets fixed with AI button. She can fix the code. And then she can express her complex requirements. The data agent goes and comes back with a plan consisting of different steps. For example, Maya is a data engineer where she can express her requirements or needs to build a machine learning model. Now the agent goes and comes back with a plan to ingest the data. To transform the data. Perform feature engineering, even do hyperparameter tuning and running different experiments to select the optimal model, and then deploy the model. This is cool. This, this game-changer for customers such as Maya. It turbocharges the productivity. So let's see what does all this means for uh our customers. Let's summarize, uh, a recap, uh some of the deliveries that we did. First one is uh one click onboarding wherein it takes just over a minute to set up uh the studio instance, which gives a clutter-free builder portal. With completely revamped information architecture. And Maya's data permissions to seamlessly carry forward with the IM role that uh Maya provides during studio setup. And then Sage make a notebook, serverless, built from the ground up. All integrated with the intelligent assistant. And then we have uh Spark Connect. Uh, wherein we have made Amazon Attina, even better with Spark Connect. These changes represent significant uh transformation that we uh Delivered this year. To simplify the experience of uh customers such as Maya. I will hand it over to Brian to show everything. Brian, take it away. Just need this alright, um, so I'm Brian, my surrogate back and, um, just a little disclaimer, I'm not gonna show you everything, um, we want to go home today. There's a lot of features in this, um, but, um, I do wanna set the stage for the next, um, bit of demo work that we're going to do here. And as a data engineer I learned um once you look at your data you gotta make it available for people so um some people um subscribe to the medallion method where you have um you know, um, bronze, silver, gold data. Um, some people also subscribe to the raw processed and curated data, and actually those are the words I'm gonna stick to today. So what we're going to do in this next demo is we're going to see how Maya goes from this raw data, um, create some process data that we could actually use for, um, production means, and then also create some, um. Some curated data and um we might demo a little bit of other things too as we are going through this. Now. This time Um, we're gonna start from the notebook screen and, uh, this time Maya pre-created a notebook that has a little bit of information in here and what we're going to do first is we're just going to remove some of that data. We know there's negative data in here and we're just gonna go ahead and remove it and say we don't wanna see that anymore. And why would we do that? Well, as we are sharing data with others, we don't wanna share raw unprocessed data with people. Um, what we actually wanna do is do a little bit of pass of, um, performing a little bit of data quality and a little bit of clean up work for the data. Now the next thing we wanna do is, um, is something that I think is pretty important is there's information inside of our data that we wanna free up for others so in this case what I want to do is, hey, you know, um, we have some taxi data and we have start and stop, um, but we announce some dates and some locations but you know, wouldn't it be interesting if we could, um, you know, put some more columns on this data. So this is what we are doing here. We're actually just gonna add a few more columns to the data and we're gonna create a new data frame. And then for all you SQL lovers out there, um, I did not use SQL in this part, um, and I actually did not talk about where this data is being stored. Um, I might have mentioned it earlier, it's in S3 tables, but notice that I'm using it like it was in, um, it could have been S3 directly or it could have been anywhere in the Glue data catalog, and I think this is a really powerful feature here. So, um. I will pause it here to show what I'm doing now. So what ends up happening is we created this new data frame and now we wanna persist this data frame back to, you know, um, S3 tables and Iceberg. So, um, what this code is actually doing here is it's, it's looking. At the data, um, building up the columns and we're going to, um, store this back to Iceberg, but once we store this back to Iceberg, you're gonna notice that, um, if it was in S3 directly or it was in GDC directly, glue datacal uh directly. Um, it still would've been fine and notice that I created this new iceberg table here, um, with not a lot of effort, and those of you who are paying it real close attention, you're gonna notice that time I did speed it up, it did take like 30 seconds and I didn't think you all wanted to hear me tell jokes for 30 seconds. So, um, now I'm actually verifying that my table was created and, and just note one thing here as I'm talking. When I create it, so whenever I have a data set and I create data and I create some data frames or I do some local data, that's my data, or maybe someone who has access to the notebook can see it. But now when I put it in S3 tables, now it can be available outside of this notebook experience, and I think that's a very important thing as a data engineer or even a data scientist to be able to, you know, make your data available in the context that people want to be able to see it. So we transformed our data and now I'm going back to the SQL folks and I'm, I'm actually creating a query here and what I wanna do is, is, you know, get a little bit more, you know, data sciencey with my, with my data here and I wanna show you a, a pretty neat feature. A pretty neat feature of notebooks. So notice I did this query and did a little bit of aggregation, just trying to see like when people are traveling and. Um, Daya mentioned that, hey, we have a data notebook which is in, or we have data agent which is available in the notebook. So I'm gonna be very honest with you all. I wrote no more of the code of this demo. The rest of this code of this demo, um, you know, the rest of this session that we're gonna talk about is, um, showing the powers of data notebook and the context that it has over the data that you're having here. So everything that I'm not writing outside of this right column is computers and, and, and I just wanna show you the integration that we have here. So one thing you'll notice is that I was talking about variables earlier. So every time we create something, um, when we create an output, it gets saved to a variable and because it gets saved to a variable, what I can do is I can actually refer to that variable when I'm talking to or when I'm conversing with the with the agent. So what I'm actually asking the agent is I'm saying, hey agent, hey, you have some data over here. Why don't you just tell me when more of the, when most of these trips occurred. And now I'm thinking like a fledgling or a neophyte or, you know, a new data engineer, you know, let's build up some confidence here and. And our data agent's like, I got that boss, and it starts writing some code. And noticed that um. I'm just accepting it, um, and the reason why I'm accepting it is because it is a demo, but, um, it actually knows the notebook, so it's writing code in the context of the notebook, and you'll notice some other things if it knows that I'm using SQL, it actually knows my preferences here. And so now the data agent wrote some code and just like every other people or other people who write code, um, sometimes it doesn't work right. So not only can the code be written, it can, um, it can write code for you, it can also debug code. So notice I have an error here and I scrolled up and I actually saw what it was pretty instantly and I was like, nope, data agent's gonna do it and it didn't it didn't include pandas. So now I am exploring my data and notice I wrote no code. Um, I just used Maplootlib and it is actually going through and now as a human I can use my human intuition to determine, hey, what does this data look like and is it doing the right thing for me? So we're gonna keep on moving on and Um, noticed that, um, I thought it was only gonna do one. It actually created a way more robust example of showing me how, um, when trips were being taken than than I thought I was going to see. So the next thing I'm going to do here is I'm actually going to say hey I am now a data scientist on the weekends and what I want you to do is I want you to help me where people are taking trips and you know and and notice I did spell the word wrong that was not in the demo I did that on accident. But um this is this is taxicab data from NYC and notice that last sentence I just said break it down by borough and now I'm trying to test the limits of, you know what does our models, you know, what do our models understand and what context can they actually bring to the table and it was like once again, OK boss, I can do that. Um, so the first thing we're going to do is we're going to say, hey, um, I know what New York looks like, and I'm just gonna bound, I'm gonna draw bounding boxes around all the boroughs in New York City, and I'm like, alright, I'm intrigued. And then it's like I'm gonna do some more things and I'm gonna and I'm actually gonna transform this data and show you a visualization and like once again I'm like I'm intrigued. And, and it's still going on and notice one thing that it's doing is it noticed that I like to use the markdown blocks to separate things so it started inserting markdown blocks. Our data agent actually doesn't just know about the cell that you're working on, it knows about the full context of the current notebook. So anything time that you communicate with it, um, it's actually taking the full notebook into context. So we're going through this and. Noticed that most of this started from a three sentence conversation and, and I'm gonna keep on going and it's creating some graphs and I'm like wow this is kind of interesting and what I'm gonna see here is, you know, we inevitably run into errors and the worst thing about running into errors in a notebook is, well, now I have to fix that error and how do I fix that error? But. The neat part here is that um the data agent will actually fix it and there is a fix with AI button and it will actually work through you using the context of your current self but also using the context of your entire notebook to determine um exactly how to make the fix and like I said my goal here was to write no more code other than the prompt and we're going to actually see what happened. So now I was thinking and I didn't actually, I do know what the issue is and I'm um working to see where I would make the fix, but it was like alright I'm gonna make this fix for you. There was a missing, uh, there's a missing, uh, data frame that I think that needed to create and and went ahead and created it and now we have all these visualizations about all of my, um, all the trips and notice that. Even as a data engineer who is used to who is well versed in moving data from here to there but maybe not as well versed in doing the analysis part of like the, the more sciencey part or you know, I don't like doing images because I'm, I'm scared of matpotlib and things like that, you know, I think that this is a great empowering faction or function that allows you to expand where you are so you can start with I know where I wanna go. And this is gonna say instead of taking the yellow brick road, you know, you know, take this road instead. And so as you see here, I'm like, well, now, OK, I believe you. So. The things that I wanted to do in this demo were two, I wanted to create some process data that I could actually share and then I wanted to create some, some curated data that I could share with the smaller subset. Maybe it's actually a smaller, it's a, it's actually a smaller data set. So this is actually what we're doing right now is we're creating a curated data set that we can use or someone else can use for further analysis. And like I said, I am not typing anymore. And you know I'm just trying to go through and I'm going through and um now it's not just me working on it it's a combination of me and the model working on this and and the reason why I'm showing this more than once is because you know one time was a you know that's a neat trick Brian but you know if I do it two or three times you're like all right well maybe I do believe that this works and notice here that we're talking in different modes. So we're doing some graphs, uh, we're, we're doing some, some graph data or some graphics data. We're doing some create some new data frame data, but also, um, we're doing some, hey, let's create a new table data, um, and now we have this curated data set that we can actually use and we can use for other purposes. And um once this creates um we'll go through and we'll actually validate you always gotta check the check if your tables were created properly and the neat part here is that you didn't have to leave this tool and you won't have to leave this tool to see if what your data looks like um I um if you come from Athena and you know everyone likes the Athena on the left side of Athena where you have the where you can actually see all your data we're providing that here for you. And you can actually see now we have a new table. So, um, how I like to end this is, is that you know you might not want to use the AI piece and I think that's um that's that's a great um choice if you choose to make it but at the same time. Um, I like the idea of having the option to be able to, you know, build, have context and build on top of that context and then share whatever you're creating somewhere else. So this is actually I think one of the biggest pieces of the data notebook. It's not just. I'm working in cells now it's I'm working in cells along with the model I'm working in I'm working in cells and I can share this and I can actually have access to multiple data types. It's not that I'm doing, you know, I can work in cells it's also I'm working in cells and I have variables that I can rename dynamically. And then there's something else that Dia kind of highlighted but I did not include in the video is that you know, um, when notebooks have compute attached to them you can change the size of compute, you can add more cores and you can add more memory, but once you outgrow that and you need to go into the place where, um, you want to get where you might want to have, um, data that is bigger than the memory of a particular machine and you want to actually have the power of spark, you can move there. And you don't have to make any of these changes. uh, Dia mentioned that this, that, um, Spark comes up in only a couple of seconds. Uh, we were using Spark for the whole entire time and I know this is not in real time, but there was no delays anywhere in this demo for, hey, we're moving to Spark now now we have to change how we're operating. No, Spark was seamlessly included in this. Um, and through Spark Connect. And you know what, if you choose to say that I wanna work with Athena sequel, guess what? You can do that too, cell by cell by cell. So the whole point of this section is we really want to provide a notebook that not only works with you but it grows you into things that you weren't, um, thinking about doing before, so. As I'm going to summarize this section here, it's like it's really is work with all your data, you know, the idea is you have data, we want you to be able to work with it. I don't want you to work with it in a whole bunch of tools. I want you to be able to work it in one place and you know, use, we can use a data agent, uh, we are used to now, you know, we're using data, uh, we're using, uh, models to write code. Well, let's write code in a data context. Um, the data agent inside of there actually does understand data a little bit better. And then also, you know, now we can, um, now that we have all of our data sets in front of us, it does become easier to create and manage our data sets. So I'll move back over to Daya and he will bring us home. Yeah. Thank you, Brian. That was a really cool demo. Uh Let me, oh, sorry. OK. So everything that we saw. Demonstrates how Maya's experiences transformed. Transitioning from a collection of AS services. And, uh, how there are 3 steps here, right, to simplicity. To begin with, there was a collection of purpose-built ALO services, and Maya had to figure things out on her own to integrate the services, to access the services and make them work together. And then we launched the Universe Studio. With this, Maya had uh an ability to access all the services under one umbrella, all the capabilities. She didn't have to set anything up in the sense she didn't have to integrate them. So everything was integrated at one place. And then with all the new releases, the simplicity is taken to the next level, where she gets the Always on notebook that is surveillance, instant onboarding, seamless, uh, permission, uh, carrying forward, and Spark Connect. With, which is connected to Athena to give a superior interactivity as well as uh seamless scalability. So, it means, uh, let's take a look at the impact. Uh, we're trying to quantify the impact here. Onboarding time is down from hours or days to minutes. And then with the AI assistance, you can write faster code, better code, and become more productive. And notebook brings home everything together, so you don't have to switch across multiple tools. You can spend most of your time in the notebook instead of hopping across multiple tools. No idle compute clusters, so you pay only for your usage. And scaling from gigabytes to terabytes is seamless without writing any single line of code or uh without uh configuring or fiddling with your clusters. And your overall data pipeline development. can be done in a matter of a few hours instead of days that it, it used to take previously. There is significant improvement in uh uh the experience for our AWS customers. At the beginning, we asked this question. Why simplicity itself cannot be the innovation, so we work backwards from customers to remove complexity to make sure everything just works. We delivered Won't click on boarding Always on notebook that is available for you instantaneously with AI systems built in and seamless interactivity. With the spark backend and scalable spark cluster with one goal in mind. To make sure the creativity take the center stage. So talking about what next, so it's available today from uh AWS console. We highly recommend you all try it out. And then tomorrow at 10:00 a.m. in MGM Grand, there's a chalk talk. So if you want to dive deep, It's a great opportunity for you to do that. And uh we also highly recommend visiting uh the expo floor. Uh, we have a booth there, so you can, uh, get the demorals, you might try out, and get, to get hands-on experience. Finally, we are really excited about what we have built. And uh we can't wait to see what customers like you all can do with this. Happy to take any questions. We'll take them down