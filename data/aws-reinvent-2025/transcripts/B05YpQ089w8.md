---
video_id: B05YpQ089w8
video_url: https://www.youtube.com/watch?v=B05YpQ089w8
is_generated: False
is_translatable: True
---

- So this talk is a
deep dive into DynamoDB. Let me start by saying,
I realize you have lots of choices on where you could be. There's a whole bunch of talks. Thank you for coming to this one, and if you're watching
this online, likewise. Every year when I do this talk, one of the things which I think
about is why we do these talks. And the simple answer is,
it's for you, our customers. The thing which I like the
most about what I do is to work with all of you and see the things which you build using DynamoDB and in the past, with other
databases I've worked on. This year, I wanted to change
things up a little bit. In years past when I
did this presentation, it was block diagrams and
architecture and stuff like that. This year I figured we'll change it. We're gonna do the same things. We're gonna do the deep dive, but we're gonna do it from
the point of view a customer. And we're gonna take two actual situations with real customers, talk about an issue which they reported, and talk about the architecture through that. Wrong button. So quick word about myself. My name is Amrith. I'm a Senior Principal Engineer. I've been in the DynamoDB
team for about six years now. about 35 years in this business. The thing which I like the most know of the roles I've had is
working with customers. So if we can, after this
talk, if you have questions, please feel free to reach out. Let's dive right into it. So what is DynamoDB? It is a document and key value store, and the objective is that we give you
predictable low latency at any scale, right? Think about those two things. Predictable, low latency and any scale. We typically see DynamoDB
used in foundational services. What are foundational services? The things like your
login, the ability for you to connect to a system, inventory management,
like most of amazon.com. AWS runs on DynamoDB. Control plane of most
services run on DynamoDB. These are applications where latency and availability are paramount. Predictable, low latency
and very high availability. Well over five nines is what we achieve, and most requests are done
single digit milliseconds. The important thing is you
as a customer don't have to bother about all the
things which we do in order to deliver these things. You get data, you put
data and that's about it. It's fully managed for you at any scale. So whether you're doing 10
TPS or you're doing 10,000 TPS or we've got hundreds of customers who do 0.5 million RPS sustained, you'll get the same latency. We aim to give you the
same predictable latency. I've talked about scale several times. This is an Olympic swimming pool. Apparently this is in Barcelona. It is 50 meters by 25
meters by two meters. Now, most people in most parts of the world understand
2,500 cubic meters, but in the US we don't do things that way. We have to tell you how
big things are in terms of football fields or jumbo jets. (participants laugh) So for all of us here, this is 17,000 bathtubs,
metric bathtubs ostensibly. So let's assume you take all
the water out of the pool and you fill your 17,000 bathtubs, and grains of rice. Everyone knows how big that is. I'm not gonna tell you how much that is. For every request DynamoDB
serves, place one grain of rice in the now empty swimming pool. How many days do you think it's gonna take to fill
this swimming pool? Anyone? - [Participant] Less than one. - Less than one day. Okay, brave person there. Anyone else? In less than the time it takes for us to talk through this slide, you would've filled that swimming pool. That's an idea of the
scale at which we operate. Every time you go to amazon.com, put thing in a cart, search or use any of the many
applications which use DynamoDB, one grain of sand, one grain of rice, 61 seconds, you got a full swimming pool. So keep that in mind
when we talk about scale. And if you have any concern
whether your application will be a, whether we'll be able to serve your application scale, remember 17,000 bathtubs, one minute. All right. So we'll talk about the first of these two case studies. Like I'm firmly of the
opinion that life is too short not to not be having fun. So I hope you have some fun and learn something along the way as well. This is an actual customer incident. I'm not gonna name the customer, but it is something
which literally happened. The first time this problem
happened, we were as puzzled as you probably are gonna
be when you see this, but it's an interesting
way of understanding the architecture of DynamoDB. So this customer came to us and said their application was receiving unexpected write throttling. Now I should probably pause and say, how many of you
here have not used DynamoDB? Okay, so most of you have, so good. So this customer complained about unexpected write throttling. This was a, this was a national crisis. If this application didn't
work, this application had not yet gone into production, but it would literally have
been a national crisis. They were conducting a nationwide poll, and they needed this application to work. There had been an older
system, it used a legacy RDBMS and that system was fully
rewritten now to use DynamoDB. The old system needed only to do 600 TPS. We'll get into why in a second. But COVID came along and this old system which
involved people going to a counter and so on could no longer happen, so they said we're gonna have a self-service mobile application
and the mobile application has to serve 45,000 TPS. So it was going from
about 600 up to 45,000, which meant they had to do a full rewrite. So the customer said they
were getting write throttling. So the first question we
asked them was, you know, what throughput are you
getting write throttle at? 800 TPS. Now, 17,000 bathtubs in a minute, 800 TPS. There's something wrong with this picture. So we dug into it, we asked
them, what's your application? It's very straightforward application. And to understand this
application, I have, I'm gonna describe the application almost exactly the way in which it is. The most important thing to understand is anonymity was really
important in this application. So keep that in mind. So the workflow itself
is very straightforward. They're gonna read a table and they're gonna write three tables. That's it. All right, what are these tables? First one, a national ID table. The one on the left. This is not real data, this
is just synthetic data. I said anonymity was very important. There's a table which has random IDs, and the survey itself consists
of two tables, a survey table and the personally
identifiable information. Now, they're very careful about
keeping the data separate. So they came up with
these two randomized IDs. One's called a PiD and one's called a SiD. And the basic idea is that the PiD ties to the PII table and the SiD
ties to the survey table. Now, if somebody has
access to one of these, let's see if this worked
here, if you have access to the survey data, you can't directly get to the personally identifiable information because you would need
this randomized ID looker. And they made sure that not
many people have access to that. So if you need to, you can look at PII, you can look at survey but you can't ever put the two together. But they went one step further and they did not tie
it to the national ID. What they did was they did a hash of this combination and
made a low cardinality hash, which means that if you
did go from the random ID to the national ID, you would
get some number of people but you could never go in this direction. You could never say what did Jack, what was Jack's choice in the survey? So this was the whole
premise of the application. So a random ID, personally
identifiable information and the survey information and they're throttling at 800 TPS. So what did the application actually... How does the whole flow work? They mail something out to everybody, to all registered voters and and so on. And in the old system people would go into a physical location and prove who they were with,
you know, a driver's license or whatever, answer a bunch of
questions and they'd be done. But because of COVID they
didn't have the ability for people to go into a place. So they did this
application, the mobile app. So what does the app itself do? It generates the random ID, records the personally
identifiable information in the survey and it puts that one way hash
in the national ID table. Straightforward. One read, three writes. That's it. Everybody good so far? Okay. So some information here,
you know they got a random ID and that's gonna map to
some particular record here. Very straightforward. How can this application so simple possibly have write throttling? Like we were not able to see
any way this could happen. So this is a sophisticated customer, they did all the right things,
they've scaled their tables, you know, high cardinality keys
and all of that good stuff. So we went one step further. We asked them, tell us
about your schema and so on. Okay, so we get a little bit more detail. The national ID table
and the random ID table also have GSIs. GSIs are global secondary indexes. And the basic idea of
a GSI is it gives you another access pattern. Now, in this case, if you have a AiD, which
is this anonymous ID, get me the national ID, you'd use the GSI for that. Or if I have a SiD and I want to get the
PiD, I would use this GSI. On the other hand, the
primary key on the table will allow you to go from the
PiD to the SiD as an example. So simple application, one read three writes, this can't
possibly be happening. So next thing we ask them,
how big are your items? So maybe they're writing
one megabyte items and that's the reason they had a problem. Nope, small items. 500 bytes, two kilobytes, 400 bytes. Next question, is the ID really random? 'Cause that's the only
thing we could think of. So like small items. Properly scaled up tables. They didn't have like, you know, LSIs or any other things, why
would they possibly throttle? So we said, this ID random, and tell me more about your
random number generator for this random ID. And they said, oh, it's not
a random number generator. For various statutory reasons, this table is populated in
advance with random IDs. So they populated this table in advance, and then at runtime all they did was they would scan this table. So this is the mechanism
to come up with random IDs, this is the random number generator. This table is populated in advance. They verify that it's high
cardinality and truly random and all that. There's some microservice
which is gonna generate the random IDs, and what
it does, it goes and reads a whole bunch of rows at once. Maybe it reads a batch of a thousand rows and it records the last evaluated key. And then when you want an
ID, it'll give you one. That's basically how
this application works. And they verified high cardinality. And oh by the way, GSIs are non-unique. We don't enforce uniqueness. So they verified that
the SiD is also unique. That is their statutory requirement. Something is funky with this
random number generator, but it looked right to us. So we said okay fine. But we still had some, you know, nagging suspicions about it. So then the next question is
where is this write throttling? So obviously we thought, you know, write throttling must be on the table with the two kilobytes. Nope, it's on the smaller write. The 500 byte write. And so here's what we know so far, and they're throttling at 800 TPS. 60 days out, they need
to get to 45,000 TPS. So anybody out there have
an idea what's going wrong? Raise your hands if you
think you know what's wrong. They're throttling on the
PII table at about 800 TPS. Okay, I don't see any hands up yet. Good. We were in exactly the same boat. Oh, good. No, that's, no, I just want to know whether you're with me so far. If you have an idea, great. We'll see if you're right. All right. So let's talk. So we were puzzled. We did not understand
why this was happening. So here's how we scale. We scale horizontally. We partition your table. You give us a partition
key, we partition it. It's basically hash-based partitioning. When you create a table,
we ask you for the name of the table, what the
primary key for the table is and you know your credit
card number pretty much. That's all. And same thing with the GSI,
how are you gonna pay for it? But notice here, this is not unique. That's the only difference
between GSI and a table. So here's the national ID table. When they wanted to populate
this data, what would we do? We would compute a hash. Now, throughout this
presentation I show you hashes. These are not actual DynamoDB hashes, this is just an MD5 actually. So we compute a hash, and then we sort that data
based on the hash value. And having sorted it on the hash value, we create demarcations and
we call those partitions. That's pretty much how DynamoDB works. Horizontal scale, hash-based partitioning,
fairly straightforward. Okay. So a partition is a
contiguous range of hashes. All right? So we compute the hash of the partition key,
sort based on the hash. Contiguous ranges become a
partition and store those on storage notes. That's pretty much how we work. So the hash which we use. A couple of the properties of hash, of course it
has to be a fast hash. We do this, what is that? 17,000 bathtubs in a minute. So we have to do this hash fairly often, and the hash has to be even
in the sense you can't have a hash which is gonna produce
the same value at a higher or some values at a higher
frequency than others. Two other properties of our hash, hash has to be deterministic of course. And there's this property
called avalanche. So let's talk about these two. The deterministic hash basically says, irrespective of which
table, who the user is, hash effects equals
hash of Y if X equals Y. Now, it doesn't have to be for
every table, it doesn't have to be for every user. That's just the way in
which we implemented it. It turns out. Keep that in mind. The other property I
mentioned is avalanche, which basically says that if
you have some value, you know, that if you make a small
change to the value, you should have a large and unpredictable change in the hash. So notice, I make a very
small change to the value, you get this very large
change to the hash. So those are the characteristics of the hash function we use. And it's worked great, you know. Over a decade we've never had a problem with this hash function. So here's a look at how the
random ID table actually look. There's a PiD and a SiD,
we compute the hash values, we sort the data based on the hash value and we store it in partitions. That's pretty much what DynamoDB
does day in and day out. And when the customer has
a request, they do a scan, and a scan walks the table
in the order of the hashes. Okay? So when we, and then we
return last evaluated key, which is the actual
attributes in the table. So the next time you come back
and you make another request and you specify the start point, we'll continue the scan
from the same place. That's basically how a scan works. Anybody have ideas now about
what may be going wrong? So here's how the random number
generator appears to work. They're scanning down the table. What they appear to see is this data. Let's look at that data. This is what the random
number generator is returning. Looks perfectly random to me. What isn't apparent is that
it's actually not very random because it is in the order of the hash. Okay? So now they go and write the survey table. This is not the table
which was throttling. So as they're reading data
from the random ID table and they're writing to the SiD, it's gonna go all over the place, which is really good because this is the right pattern
which they're gonna see. This is how you get horizontal scale. You've partitioned your table into, in this case I think I
have eight eight partitions, and I continuously write, but the traffic is distributed
across multiple partitions. Let's look at the other one. I'm reading on the value of PiD, implicitly I'm getting
it in the hash order, therefore the data is being
written into the PII table, which is also on the same key. Therefore the write is
not randomly distributing across the entire table. (Amrith clears throat) And for the first block of some number of random IDs,
you go hammer one partition, then you switch to the next partition, you hammer that partition. So you're not getting the
benefit of horizontal scale. And the reason for this of course is this is the
unintended consequence of our deterministic hash. No matter which table,
no matter who the user, no matter which region,
the hash value is the same. Now, let me also say
we intend to fix this. So don't take a dependency on that, because you will probably be hurt later. When you are scanning a table, when you perform a query operation and you're getting multiple items back in an item collection,
remember that the data may appear random to you but it's coming back to
you in sorted hash order. So now how do we fix this? So of course the first thing
which we told the customer is get yourself a real
random number generator. And their answer was,
yeah, no, can't do that. Statutory requirement, random numbers have to be populated upfront. Okay. So 60 days out before
this thing has to work, they were also not willing to make huge changes to the application. So one of my colleagues came
up with this clever idea where he said, what we really need to do is that you are gonna continue
to read down this table. This is your random number generator. We're not gonna change that, but we want to generate, we want to distribute the right
traffic on the PII table. We don't want it to go
hammer one partition, then hammer next partition and so on. So what we came up with was we just said, when you're writing to the other
table, how about you prefix the ID with some two characters? Like we said, use TX in front of it. So the reason for this,
you make a small change, it changes the hash value a lot. So now as you read down the table, the traffic is getting
distributed all over on the PII table. So now we're back to distributing the load across multiple partitions
and we're in a good place. So this team was asked
to benchmark the system for 45,000 TPS. I think they happily
benchmarked it at 90,000, and the survey went off without a hitch. So thing to remember is the way we scale is through hash partitioning. If your traffic has items
which are like on a GSI as an example, if you have
a large number of items in a table with the
same value for the GSI, you're gonna get poor performance on that GSI. High cardinality and uniqueness
are really important. And if you're ever in a situation where you're reading one table and writing to another table
which shares the same key, remember, you could be susceptible to this problem till we fix it. So it's gonna take a
while for us to fix this, but we do hope to fix it pretty soon. So benchmark was a success. The survey was conducted
and life was good. So that was the first one. And when this happened, we literally had never
realized this problem. But now that we know this problem, this is the way Murphy works. Once you know there's a
problem, you'd run into it. Every now and again, every
couple of weeks we hear of some customer who says, oh
my god, here on fire crisis, my application is throttling. Turns out to be the same problem. So if you are building an
application, keep this in mind. It's a pattern which can
give you sleepless nights. So to you, the key takeaways are, we have consistent hashing. This could create a problem for you. This is the way DynamoDB works today. Hopefully we'll fix it soon, and probably, yeah, I'd say certainly a
couple of times a week we hear, or a couple of weeks we hear this. And it is easy for you
to fix if you're able to use the power of the avalanche
and make a small change. So if you're reading
one table, as you write to the other table, don't
choose the same key value and you won't have this problem. So debugging throttling
used to be very hard. When we ran into this
problem for the first time, it was about four and change years ago. CloudWatch Contributor Insights is literally the only way to do it, but it used to be very expensive, and when we told customers we enable CloudWatch Contributor
Insights, they would say it's too expensive, can't do that, and they can't enable it
once the problem happens. You need to have it when
the problem's happening. So we introduced throttled keys only, which means we only log
when there's throttling, you pay substantially less. The other thing which we did was when you do get throttling, we return enhanced error codes. There's, these are backward-compatible. You don't have to make any
change to your application. The exception just identifies
the exact resource and why. So in the case of write throttling, you sometimes will get
throttled if your GSI is generating back pressure, it'll identify which GSI for you as well. So if you do face throttling,
keep that in mind. There's 10 new metrics which we now have in CloudWatch Contributor
Insights with this. One thing which I will
mention very briefly. I said partition key and sort key in the past, like up until a week ago
or so, a partition key and a sort key were a single attribute, which meant that if you
had a complex schema, it became a little bit
hard for you to model that. So we had customers who needed
to do these funny things, like this hack where you put
a customer ID and an order ID and so on and then use
begins with and contains and so on, on the sort key, all very hack-y. And also, if you want to have the customer ID in the key and you want to store it as
an attribute unto itself, there's a problem, you're
paying twice for the storage, and the two of them can get out of sync. The problem with redundancy is always you could have
conflict between it. There's correctness issues. If you didn't and you did
not want to pay the price and have redundancy,
it became hard for you to evolve your schema over time. Because if you need to build
another GSI as an example and you wanted to use
order ID, now you have to go backfill it, pull it out
from the middle of this key. Became a pain. So as of, I think this was a
week ago, we launched this. You can now have up to four
attributes in your partition key and your sort key, makes for substantially
easier data modeling. Right now this is supported for GSIs only and in the future we'll see
whether we do this for tables. The links there are to our documentation. Of course if you want to see Alex's LinkedIn post,
you can see that as well. We also now have MCP
support for schema modeling. So if you are so inclined, we have a very good schema modeling advisor, which will take the benefit
of these new capabilities. So with that, let's
move on to the next one. And again, this is, it's similar, this too is a customer-reported problem. So here is a view of a
tool we use internally. It's called (indistinct) internally. If you call support, one of the things which they ask
you is for your subscriber ID, and with your subscriber
ID, we can look at some of this kind of performance
metrics about your application. So here's data for seven
days for an actual, for a real customer. That's seven days worth of data. That's the traffic history. And this shows reads and writes on, reads on the left, writes on the right. And the peak to trough is about 2,000 X. Now, this particular
application once a day runs some job which drives
some bad job, which runs a fairly high rate relative
to the rest of the day. So this customer application,
this is the perfectly, this is how the application
is supposed to work. This is not an anomaly, this
is literally what they want. And their issue was, this is the continuous interactive workload, which is the low traffic period. And then these are the spikes, which is the batch part of their workload. Now, this is the peculiar
thing about this application. When the traffic is low here, their latency is high and variable. And when the traffic is
high, the latency is low. Very acceptable, but
also just a flat line. Predictable low latency. Now, this is completely counterintuitive. Normally you would
assume that high traffic, you have high latency. Nope. High traffic, very good latency. Low traffic, horrible latency. Have any chance one of
you has run into this? Ever seen this before? No. I know there's one customer who has. They're here, but this is literally data for their application. Now... I can't see what it says there. Oh, this is the average. And in case you're interested, we do store other metrics as well. We store P90, we store P99. This is the P 90th percentile. Same exact pattern. So one of the things which we did was to try and understand
this, built a simulator so that we could simulate this in the lab and we were able to do that. High traffic, low acceptable latency, low traffic, horrible latency. Peculiar situation. So to understand what this is doing, what this application
is doing, by the way, these are, those are the request types. That's get item and put item, which aren't used here Next day, same behavior. Okay. Anybody have ideas on
what's probably going wrong? Just show of hands if you
understand what it is. Yes. Anybody? Good? All right, so we asked them, what's your application look like? And this is more of a description
of the simulator we built, but they have the continuous
traffic, they have the batch. The batch is the one which
drives the high traffic, and they've got this fleet of instances. In our simulation we had 150 hosts, which we were driving traffic through. And that's DynamoDB behind,
very straightforward. What could possibly be going wrong? How is... How can you have a situation where you have high traffic
and you have good latency and you have low traffic and
you got horrible latency? So to understand why this happens, let's dig into the
architecture of DynamoDB. This is the application, it
connects to us through an SDK. Resolve the public endpoint. You get pointed to a load balancer. Behind the load balancer, and by the way, load balancers are in multiple AZs. (Amrith coughs) Behind the load balancer,
we have request routers. Every request which you make needs to be individually
authenticated and authorized. Are you who you claim to be? Is your SIG B four signature correct? Are you permitted to do what
it is you're trying to do? Every single request, we have to do that. To do that, we look up some metadata, we rate-limit you if you are going over whatever your provision limit is or if your table is not able to scale. Your data is actually
stored on storage nodes, and the request router's job is to figure out which storage node it should send the request to. And down on the storage nodes, all data is encrypted at rest. So we need to go to KMS to
get the decryption keys. And again, there's rate limiting on the storage nodes as well. So these are the throttling limits which we talked about in the previous one. Now, we have to do this stuff stuff, what did we say? 17,000 bathtubs a minute, right? So we use extensive caching
in all of these things. So the request router is
gonna maintain caches, it's gonna cache
metadata, it's gonna cache your identity information. Your TCP connection from
your application goes from your application to a request router. So as long as you use
the same connection over and over again, you get
the benefit of this cache. If you go one request here,
one request there, eh, you may not get the cash benefits. So we do this hundreds of
millions of times a second, and it is physically not a good thing for you if you're gonna be
reconnecting on every request, because you have to go through
the entire TCP socket setup, you have to go through TLS. Okay. Why am I telling you all of this? And of course there's
authentication, there's caching for authentication, which
you have to keep in mind. And all of the metadata
which we're gonna use, data is on the storage nodes. We've got encryption keys to deal with. And in your application,
you have connection pools. So going back to where we were, if you managed to get to
the same request router over and over again, you get the
benefit of these caches. If not, you don't. So wherever possible, try and
use long-lived connections. In the case of this
particular application... in the case of this
particular application, the issue they were having was they were not getting
the benefit of the cache. So if their large traffic used
this large fleet of hosts, when they were doing
their continuous traffic, when the batch was not running, that was almost 2,000 times less traffic. So our hypothesis was that they're not getting
the benefit of the cache because of the number of hosts which were
involved on their side. So we asked them how many hosts they had. For the purpose of my
simulation, I had 150. So I'll just stick with that. I think the customer said they had a few thousand containers,
don't quote me on this. And the bulk was 20,000
TPS and continuous was 10. This was the numbers which
I said, you know, 2,000 X. The tables, these thousands of hosts or thousands of containers,
they're provisioned this way because they're statically stable. And statically stable
basically says that you want to be able to continue to serve whatever traffic without needing to make a control plane change. God forbid you're not
able to scale up in time, you're not gonna be able
to serve the traffic. So they're scaled to peak all the time. So that's the requirement. And maybe they want to accept
bulk injection at any time, maybe it's not once a day. So one option is reduce
the host during continuous, during the interactive time. That is not something which
they were willing to do, and also they're in a regulated industry. It'll take a while for them
to actually try it out. So our simulation was this. High traffic, low latency. Low traffic, high end variable latency. We were able to recreate
this in the lab perfectly. And so we... Oh, if you wanna take a picture. So what we did was to reconfigure it so that the continuous
interactive traffic only went to a small number of hosts. That's the point where we
reconfigured it flat line, because now they're getting the benefit of all the caches they need. So we talked about, so
that's the configuration, which gives them the good performance. Now, these are two of the things which they
could do if they wanted to save money, run that at low traffic and scale up when they
have higher traffic, but this is not statically stable. So while auto-scaling
would save them money, it does leave them with an
application, which is bimodal. So I believe that the
configuration they have is the right one to run always with the same number of hosts. But the recommendation we made to them was that they should send the
low traffic to a small number of hosts and not to all the hosts. In our simulation certainly, we showed that that would be beneficial. So I'm gonna wrap up by
saying, I've built applications with databases for several years, and it is always important to understand how the database actually works. Not knowing how the database
works means your ability to build an application
is substantially less. Understanding what goes on
inside is really important. And equally, anybody can
go to a hardware store, anybody can buy paint, but it takes a certain amount of thought and some human creativity
to come up with art. What our customers build,
what you're building are some really hard
complicated applications which take a fair amount of thought. So we're here to help you and help you build these applications, and please keep the feedback coming. Thanks a lot. (audience clapping)