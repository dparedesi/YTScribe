---
video_id: 6T7HkQ5OsKg
video_url: https://www.youtube.com/watch?v=6T7HkQ5OsKg
is_generated: False
is_translatable: True
---

Hey everyone. My name is Farah Elbe. I'm a product lead for Amazon Nova, and I'm really excited to talk to you today about Amazon Nova 2. I'm joined here with my colleague Ryan Holm, who leads the Applied AI Solutions architecture team at Amazon. And we've got Abone Kura, who also joins us from Zendesk, and he leads the AI platform teams. So we're gonna quickly go and give you an introduction on Amazon Nova 2, and then we're gonna dive a bit deeper into some of the capabilities, some of the results that we've seen. Then Ryan's gonna go deeper into real world evaluation, some of the key use cases that we've seen customers have success with. And then Abuna is going to then dive deeper into his success that he's had with Amazon Nova. So Amazon has been, has been shipping AI innovations, you know, for decades. And at each critical inflection point, we've been there. Building industry leading technologies for our business and our customers. So, for example, starting at rule-based and decision systems, we've built, you know, early recommendation systems. And then when you move to things like classical ML we do things like inventory forecasting. And then if you move to early language models, we, you know, built many models for products like Alexa, you know, speech recognition, natural language processing and understanding. And then moving on to foundation models with Amazon Nova, and then, last but not least, agents with, you know, Quiro, Nova Act, and so on. So, we've really been innovating in this space for the last few decades. And to bring these innovations to you. We ship at each layer of the stack, from custom compute to turnkey solutions. So starting from, you know, the bottom, we've got our compute, Oedipus Trainium, Inferentia, and then we've got the, the tools that allow you to train these models and process the data. And then we have like the applications, agents and models, you know, Amazon Bedrock, Amazon Nova, so then you can build on top of that. And then we have our kind of vertical solutions, our turnkey solutions like Amazon Connect, you know, software development, we've got Quiro that you've heard about, as well as Edbus Transform for migration and modernization, and QuickSuite for business productivity, as well as the Oedipuss Marketplace. And then on top of that we've got our specialized expertise. So with, for example, the Gene Innovation Centre, they work with you on proof of concepts, really help you get the most out of, you know, your Gen AI applications or the outcomes that you're looking for. And then we have our broader partner network who also work with customers to enable them to get the best out of Eddie B. So, OK, let's zoom in a bit. We have Amazon Bedrock, a comprehensive service for generative AI application and agent development. Bedrock offers access to the leading foundation models and tools to enable you to build your DAI applications. You can customize models and applications with your data, apply safety guard rails and optimize cost and latency, and rapidly iterate. OK, let's jump into Amazon Nova. So we've got thousands of customers across many industries using Amazonova to build AI solutions today. And you know, since we launched this time last year, we've seen lots of, you know, great successful stories, and here's a selection of customers that use Amazonova today. And as we spoke to those customers, they told us, you know, one. They wanted more intelligence. They wanted smarter models for those more complex tasks. Then they wanted more modalities. There's a lot of different models out there that offer different capabilities and you know you have to kind of chain these things together. Customers really didn't like that. And you know, Gen AI is great, it solves a lot of problems, but like how do you scale it? So they wanted lower costs and, you know, whilst also being, you know, faster, scalable in the production environment. And for our more advanced customers, they wanted more options for customization. You know, where you need to go beyond context engineering, you need to do fine tuning or even go deeper into model training, into the pre-training side of things to get the most out of models and, and deliver the experience that you're looking for. So I'm really excited to introduce, as Matt mentioned in a keynote, Amazon Nova 2, our next generation models that deliver industry-leading price performance across reasoning, multimodal, um, use cases, as well as conversational AI. So today we announced 4 additional models to make up the Nova 2 model family. So starting with Amazon Nova 2 Lite, this is our fast, cost-effective reasoning model for your everyday workloads, your general NLP tasks, your routing needs, your classification use cases, etc. Then you have Nova 2 Pro. This is our most intelligent reasoning model for those more highly complex tasks. Think agentic coding, think multi-step agentic workflows. And then we have Nova to Omni. This is the industry's first unified multimodal reasoning model. I was saying earlier, this model can handle all modalities on the input whilst also generating text and images in industry first. And then we have Amazon over to Sonic, our next generation speech to speech model for your real-time conversational AI needs. So let's dive deeper into Nova 2 Lite and Pro. So here's a few key attributes that I wanna call out. So Nova 2 Lite is available, generally available today. Nova 2 Pro is in preview, um, early access for our NovaForge customers, which Ryan will cover a little bit later about a little bit about NovaForge. And you can see both models support long context, you know, we. We see a lot of applications, especially agentic workflows where, you know, there's agents that use compress um compression, context compression techniques that don't, you know, quite keep all the necessary context to make the experience delightful. So we're ensuring that our models support longer context. These um models support, you know, up to 200 languages. And as I mentioned earlier, Nova um Nova 2 Lite and Pro support multiple input modalities, as you can see on the screen. And Nova 2 Pro actually supports, in addition to all the other modalities that light supports, it supports speech as input. And on Nova 14 customization, Nova 2 Lite is available, and for Nova 2 it will be coming soon. So these models represent a significant leap in intelligence from the previous existing Nova family. For example, Nova 2 Lite outperforms Nova Premiere, our previous flagship on multi-step reasoning and agentic workflows. So you can see here, you know, Nova Premiere was our kind of flagship model um earlier this year, and Nova 2 Lite, our cost-effective reasoning model is, you know, winning in a number of areas. And it does this whilst being 7 times lower in cost and up to 5 times faster. So it's quite significant in terms of where we've come from the original Nova family. But, you know, the key question is how do these models compare to other models in their similar intelligence tiers? So here you can see Nova 2 Lite delivers incredible price performance. For many workloads, we see customers looking to deliver in production today. Nov 2 Lite excels in areas like instruction following, tool calling, generating code, extracting information from documents, and often matching or exceeding performance of comparable models. It compares favorably in industry benchmarks to the likes of Haiku 4.5, GBT5 Mini, as well as Gemini 2.5 Flash. And Nova 2 Pro, our most intelligent reasoning model, is great for, as I said, your highly complex needs, in particular when you look at some of the more important areas like instruction following again and extracting documents and tool use, you can see it's one of the leading models when compared to models in a similar intelligence tier. And not only that, these models are incredibly fast and cost effective. So you can see here, you know, Nova 2 Light is in what is what artificial analysts like to call the magic quadrant. And uh it's, you know, it's. Achieving over 220 output tokens per second. And it's um in terms of input output price, you can see it's you know far on the left side, so we're not only trying to give you intelligent models, we're trying to make them scalable for you in production. OK, so next I want to cover some of the key capabilities of these Nova 2 models. These include things like developer controls, multi-step reasoning, native tool use, built-in tools, and extended context. We'll jump into each one by one. Let's start with developer controls. So with Nova 2, we wanted to give developers control. The ability to toggle reasoning on or off, adjust how much the model thinks based on, you know, the task. So in terms of the uh a dial that you can control from low, medium or high, and you can tune this to a specific use case. And this enables you to manage, you know, latency and control your costs. So you've got multiple options, you can turn reasoning off or you can use reasoning at different levels to really tune in what you need to ensure that you get the right price performance for your use case. So, OK, in, in this demo I'm gonna actually show you an example. So first I'll show you with reasoning off, and then I'll show you with low reasoning. Here, I've actually attached in this user interface, um, two documents around 70 pages in length. And your papers where I want to see if these, if the model's able to determine if it supports the hypothesis that I'll have on the screen. So here you can see it's processing the document, and this is the hypothesis I put and I'm asking like whether or not this er these documents validate or invalidate. So you can see here the model says supported, but that's actually the wrong answer. Um, it's actually meant to be not supported because one of the papers actually disagrees with the hypothesis. So let's take a look. But this time we'll put um you'll see a toggle on the bottom and I'll speak about this uh chat interface later in the session. You can see we've set it to low, and now we're going through the same exact prompt, same exact documents. And you see this time, it was able to reason through and actually picked out the missing mechanism at the bottom. So it's able to reason through those 70 pages and effectively use this new reasoning extended thinking capability to get the answer right this time. So let's move on to multi-step reasoning. We hear this a lot, but it's really critical that we enable our models to be able to take these kind of complex tasks and break them down. We really optimize Nova for this kind of capability. Nova's able to look at a problem, reason through it, break it down, and then call the right tools in the right order, so. We've seen, you know, customer support type use cases, whereas any assistant interactions where you have multiple, um, um, tasks that the agent needs to do and it's able to kind of reason through all of that. And it's able to detect failures and course correct along the way as well. So in this example, a Nova 2 powered agent updates a repository to enable support for Nova's new extended reasoning capability. So this means Sorry, let me just pop my. So this means um we're reasoning through planning, executing a full workflow, including executing multiple tools. OK. So in this, in this in this situation we're running an agent using Novo and Agent Core with Strands. And Strands is the genic framework for building multi-step workflows, and Agent Core is the fully managed runtime that hosts and orchestrates these agents at scale. Our agent is able to navigate through, analyze the issue. Generating the right code and files, and you can see he's making multiple tool calls. And then it's able to put a comment to say that the actual uh merger was complete. So it's able to essentially use, you know, be plugged into an ergentic framework and then effectively reason and and plan and deliver that specific uh use case. So it's really critical for models to handle tools effectively, enable models to access external knowledge or take action. Nova has been optimized for tool use, offering higher reliability and accuracy. It can take calls in a sequence, or it can do parallel tool calls. Offers built-in tools such as a code interpreter, where you can use Python to do complex math calculations, as well as web grounding to access external information, and this is us orchestrating all of this on your behalf, so you just simply turn it on in the API and you're able to get these two additional tools available to the model and use it for your use case. So here's an example of a query that involves math. We also enable code interpreter in this query, you can see the user prompt and then the tool config, it's really simple just to enable it. And you can see here the model chooses to use the tool, generates the Python code to perform the calculation, gets the correct result. And then tools like this, especially in this context, it's great to improve accuracy, really lifting. The overall performance, we know that LLMs are really not great at maths, so these kind of additional tools really help um improve performance. And lastly, we have long context. So longer this is is critical, as I mentioned earlier, for things like document processing, you know, if you have 10s, 20, hundreds of documents, 100, 100 page documents, they could really consume a lot, a large amount of context. And the same goes for things like video as well, this can take up to, you know, 90 minutes of video, which is, you know, pretty long, a larger, um, video file. And is able to reason over it, understand it, and so on and so forth. So we'll continue to work on innovating in this space, try to extend the context so that you can, you know, build these complex agentic workflows and, and, uh, leverage, uh, these capabilities. So here's a great use case where long convex becomes useful. Here Nova 2 is building a web app for a real estate agent. Using Klein, we give it a prompt to build the app. And you can see it's generating code and the multiple files needed to make the app functional. And once complete, we can then launch the app, so you can see. You know, created a plan and it went through and it's now generated a code. And then once er once complete, it will er launch the app, so this is completed now, it'll run the command to actually launch the app. And then we should see the resulting webpage. You can see, you know, this is the kind of the first iteration, you know, it's created like a basic CRM system, property listings, you know, pretty good job on a first class, but what's powerful is that you can keep iterating multi-turn and using that long context to really get the outcome that you're looking for. So here are a couple of customers who have tried Nova 2 that I was just gonna walk through. So Siemens would have been trying to use it to improve search. And they've said that Nova 2 Lite is up to 3 times faster than other models while still offering the high quality responses that this specific application needs. So again, the, you know, the speed and cost efficiency really comes to play here. And next we have Trellix. They've been using our Nova 1 family, and they specifically were using Nova Nova 1 Lite. And for them, Nova 2 no longer has any failures in tool calling. And they have a 39% improvement in threat classification and over 3 times more detailed responses with better technical analysis. So again, it's really great to see how these models are kind of moving from the first generation, customers are continuing to get value and um and, and, you know, scale these things into production. I'm gonna hand it over to Ryan now, who will talk a bit more about real world use cases. Awesome, thanks for that. Can you guys hear me OK? Awesome. Cool. Yeah, so I'm Ryan. I lead a solutions architecture team within the, uh, the, our AGI organization. Um, been working on the Nova model family for the last 2 years. One of the things we learned, and I hear a lot from customers is that, you know, a lot of these model providers, us included, last year, we, you know, we worked pretty well at the benchmarks, but then when people try to use these things on some of the problems that mattered the most to them, model didn't always work or it took a lot of extra work in order to make it work for those use cases. And so we started this effort to really kind of get a set of evals that was outside of um the academic benchmarks that you see on the marketing slides and really modeled after the kinds of use cases that you guys are all building for, for your enterprises. And so I'm gonna talk a little bit about our mental model for that and we'll talk through an example here. Um, so I kinda talked to most of this, but, um, we start by just really working with our customers. A lot of these are internal to Amazon, but also folks, maybe some here in the room. Um, just to understand that use case, um, and I'll talk a little bit about what that means, but what are they, what, you know, what are you trying to accomplish with the model? Um, then we build eval sets. We either, you know, get data from our customer, our partner, we procure it, um, and then we run these things continuously every, every day, sometimes every night, once a week, once a month, depending on the eval set. We run these to see how the models that we're building are performing, um. And then we use that to, you know, improve the, the data that we train with, as well as the, the, the prompts um that we, you know, use for the applications that we're building ourselves or helping our customers build. Um, so we'll, we'll dig in here, um, and talk a little bit about the, the process. Um, again, I, I had the opportunity to talk to, you know, dozens or hundreds of customers over the last couple of years. And a lot of people don't necessarily know how to do this. Um, so I, I kinda mentioned we, we, we start by understanding what that business problem is. Um, what are the dimensions that matter? What's the rubric that the subject matter expert that you're building that application with cares about? What are the dimensions over which the, the data will vary, um, from the user base? Um, maybe there's 4 or 5 different categories within the prompts that are going into the system. Maybe there's 10 or 15 different tools that need to be used. All those things are really critical that you understand at the kind of outset here. Um, then we either collect prompt and prompt and response data, we call that traces. We collect that data. Um, in some cases, we use the understanding of the business problem in order to synthesize that data or procure it from a vendor. Um, and then we, we run those evalves, if you will, um, and we analyze, uh, we analyze the output of the model, um, and code the failure modes. We'll talk about this in a little bit more detail here on the next slide. Um, and then we measure that by building metrics, um, as often as we can. We use programmatic metrics to do that. Um, other techniques, LLM as judged, everybody's heard about. Um, and then as we, as we, you know, get those metrics instrumented properly, um, we continue to refine the data, the metrics, and the prompts that we're, that we're using, um, uh, in order to kind of run these emails. Um, so we did this, uh, you know, on many, many different use cases over the course of the last couple of years, um, but we, I'll just talk about three here. Um, one that we see every day is classification. Can I classify an email? Can I classify a customer service case that came in? Uh, Abby's gonna talk about one in a few minutes that's important to their business. Um, super, super common, uh, use case. Every business that I've talked to, um, has that. The second one that we, we, we really zoomed in on this past year forra mentioned. Um, can we really understand what's going on inside of documents and specifically can we extract the content from that document in a structured way that is useful to your business processes? Um, and so we'll talk about that, uh, and then the last one is agentic workflows. Um, can we actually make agents that work for business processes that matter to you so that you guys can, um, use humans for high judgment tasks instead of the tasks that are easy to automate. Um, so we built, again, we, in each one of those cases, we have anywhere from 2 to 10 different customer eval, uh, eval sets that, um, that we are running on that kind of continuous basis in order to make sure these models are starting to generalize and, and, and work outside of what you see on the benchmark slides. Um, so talk a little bit about kind of how we do the error analysis part. This is the part that I get the most, the most questions about. Um, so we take a, a sim a very simple methodology. Hopefully many of you are already doing this, um, but we try to apply it with very, very high standards. Um, so simply go through and read these prompts and these responses. Um, study them in detail. A lot of people just look at the end result and look at, you know, did I get the answer right? Did I get the answer wrong? Did I agree, uh, with the thumbs up? Did I agree with the thumbs down? You gotta look deeper than that. Look at what happened, um, compare the model output with what a subject matter expert would say is the right answer, and then notate what are the differences. Why are those differences there? We'll talk about an example of what I mean. There's some really nuanced things that come out of these models, um. Three, you, you know, highlight what, what are those errors, um, in detail. Take notes, long form notes, little notes in the margins. What are those notes? What are those common, the commentary on kind of what the model got wrong and why it was wrong. Those will help you ultimately code these failure modes into buckets that will help you improve your prompts, improve data if you're fine tuning your models, um, and, and ultimately make your system better, optimize your prompts better, and, and continuously improve your systems. Um, and we do this for at least 100 traces just to make sure we have statistical significance. For some systems, it has to be more than that. If, you know, if you don't have, uh, if you have a really, really broad distribution of your, of your underlying data sets, but Um, at least 100 traces, we try to get through that, bucket those failures, and you learn just a ton. Your prompts will get better really, really quickly. And if you're building custom models like, like we are, obviously, we're building foundation models, um, it helps you really make sure you're generating data and choosing data mixes that will help you make the, make the models better. Um, so I'll talk through an example. I don't know how I'm doing on time here. Um, I think I'm OK. Um, so what you see here on the screen is a, it's a pretty simple prompt. It's a little bit contrived. I simplified it so it fit on the screen, um, but probably everybody in the room has done something like this. Can I, can I take a product? Can I take a reference product? And can I determine, are they the same? You know, there's 5 categories there, um. These you know business rules, um, are kind of described semantically in a way that a human could make this decision reasonably, I think, um, and then I give a very specific output format. um, now I've confused the model a little bit here you'll see where the mistakes are and you're probably already guessing where they're gonna be, um, but you can see I just want a simple explanation for, you know what, why is the model making the decision it's making. Um, and then, uh, you know, give an answer in, in, uh, JSON format, you know, everybody's done this. Everybody, every customer I've talked to has done this. Um, so let's see how the, how the model did, um, so you can see, uh, right off the bat, uh, you get a response, um, it sort of looks right, but it's not. Um, so you can see, instead of giving the rationale first, it gave the answer. Um, sometimes these models are jumpy and they like to answer first. Um, uh, I know some people that like to answer first and explain later too. Um. Um, but you can see it, it got the wrong answer, right? It gave the classification first and it's the wrong answer. Um, now, it followed the format that I used in the prompt but not in the output format. Um, now, that's a bad prompt on my part. Um, I'll fix that. Um, but then you can see, you know, they repeated that wrong answer within the JSON, but interestingly, the, the rationale is there. Um, so the rationale came after the answer. You're like, oh wow, that's interesting. And you can see the rationale actually makes it sound like the answer is correct. It says, you know, Brand Y has a different scent profile than Brand X, and so these products are incompatible. Well, a reasonable human wouldn't make that judgment based on the description on the screen, but the model kind of backed into the answer in this case. And, you know, it's surprising, but you do see this from most models. This, this kind of behavior can happen in, in certain situations and, um, and so I learned a lot about how to improve the prompt in this particular case. And so. Um, you know, taking that methodology I described a step forward, you know, we coded these, these out, bucketed these things into buckets. And started to figure out how do I measure these things against these, I think on that classification example I shared, we have about 8 or 10 test sets that are out of sample of what the science team uses. And uh we built simple metrics. Um, these are overly simplified here. The, the real ones we use are a little bit more complex, but we built very simple programmatic metrics. No element is judged for this use case. We had ground truth um that was annotated by humans, but we built a bunch of additional simple metrics in order to measure the failure mode the model was exhibiting, um, and the result is really awesome, I think. I mean, I'm, I'm super excited. What you see here on the screen is those 3 use cases I described. Um, and the, the, the blue bar is Nova 1 Pro. So at the time, our best model. Um, the middle bar is Nova 2 Lite, which is our mid-tier model now. So we want our, our mid-tier models now beaten our best model quite a bit in these three cases. And then on the right, the pink bar is, uh, Nova 2 Pro, um, which is in kind of a gated preview mode right now. Um. And so we're really excited this methodology works. These are all out of sample. None of these are in any of the, you know, public benchmarks that you read about or hear about. Um, and so we're really excited about, you know, the fact that we're starting to get hopefully more generalized for the types of use cases that, that, that you guys need. But we recognize that, you know, a lot of times your, your, your, you know, you, you guys need to make the models really, really good at tasks that are important to your business. And so, earlier today we announced Nova Forge, which is really the ability to build custom models um in a very, very tailored way for your organization. There's a talk tomorrow. My colleague Karen, um, is, is speaking with a customer on Nova Forge. You should go check out that talk. I don't know the exact time, noon-ish tomorrow. Um, but check that out. It's a, it's a great, um, set of, of, of tools that will let you build very specific models, either foundation models for your organization or very awesome task models that are really tuned for the types of tasks that you're doing. Um, it has all the techniques, data mixing, reinforcement, fine tuning. Uh, SFT continued pre-training, um, everything you need in order to kind of get started building, uh, building custom models, hopefully without all of the undifferentiated heavy lifting you have to do if you do it yourself. Um, but the results are also good. So we took that classification use case that I shared in the previous slide and you can see, you know, the, I, I should have had Nova Pro on the left, but, uh, you can see Nova Light 2 was 7% absolute over baseline improvement. And then when we actually went and, and fine tuned that using NovaForge um with like annotated reasoning traces, we were able to get a 21% lift over baseline. So it's really compelling results and particularly in these classification use cases, as you'll hear from Abby in a second, can have a huge business impact uh if you have higher precision in, in some of these class classification use cases, so. Um, that's all I had. I'm gonna pass it off to Abby, um, and I'd love to take questions after the, after the talk from any of you that may have them, so. Thanks, Abby. Thanks, Ryan. Um, hey, everyone. Um, my name is Abby. I head up the AI platform group over at Zendesk. Uh, I run engineering teams which build out the core AI platform, run evalves, and build customer-centric services. Um, hope all of you have been enjoying Reinvent so far and have really enjoyed the Nova launches. I definitely have. Um, for people who don't know Zendesk, at Zendesk, our mission is core and simple, is to make customer service radically better by building intuitive products powered by AI. And today, I'll walk you through what we're doing at Zendesk, how we're partnering with AWS, how we're partnering with NA Models, how we're partnering with Ryan Ferra, and, and making it work for our use cases. Um, I'll also give you some sneak peek about the Nova 2 results. Um, spoiler alert, they look great, and I'll deep dive into it in a bit. Um Today, before we start into like diving in directly into the, the use cases, I wanna talk about like what we're doing at, at Zendesk around the AI story. Um, we're really reimagining customer service at its core, um, not at tickets anymore, not as users, really a journey towards resolution, and that's what you see here, which is the Zendesk resolution platform. Um, if you look at the, the slide or the, the circle on the right, at the centerpiece, you see agents, right? So these are a network of AI powered agents that can adapt, that can reason, that can actually solve, uh, real customer interactions. Um, they're, they're not chatbots anymore. Uh, they can actually go end to end, um, solve a, a, a complex customer query. Um, and they can also connect and escalate to humans as needed, so that you're efficiently solving for tickets and queries without, um, compromising on quality. Um, if you look at the next part, um, which is our, um, Knowledge graph, right? So a knowledge graph is basically a unified self-service knowledge base which connects all our knowledge bases, our past tickets, external systems, so these agents have context that they can refer to, um, and that's basically forms the, the, the foundation of agents and how they can solve. Um, on the next piece you have actions and integrations, and I think like actions and integrations are a core part of reason of, of agents to be able to actually solve for user queries. Um, these interactions and actions and integrations basically look like your ticket call, being able to call external systems, being able to call APIs, um, and that's basically gives that agentic part to your AI. Um, and last but not the least is your measurement and insights and governance and control, basically giving Zendesk admin the insights into how AI is solving your query, being able to reason, being able to see how it solved your query, why it didn't do it, why it escalated to humans, um, so it really gives you the metric of what's happening behind the scenes. So Zendesk had had a really great year of growth, and it's no surprise to anyone at Zendesk that AI is our fastest growing product in the history of Zendesk. By the end of this month, we'll have 20,000+ B2B customers who are going to be using our Zendesk AI platform. That's almost more than 200 billion. ARR just coming out of from starting from 2 years of zero revenue in AI to 200 million plus and and so that's where AI is transforming core of our resolution journey today if you see 60 to 80% of our end user interactions are now being s solved through AI, not just touched, not just interacted, actually beings solved through AI, um. Human agents can now focus on the most complex queries and can give that human touch on the customers that actually need that, um, so what you see on the screens right is kind of like what forms the foundation of our resolution platform we have the AI agents that I just talked about, the co-pilot which allows for human assistance. Um, you have the auto QA so that we are checking for quality both on human interactions and uh bot interactions. You have the Action Builder which allows you to build apps on top of Zendesk. You have the knowledge bases and you, you have the AI Insight Hub. So overall that basically forms the foundation of our resolution platform. So enough of what Zendesk is and kind of deep diving into what everyone is here for, which is the use cases and how Nova is solving that, um, so the first one that I'm gonna talk about is translations, right? If you can imagine, right, like global service is global, um, and so like language shouldn't be really a barrier to solve these queries with Zendesk AI every human agent becomes a multilingual agent. And how we do that is basically supporting bidirectional translation for every ticket, every conversation that is coming in. Um, what that actually means for our customers is they can have naturally supported translation within the product without having to go outside, which helps them improve CSAT, solves query fosters, um, but also does mean that they can have an expanded coverage of support over multiple languages without having to go through hiring specialists and language specialists. And the next one that I'm gonna talk about is automated resolutions. So if you think about like how many times I've talked about resolutions today, uh, automated resolutions is like super critical for us, and the reason being this is how we think about how many resolutions our customers are making, right? Um, so what a successful resolution means is an issue is solved without a human in the loop at all. So an end to end interaction that gets solved through AI. So, how is Nova one is already having a massive impact for us in translation. Uh, today, we are translating over 300 million words per year, um, across our customer base. Um, that's 300 more moments where language is actually not a barrier to customer support. Um, we support more than 30+ languages across 3+ channels, um, email, web form, API, and so that's kind of how Nova One is already impacting, uh, or, or as having a massive impact, uh, but. So before we go into Nova 2, this is kind of what it looks like, um, if you look at the screen, um, the user is, um, messaging in Chinese, but that automatically gets converted into the human agent thing which is English, so that allows for a human agent to be able to reply in English and get a bidirectional, um, translation in. Talking about what everyone's more excited about, which is the Nova 2, what we're seeing is like core three parts that Nova 2 is already solving for, um, so three issues that we had with NA 1 was it was hallucinating pieces of information, uh, while it was translating. Second was mixed language or language mixing where the end text still had some part of the original language. And then the third one was the mistranslation where it didn't know how to translate, so the, the output was exactly the same as the, the input um on all of those we are seeing a massive impact. We've seen 5% less hallucination rates, 2% less on language in mixing, and 3% less on mistranslation, even though these numbers seem small, but on a volume of 300 million words, that's a massive impact that we're already gonna have. And not just that, It's also helps us scale from 30 to 60+ languages, so we're already seeing Nova 2 improving the coverage of languages, um, on how good it does, um, that allows us to not only just have multiple languages, that also helps us having an uh a massive customer problem that we can solve around 5500 million words across 5 channels. Um, and that leads to, at the end, improved customer support and customer experience for our end users, but also their end users. Kind of talking about the next use case which is automated resolution as I said, uh, this is very key to, to us at Zendesk, um, as I was talking before, uh, a resolution and an automated resolution is where an AI agent can solve an end to end user query without human intervention. Um, how do we think about automated resolution? How does AI come into the picture? Um, so if you can think about, right, like a customer comes in, has a conversation with a chatbot, um, makes, um, it can happen within a matter of seconds or it can happen in a matter of days. Once that's finished, we wait for 48 to 72 hours, um, making sure the conversation has ended. Um, after that, we have a bunch of product metrics that we can see like what happened and based on that we, we kind of know like it's a automated resolution or not. But because automated resolution is so core to our resolutions and so core to our trust of our customers, uh, we send that conversation back to an AI model to make sure it can capture all the nuances of a language, and that's where the LLMs and AI come in. Today at Zendesk we have, we've already sold for more than 5 billion resolution this year. That's a massive number that that is already gonna keep increasing at Zendesk we believe within the next couple of years 80 plus interactions are gonna be solved through AI end to end without any human intervention, and that's what we are aiming to, to achieve, um. Talking about Nova 2 key results, right? Um. If you can imagine, right, like there's 3 core parts to, to this as well. Um, there's the false positive, there's the false negatives, um, so on the false positive, that means basically where the, with the AI thought it is a resolution, but it wasn't, right? And that impacts customer trust massively. Um, false negative is where we didn't think it was a resolution, uh, with the AI I think it wasn't a resolution, but it was, and that means we are missing on resolution potential. And the third problem that we saw with NA 1 was the Jason format. I think a lot of customers have, have provided that feedback. Going back to looking at it, it's looking pretty good across the board right now. Today we don't use Nova for this use case, but looking at Nova 2, it's looking super promising. These are early results, and we're going to go and do a deep eval on this, but so far it's looking really, really good, um, and what that allows us to build customer trust, expand our potential, and also reduce our cost to serve. And that's me, and I'll pass it over to Ferrat. Thanks, Abbi. And thanks, Ryan. Um, so it's really refreshing to hear from Abbi, how, uh, Nova generally has improved, you know, some of the customers' experiences that they're having. So just to recap, Today we, you know, announced our Nova 2 Foundation models, um, you know, across these different modalities and capabilities across reasoning, conversational AI. And Ryan mentioned Novaforge, our customization offering. Um, allowing you to go deeper into model training, to really get the most out of foundation models. We also uh announced a Nova Act, which is a kind of browser use agents, and there's a number, a number of kind of agent capabilities that we're, we're delivering. And a, a lot of these talks are happening tomorrow, so please watch out for any sort of Nova talks. We've got a lot of talks around how to build effective agents, you know, NovaForge customization, so please keep it out. A lot of, uh, 34 of these sessions are tomorrow, so please, uh, take a look if you're interested. And you can navigate to Nova.Amazon.com. So earlier I showed you a demo of this site. You can actually go there now, Nova.Amazon.com. You can play around with some of these models. You can also go to Nova.Amazon.com/Dev, where you can have interoperable APIs, where we can experiment and play with some of these models today. So, you know. Chat with Nova, build with Nova, it's all in your hands and you know, give us your feedback. We're really excited for you to, for you to try them out. And thank you so much for your time, really appreciate it. If you have any questions for any one of us, we'll be around for a few minutes, so please feel free to come to us and then ask us a few questions, but thank you for your time, thank you.