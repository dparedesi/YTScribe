---
video_id: L2v8WdBGjvg
video_url: https://www.youtube.com/watch?v=L2v8WdBGjvg
is_generated: False
is_translatable: True
summary: |
  Sachin Hola introduces Ashi Singh, who details how Pinterest serves 600M monthly users atop a 500 PB S3 data lake (100k Hive/Iceberg tables, 20k Spark nodes, 1k Trino nodes, 400k jobs/day) by moving from Hive to Apache Iceberg. Pinterest spent two years socializing Hive’s limits and began Iceberg buildout in 2022; first production use in 2023 focused on GDPR-scale user deletions. Today 15k Iceberg tables hold ~200 PB, growing table count 300% YoY while data growth stays moderate thanks to better compression. Iceberg is accessed via Trino, Spark, Flink, Python metadata reads, MapReduce writers, and Ray.
  
  **User deletion:** In Hive, deletions rewrote entire partitions, disrupting downstream readers and incurring high costs. Iceberg’s snapshot isolation allows rewriting only files containing deleted users, but naive batching still touched many files. Pinterest sorted data by deletion key to cluster a user’s records, but full-table sorts on 40–50 PB tables were prohibitive. They added logic to sort only unsorted files and contributed related changes upstream (Iceberg, Spark), ultimately scaling deletion throughput 10x, cutting storage and compute costs ~30%, and improving job reliability 90%.
  
  **Table sampling:** Data scientists needed reproducible samples and meaningful joins between independently sampled tables. Engine-level Bernoulli/system sampling produced non-overlapping keys. Pinterest bucketed Iceberg tables and defined sampling as reading whole buckets (e.g., 1 of 100 buckets for 1% sampling) using consistent bucketing keys across tables. This guaranteed key overlap for joins, delivered ~90% query speedups for exploration, and kept deviation from full-table results under 1%.
  
  **Feature backfills:** Recommendation and ads models rely on tens of PB spanning months. Forward logging new features required 3–6 months of data accumulation and intertwined experimentation with production. Backfills were blocked by expensive sorts/shuffles on petabyte-scale joins. Using Iceberg bucket (storage-partition) joins on Trino and Spark (backported to 3.2) and enabling Ray to read bucketed data, Pinterest avoided massive shuffles, saving ~65% on join costs and boosting feature development speed 90x, enabling counterfactual backfills over forward logging.
  
  **Operational learnings on S3:**  
  - User-agent-based access control at the bucket/prefix level prevented accidental writes from non-Iceberg-aware clients during in-place Hive→Iceberg migrations (table definition swap without data copy).  
  - S3 inventory and request logs surfaced orphan files (unreferenced by snapshots) and misuse of Iceberg tables; inventory comparisons guided cleanup of unreferenced data.  
  - Throttling (503s) persisted because legacy Hive partitioned paths had low entropy. Adopting Iceberg’s hashed object locations (20-bit base2 prefix) for 66% of top datasets eliminated user complaints and request throttling.
  
  Singh closes by emphasizing Iceberg’s role in cost control (reduced rewrite scope, better compression), reliability (snapshot isolation), and developer velocity across privacy, analytics, and ML pipelines, with key improvements contributed back to the open-source ecosystem. The trajectory—table counts tripling while data growth is tamed—shows Iceberg enabling faster experimentation without runaway storage or operational drag.
keywords: Apache Iceberg, Pinterest, S3 data lake, user deletion, bucketed sampling
---

Good morning, everyone. Um, thanks for coming. Um, my name is Sachin Hola. I'm a principal solutions architect here at AWS. Um, during my time here, um, I help, uh, customers achieve massive scale with AWS services, and this session is one I'm particularly excited about. Uh, we have here, Ashi Singh. Um, a principal engineer at Pinterest who's going to, uh, talk us through some of the challenges, uh, that Pinterest faced, uh, with their data challenges, and, and, um, part of it is how they serve, uh, uh, 600 million users with their petabyte scale data lakes. So, uh, I think you'll find this session exceptionally interesting. So thank you. Yeah. Thank you, Sach Chen. All right. So, thank you all for showing up here. Um, today we'll be talking about how we scale Pinterest with the help of Iceberg. We'll talk about some of the use cases that are powered by Iceberg, the adoption journey, and we'll share some of the, uh, key, uh, learnings that we had while operating this at, uh, on Amazon S3. All right With that, uh, let me talk about the agenda. We'll start with the, uh, discussing the scale at Pinterest, a little bit of that that you know already talked about. Um, we'll look at how table format evolved at Pinterest and in the industry, uh, we'll talk about the Iceberg, uh, adoption at Pinterest, um, and then we'll dive into some of the key use cases that are, uh, power, powered by Iceberg and the ST specific learnings is where we'll end the session with. So scale at Pinterest from a business perspective, we have over 600 million monthly active users. Every week we save around 1.5 billion pins. We have over 10 billion boards, but more relevant to this audience, I would think is going to be the data scale. So our data lake is about 500 petabytes on Amazon S3. 100,000 tables in high mast, but when I say high meta stir, these are actually hive and iceberg tables. We have, uh, over 20,000 nodes that are, um, running Spark, um, and, and over 1000 nodes that are running Presto, uh, sorry, Trium, and in total, we run around 400,000 compute jobs in a day. The table format evolution at Pinterest is aligns very well with the how the evolution happened in the industry. Uh, until 2019, Hive table, like everywhere else, was a de facto, uh, data lake table format, and, uh, around 2020, the industry and within Pinterest, we started realizing that, oh, you know what, like we have a lot of new use cases and Hive is just not getting it. Um, and, uh, so like in the industry like there were, um, alternatives like Hoodie, Delta Lake, Apache Iceberg that were farming, um, we were involved in those discussions early on, but it took us two years to get the approval, make sure like the whole leadership team understands the challenges of Hive, and we started the buildout in 2022. Um, 2023 is when we will actually go to production. The first use case and the key use case that we went to production with was user data deletion. Um, and since then we have been ramping up. That takes me to the next slide, uh, where we'll talk, look at the current, uh, numbers, we are operating at, uh, for Iceberg Adventures. Around 15,000 tables, um, are, are on Iceberg. We have around 200 petabytes of data um on Iceberg. And if you look at the growth numbers, The table count on Iceberg has been growing over 300%. By data volume it's much, much more or less, which we want. We want the data to be not growing as quickly because it's expensive. And one of the reasons why we are able to do that, that even though our table counts are growing, our data count is not growing as significantly, is, is because we are able to utilize compression much better with Iceberg. All right, so we have Apache Iceberget printers. How do users use it? Uh, we, uh, enable users to use uh Iceberg from, uh, engines like Trino, Spark, Fling, uh, from Python, we can do, uh, we allow users to do metadata reads. Uh, we have some, uh, MapReduce-based frameworks that write to Iceberg tables, and then we also support tray. So that was pretty much about like how we um uh sorry, adopted Iceberg Apprentice, but now like, let's talk about some of the key use cases that are driven by Iceberg and the use case that I just talked about, that was the first use case that we went after is a user data deletion. User deletion is not a new thing. I'm sure you all have heard of it and maybe you do that at your companies too, but essentially in this example, like if there are 3 users, they come in, they say, oh delete me, our deletion service used to look at all the tables that had user data, right, and then perform deletion on them. In the high world, what that means is we are rewriting entire tables, right? That's super expensive, takes a lot of time, and because it takes a lot of time, it was not as reliable. So we we thought that we'll solve this problem. Oh by the way, like the, the cost of the rewriting was one thing, but then the fact that if there are downstream jobs and users reading those data sets at that time and we change the files on the fly, uh, those jobs will likely fail because at the query planning time or the job planning time they saw there is a file, but when they were actually going to read it, it didn't exist, so they, they will fail. So we try to solve this problem with Iceberg. So in the with Iceberg, you don't have to do the whole table rewrite. You can just focus on the files that have the content or that has the data for these users and just rewrite those files. And the good thing with Iceberg is that you get the snapshot isolation. So while you are rewriting the files, the users that are reading from the previous snapshot, they remain unaffected. So great, right? Maybe. Um, the problem that we ran into is that our cost of deletion was still the same. And that was because the cost of deletion is proportional to the number of files rewritten, right? If you are rewriting all the files, it's as bad as doing it on Hive. If you're rewriting just a few files, it's going to be super fast. Now the problem with the way we were doing the user data deletion is that we will batch the deletion request and then perform the deletions, but the user that will perform asking for data to be deleted. Their data could be spread across all the files, so that would mean we are rewriting the entire table, even with iceberg. To solve that problem, an easy solution is we just sort it by the deletion key, right? So that like we make sure that only a few files have the data for a particular user who is asking to be deleted. Um, that sounds good. It does help. So in this example we were able to reduce the number of files that get deleted. For example, in table 1, it's just one file that that is getting deleted now. Table 2, just one file, much better than the previous scenario. But this requires sorting. Our tables are on the scale of 40, 50 petabytes per table, and doing sorting, that's going to be super expensive. Um, so how do we handle that? We added optimization where, um, we only sort the files that are not sorted, and this required us to make some changes to, um, Iceberg. Some of those changes we contributed upstream, uh, required us to make changes to Spark, um, some of that is in-house. But with that we were finally able to meet our requirements where we were able to Scale the deletion capability at printers by 10 times, um, reduce the data storage cost by 30%, and also reduce the data compute cost of those deletion jobs by 30% while increasing the reliability by 90%. So that was the first use case. Let's talk about the 2nd US case. Table sampling. Table sampling is a very common thing. All the engines, they come with the sampling functions. Essentially you can take sampled data from your large data set, and it's very good for data exploration. Great. But then our machine learning engineers, our data scientists want reproducibility. If they are doing something on a sample data set today, they want to be able to reproduce tomorrow. Add on top of that, like if you have 2 tables and 2 large tables and you want to do a joint between them and you want to do a sampled version on both the tables, you want to make sure that there are keys that exist in both the sam the output of the sampling from both the tables. If not, the joint would just be meaningless. So to do that, um, we introduced bucket-based sampling. So with Iceberg we we bucketed the iceberg tables and then instead of using like sampling milgarm like Bernoulli and and system sampling, we started doing the the bucket. So for example, if you have 100 buckets in a table, we'll say if you are doing one person sampling, we just pick one bucket and we read that entire bucket. Now the good thing with that is like if you have two tables, right, you are doing one person sampling on both the tables, as long as your bucketing key is consistent between the two, you are guaranteed that the same uh same keys are present in both the sampling output from both the tables. So great. Uh, with that, like our data scientists and engineers were able to use sampling for their data exploration which required reproducibility or even joining between multiple tables. And, um, with that, of course, uh, the, the, the developer last year went up. We saw speed ups of like 90% uh with our users and the deviation from like if you were to do the entire table scan was less than 1%. The third use case is feature backfills. At Pinterest, our mission is to inspire users to curate a life they love. To achieve this, we rely on state of the art recommendation and ads model trained on tens of petabytes of data over the span of many months of engagement blogs. These models drive personalized recommendations showing user content that resonates with their interests. These models show significantly better performance when trained on large data sets with events spanning over multiple months. Our ML models are trained on a wide range of features including pens, users, advertiser level, and session-based features. Machine learning engineers are constantly experimenting with new features, and, and then like they do the AB testing and based on that they will decide which features are going to live for a longer period of time. A traditional approach of doing this is forward logging, so you add the feature in production, you wait for the data to accumulate. Usually that takes like 3 to 6 months, and the worst part is like you are doing experimentation in production. A very easy way to solve this problem, and it's very popular too, is rely on feature backfills. So we counterfactually compute historical feature values and join with production data. The benefits is like we are not waiting for our data to accumulate, uh, we are isolating the production and experimentation environments. 1 may ask, OK, then why were we doing the, the forward logging in the first place? That's because of the joints. The joints are super expensive when you're doing a joint between cables which have petabytes of data. Especially if you have petabytes of data, the, the most common joint, ah, mechanism that works is like sortment joint, right, which requires a huge shuffle. The shuffle was so expensive that we couldn't do the fissure backfill approach in the past. And every time, like if you were to use it, like every time we add a new feature which happens multiple times in a day, the cost keeps going up. So we went after the shuffle with an iceberg, and we used the bucket joint, also commonly known as storage partition joint in the iceberg community. And with that we were able to avoid expensive shuffles, um, achieve like 65% cost savings in these large joints, um, and we were actually able to enable our machine learning engineers, or feature engineering folks to do feature backfill instead of forward logging and achieve 90 times better feature development speed up. We added the support on Trino Ray. This was already on Spark 3.5. We back forwarded to 3.2, which is what we used to use. And on top of that, we also enabled users, for example, on Ray, people can just read these tables, these buckets, do the join on the fly on Ray itself without doing any materialization before. Um, and yeah, that was the feature backfill use case. Next step, um, I'll share you some of the learnings we had on operating this at the scale on Amazon S3. The first one was user agent-based access control. So when we decided that we want to go from hive to Iceberg, um, Very commonly people do data rewrite. They'll read the entire table from, uh, from a Hive table, convert that into Iceberg, and then start using from Iceberg, right? The problem with our uh with us was that like it was like we have 200 petabytes of data on Iceberg today and we couldn't do that copy like that would be super expensive for us. So what we did was like we decided to do in place migrations so we what we did was like. On the Hive table we would build a snapshot and then atomically swap the table definition to use Iceberg. Now to do that, we want to make sure that only Iceberg capable or Iceberg clients or catalogs are are are manipulating the data because in high world one can go write a new file in a partition and then that file will show up the next time you read the the table, but in Iceberg that's not the case. So we wanted to prevent accidental writes to the Iceberg data sets uh by directly accessing the S3 location. and not modifying the table definition of the or creating a new snapshot. So to do so, um, there were various mechanisms, but the best mechanism that we thought that would work for our scale was using user agent-based access control. In Amazon S3 you can define, you can, while making the request, you can add some information in the in the user agent of those requests and have policies on your bucket level or even prefix level that either allow or deny based on the information present in the user request. This wasn't intended to block malicious access. This was intended to block accidental access, like people who didn't know that, oh, my table definition has now changed to iceberg. So this was very instrumental for us. Like we were able to modify all of our iceberg catalogs and clients to add additional information in the user agent and then have bucket level policies with which we were able to deny anything that wasn't coming from Iceberg ready surfaces like Chino, Spark, Fling. Um, The second learning we had was the power of um uh ST request logs and S3 inventory reports. In Iceberg, like, it's very common because like every time you have to, we are, we are generating a new snapshot whenever we do a write, right? And the promise that we make is that if you, if a user has started reading a data or a table. And we do the concurrent right at that time, the previous read is not going to get affected. But under the hood, the way we do it is like we write the new data, we keep the old data, and there are multiple versions of data sets living together. And this very easily can lead to a problem where the old data is just sitting there, it's not referenced by any snapshot, commonly known as orphaned file or unreferenced files, um problem. To do to figure out like if and how much of unreferenced files or the orphan files are existing within our iceberg data sets, we heavily rely on essay inventory. So we can look at the essay inventory and and compare that with the files in the iceberg table definition or the metadata. And find out like, OK, which files are unreferenced or orphan, and then get rid of them. The other place we use this is to identify the accesses that are being made to iceberg tables and figure out, oh, are they, are these accesses legit if they are not, like again like this wasn't meant to block or find out malicious users, but then to find out users who are not using Iceberg in the intended way. And we did uncover a lot of that with the help of the SA request logs. OK, The third learning I would like to share about is the throttling. It's very common, and one of the promises that Iceberg made was that this will solve the problem. But we at Pinterest, we chose to do the, um, in-place migration. Now when we do in-place migration, what that means is that we are still keeping the old way of organizing data in a table or on S3. Um, so in the high world typically you have the table name, the partition name, the partition values, and then the the goes on. Like if you have multiple partitions, it goes on. The Amazon SC recommends that you have high entropy in the object paths as early as possible, but because like your table name, partition name, all that like super long strings are pretty static, right, so we aren't able to introduce any entropy until we reach the file name. And what this essentially means in terms of throttling, we usually would, if the data set is large, it is being written by a lot of concrete executors, we would hit the 503s, which is expensive because it means wasted compute and wasted time for our users, so. With Iceberg, even though we did the in-place migration slowly, we, once we made sure that everything is going through the iceberg table definition. We were able to modify how these object paths are the the the objects locations are decided, and slowly we were able to use the recommended approach of using entropy in the path as early as possible. One of the recent things that Amazon contributed to Iceberg was the writing objects on 20 bit base to hash locations. And we started using that. We rolled that out to 66% of our top 10 data sets and boom, like we don't have any more user complaints on 503s. We don't see it in the in the request logs, and this was one of the top concerns we used to get from our users, not anymore, so I highly recommend using that. And with that I'm out of time. But thank you all.
