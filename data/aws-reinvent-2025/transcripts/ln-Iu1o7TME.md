---
video_id: ln-Iu1o7TME
video_url: https://www.youtube.com/watch?v=ln-Iu1o7TME
title: AWS re:Invent 2025 - Implement Agentic AI at the edge for industrial automation (HMC317)
author: AWS Events
published_date: 2025-12-02
length_minutes: 55.02
views: 9267
description: "Manufacturing and industrial operations demand real-time, intelligent decision-making in low-connectivity environments. This code talk demonstrates how to deploy SmolVLMs (small vision language models) and AI agents to automate site operations using AWS Outposts and Strands Agents. The solution enables on-premises, multilingual interaction with safety protocols, documentation, and equipment data to enhance the productivity and safety of frontline workers. Youâ€™ll learn how to combine edge infrast..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

My name is Karim Aknou. I'm a senior solutions architect at AWS. I've been with the company since almost 3 years, and I've worked with many of the big manufacturers in Germany, helped them navigating the complexity of deploying G AI applications. OK, perfect. My name is Mohammad Salah, working as a solution architect at AWS looking after the public sector in the Middle East. My job at AWS is solving the similar problem, not Kareem's daughter's problem, but the manufacturer, a real manufacturer's problems. Picture this. The same silence. Inside your factory And you have machines stopped. Your production line pause. Everyone. Starts running around. Every minute, your products are not made. Your orders get delayed and costs pile up. Simply, this is what we are calling. This silence, unplanned downtime. The top 500 manufacturers globally, they are paying for this. $1.4 trillion US dollars due to this unplanned downtime. They are losing around 11% of their revenues every year. To make this more clear, we are talking about. This is equivalent to the GDP of the nation like Spain. When we take a closer look about this, we found 4 different challenges. First is the data silos. You have a production line, and as part of this production line, you have multiple machines. Each machine has its own alarms and telemetry data, but each machine has its own separate database. It will end up, you don't have this kind of a view, end to end view. To understand exactly the relations between the machine inside the same production line. The idea here is you have the data, but the data is disconnected. Second problem, which is the skill gap, it's all about people. For each manufacturer, you have this senior expert. Who can tell you from the sound of the mixer, there is a problem. This is over mixed dough. I can tell you from the oven, the right side of the oven is much, much more heated than the left side. This kind of expertise. If they are not there and you have a junior receive this kind of a warning, they will not be able to act, and this is the second idea. You have the knowledge, but knowledge is not shared. Third is the production delays, and this is when this silence. Feasible for everyone. You have the junior, you don't have those seniors. You have the data, but the data is scattered, and they are not able to act because they don't have the expertise, they don't have the knowledge, and the data is disconnected. Last one is the operational disruption. I know this is. Very important for every factory because most of the factories are in remote areas. When we are talking about the factory, they are not having most of the time stable internet connectivity. And they don't have a cloud access. If you have a smart manufacturer, And this disconnected under any reason, this means Your manufacturer will be blind. You will not have this kind of Data-driven decisions and the insights. That's why we thought about simple solution. Let's put all the data together in one single data link inside AWS outpost. But when we did this, we found a different challenge. Each machine has its own integration type. I bought this machine from OEM X, OEM Y, and OEM Z. However, each OEM. Has its own integration even technology if we are talking about modern IOT integration, MQTT or even Laura WAN, or we are talking about rest APIs for post HTTP and get HTTP and most important part about your manufacturer is the documents. Those kind of standard operation procedure document, manual documents, you can get it using a legacy SFTP. We thought about why not to have this outpost underlying infrastructure over ETS local cluster doing this unified API deployment to absorb those kind of different integrations. And accordingly integrate this with a one consolidated data lake. And then to get the consumers of this data lakes will be an AI, in our case will be Bedrock, and also to get much more insight with the dashboard using Amazon Quick sites. This will be the result. You have on the left side, you have a production line, you understand everything. You have the full details and full understanding about your production line, and you can see the inspector is telling you, you have a cracked cookies. The machine stays down for 5 minutes. Second problem Is the resiliency of what we are calling always on how to stay always on. If you got disconnected outposts disconnected from the cloud under any reason, you will not be able to act as a smart manufacturer anymore. You will not be able to have this kind of a data driven decisions. You will not get automated AI recommendations. That's why we thought. Why not to add this intelligence at the edge by deploying multiple small language models? We'll we'll go through this later as part of the sessions and to have two different features. First, To provide Intelligence for the operators using text and even voice to chat with the data. Second is to apply this kind of an automation. You are digesting your data and telemetry data every 5 minutes, understanding the insights, getting the recommendations, and if you feel this is recommendations. Require a notification, notify the operator with the required actions. So before we jump into the coding session, which I know you're so much looking forward by now, please allow me 2 more minutes to explain the big picture so you understand how the end to end workflow looks like. We imagine having a cookie factory which produces the best in class cookies here in Vegas, and for simplicity, this cookie factory has 3 big machines which are the cookie former, followed by the freezer tunnel, and lastly the cookie inspector. Now imagine the situation when the cookie inspector, which does the visual quality inspection, detects a cookie which is cracked, misshapen, or a cookie with some air pockets. Then we have our Agentic AI application running inside AWS outpost for the always on connectivity thing. This Agentic AI application has access to different data sources, including the machine manuals, the standard operating procedures, and real-time telemetry data and alarms. And then operators inside the production site can chat or voice chat with the agent. To understand what is the underlying problem and then it can finally instruct the agent to itself take some actions to correct the problem inside the production site. And finally we move from the state where we had a rat cookie to a state where we are producing delicious cookies and everyone is happy again. But you know what the problem is with what I'm showing here? I wish that the solution was as simple as plugging in the icon of AWS outpost. Under the hood, there is a little bit of a more complex architecture. So on the right hand side we have the Um, we have the pipeline for data preparation and model fine tuning which we'll go into very details at the rest of the session. And on the left hand side we have our Agentec AI application deployed on AWS Outpost, and then we have different specialized small language models. We have GPT OSS model which will be the routing model to. Navigate the different tasks from the users to the different tools we have the fine tuned LMA 3.2 billion parameter model which will be specialized into the rag application and we finally have the small VLM for visual quality inspection for the sake of time, we will not be implementing the small VLM, but we will go through every other details of the implementations. So let's go. That's what you want. OK, so I hope you're seeing my screen. You do? Right, so, um, I will start with setting up the infrastructure deployment. Um, and I will be mimicking this with a couple of EC-2 machines. I will be mimicking the AWS outpost environment with a couple of AWS of EC-2 machines. So let me navigate to the first method here that we have, which is the create EC2 instance. And here, as you can see, I'm using the G4DNX instance, which is relatively small but capable instance. It can be capable of running a small language model. I'm selecting a specific AMI which is the Nvidia one, and then I'm creating some security groups, some VPCs, some IM role, and so on and so forth, but the most important part that I would like to stress on here and grab your attention is this part, the user data script, which will be uploaded to the EC2 instances because this is the one that will be installing the actual dependencies that is needed to run the agentic AI application. So if we take a look at for example this EC2 machine, the user data script of this EC2 machine, you will find that it's 148 lines, so it's not very realistic to go through this line by line, but instead what I would like to do is. I would like to use here your brains to brainstorm together what should be the dependencies that should be added to this EC2 machine. So please take a moment, scan the QR code, and think. As if you are deploying this application, what dependencies do you need on the EC2 machine in order to host an AI agent and a small language model which is the router small language model? Please think about it, and in a moment I will be showing some results. Get him. OK. OK. So, We're still collecting the votes. So you guys almost nailed it, right? Yeah, so of course we need the agent code. That's, that goes without saying. We need Docker runtime because we will use Docker to um pull the container of Olama, which will be our uh software for hosting the small language model. We don't need the fine-tuned model for this specific EC2 machine because we are having another EC2 machine we will be where we will be deploying the the fine-tuned model. We need Nvidia toolkit for the Auda software. We need a custom Python version indeed because the SDK that we will be using for installing the AI agent is strands, and strands works with a minimum Python version of 3.10. And the default Python version in the G4DN instance is Python 3.9, which is why we need to install a custom Python version, and the one who said Olama, that's actually correct because we will be running Olama as a Docker container. OK, so that was for the first EC2 machine for running the agent. Now please again scan the QR code and think what the dependencies do we need for the EC2 machine that will hold the fine-tuned EC2 instance, uh, the fine tuned small language model. OK, so let me view the Results. Yes, yes, yes. We need Docker. We need the fine-tuned model. In that case, we don't need a custom Python version anymore because it works with the default version of the G4D and XL. That was a tricky question. Um, we don't need the agent code that was deployed on the first EC2 machine. And we need OLA, and we need the SLM of course. OK, so I'm happy that we will brainstormed the dependencies now. After we write the user data script, like I said. We create a VPC. We create security groups. We create an IM rule with access to S3, and when we are happy with that, we need to deploy our application. OK. Perfect. Now we have infrastructure starting to deploy. Kareem will debug for sure what is going on, but during this time, let me give you some tips and tricks how to do a deployment of for a small language model over AWS outpost. We'll take a closer look here. The GPU innocence that we have over outpost is the G4DN. And to deploy any model, you have to consider two things. First is the hardware constraint. We are talking about G4DN12X large, which is 4 T4 Nvidia ins, each one of them powered by 16 gig of memory, gigabytes of memory. At the same time, and this is the second consideration, is the model itself. We are taking an example of an open AI model, OSS, 20, around 20 billion parameters. But here I mentioned different parameters, which is very important to consider while doing a deployment over the outpost. One of them is if this is a mixture of expert or not. How many layers? And the active parameters per token. Now, let's focus on the first strategy to do the deployment is the quantization. Quantization, in a nutshell. is reducing The number of bits. Presenting your weights. In order to reduce the memory footprint. This model trained by using the precision of FP 16. This is the base model. However, we can have another quantization, which is MXFP4. We are talking about how to present each weight. We are talking about how to present each parameter. Instead of presenting each weight with 16 bits, you can present the same weight with 4 bits. This will make a significant reduction in the memory footprint instead of having the full precision, which is the base model, 40 gigabytes, you can get only 13 gigabytes, but everything came with a cost. The cost here is accuracy for the base. Line, you don't have an accuracy impact. However, for a mixture of XFP4, because you are going to reduce the number of bets presenting your weights, you will have 1 to 2% accuracy impact. This means that you can save 65% of your memory trading this by 1 to 2% of accuracy loss. By doing this, you will have. The full model can be deployed with the 13 gigabyte in your memory. Do you remember the hardware constraint? We have 16 gigabytes. Now you can have the full model deployed inside GPU. One of the models, the deployment strategy is to deploy the full model in a one GPU, as you can see here. We are going to replicate the same model across the GPUs, and the significance of doing this is to have. Latency sensitive workload implementing by doing a parallel processing for multiple clients at the same time. Your application will not have any kind of a tolerance for the latency. You are going to do a processing in a separate GPU for each client. Second, the strategy is This You have one single model, weights, shored across GPUs. Where for each GPU you will have 3.2 gigabytes for the model weights. And the remaining will be for the KV cash. KV cash is simply, this is. How to cache the tokens as part of your session in order not to let the model recalculate the token every time generating a new content. If you have a larger KV cache, this means that you can get. Large number of users and at the same time you can get A larger context window, and this is important for your session. Let me Show you the foundational part of this. We started to, with the architecture, high level architecture, where we can start with the data preparation. Let me show you something with the data here. As you can see, Those are the data sources. You have a different types of data source. You have a CSV files, you have a text files, and you have a PDF files. This is the normal data source, how it looks like. You have a different files, you have a different format because you have in your production line, you have a different OEMs, and each one of them has its own structure to do this. What we need to do is to fine tune the model, but what is the objective of the fine tuning the model? The objective here is to give an assistant for your operators. This assistant will help your operator to have a detailed, instructed steps with considering the severity of each problem, considering the compliance, considering the safety, and so on and so forth. But the most important part is to let the model understand the skills needed and let the model talk with the same tone needed. That's why we decided to use a fine tuning the model using instructed fine tuning. Let's start with what's exactly happening under the hood. I will show you here on the right side as you can see this is a data pipeline where you can read the different documents, PDF file, text file and document files, and then you can invoke a larger model in order to generate a structured data to fine tune your model. The most important question here, why do not. Through the data to the model directly to find you in the model. The answer is simply, is if you are throwing your data right away to your model, you are training a hallucination machine. The important part here, reading the document, invoking the model to return a structured data accordingly validate those data, and then you are happy to have a full structured documents in order to fine tune the model. Let's start with. Running this As you can see, The pipeline start reading this and at the same time start invoking the bedrock to generate questions and answers. Why to generate the questions and answers in order to have instructed. Data set in order to fine tune your. Let me show you something here. This is the data preparation class. That we are using to invoke the model as you can see here we are using this model. To generate the structure data. And the most important part of this is, let me show you something very important. Is a converse API as you can see here, we are using the converse API. Conversely API is giving you the flexibility of changing any model at any point in time, because it's using for different models, same API signature. At the same time, and the most important part is the inference configurations. If you are configuring the maximum number of tokens correctly, you will be able to get your answer correctly without having any kind of data split or something. The most important part here is the system prompt. This is an instructed prompt with the one, only one shot. I'm giving the model a clear and detailed instruction to generate the realistic information, to include the severity, to answer the safety questions correctly, and include the needed. Information By doing this, you will be able to have a full data set with your structure, with structured data. Second, and this is very important. Why do we need to trust the LLM? Maybe the LLM itself will hallucinate. In this case, if your LLM is hallucinating, you have to apply what we are calling a deterministic validation. Yes, the LLM generated the content, but if this is the content is correct, if this is a right question and answer, if this is following what the standard operation procedure is telling you, if this is following even the needed structure for requests for your response in adjacent way. Why this is important? Because again, we are, we need to make sure that we are not having a single record in your restructured data for the fine tuning the model. Second is sharding. If you are starting to shard your data, you have to start shard your data in a smart way. If you have a very long paragraph, you cannot cut the paragraph in between. If you have a large section, you have to make sure that this section is. Split it in a correct way. You cannot cut a split a sentence in the middle. Why this is important again, same objective structured data implemented correctly as we can see on the right side, the job is done. And we have generated Q&A done validation done and as you can see we have some statistics here we have a 50 pair of questions and the answers requested, 45 generated, and we have 5 rejected. We can check here the output file. And from the output file you can see this is rejected. Because of the frozen belt misalignment section, this is the content and this is the critical information which is wrong. Why? Because the model itself or the pipeline itself is doing this validation across the needed structure and across even the data inside the SOP. Accordingly, now we are, we uploaded the data inside S3 in order to start, start the fine tuning the model. I'm not sure if you are familiar with this. This is. AWS Sagemaker AI studio, where you can start to fine tune the model. If you go on the left, you can see this is a jumpstart. Inside the Jumpstart, you can select the model. Here, the model family is meta. This is a model family. Perfect. Lama 3.2. Billion Instruct. This is our model. Here you will find the 3 different options, either to evaluate, deploy, and train. In our case, we are going to train the model. I'm going to select training the model. After selecting the model, you have to select the data set. We did as part of the data preparation for the data set. Here is the detailed set. Here is the data we just uploaded. We select the data, and then we need this fine tuning will be, there is an output of this fine tuning or updated weights of your model based on your data set. You are going to set the SS3 bucket. To get your updated model. Accordingly, you can go through this. I think it is normal to have 5 epochs. And this is very important. This is AWS recommended the recipe to do the fine tuning using this G5 innocence. If you want to select a different innocence, it is fine. This is fully flexible, but this is what we are recommending as part of fine tuning the model. Accordingly, we are going to use the same identity access management, same VPC, same encryption keys, and then we'll submit the model. We have to agree on those things. And then Jo started to do the fine tuning for the model. OK, so now I bet that many of you are asking themselves one question which is in my opinion very valid if you are asking it, and the question is, why are you guys doing this model fine tuning and data preparation when you will anyways be using a rack task in the edge. Maybe you are asking yourself if we just added this piece of architecture in order to make our architecture looks a little bit more fancy. Well, while we may love doing so, but in fact we have done extensive experimentations in order to prove that this approach is indeed valid and in order to prove that we have used a technique which is called LLM as a judge evaluation and let me show you how this technique works. That's a lot. Hm? When I try. Please allow me a moment. Clos to men. Yeah, for some reason, um. The aspect ratio is not working well. OK, so let me explain how LLM as a judge evaluation works. I had some fancy um animation, but anyways, so we have our fine-tuned model, and we have our base model. The lemma-based model that is produced by Meta and we want to compare the performance of both models on the rag task. So how it works at the very first step, what we provide to both models is the question and the context from the test data set. And we ask both models to generate an answer based on those questions and context, because usually this is how you do, um, how you do it in RAG applications. And then as a next step, we have our three LLM as judge evaluators. We have Cloud 4.5 Sonnet. We have Cloud 4.5 Haiku, and we have Amazon Nova Pro as judges. And we pass to each of those 3 judges 5 different inputs. We passed the question and the context from the test data set that Salah has generated. We passed the answer from the fine-tuned model from the previous step that I just showed, and we passed the answer from the base model based on the question and context. And finally, we also passed the ground truth answer from the test data set. And we ask each of those three models. To evaluate the result, the results generated from the fine-tuned model and the base model based on three evaluation criteria which are number 1, accuracy, which measures how well the result from the fine-tuned model and the base model agrees with the ground truth answer from the test data set. Number 2, evaluation criteria is the completeness, which measures how well the answer is addressing all the different aspects of the question. And number 3 evaluation criteria is the relevance, which is, in my opinion, the most important metric in the RAG application because this one measures how well the answer is following the context given to it, because this is how RAG works. And then we asked the 3 judges to evaluate the performance of the base model and the fine-tuned model. And here we have the results of our LLM as a judge evaluation. In my opinion, this is really great results because this shows that the different evaluators, these different evaluator models, shows that the fine-tuned model is outperforming the base model in the rack task for all the data set aggregated for all the three different evaluator criteria. So for example, Claude 4.5 Haiku says that the fine-tuned model is outperforming the base model by 17%. And Cloud 4.5 Sonet says that the fine-tuned model is outperforming by 15%, and Novaro says it outperforms the base model by 10%. So on average we have 14% points improvements for adding the fine-tuned model on the rag application for our Agenec AI application. So now let me quickly recap before jumping to the next step. We started with deploying the infrastructure to our EC-2 machines in order to Replicate or mimic the environment of outposts to have our gente AI application. Then Salah thankfully showed us how to prepare the data and fine tune a small language model for the rack task, and I just showed you how important this is in our architecture. Now we need to put this together into an Agentic AI application that runs on the edge on AWS Outpost, and this Agentic AI application will have access to the rack tool and also the telemetry data tool. So let's start doing that. OK, as you can see that our 2 EC2 machines are up and running now, the factory agent in instance and the fine-tuned small language model instance. So what I would like to do. is to log in. To the factory agent instance, so I would use Kiro for that. I installed a plug-in for a remote SSH. I connect to the host. And now it's opening an SSH session inside the EC2 machine which we instantiated at the beginning. So now we are in the AC2 machine. I need to go inside the factory agent directory which I copied in my user data script. I'll zoom in a little bit. Hm I zoo in is not working. During this time, let me explain something very important. We used two different approaches here. We used fine tuned model at the same time we use drag. Anyone have an impression why we did both because most of the time you are reading one of them. Any answers? Huh? Information that ensuring the right database database may be. Fine tune model or. Exactly. And let me dive deep into this. The most important part here is to have the knowledge about your document and the skills needed to fine tune the model is you are gaining the skill needed. You, you are setting the tone of your agent to respond correctly. By having Iraq, you are retrieving up to-date information from your knowledge base. That's why we combine both to achieve this LLM as a judge evaluation results. We are talking about 15% increase from the base model. Cool. So now, as Salah is talking about the rag tool, I would like to start with showing you how to develop that rag tool that I will be using for my agent application. And I pre-created this class which is called Rag Retriever. Previously I also in my environment, I pre-ingested the documents inside Chroma DB as a vector restorer, and I created a very simple method which is a search method that you can provide it with a query. And the number of results and then it uses this query to look up the information inside the vector store and retrieves the most relevant information from within your vector store. And then I used that tool decorator from Strands SDK in order to have. This search documents method and here I'm providing some dock string which will be used by the agent to understand what this tool is doing and how it can actually be used and then as you can see. We are iterating over the results of the retrieval from the vector store and we are printing them in a nice format. So this is a tool for reg retrieval, augmented end generation. Do you think in this implementation here anything is missing here? Again, we have retrieval, augmented end generation. Anything you see missing in this tool implementation, a major thing. OK, just to save your time, we have the G missing, the generation missing. If you remember the model that Salah spent some time fine tuning. We're not using that model here, right? And what we, what I would like to show you is how to use that model in order to enhance the results like I showed in the LLM as a judge evaluation. So what I will do. We will be instantiating Olama client using the IP address of the machine which has the fine-tuned model with the port that opens up the Olaa connection. And then I need to just copy and paste um a couple of code parts. I will just paste it here. Yeah, and then I changed this to response. OK, so now we have our rack tool complete, at least in my eyes. We have the retrieval part and we have the generation part asking O Lama to invoke the endpoint of our fine tuned model, passing to it the query and the result. So this was the first tool, and this tool was to retrieve the standard operating procedures and the manuals from the different machines inside your production line. Now the second tool is the telemetry tool, and this telemetry tool is to retrieve real-time telemetry data and alarms from your machines inside your production line. And I've marked this real-time data by a very simple CSV file, as you can see, I'm not sure if you're seeing, let me zoom in a little bit. As you can see, the CSV file has a device name, the sensor type of that specific device, the reading, and the time stamp for that reading. And we have two devices if you remember from my first slide, we have freezer tunnel machine and the cookie former machine, and for each of those 2 devices we have 2 different sensors, the temperature and the speed, and then we have the reading for those different sensors, and finally the time stamp at that specific reading. So for the telemetry tool, I also pre-created a class which is the telemetry reader that reads this CSV file and then I also created another helper method which converts the CSV into a time frame. And we passed through this method 3 things. The device name that we want to look for, that we want to look at the telemetry for the current time where we are standing, and the number of minutes back in time where we want to look at. So for example, if we want to look at the telemetry data for the last 5 minutes, we set the minutes back here to 5. And here if we for example provide it as an input the device name corresponds to a freezer tunnel, this is how the output can look like. So the freezer tunnel has temperature and speed sensors, and those are the readings for the different time stamps. Now I want to create a strength tool out of that one. So let me create it as we go. If you remember, I would use the tool, the curator. I would create a method called. Get telemetry. Data and for that method I need a device name which is of that string. And I need minutes back. Which is of type integer. And the return should be here uh formatted string. Again, for any tool we need to have a well structured dock string, so I will be copying the dock string from my notes here. And then we need to code the tool, so this method here instantiates the class telemeter reader. We need to have it. The telemetry reader. And in order to call the method which I showed you before, which is this one here, we need the device name which is passed as an input. We need the minutes back which is also passed as an input, and we need the current time string. Luckily enough, strands has its own implementation of current time, so we will be using that one. Current on string equal. Current time class from strands. And finally, now we need to call the method that I just showed you. Data frame equal. Till. Reader. Get device sensors in time frame. And we passed with the device name that we have as an input, we passed with the current time string, and we passed with the. Um, minutes back. And now we just need to formulate our results in a nice F string. Readings for device. Devi name For the last. Minutes back Minutes And then we just need to concatenate the results that we get from the methods. Result. Plus equal Jason. dumps and then we have the data frame and we set the indentation. Tutu. And finally we return the results. Cool. So as any good software engineer, I wrote a tool, I just need to test it. Let me do that very quickly in order for the application to work at the end. Get telemetry data. And we need to have one device from our CSV file, which is, for example, the freezer tunnel. For the last 5 minutes. OK, I always do this problem. Should be only return result. And now it works. So now you can see that. We have our um our readings for the freezer tunnel for the last 5 minutes structured in a nice JSON format. So the final thing that I would like to add here is a very simple tool inside this telemetry telemetry tool, which is a tool that can be used as a lookup for the agent to understand which devices that it has access to. Think about the situation here for example, I type freezer underscore tunnel, which is the name that I just copied and pasted as is from the CSV file. In real life scenario, users don't know how the exact name is written inside your database, so we need to have some sort of a lookup so that if, for example, the user type uh uh had a typo or type it without the underscore or had a capital small letter or so on and so forth, this shall work. So let me just copy and paste um. The last tool, less available devices. And that should be it. And now the last fun part, which is the agent, which puts all those things together. And so I pre-created this class which is called Factory Agent. It uses the GPT OSS model, the edge model that I deployed at the beginning of the session. And here It uses this big system prompt. I would like to walk you through that prompt and how I structured it because it might be relevant to you. So the first thing that I pass to the prompt is some orientation to how the tools look like, what tools does it have access to. And the second thing is some scenarios, some possible scenarios and examples of users' interaction, and based on those user interactions, what tools that the agent should be using one after the other. So it's some sort of a few short prompting here. Right. And then we have the system prompt. We have the model, and we have the tools I imported from the class. We instantiate the trans agent and we are good to go. Finally, we need to test our agent. And see how the result looks like. Before the session, I asked my best friend Kiiro to create a nice UI that I will be using to test my agent, so I will be just invoking this agent, this UI. Yep, and here we have our UI. So now since we have our agent, I would like you to put yourself into the shoes of the operator inside the cookie factory. Remember my first slide when the visual inspector has detected a problem inside the cookie? Now you are a junior operator inside this cookie. Factory and you want to understand what is the problem. You know that the visual quality inspection has two devices in front of it which is the freezer tunnel and the cookie former machine. So as a junior operator now, what I would do is ask a very simple question, debug. Freezer Don't. That's a very simple, but it's a powerful enough prompt in order for the agent to understand or try to resonate what is wrong with this freezer tunnel. And as you can see, It used the listing devices because I didn't use the 100% correct name of the device inside my CSV file. It used the get telemetry data tool that I just coded and it used the search documentations tool that we also have written for the reg. Let's see why that. And as you can see in the results, I will make it bigger in a second. What it did is it retrieved the telemetry data and it compared the telemetry data with the correct results from the documentations to see if those measurements make sense or not. And in our case it detected that. The temperature is not in the correct range, and here it also suggested some follow-up actions in order to debug this machine in order to restore it to the operating state. OK, perfect. This is what happens when you have a factory start listening. Everyone And everything inside your factory are now talking with the same language. Which means machines, IOT sensors, systems are fully integrated together, where you can have an AI system that can give you, gives you a different types of recommendations, explanations, insights about your factory. The important part here, and this is a key takeaway, is We connected the operations by having a full deployment over outpost EKS local cluster. In case of any disconnection, you still have access to your control plane. You still have access to your data plane, and you can manage your infrastructure completely in case of any disruption. Second is deploying a small language model at the edge, which makes this kind of intelligence, real-time intelligence, you're understanding what exactly you are deploying the agents, getting the insights, and then giving a recommendation on the fly. Third, and this is a very important part is You are still generating more data every day, and you have to stay continuously learning and continuously understanding because the data is your gold. Once you get this data, update your model, trigger the pipeline again, and fine tune the model. I encourage you to scan those 3 QR codes to do the full end to end deployment of agentic AI, a small language model at the edge. OK. Now it's your turn to start with the one single process, get some data, start to understand your manufacturing production line in a more detailed way. Start to start small, fail fast, understand your data, structure the data in a correct and proper way, and then fine tune the model, get the fine tune the model, speak your language. Generating the correct information and then doing the deployment on the edge over the outpost. Thank you, and I, it was really a pleasure having you as part of this session, and I encourage you to uh take the survey and give us. The season thank you, thank you.