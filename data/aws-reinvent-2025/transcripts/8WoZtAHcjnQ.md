---
video_id: 8WoZtAHcjnQ
video_url: https://www.youtube.com/watch?v=8WoZtAHcjnQ
is_generated: False
is_translatable: True
---

Well, welcome everyone. Thank you for joining us today. So whether you're managing your IT infrastructure or steering your organization's IT strategy, there's one challenge that keeps us all united, that's the complexity of large scale migrations. You are here today because you recognize the significant challenges large scale migrations present in your cloud journey. My name is Tulba Oxal. I'm a go to market specialist at AWS, and today I'm joined by Jeff Bartley, principal product manager for Data Sync, and we're super excited to have Aditya Dut, VP of Engineering at Path AI, join us today. Aditya has a very exciting story to share with us. So from an agenda perspective, I will introduce you to Data Sync, go over our use cases, and then cover our recent launches, and then I'll hand it over to Aditya who's gonna talk about the customer success story and the path AI use case. And then Jeff is going to provide a deep dive on Data sync and wrap up with some resources. We're living in an era where enterprises are creating exabytes of data every single day, and it's not slowing down. In fact, data shows that it is growing at a steady pace year over year with no sign of plateauing. An average enterprise works with hundreds of applications and we're talking about sometimes more than 500 systems creating petabytes of data across multi-cloud environments. And it's not just about data, it's about maintaining data governance and data quality across these systems that were not built to work seamlessly together. Security and reliability have absolutely become critical for every data transfer, every backup operation, you have to maintain the highest security standards, ensuring zero data loss. You're moving data which represents the lifeblood of your business operations, so any compromise in security and reliability may have consequences. Enterprise data does not live in a neat, tidy data center anymore. It's distributed across regions, some living in edge locations scattered between multiple on-premises data centers spanning multi-cloud environments, so it becomes very complicated to work with this data that's all over the place, and it requires sophisticated orchestration and management strategies to maintain security and performance standards. right. So as organizations go through the different stages of their migration and modernization journey, they face the complex challenge of working with massive amount of data. And some organizations will do it using do it-yourself tools, perhaps combined with some custom solutions or open source tools. While it sounds plausible at first, it can become very complicated fast when you're working with petabytes of data, billions of files. Data verification is an area that often gets overlooked. I've had customers who had to restart their entire data process because they had not implemented the proper data verification processes in place. Errors are going to happen. The question is how do you recover from errors gracefully and keep on schedule? How do you transfer your data in the most secure way, in the most efficient way? And most importantly, how do you assure the performance that you need? AWS Data Sync was introduced to overcome these challenges associated to large data transfers. It's our online data transfer service that moves file and object data from on premises, other clouds, and AWS. It's fast and easy to use with built-in features such as advanced filtering, flexible scheduling, precise bandwidth control, and more and more reporting for your data transfers at scale. It's secure and reliable. We encrypt your data at rest and in flight. In fact, it's built on a custom network protocol that maximizes your available bandwidth through parallel throughput operations, and it will recover from some of those network failures. And you can scale it to any data size. And then lastly, it's fully managed, meaning we take care of all the heavy lifting for you so you can focus on your data migration strategy. And perhaps one of the most differentiators for DataSync is its deep integration into the AWS ecosystem. So when we go out and talk to customers, we see them using DataSync for one of these 4 use cases. Our primary use case is migrations, so customers use Data sync to quickly and easily bring in their file and object data to AWS. They could be moving from on-premises environments or from other clouds or moving data between various AWS storage services. Some customers use DataSync to replicate their data. They make a 2nd copy of their data primarily for disaster recovery purposes. Archive data is cold data, infrequently accessed data that takes up a lot of space from your on-premises storage. So we see customers using Data Sync to bring in their data, freeing up that space from their on-prem storage environment, moving it into S3 Glacier or S3, where they also take advantage of the cost and the durability of these services. And then lastly we see customers using Data sync to accelerate their business workflows, so customers in life sciences industry work with these on-prem equipments like genome sequencers that output a lot of data, and it's really critical for these businesses to bring in this data into cloud for processing. So we're seeing more and more customers using DataSync for these types of recurring. Data transfers. So what can you do with Data Sync? There are 3 data movement scenarios Data Sync supports. First, you can use DataSync to move your data from your on-premises storage environment into AWS. Some common use cases we see here are with customers exiting their data centers or shutting down their storage service. You can connect any storage service that talks to NFS or SMB protocols, object storage, or Hadoop. And you can move your data into Amazon S3, any of our FSX file systems, or Amazon EFS. And DataSync has the ability to move your data and your metadata. So it's really critical to preserve the metadata information such as time stamps, permissions, or Windows. You can also use Data Sync to move from and to other clouds into AWS. So we support Google Cloud Storage, Azure Blob, and others on this list. So if your data is in another cloud and that cloud offers S3 compatible object storage, it's very likely that Data sync can work with it. It's in both directions so you can move to and from other clouds to AWS. And at all times we use secure protocols when communicating with the other class so you can be assured that your data is encrypted. And we support S3 EFS file systems or as the destination. So customers who have a multi-cloud protocol or customers who are trying to consolidate their data in one single cloud use DataSync for their use cases. Mr. And then thirdly, if your business requires you to move data between any combination of these AWS storage services, you can use DataSync for your use case. That could be moving data across accounts or across regions. Um, you could be moving between two Amazon S3 buckets to S3 regions or moving from S3 to EFS, so any combination of these storage services. And the and the data is transferred over the AWS backbone backbone. There's no infrastructure to manage. It's fully service managed end to end. AI is an emerging use case for Data sync, so customers use DataSync today to bring in petabytes of data into Amazon FSX4 Luster or Amazon S3, both of which provide high throughput and high scalability. So customers bring in their data into AWS to build their data lakes which then is used to build their training models and then they can use services such as Amazon Bedrock to do for the processing of their data pipelines. Another use case we see is customers using DataSync to move their training data around. I had one customer who used DataSync to move between two Amazon S3 buckets for GPU optimization to support their expanding models. We'll also hear Aditya talk about how Path AI successfully used DataSync to bring in their image data sets to S3 to support their diagnostic workflow. Let's go over some of our recent launches. Data Sync enhanced mode is a new capability of Data Sync that we introduced last year, and it enables our customers to virtually move unlimited number of files for their S3 transfers as well as their cross cloud transfers. You also get enhanced metrics and reporting for your data transfers. And enhanced mode has the ability to transfer very large files with increased transfer speeds. So and the way enhanced mode works is it will break down those large files into pieces and then transfer them in parallel which leads to higher performance and increased transfer speeds. Our customers, especially in the media and entertainment industry who work with very large files like in the terabytes of data or hundreds of gigabytes in size, are finding increased transfer speeds with enhanced mode. Enhanced mode also simplifies your cross cloud transfers. There's no infrastructure to manage. You no longer need to deploy an agent in the other cloud, and it provides a very easy setup. All you need to do is create an object storage location and point it at the endpoints in the other cloud and get started. So in summary, Data sync enhanced mode provides increased scalability and performance for your data transfers. I'll hand it over to Aditya. It Hey everybody, I'm uh excited to talk about how Pathea is accelerating digital pathology using AWS. So at Path AI our mission is to improve patient outcomes using AI powered pathology, helping labs and clinicians get access to more modern tools such as AI to be able to do their diagnosis and do case reviews. But behind this innovation is a large data infrastructure problem. How do you move petabytes of tissue imaging data from on-prem into the cloud, securely, reliably, and at scale. So in this session I'll talk about, you know, how at Path AI we've been able to develop a data pipeline using AWS Data Sync to solve this particular problem. But before I get started, let me tell you a little bit about how pathology works. So it all starts when a patient needs to get a biopsy. So after a biopsy is taken, processed at a lab, this biopsy is mounted on a glass slide, and then pathologists look at this glass slide underneath the microscope to be able to assess the, the, the diagnosis. This particular process has not changed over 100 years. It's been practically the same. It's very manual. It's very physical, and it relies on the pathologist's ability to kind of look at these glass slides underneath the microscope to make this assessment. And as volumes, case volumes go up across the world, there, you know, these, these particular manual steps have become a huge bottleneck. So let's dive a little deeper into it. So pathology is at the central of almost all cancer diagnosis. Yet it's the last imaging modality to go digital. A lot of these labs are extremely manual, they're still using glass slides, they're still using microscopes, and all the cases that they review are manual. And once again, as case volumes go up, there simply aren't enough pathologists in the world to keep pace. And this is where digitization can really help. Once you digitize these glass slides, you're able to push them into the cloud and then run AI on these glass slides, uh digitized glass slides, so that you could get a more consistent diagnosis. You're able to do remote review with pathologists and also enable a more data-driven case workflow. But this particular innovation also means that all of these glass slides now need to be digitized, and there are a lot of data that's created that needs to be managed, maintained and stored. So at Path AI we're tackling this challenge head on. So AI site, the platform we've we've developed, is used by labs and biopharma partners to be able to manage these whole slide images, share them, and then run AI on top of them. But to give you a glimpse, every glass slide that's digitized could be over 1 gigabyte worth of data. So these glass slides are quite massive. So AI site is built on AWS. It uses fast, secure architecture and storage and uses automation tools like DataSync to be able to push these images into the cloud. And this is what has allowed us to be able to move labs from a very manual workflow into the cloud and digitize them so that they can get access to more modern tools. So what exactly is the bottleneck within these labs? So these labs generate a ton of terabytes, petabytes worth of imaging data on a fairly regular basis once they're scanned. So these whole slide images are stored on local storage systems behind strict firewalls, and these labs have never been built to be able to push this data seamlessly into the cloud. And oftentimes these IT teams are overburdened. The lab setups are unique. They use different set of network configurations, different scanners, and they simply need a solution that works fairly seamlessly. And this particular problem compounds even more because health systems oftentimes they're dealing with multiple labs that operate in multiple geographies across state lines, across countries. And every single one of these labs have a different network configuration, different set up altogether, but they need a standardized solution such that all of these whole slide images that they're generating can still end up within a centralized cloud environment where pathologists can then log in, access these cases, and do their review. So the challenge here was how do you build a seamless data pipeline that allows all of these labs to come online and go digital. So what we've utilized to enable this architecture is use AWS DataSync. So at labs, the data sync agent lives, uh, runs on a local hypervisor that talks to the local storage systems and is able to push this data into the cloud into a lab's S3 bucket. This is what allows the uh data sync agent to run in the background to be able to push these hold slide images every few minutes into the cloud for AI site to be picked up so that pathologists can look at these cases and then run AI tools on them. Yeah, but So let's look at this particular architecture that we've deployed across many labs a little bit more in detail. So on the left hand side, you've got the on-prem lab environment. Every lab has a unique set of hold slide image scanners. They're talking to, they're storing these local hold slide images into a local environment. And then a data sync agent is able to push these hold slide images into the cloud, particularly in the lab's AWS environment in their S3 bucket. From there, these hold slide images are pushed into Path AI's AWS uh environment so that AI site can uh uh utilize these images, process them. But there is another piece of metadata that's also important that we need in order to generate and make the case ready to be reviewed by the pathologist. This is the patient metadata. This is the case metadata. So we use our middleware AI site link that communicates with a lab's information system, uh, to be able to retrieve this patient metadata using something called HL7 messages. Together with the digitized whole slide image and this metadata, the AI site back end processes these images and then makes it visible into AI site for the pathologist to be able to view and then for them to also run a variety of different AI algorithms uh to get a better assessment of the diagnosis. So this particular architecture has allowed us to uh bring a lot of labs, digital and bring them online and use our solution AI site. So what's the operational impact of all of this? That solution that I described had allowed us to bring labs online in the US, in Europe, and also in South America. Petabytes of whole slide images on a regular basis can be moved into the cloud and be, uh, used, uh, through AI. This has also streamlined a lot of IT operations as well, where they don't want to deal with complex solutions, uh, no manual uploads. They can deploy data sync agents to manage this data into the cloud fairly seamlessly. And then altogether, uh, this is what has allowed us to bring digital pathology into kind of the modern era where labs are now going digital from a physical to a uh a more whole slide image based uh pathology. I thank you, and then I'll pass it over to Jeff. WS is being able to see how the products and services that we built impact customers and customers' lives, and I, I, I love seeing stories like that. So thank you very much for sharing that. Uh, so my name is Jeff Bartley. I'm a product manager on the Data Sync team and, uh, gonna do a deep dive into Data Sync, and we're gonna frame it. Kind of in a uh kind of a real life scenario we're gonna frame it in a migration use case, but a lot of what I'm gonna talk about is applicable to the other use cases that Tuba mentioned or whether it's archive or replication or the ongoing data movement use cases pretty much. The same things apply to what I'm gonna talk about here. So our deep dive configuration, I'm gonna walk through just an example of how to use DataSync in this configuration that we have here, which is, um, the goal being to migrate data from an on-premises NFS server to uh our S3 bucket located in the US West 2 region. Um, in between there we've got a direct connect link. So Direct Connect provides a private network between your on-premises environment. And your VPC running in AWS and in our case we've got 210 gigabit per second links that are bonded together to give us a total of 20 gigabits per second of network bandwidth and that's gonna come into play later uh maybe you're thinking wow I'd really love to have 20 gigabits per second of performance up into uh data into uh AW. I actually work with customers who have like over 100s of gigabits per second of bandwidth, so it is possible, um, but uh you know and Data sync works in those kind of environments, but we'll talk about how this works with Data sync here in a second, but that's our goal is to get that data migrated and I'll I'll walk through. I'll use this to frame our deep dive discussion. So specifically we're going to talk about kind of 3 general areas which cover a lot of the questions when I'm working with customers who are trying to utilize Data sync, often the questions are coming in these kind of 3 areas. So the first is we'll talk about the data sync agent, what it is, how to deploy it, some of the decisions that you have to make and considerations you have to think about as you're working with the data sync agent. Then walk through running a test and and you know often I'll see customers who they're in a rush to get their data moved, particularly if it's a migration they need to exit a data center or get that data moved quickly and they'll just run right into trying to do their migration, trip over themselves, and you know realize that they missed some steps. And so running a test is always a really good best practice whenever you're utilizing data sync. And then based upon those test results, I'll then show you how you can think about optimizing performance for data sync. So let's start with talking about an agent, so. Aditya mentioned the agent Tuba mentioned it as well. So the data sync agent is a virtual machine that you deploy outside of AWS. It's used to access storage that's outside of the AWS environment. This could be storage systems located on premises. It could be storage systems in other clouds, but it's, it's storage that we as a service, a data sync service, can't access directly. Um, as a virtual machine, it deploys, uh, various hypervisors. We support VMware, KVM, Hyper-V. We recently, uh, enabled support for Newtonics. So if you got that, we can work with that as well, and you can also deploy it, uh, as an EC2 instance. And the agent brings a number of advantages to these environments which I'll talk to you in a little bit, but one of the big ones is that it compresses data in flight and so can often help with increasing. Uh, your speeds or optimizing your network utilization. So, um, so let's dive a little bit deeper into it. So one of the common first questions that I get from customers when they're looking to utilize Data sync is do I run the agent as an EC2 instance in AWS or do I run it as a virtual machine either on premises or maybe in another cloud. And there are various things that that you need to consider um in these in these scenarios. So if you go with running the agent as an EC2 instance, um, some of the advantages are the simplicity of installing it. So I, I work with a lot of customers who may operate. Their environments where there's a lot of overhead for getting an actual virtual machine deployed in their environment. There might be maybe it's another team who controls deploying virtual machines, or it could be that it's just difficult to get VMs deployed on premises and it would be so much easier if I could deploy as an EC2 instance and you can certainly do that. What you need to consider though is that the network protocol that the agent is going to use to talk to your storage might be sensitive to latency. So for example, in our case we're going to be working with an NFS server, so the agent needs to be able to communicate with that NFS server over the network. That means that that path from the EC2 instance to the on-premises storage needs to be generally low latency. Um, protocols like NFS or SMB or others are sensitive to latency and so typically you want, you know, single digit millisecond latency that's gonna give you the best performance. Uh, you can, it'll work if you're up, uh, into the double digit milliseconds, but if you start going above that you're really gonna start hitting issues. So that's one of the challenges, uh, with deploying, uh, the agent as an EC2 instance. On the other hand, If you can deploy it in your on-premises environment, you get a couple of advantages. The biggest one is that DataSync is now able to use our custom protocol to communicate data over the network. So now what's happening is that the agent has that short network trip to get to the NFS server. It's low latency, typically very fast, and then we use our custom protocol to move the data over the network and we're able to better optimize data movement over. The network, like I said, we use compression. All data is encrypted in flight and as it's moved over the wire, and we're also using parallel streams to maximize bandwidth and we're also very we're redundant to things like packet drops or you know network retransmits, things like that so we can handle all of those situations and provides for a much faster experience moving data over the network. So if you can. And you're working with data in your on-premises environment, we recommend running the agent on premises if you can for that reason. Now when you set up and deploy an agent you've got two decisions that you need to make like how am I gonna connect my agent to the data sync service that's running in the cloud and we offer two types of endpoints. The first are public endpoints which enable you to connect the agent over the Internet, and the second is using private endpoints which utilize something like a direct connect or a VPN providing that private connectivity from the agent into the cloud. Now when you're working with private endpoints, one of the things to understand is the communication path that Data Sync leverages in order to move data. So we have two kind of traffic streams. The first is for control traffic, and that traffic goes from the agent through a VPC endpoint that you would create in your VPC and subnet, and that's things like instructions from the data sync service on when to run jobs or tasks and move data. It's for uploading logs, things like that. But the majority of the data is being moved over the data path, and that data path actually takes a separate, uh, network path. It actually goes through, uh, ENIs or, uh, network interfaces that the data sync service creates in your subnet in order to actually connect the agent directly to, uh, our system running in the back end and give, give that direct. Uh, path of data movement, highly optimized and the other thing that it does as well is that it avoids routing data through the VPCM point which adds an additional per gigabyte charge for data movement. So in this case you know we're we're bypassing that while at the same time achieving the high levels of throughput and performance that our customers expect from data sync. And this, this is important to understand because especially if you're setting up firewalls or thinking about you know how uh you know another area that customers can sometimes run into challenges with data sync is that we create multiple ENIs per data sync task that you create and so you want to make sure that your subnet has enough IP address space to support the various tasks that will run as you launch your data sync tasks. So something important to keep in mind there. So in our case we've got our direct connect links so we're gonna wanna use a private endpoint. So I'm gonna in my subnet I'm gonna deploy my VPC endpoint and then I'll deploy my data sync agent in my on-premises environment and then I'll configure it and activate it with the data sync service running in my region and in my account and that associates that agent. Uh, with the data sync service and from that point on, the agent can only be used with that account and with DataSync running in the particular region that you activated in. OK, so that's deploying and working with agents. So the next step that I would wanna do is I would wanna run a test of Data sync and get a sense of, you know, what is the performance like, um, you know, validate that I can correctly connect to my storage, uh, validate connectivity over my network like I mentioned, firewall settings, um, and you know, typically I'm gonna do this with a small data set. I don't wanna transfer everything. I just wanna get a sense of how quickly I'm gonna be able to transfer data. So when I, when customers first come to me and they're asking about data sync, I often come back to them and tell me about your data set. Are these large files, are they small files? Is it a mix, um, you know, what does the folder layout look like? And um so there's obviously a wide variety of data and understanding that can help you in optimizing the use of data sync. In our case, for our example, we're going to use a data set that looks like this where data is split across multiple years. Older data is read only. There's uh it's not being modified and it's a mix of large and small files. Now when you're using Data sync, as Tuba mentioned, we have built-in filtering. We have different ways for you to specify what data you actually want to move. So you can use include filters to specify the data that you want to copy. You can use exclude filters to leave things out by, you know. Temp files or things like that or you can use a manifest where you can specify a list of files to copy to us and we'll only copy those direct files and manifests can be useful in situations where you have a well known set of files that you need to move on a regular basis and you want to avoid the overhead of data sync scanning to figure out what's actually changed in your data set. Now in our case I'm gonna start my my test with just using a simple small set of data. So I'm gonna copy the January folder from 2025 and I'll put that in as an include filter so that data sync only focuses on that one particular folder and doesn't copy anything else. My next step would be to create a data sync task. So it's a task consists of a source location which tells Data sync how to connect to my NFS server using the agent that I deployed, then my destination, which is an S3 bucket, and then I configure my task option using an include filter to specify what it is that I want to transfer. And then when I go ahead and run this is an example of me running a data sync in the console you can see obviously it's it's a time lapse video, uh, but you can see how it runs uh as it's as it's running it's transferring, giving you an idea of what's being verified. And it transferred a little bit, about 550 gigabytes of data in about 17 minutes, achieving about a 550 megabyte per second throughput. So in my case, where I'm looking to get 20 gigabits per second of performance, which is, you know, like 2400 megabytes per second, I'm well short of that. And so in my case I would go back and I would start thinking about well where is, where's the issue that I'm hitting and for our example I've gone and I talked to the networking team and the networking team told me well you know uh there's a limited, there's a bottleneck here between your agent and the router that leads out to the direct connect. A single agent can only achieve up to 5 gigabits per second of performance, so. In this case, I need to, if I want to take advantage of that full 20 gigabits per second of network bandwidth, I have to attack my problem a little bit differently. So let's talk about how then to optimize performance using Data sync. So in this case we're going to focus a little bit on migration patterns, um, talking about first time versus incremental transfers, and then how you can scale out tasks and agents to achieve higher performance levels. So most migrations, uh, which is what we're trying to achieve here, uh, in our use case, um, typically follow this kind of pattern where you start with an initial transfer of your data that's most of your data is being copied to your destination and then you're running incremental transfers over time where um you're, you're uh getting the differences that have occurred, any changes that were made between the first time you copied your data. And your cut over and your cut over is where you actually move your application from your original data set to your new, uh, to your data set that's now located at your destination. In this case it would be, um, in S3 and, um, and knowing that cut over time is super critical when it comes to migration. So Data sync's got a couple of capabilities that really help customers, particularly when it comes to migrations. So one of the things that it does, and Tuba mentioned this is that it copies file data and metadata. So if you're migrating file systems, this is obviously super critical to preserving your permissions that are on your files and folders. Uh, it has the filters that we've talked about enabling you to copy only the data that's necessary, and then, uh, it can really scale to maximize your bandwidth, which we'll talk about in a little bit. For incremental transfers, DataSync has built-in scheduling so you can run Data sync on a schedule, which you might do in a migration scenario where you set it up, let's say, to run every day. It's picking up the changes, um, and it'll run automatically at that time and as it runs you kind of get an idea of what your cutover time is gonna be and DataSync also has detailed logs and audit reports that enable you to really make sure that the data that you expect to be migrating is being moved. So in our case, you know, with a single agent we achieved about 550 megabytes per second. We want to, in order to really maximize that 20 gigabits per second of network bandwidth, we want to scale that up, and this is a common pattern that we see with Data sync customers where they're. They've got a lot of network bandwidth and a single task alone can't achieve the level of performance that they need. And so what they'll do is they'll partition their data set into by folder typically and then they'll run multiple tasks in parallel, each one of them using their own separate agent. Um, and if you're looking to better understand, uh, this pattern and how to apply it, you can go ahead to that QR code there, uh, read a great blog that was written by one of our solutions architects. So we're gonna use this pattern in our case and we're going to partition our data set by. By those years, so we'll copy from 2022, 2023, 2024, and 2025 in parallel to maximize our our available bandwidth. Now we're not going to copy all of it. We're just going to start again with a test to verify that we really can scale. Uh, our performance, but the first thing we'll do is we'll, uh, replace that single agent with 4 data sync agents on premises, um, and activate them the same way we did previously, and then I'll create 4 separate tasks. Again with my source, my destination, and this time I'm going to set up an include filter for each task to point to the February folders in each one of those years. So in this case we're going to copy similar size data set as we did with our first test, but we're now going to do 4 times as much data. So let's see what that looks like here. So this is actually 4 tasks running in parallel. You can see the start times there. They're all kind of running at the same time. Um, and you can see that, uh, again just like the previous one they're transferring data in parallel. Um, all of them are achieving roughly 520-ish, uh, megabytes per second. So the good news here is that our, our on-premises. Storage can scale to that level of performance and you can see they're all kind of flatlining as they get to that, you know, like 550 megabytes per second showing us that they're hitting that limit of the network that we pointed to previously but in aggregate in aggregate. They're achieving well over 22 gigabytes per second and you can see we moved about 4.2 terabytes of data in under 40 minutes, so a pretty good, uh, pretty good transfer rate. And really being able to move a lot of data quickly. So with this, what I've shown myself is that yes, I can scale Data sync out pretty massively. In fact, I've had customers use this same pattern to move petabytes of data a day using DataSync, running tens to you know dozens of tasks in parallel. OK, so we've talked about optimizing performance. So let's go ahead and, and just wrap up, um, and so some of the things that we talked about, so Data Sync, um, provides, uh, Tuba talked about our, our use cases, migration, archive, replication, data movement workflows, all of these can benefit from DataSync and its ability to move data quickly and reliably. Um, you can use it, uh, data sync with a variety of, uh, storage systems on premises, other clouds, and AWS storage. Uh, Tuba talked about enhanced mode and its ability to increase scalability, able to move virtually unlimited numbers of files, um, at very high levels of performance, and then, uh, we showed how you can use Data Sync to scale out your data transfers with multiple tasks, maximize your bandwidth, and achieve high levels of performance. If you wanna learn more, head to our our website. We've got, uh, blogs and demos and use cases and more that you can learn more about Data sync. And then we've got a chalk talk coming up tomorrow if you wanna get, uh, a little bit deeper, uh, specifically in moving data between other clouds, definitely would recommend that, uh, go ahead and check that out. And with that, I wanna say thank you and appreciate your time and thanks for being here.