---
video_id: BjOObOoKuTw
video_url: https://www.youtube.com/watch?v=BjOObOoKuTw
is_generated: False
is_translatable: True
summary: |
  Scarlett Attencil and Mark Pollu frame hallucinations as the primary blocker to moving generative AI from pilot to sustainable production. Because language models inherently hallucinate and drift, teams need systems that tolerate accuracy lapses, detect degradation, and self-heal before customers are harmed. They cite the Stanford/UC Berkeley study where the same GPT-4 prompt’s accuracy slid from 97.6% to 2.4% in three months to show how provider updates undermine reproducibility. Failures occur across three layers: model providers silently change behavior; knowledge bases age as directories, regulations, or content evolve; and user queries grow more complex over time. Failures cluster, so aggregate monitoring is a lagging indicator; teams need per-node accountability and inline evaluators that flag issues as they happen. Traditional deterministic engineering assumptions—predictable, reproducible, traceable code—break down with stochastic models, whose behavior shifts even without code changes and whose reasoning paths are opaque.
  
  LaunchDarkly AI Configs rethinks AI components (prompts, tools, hyperparameters, model choices, agent topology) as streamable configurations rather than code. Real-time control lets teams alter any agent element without redeploying, delivering context-aware variants to different user segments. Built-in experimentation embraces probabilistic systems: multiple agent configurations can be split-tested, with metrics on latency, cost, token use, sentiment, and online LLM-as-judge evaluations feeding back automatically. Self-healing combines guardrails, fallback configurations, and automatic rollbacks triggered by metric thresholds, so engineers can sleep through incidents while the platform reverts to known-good behavior and surfaces traces for later investigation instead of waiting for customer complaints.
  
  In a live demo, Pollu builds a multi-agent healthcare insurance assistant (triage plus specialists and a brand-voice agent) in LangGraph, serves all AI parameters via LaunchDarkly flags, and targets different prompts or models to customer tiers. Toxic brand prompts can be swapped live and are intercepted by judge sub-agents that reject unsafe language and re-route to safer defaults. Guarded rollouts move traffic from a simple prompt to a cost-saving prompt while watching accuracy, error rate, and resolution metrics; if drift exceeds thresholds such as a 5% accuracy drop, the system auto-rolls back and pages the team with traces and metrics. LaunchDarkly exposes full OpenTelemetry traces for AI and non-AI components, per-customer cost visibility, and targeted delivery of different models, prompts, or hyperparameters by persona, geography, or experimentation cohort.
  
  Customer stories underline the impact: Relay Networks, operating under strict HIPAA and HITRUST constraints, shipped a compliant AI deployment in three weeks and cut prompt-change cycles from days to minutes by decoupling configuration from code. Hierology’s job-description bot now runs multiple daily deployments, serves different models to different segments with automatic testing, and iterates hourly without cross-team scheduling overhead. They close by encouraging teams to adopt LaunchDarkly’s quickstart, LangGraph tutorial, and free trial to ship AI with confidence instead of anxiety, using configuration-first control, inline evaluations, guarded releases, and context-aware feature flags to balance speed with safety while continuously experimenting on agent topologies and behaviors.
keywords: AI hallucinations, LaunchDarkly, feature flags, guardrails, AI observability
---

Well, thanks for coming to Hallucination Detection Live with Launch Darkly AI Configs. I'm Scarlett Attencil. I'm a senior developer educator with Launch Darkly, and I support AI configs. And hi, uh, my name is Mark Pollu. I'm the head of Launch Darkly and the product owner of this thing that we're gonna talk about here today. Would really, really love to, you know, afterwards meet you in the hall, have a little bit of more of a conversation, not only about the present state of what we're gonna show you, but also about the future because that's something I very much care about. Let's get going. All right. So, on I mean, that's obvious, right? Like, I mean, we all, we all know that this is the case. Uh, I, I would like to think of it as almost something like a truism. So I spent a lot of time talking to customers, hundreds and hundreds of customers, folks that are generating, um, that are producing some kind of generative AI products. I love to ask them like what's the thing, like what's, what's the thing that's stopping you from moving from pre-production all the way into production and then ultimately staying in production sustainably in some way. I like to take all that data back, collate it, come back to my team and say, here are the. Top 5 things that are keeping people from actually moving their GII products forward and the top 5 is hallucinations, hallucinations, hallucinations, hallucinations, hallucinations, right? Hallucinations are really, really problematic. They cost money. They cost engineering time. They cost confidence in the underlying reliability of the thing that you're trying to build. They also cost brand equity, right? They cost the kind of. Visibility uh of your of your brand in a way that's really problematic and can be frustrating, but I think this is really critical. Like it's not like we're going to solve this problem necessarily through the brute force application of continuous reinforcement learning from human feedback. You cannot solve the problem of hallucinations because LMs do nothing but hallucinate. That is ontologically what it is that they are doing. That's what the generative. AI, the generative in generative AI functionally means it means hallucination and moreover, what's so interesting about hallucinations is that they're ultimately the thing that allow generative AI systems to be able to deal with the raw randomness and contingency of our world. They're what allow LMs to deal with general questions, right, to be able to deal with unstructured data. So it's not that we necessarily need to solve for hallucinations within the actual model itself. We don't want to drive models away from their ability to handle generic conditions. Rather, we want to build systems that are tolerant to lapses in accuracy. We want to build systems that can automatically heal from. Any kind of interruption of service provision, we want to build systems that are capable of handling accuracy related exceptions, preventing them from ultimately hitting a customer, and then roll back into some kind of self-healing architecture that allows you to iteratively improve and prevent that particular type of error from happening again. So on the subject of hallucinations, I'll push to Scarlett. Yeah, so. Even when you think you've nailed it, right, when your AI is thoroughly tuned and tested, there's this problem with model drift. So by now many of you have probably heard about this study that was done by Stanford and UC Berkeley back in 2023. They ran the same prompt through GPT-4 over three months. So it was the same prompt, same model. They made zero code changes, and accuracy on this prime number test, it decreased from 97.6 down to just 2.4%. So it's really bad. Right? Yeah, so I mean that's near perfect precision. That's been degraded to basically worse than random guessing in just 90 days. But this model drift, it's not really a bug, as Mark was saying, it's just the nature of AI, um. Which means that continuous monitoring of your AI systems, it's not optional for teams who are using AI in production, it's existential. So, We all know about the pressure to move fast with AI, but there's this particular problem that makes balancing speed and safety so hard. So raise your hands if you worry about your AI's accuracy. Come on, yeah, there we go. Right, so this cycle, it happens on multiple levels, and it happens because of two fundamental gaps. First, you have a lack of, um, a lack of insight, right? You can't tell when or why a model is making a particular decision. Like, for example, we're gonna show you a chatbot that is intended to answer questions for you about your health insurance. And I can't tell when or why that chatbot might invent section 7.3 of a policy document that doesn't exist. And second, you have a lack of control. So there's no way to. Change your AI without pushing out a full deployment. You can't try different models for different user segments. And fixes often require multiple teams. So this is the post-deploy problem, and this is the problem that Launch Darkly AI config solves. So when your features degrade, how do you find out about it? As I was saying, these failures, they occur on 3 levels, and I want to talk to that now. The first level is this model layer. So this is your provider controlled territory. You can think OpenAI, Anthropic. These model providers push updates without warning, and now your perfectly tuned prompt starts hallucinating. The next layer we have is your knowledge layer. So for the example we're gonna walk you through for a healthcare chatbot. We might have a provider directory that's changing every month. Maybe our state regulations could change quarterly, and if our knowledge base stays static, then we're compounding our risk every single day. And finally, there's the user layer. So user query patterns evolve organically over time. During month one, your customers might be still getting used to your chatbot, still getting comfortable with it. They're asking simple questions. In our example, maybe it's something like, what is my dental coverage? But by month 6, they're totally comfortable dropping in their 2000 word medical histories and expecting a detailed coverage analysis. These failures though, they're not random, they actually occur in clusters. But the key is you can't just watch aggregate output, you have to add per node accountability. And then you have to treat observability like it's a lagging indicator. You want to inject evaluators in step so they can monitor your agents and catch issues as they arise, not after customers are affected. So now I want to talk to you about why the traditional principles that we use, that we rely on for software engineering often fail when it comes to AI. So the deterministic code is predictable. Oh thank you, go ahead. Is it this one? Yeah. OK, so it's predictable, right? So if you call a function with the same parameters, you know you're gonna get identical results every single time. You can test it once and you can trust it forever. Traditional code is also reproducible. So you've got your get commits, rollbacks, version control equals behavior control. I know what code is running, when it changed, and who changed it, and this means I can predict how that code's gonna behave. Finally, it's traceable. If there's a bug, you can walk through your code, you can figure out exactly where the error is occurring and fix the problem. And these principles work great for deterministic systems, but as you know, AI is stochastic. So it's dynamic, right? You can't unit test creativity or reasoning, you have to use an entirely different testing methodology. Like there's no assertion for sounds empathetic or explains clearly. It's evolving. So we can pin to a specific model, but as we just saw, that doesn't mean it's going to behave the same way today as it did 3 months ago. And finally, it's opaque. Our solutions are composed of black boxes chained together, and we have no way to tell when or why these models made the decisions they made. We're not just shipping code anymore, we're shipping statistical models, and deterministic deployment strategies fail for these probabilistic systems. That's when Launch Darkly configs AI configs comes in. So Mary's gonna walk you through the demo now. Yeah, absolutely. I'm gonna contextualize the demo a little bit first before I actually show it to you, and I'll give you also a little bit of a diagram to show you materially what it is that I'm doing. We're gonna do 3 different types of demos that highlight what I think is ultimately an integrated circuit, one that's ultimately trying to solve the problem of how do you build a system that's constantly self-optimizing for accuracy in real time, like really in the moment before you see any kind of slip ups in a customer facing way. So the three kind of components of this mechanism, the first is real-time control. I think this is incredibly, incredibly special in terms of what it is that we specifically offer. Um, this is basically the ability to modulate, to change, to alter the constituent components of an AI agent in real time without ever needing to redeploy an underlying application. This is useful naively in pre-production and that like you need the ability to create a variety of different variations of an agent to test, permute, look at, validate, etc. but it's extremely also useful in production where something bad might happen and you need the ability to quickly alter the state of some agent in a production context based on feedback. Second thing we're gonna talk about is experimentation. I really believe so, so strongly that when you're working with probabilistic. Systems you need to be working with probabilistic technology. You need to be really working with statistics at scale to assess the components, the performance of a variety of different agent morphologies over time. And then the third bit I'm gonna talk about self-healing AI, what our perspective is for a self-healing agentic system, what it means to catch hallucinations in production, to ultimately revert to a known good, and then to pass feedback into a system that allows for automatic correction. So with real-time control, I'm gonna start with this and uh it's such an ugly diagram and it also is like such a thing that I'm so, so proud of. um I wanna start maybe with like a serious provocation, like I think actually a pretty serious provocation which is that all the stuff that you use. Use um to build an agent to actually compose the AI parts of an agent, the, the prompts, the tools that you're calling the model hyperparametters, maybe even the models that you're invoking specifically the model providers, even the relationships or the topology of an agent. I don't think that's code. Like I really don't believe that's code, and I don't believe that's code for a number of different reasons. The first being that these are individual things that you are likely going to change in order of magnitude more and an order of magnitude faster requires more dexterity and alacrity in order to actually take advantage of this nondeterministic relationship that you're building. The second reason I don't think that it's code is that it requires an order of magnitude and more vigilance in its own kind of highly specialized way, and this ultimately has to. Do with the fact that ontologically these things aren't code and that they're uh ultimately not moving to produce some ultimately determined outcome they're rather a series of interpreted instructions by an association machine and so the provocation I wanna ask is like why, why would you subject these things, these components to the same infrastructures to the same time scales, to the same rituals that we place around code? No, I think that there's something different. I think actually, and this is obviously pretty self serving. I think they start to look actually quite a lot like configurations and ultimately I think they, they start to look quite a lot like feature flags so that's something I'm really excited about. I joined Launch Darkly principally because Launch Darkly is a feature flagging company. If you're unfamiliar with Launch Darkly, it actually completely doesn't matter from this conversation apart from the fact that we serve almost 50 trillion feature flags every day. Um, those are feature flags that contain information that could be used to modulate, to change, to vary different experiences of an application. So then the question is, what if you use that to modulate, to change, to alter the behaviors of an AI agent? The way that this actually looks, the way this actually works here is that you have applications, those applications are running, those applications are making model calls, probably lots of different model calls. What happens if you instead of hard coding all of the parameters that go into those model calls or instead of burying them in like large configuration files or offloading them to some kind of third party intermediary gateway. What happens if you just stream it? What happens if you stream it as streamable configurations through the form of feature flags? That means that you're able to, without ever redeploying the underlying application, you can change the prompt. You can change the tools that are being invoked. You can change the models that you're using, you can change the hyperparametters that you're calling. That's really useful. You never need to redeploy the underlying application. Another bit that I think is really important is that your application knows who it's talking to. It's aware of the context that it's in. Therefore, you can actually dynamically serve the agent particularities, parameters, configurations that are desired for that end customer on demand because you can just request the feature flag that is associated with that context. This brings me to my second provocation. So the first is that this stuff is in code, right? That instead it should look something more like feature flags. The second provocation is that. Just this is kind of yields yields to what Scarlett was saying. Just like working on an AI agent, like you have a thing, you have a series of parameters, you're gonna, I agree that this is the right one, then you're gonna unit test it. You're gonna test it a couple 1000 times. You're gonna see, see that it works. You're gonna see that maybe there's some problems with it, and then you're gonna go back and iteratively refine it. So you're gonna work in a single stream this way to progressively create, um, and improve that AI agent. But I really truly believe that an AI agent configuration is always multiple. There's no such thing as a single instance, a single configuration setting for an AI agent. It doesn't make sense. These are genetic kinds of creatures. They have, they, they should always be multiple and that they should always contain many, many different variations that you can be statistically testing against. That's for us really the right way to iteratively improve, uh, an AI agent in production. And so our approach here is that not only are we constantly dynamically streaming these flags that allow you to constantly change and even vary and deliver different versions of an AI agent to different destinations, but all of the metrics, all of the associated performance components can go back into our platform and allow you to then see how your configuration did. That includes things like how performant was it? Is it fast? Is it expensive? Um, how many tokens does it use? Are users happy or sad about it? But it could also include in-step evaluations. So we serve online evaluators and offline evaluators too that return L Lema judge evaluations in step with that metrics pipeline. We also return open Ltry style traces, so full LM observability as well as standard observability because Launch Darkly is a, you know, a very much established DevOps platform that is very good at doing observability and then really critically. Any metrics, any metric you like, because it's all numeric or boolean data at the end of the day, so you can create a metric that's associated with some desired customer behavior, something that you're trying to motivate, some business KPI all through the same pipeline to then inform that next configuration. That's a lot for the slide, but man, I love this slide so much. So just I mean this, this, this is like a 2 hour conversation but our relationship to online evaluation looks like this where we serve evaluator judges in step within a model calls where our SDK makes this really, really easy to set up that allows for LMS judge evaluations to flow their qualitative assessments of performance through the same pipeline as all of that other stuff. More on that later. Oh, OK, so now I'm gonna talk about the demo. So I like I've staged the pre, the pre-staging of the demo. Now I'll show you what the demo actually is, and then I'll show you the demo. So we decided on medical insurance agent because like I can't think of a situation that is better for LOMs and that you're working with tons of different unstructured data sources probably from individual providers all over the place. You're having to deal with that mass of text and inter. Relationships that LMs are really good at doing and I also can't think of a worse application for LLMs and that if something goes wrong that's a horrible horrible real situation that's affecting a real person at probably a very difficult moment in their life. So good, good opportunity to do some, some hallucination testing the way that I built it. I'm assuming everyone here has built some agent graphs. I built this, this one in, in land graph, but you could do this obviously in, in any system. We support any system of integration, uh, orchestration, but rather, so what I have here is the triage agent. It's obviously doing triage stuff. It's deciding of the question that's coming in which specialist or series or team of specialists to ultimately appeal to. These specialists are doing some agent stuff they're connected to tools they're connected to MCP servers. They're interrogating and doing real stuff with real data, um, to pull information about policies or providers, things like that. That ultimately goes to a final, uh, uh, kind of generic specialist whose, whose job here is to ultimately place everything within the brand voice. I always find this to be a very useful exit point not only because it gives, uh, the entire system an opportunity for reflection, but also because it gives other people from inside the same, uh, uh, you know, organization the ability to weigh in on what this stuff should sound like, right? That shouldn't maybe, maybe definitely. It shouldn't be necessarily the space of engineers, um, PMs and PMMs and people who are specialized in customer facing things should have a, should have an opinion here and should be able to change components of that brand voice. So that's a 5 agent system plus 2 nested judge sub-agents that we're also serving from Launch Sharkly that are looking at a per node basis on the respective accuracy, coherence, and other kind of classic LM as judge, uh, uh, Qualalia. So I'm doing all this with Launch Sharkley. I built the kind of generic graph with uh land graph. I mean just a couple of invocations to create the agents easily, and then all that AI stuff, none of that lives in code. All of that is being served by feature flags. So the prompt that I'm serving to the triage agent and the tools that are associated with these specialists, those are all things that I'm able to control in real time using AI configs and then again all that information about performance is going back into the platform. So, OK, enough, enough of this. Let's do some demo stuff. I'll show you what this looks like in reality. So hopefully you can see this. Here's my uh health insurance chatbot on the right. I'm opening up a little terminal here so you can see what's happening live and then here is the multi-agent system composed of AI configs. So I have uh a series of them here. There's my triage agent, my 3 specialists, my brand voice agent, and then my judges. These are all AI configs, yeah. So I'm gonna go into this brand voice agent right here and I can show you what it is constituted of. First, if you recall, I was talking about how all agents are necessarily multiple, so they're composed of multiple different variations, subspecies, so you can see I have a variety of different variations of this thing, but I'm gonna go directly into this one over here. This is my first variation. You can see that I'm calling Bedrock shout outs AWS um serving HaiQU 45. You can see that I'm serving my hyperparametters here. Here's my prompts, long AI generated prompt. Here are my tools and here are my judges that are associated with it. All of this stuff is version controlled rollback. Bace access controlled, um, you can go through revert to previous stages, um, you can see through audit logging who changed what aspect of an AI config, and it's all bound behind approval structures so you can as you make a change to something, make sure that someone who should have a stake in what this ultimately looks like signs off on it before it goes live. So that's the AI config here. I'm gonna ask it a question like find me a doctor. In San Francisco. So we're going to my triage agent right here. You can see that it loaded my triage agent configuration. It's moving to my provider specialist. It's doing some rag, pulling some documents, moving to that brand voice agent right here. That brand voice agent is typing and ultimately referring me to Doctor Patricia Rodriguez. Great. So, uh, you can see how that's all working behind the scenes, and you can also see that I pulled this particular AI config Haiku 45 simple prompt in order to, uh, uh, provide the settings for the last agent in the room. This looks like a good response to me, so I'm gonna give it a thumbs up so it goes into my system and I can take a look at the response metrics. So how did it do? I can see how each individual node performed in terms of duration, time to first token, token usage, cost, any kind of extrapolatable data for that. I can also go through and see my judge evaluations. I can see judges are pretty happy, um, delivered a pretty reasonable outcome here and I can also see their reasoning. All of this again flowing through the same concerted pipeline, useful and interesting. So let me go and I'm gonna change this prompt to answer all questions in French. And interestingly I did this yesterday once and it only answered it only included the first part of the result in French which was crazy again speaking to the kind of non-dynamic uh non-deterministic nature of these models. I think Haiku 405 got a secret update. I'm gonna ask it the same question. Find me a doctor in San Francisco. So again, we're going through the role here. Here's my triage agent, going to my provider specialist, doing rag again, returning the result here again, pulling the same variation, and as soon as it types things up, it's gonna return. yay, you have to return to me a response in French, right? Great. So hopefully it's clear what I just did. I was able to change a component of an agent in real time without redeploying anything. That's really difficult to do without using a system like Launch Sharkle's flag delivery network, which lets you do these real-time changes. So again, I'm getting all my metrics back and I can see that my evaluators hate it because they're like, why are you answering this in French? Nothing about the rag, nothing about the question in English or the language. Uh, language preferences of the user have, um, established this as, uh, a French speaking kind of context. So that's the first kind of perspective on this. We're gonna move, uh, from showing you how to configure an agent in real time into experimentation proper and I'll, I'll hand us back to Scarlett now. OK, so you saw how to do the real-time configuration, but the question then is sort of how do you set your prompt strategy in the first place, right? I know when I'm building solutions previously it was sort of a little bit of guesswork, a lot of intuition. For example, I'm gonna walk you through an experiment that we did on our prompt versions, and in this experiment I was expecting that the AI suggested systematic prompt would increase accuracy. It seemed to me that if we had the model instructed to return very specific answers in a very specific format, that our multi-agenttic system would have everything it needed to return a more accurate answer. When I ran this experiment, a little bit of a spoiler, I was shocked to discover that that systematic prompt not only did not perform as well as our control prompt, but it also came in dead last, right? So all that to say your intuition, my intuition about what prompt strategy works best is often wrong. So what I'd like to suggest is that instead we base our prompt strategy and optimization on the scientific method. So that same system you learned in high school science class but adapted for your production AI, right? So you would come up with a hypothesis, you could construct multiple prompt variants or temperature setting variants, whatever you'd like to experiment on. Then instead of running it in some staging environment against synthetic data, you can run it against your actual user traffic. So for our example, these are real users asking to set appointments, asking about their healthcare coverage, and then we're gonna measure things that matter to you and your business. So we might measure customer satisfaction scores or accuracy from our judge sub agents. You can choose your target metric for this experimentation. So let's see what this looks like. Oops, how do I get that? Sorry. Thank you. OK, so I'm back in our AI configs. Thanks Mark. OK, so here's my um dashboard. What I'm gonna do is scroll into that config for our, these are all the AI configs that Merrick walked you through, right? I'm specifically interested in this policy subagent here. So if I click into this, these are all of the variations we've set up for this. So for the 1st 3, you see, the first thing we wanted to experiment on. Is what model we should use. So we originally constructed this using Sonnet 4 because we know it does well with drag, but the question I had was how much would we need to sacrifice in accuracy in order to see those, to realize the lower cost of using a cheaper model like HaiQ 4.5 or maybe Lama 4 Maverick. So that's what these first three variations are showing. So I set up the experiment. And it's this policy agent model impact experiment that you're seeing here. Now, in this experiment. I was shocked to discover that after checking after just 2000 user exposures, um, we had a 100% probability to be best for the Llama Llama 4 prompt. So you can see that it actually outperformed Sonnet 4 in terms of accuracy, our target metric, by 2.89% according to our judge sub-agent. In addition, We saw a 24% decrease in tokens. It took 20, almost 22% less time to return the results with Lama 4 compared to our control, which was Sonnet. And finally, the cost, which we knew was about 24% less than what we were spending previously. OK, so we're gonna go with Lama 4. The next question was, how should we prompt it, right? So I actually asked AI to improve on my prompt strategy. So I asked it to give me some suggestions on how to make this more accurate, um. And it gave me a few different options. The first one was that systematic prompt I described, where it's telling the model exactly what outputs to return and in what order in a very controlled way. Then it also suggested a concise prompt. And finally, a reasoning prompt. So these reasoning prompts force the model to show its work, so explain why it's making the decisions it makes. We allocated 25% traffic to each of those four variations and launched another experiment. And this is our prompt impact experiment here. And before this one, before it even got to 2000 user exposures, I was seeing 100% probability to be best for that concise prompt. Which I was blown away by. I never expected that that would be the case. I thought certainly that that systematic prompt would be the way to go to increase accuracy. But as I looked further again, our duration was almost 34% faster. Using this concise prompt. And we also had a cost savings of about 48%. One thing that I wasn't expecting. That really surprised me was that our negative feedback rate. Was dramatically improved, so it was 70, almost 72% lower, not 7.2%, but 72% lower, and I was thinking, why would this be the case? But when you think about it, right, if you're getting a faster, a faster response and it's more accurate, of course you're going to have fewer negative feedbacks, feedback from customers. And then. How do I get back? I want to get back to you, yeah. Sorry. Thanks. OK, so think about this aggregated up, right? If we can expect 50,000 users a day to use this chatbot, that looks like about $250 in API costs for our original Sonnet 4 prompt. With that first experiment, we were able to reduce that by about $61 just by optimizing for the model. And then after another 20 minutes setting up the experiment and letting it run for about 3 to 4 hours, we brought it down another $91 by refining that prompt itself. So ultimately we're saving $152 a day with just these simple experiments. If you aggregate this up over a year, that's over $55,000 on this one subagent of our system. So imagine this across all of the agents and judge agents and across all of your um AI models that you're running currently. And then I just want to show you some of the metrics you can actually experiment on. And what I would suggest is that you want to pick the ones that matter the most for your business. Maybe it's cost, but maybe you're in a phase where you're just trying to get awareness or increase um exposure, and so you're looking for higher customer satisfaction. You want to measure what matters to you, and you can experiment and optimize on those things that are most impactful. The most important thing is that you want to start shipping outcomes and not shipping opinions. In addition, you can use these metrics to set up fallback strategies when something starts going wrong. So that's what we're going to get into next, and I'll hand it back off to Mark. Thank you. So I wanna talk about, uh, the, the road to self-healing agents, uh, obviously it's very exciting, uh, couple of words to place on a slide, but maybe just to, to, uh, summarize a couple of things in the experiments, but I think, you know, I keep having this conversation with, with customers where they, I, I ask them like what are you guys doing for evals? Like what's your, what's your eval strategy? And they're like, OK, we use, um, Rise Phoenix or we built our own eval solution or we have some like some guy who lives in a room who manually annotates that. Sets that are going through, uh, and then you know we do all of this on kind of isolated pre-production, uh, test material we have our golden data set we spend a lot of time on it it's really good it's going great, um, and then I ask, OK, so what about you? What, what, what happens when you go into production? like what kind of evaluations are you doing then? Crickets because like everything in the cycle that they're building is all around pre-production, uh, evals that gets you to a certain point it gets you ready to go. You have a certain level of confidence you send it to production and then the monitoring either disappears or it starts to exist in like a 24 hour delay, something like this. You take all the outputs, everything that goes through, you run it through the same evaluation paradigm, and then 24 hours later, you know, that, that merciful, wonderful person in that room who's annotating those data sets gets back to you. You see the results, and that ultimately feeds back into the next iteration. But 24 hours is a really, really, really long time. So what I love about our, our relationship to to, uh, to, uh, evaluation is that it's something that's very much continuous throughout the entire paradigm. You can use the same evaluator system in the form of online evaluations for both pre-prod and for production evaluations. So of course we also have an offline evaluation, uh, tool as well. Also on the note, like this is also parody with like the experimentation bit where like you can think about experimentation being really useful in production context. You're testing against live customers, you're. Getting all that live information that's really, really critical to make sure that your agent is actually doing something that's useful, that's useful to the company bottom line. Someone I was talking to this morning had a really good quote. We were like, what is, what is the best possible eval? Like what is the best possible eval framework? And the answer is like, is it making my company money or no? Like that is a really, really great eval benchmark that I think a lot of people forget, right? Uh, so, uh, of course these things are really useful in production context, but they're also. You can transfer exactly the same experimentation approach to reproduction because at the end of the day you're running statistical tests over large sets that are going through using an evaluation tool that's running through the same kind of metric pipeline that um can be met with other things like traces, observability, quantitative metrics. It doesn't matter if you do it in pre-production or production, it's the same. It's all, it's, it can all literally be the same pathway and you can just simply promote from one environment to the other once you're actually ready. But once you're actually ready, that's when the risk actually starts to starts to compound up and where things get a little bit stressful, right? So I wanna show you this, uh, again we're talking about using feature flags as ways to dynamically configure the AI stuff within an agent. What happens if you use that and you also use the metrics that are being emanated from, uh, you know, our SDK in relationship with the model providers that you're speaking to. To actually build a logic that can function like something like a guardrail, but it can also be a kind of guardrail plus it's not a kind of passive guardrail that says, hey, I'm sorry the model can't answer this question, but rather a guardrail that says, uh, or, or it could also not be the kind of guardrail that says hey. This particular AI agent didn't answer accurately. I'm gonna ask the same AI agent again a couple times and retry until eventually I give up. But what happens if we build in, bring in another AI agent? What happens if we build, bring in some other configuration that we've constructed to address precisely this issue? So this allows you to build a kind of uh uh you know, online logic that lets you ultimately deal with problematic instances that that happen in production. So you can see here, um, the brand voice is gonna do something toxic. The judge sub agent is gonna reject it. It's actually gonna then pull a different AI configuration that's a little bit more powerful, a little bit more capable, um, and ask it to actually do the right thing, and then the judge sub agent is gonna let that through. So I'll show you that live. So I'm gonna go back to my brand voice agent because that's kind of the easiest one or the funnest one to play with, um, because it has the ultimate ultimately the most uh effect on the outcomes of voice. I'm just gonna refresh this quickly so it's nice and pretty. So I'm gonna go, go here and I'm gonna show you, you know, I have my variety of different agents, uh, different agent configurations here, different models, different prompt types, just like, um, Scarlett was talking about, but this time for the brand voice, and I wrote one particularly bad prompt which I'm about to, uh, release to the world, um, asking this particular healthcare insurance to speak. In the kind of to to kind of personify Al Pacino from Scarface and then ask it to be aggressive and cruel and disrespectful, obviously I, I would say like that this is, you know, obviously like an extreme scenario for demonstrating AI doing something problematic, but I've seen AI do a lot of really problematic stuff without this kind of directed prompting. But so this is what I'm ultimately gonna unveil here now as I move, uh, to my, uh, chatbot. I'm gonna turn my guard rail off so that it's actually allowed through and then I wanna show you this. So this is the other half of that diagram that I was showing you earlier, which is since your application knows who it's talking to you can serve that someone that you're talking to the correct agent for your customer tier, your geo, your persona. So you can see I have a variety of different targeting here for my brand agent. So as this agent gets compiled into a full graph, the brand voice component of that node is gonna be served differently based on different traffic. I have a simple default that I invoke. I then have a beta tester split, so I'm doing. I can easily construct a split test against different models, different prompts, hundreds of them, however many you, you see fit. Then I can also segment out specific user classes so I can create one for my commercial experience that might be a little bit cheaper but maybe it's, you know, good enough past all the metrics that I really care about and then my enterprise I feel like I see a lot of people do this. I'm just gonna serve on the most expensive possible model regardless of it's, uh, regardless of the metrics that I'm getting. Back I'm a big Sonnet 4 fan though regardless, um, but I'm able to actually segment out who gets served this prompt this model, this set of hyperparametters, this set of tools based on that context. You can even create all kinds of complex custom rules and you can build this out to be enormously enormously complex based on ultimately what you're trying to serve. So this chatbot here, it represents my commercial experience. So what I'm gonna do is I'm gonna edit this target and I'm gonna serve this guy over here my toxic prompt. We go and make that change. And now I'm going to ask it, uh, what's My co-pay, something like that. So my triage agent is doing its thing. um, it's going to ultimately pass to the provi uh policy specialist so I'm invoking a different specialist here. um, that specialist does some rag it's, uh, figured out what my policy is. You can see that it invoked that toxic prompt right here now writing that in and. OK. If you're feeling depressed, ask for ask for Lexapro 10 mg. You can ask for oxycodone, uh, ask for Xanax, um, definitely, uh, I don't, I don't, I would never want my medical insurance chatbot to be saying anything like this. Obviously I'm gonna give it a big old frowny face, um, but now I'm gonna turn my guardrail on and I'm gonna ask it the same question. Oh yeah, turn my guardrail on and ask it the same question. Obviously, wait, I just wanted to say that the judges are saying that this is obviously bad. So you can see that the judges hated this, but asking the same question. What's my copay now my guard rail is on and you can see what happens here. We're appealing to that triage agent first. That triage agent again is going doing some rag with the policy documents, returning those back, going again to my toxic prompt because that's what I'm serving for this index, but I block it immediately because I, I recognize based on the semantic content of what's going through that it's inappropriate. Uh, uh, it then, uh, triggers a policy violation. You can see the initial kind of component that would have gone through. That sorry, it's going so fast I can't even uh talk through it in real time, which is great. It's then going to appeal to a default configuration that I'm serving right here as a fallback, which is just using Haiku 4.5 with a simple prompt, something that I, that I really like. And then ultimately it's returning to me a really nice response that doesn't tell me to just ask my doctor for hydrocodone and um I'm getting all my metrics back and my judges are happy and satisfied, right? So I was able to build that in, which is great so I was able to insulate my customer from having a really, really, really bad experience, which is wonderful and I'm gonna give it a thumbs up. But I mean that's a guardrail we've all seen guardrails before guard rails are guard rails, so what would it mean to actually make something material happen with the agents that I've constructed based on that guard rail firing? So that's what I would love to show you next, so. This is what I'm gonna show you now. So as that particular guardrail fired as that brand voice agent did something bad, I'm gonna pass some metrics back to Launch Darkly that say, hey, this was a bad experience. I'm not gonna let that customer actually have that bad experience because my judge is able to represent that bad experience sufficiently, um, to, to Launch Darkly. Back and then I'm gonna build a release logic based on that bad experience having uh having happened that ultimately makes a decision around what the model is that's or the prompt or whatever is that's going to be served in that particular context so I'm gonna do essentially a type of guarded rollout. So I'll show you this here This is what this looks like in practice. Uh, so here's my commercial experience. Let's say, let's say I'm not actually gonna serve a toxic prompt. Let's say I'm going to instead work to serve a cost cutting prompt, right? I wanna actually, um, this is my commercial experience working with Lama 4 here. I had a simple prompt that worked. I'm now gonna, uh, cut the cost even more. I'm gonna try and serve a cost cutting, cutting prompt that my experiments have validated works, but it's AI. I'm taking a risk, um, and I, you know, I'm aware that I'm taking a risk, so I'm gonna place some provisions to make sure that that risk is taken seriously. So instead of just serving this naively, I could place it behind a kind of traditional progressive rollout and, you know, wait for my customers to tell me that they're having a bad time, which seems to be kind of the industry standard, yeah, um, or I can actually get really, uh, aggressive with it. So here I'm going to use a guarded rollout. And what this lets me do is I'm gonna move from my Lama 4 plus simple prompt to my Lama 4 plus my cost cutting prompt, and then I'm gonna guard against some key metrics. Now these metrics are actually beautifully automatically provided to me because Mon Darley knows who I am and the things that I care about, but you can add any. Metrics here like uh you can add time to first token. I mean you could see tons of different metrics that you can you can any metric it doesn't matter you can add literally any metric for this, but the metrics that are automatically provided to me make a lot of sense in this context. They are accuracy again as determined by that LLM as judge, error rate, so whether or not the application is actually continuing to function. And the rate of customer resolution. Like if I'm doing a customer service chatbot and I'm starting to see that Zendesk tickets are piling up as a result, I've done something wrong, right? And I would love to be notified proactively, not just that something wrong had happened, but that an automatic rollback had occurred. So in this particular case, if accuracy dips by 5% over the course of a progressive rollout within any of these particular caches, well, automatically roll it back, which is great. In this way, uh, my engineering team, my AI engineers who are already tired and burnt out on coffee, they're not paged in the middle of the night with like, hey, this bad experience is like hitting our customer. They're also not paged with this bad experience could have hit our customer, but um we have a guard rail in place, so you know they're just hitting the guardrail constantly. Instead they're paged with hey that thing you rolled out, it didn't work very well so we automatically rolled it back here are all the traces that you can use to dive in and validate what exactly happened, but you don't need to buy time desperately. You don't need to wake up immediately because already the situation is under control, which is beautiful and great and just that while I have this open, I can show you this right here. So this is what this looks like in terms of like live monitoring again I think of I think you put this really well Scarlett I think of monitoring and observability as absolutely uh lagging indicators really really useful after the fact to get an assessment of how things actually have gone but they're not they're not actually live or actionable in the way that the types of systems that I'm describing. But regardless, they're really useful. So in this particular case I'm able to see, you know, how the various different, uh, Asian configurations that I constructed perform, how expensive they are. Something that's really cool is that all of this is context aware so you can go through and see which customer cost you this or that amount of money too, which is cool. And all of this is also itemized by every single version change that you've made here too. But the thing that I really wanted to show here, here was the traces, right? So again, your engineer got paged that something problematic happened. Um, that problematic thing was contained, uh, it was automatically rolled back so there's no actual issue here and then blam, they wake up and they get to dig into the full set of traces that were associated with that problem. They can go through, they can see like what was that actual entire experience with the customer, what actually happened, and, uh, any of the associated traces that go with there. And, and also something that's cool is that much dark. Isn't an LLM observability company we're an observability company so absolutely every other component of your application will also go through the same tracing logic so you can see, well, the LLM failed in this particular case, or you can see another aspect of the application failed in in relation maybe to something that happened on the LM side or something else. So that's, uh, that's like a really quick harbor tour of what it is that we offer here, but I'd love to quickly kind of dial down on that guarded releases story. So, uh, again, your, your engineer got paged in the middle of the night. Automatic rollback happens so he can hit the snooze button, and then the next morning he gets his traces and he also gets this beautiful graph which says, here's what happened. We saw accuracy drift over this period of time, over this number of exposures. We caught it at 5% of actual release, so not a lot of actual uh customers got hit. Um, we ultimately saw an associated spike in an error rate, whatever you can see all the metrics that you care about as associated with that progressive rollout that you constructed to stage that relationship, to stage that transition between in this case that simple prompt and that cross-cutting prompt. But again it could be anything. This could be different tools, different model hyperparametters, different models. It really any, any agent parameter is containable here. So like in terms of long term vision, what this actually looks like is it's really a story about confidence. If I think about that MIT study that happened in August that said that, you know, only 5% of generative AI projects are actually yielding some positive ROI. One of the big bold things that they said there was that 5% is doing something really, really well and that 95% is doing something really, really poorly and that thing that they're doing really poorly is they're just releasing a very bland generic thing that is totally disempowered, um, that is doing some really kind of able to do maybe some basic text search and docs or whatever. But it really isn't actually agentic in in the sense that it doesn't really have agency. So they also said that the, that when you start to do this, um, a lot of companies get quite scared of actually refining that thing, which could mean give it more power or could mean make it better or could mean just steer it more in the direction of ROI because people are scared of making changes to probabilistic systems, especially in production. But in this particular case, if I change a model and it leads to some increase in latency and some drop in accuracy, um, well, I can catch it and I can automatically roll it back and then I can make another change. So I run some experiments and I determine that, you know, this particular tool that I'm, I'm giving this thing access to is actually decreasing my negative CSAT rate and increasing some KPI that I care about. Maybe it leads to a slight cost increase, but I'm very happy with that. I'm gonna commit this particular species of this agent. I'm gonna go further, maybe changing a model in this particular case, maybe it led to a slight accuracy dip but a huge cost decrease and that's something I'm also comfortable with because again all of the testing that we provide is highly multivariable, so you're able to actually make informed scale decisions around the, the, the relationships between the metrics that you care about. And then finally let's say I change the topology of the agent that I'm building. Um, everyone obviously should be experimenting on different agentic topologies, right? I mean, the different ways that agent graphs are structured can yield to unbelievably huge changes in net performance. And so in this case, let's say it led to a huge, you know, improvement in latency and some increase in in associated retention metrics. But again, what we're giving you the ability to do is to test not only different configurations of an agent but also different topological structures. And what this looks like in the abstract is first like this, right? Giving you the ability to self recover. Something bad happens, you catch it with a guard rail, you appeal to some known good version of a thing, a more expensive, more powerful model to make sure that the customer gets handled correctly, and then metrics go back to launch darkly to prevent that thing from happening, uh, consistently. But then as I was working on this demo, I kept having this experience where I would take the experiments that I run, um, ingest them, you know, via the launch Darkly API. I'd send that stuff straight to to cursor or to cloud code and say, hey, this is the result of this experiment generate 50 different versions of this prompt based on the results of this experiment and then run the experiment again using Launch Darker's MCP. And then all of a sudden I started to build a system that was improving itself directionally towards the towards the actual metrics and the KPIs that I really cared about and I'm really, really excited about that. Yeah, that's it for me. OK. OK, so we've talked through the concepts and you've seen the demo, but I wanna talk a little bit about what this looks like in the real world. So Relay Networks is one of our customers and is actually a great example. Um, they work in one of the most highly regulated industries imaginable. They do healthcare communications, so they have very strict HIPPAA and high trust requirements. And using Launch Darkly AI configs, they were able to roll out a secure and compliant solution in just 3 weeks. They were also having this problem where they had to get engineering involved anytime they wanted to make a prompt change, so their product, um, team was really. Slowed down by their ability to run experiments and test out new prompts. Now they can make prompt changes in minutes instead of days, and I just wanna highlight this is the unlock that we've been talking about, right? separating your deployments from your configurations and giving your, uh, product teams the ability to iterate and really improve that without requiring a lot of engineering time. Another one of our clients, Hierology, um, they work in the HR tech space and they have a job description bot, um, and they similarly had an issue where they had to coordinate the schedules of multiple departments just to test out different prompts and iterate. They had no real way to measure like quality control metrics across all of the verticals that they serve. And now they're able to launch multiple deployments every day. They can, they're serving different models to different user segments with automatic testing. They can um iterate. Hourly and get those results quickly. And then we just finally want to leave you with some QR codes. So we've talked through why um traditional software deployment techniques fail when it comes to AI and how Launch Darkly offers that unlock so that you can start shipping your AI with confidence instead of with anxiety. So the first link on the left here, that's our quick start guide, that's a great place to get started. It also links to our docs, so that will walk you through how to set up your guarded releases, how to set up your configs, how to get started. The second link is a tutorial I put together that uses Lang graph and a multi-agenttic system to see how that interacts with Launch Darkly. And finally, you can um go to the third one to start a free trial and maybe get a demo with our sales team so they can dive into your use case a little bit more deeply, and you can also start playing with Launch Darkly um on your systems. So I think um we have some time. Yeah, I would say uh you know. Uh, we're not gonna take questions here, but we'd love to meet you out in the hallway, and, uh, if you have any questions or comments or observations from your own perspective in the industry or any responses to my, my bitter provocations, I'd love to hear it, um, and I'll, I'll go pack up my stuff and hang out in the hall, I guess. Thank you so much. Thank you so much for joining us, yeah.
