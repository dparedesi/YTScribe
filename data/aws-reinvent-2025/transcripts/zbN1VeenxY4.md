---
video_id: zbN1VeenxY4
video_url: https://www.youtube.com/watch?v=zbN1VeenxY4
is_generated: False
is_translatable: True
---

Good afternoon, everyone. Hope you're having a good rain even so far. Well, let's start with a quick poll. How many of you are familiar with Apache Kafka? All of you, right? Great. And how many of you have hands-on experience in using a Bache Kafka? Well, majority of you, that's great to see. Well, this is a 400 level talk, so we will deep dive into the internal, so any hands-on experience will be helpful. So today we're going to talk about how Kafka is going to be used in modern architectures, extending Apache Kafka to support messaging at scale, along with its traditional streaming benefits. While Kafka has been a go to technology for handling high throughput event streaming. Many engineering teams still need patterns that requires um work independent work item processing, workload distribution, and decoupling of application in a queue-like manner. In this session we will set the context for why Q semantics is going to be very increasingly relevant for Apache Kafka and the challenges this particular approach is aiming to solve. We'll talk about the QP 932, which talks about the cue-like capabilities. We'll deep dive into the internals and learn about the mechanics of cues for Kafka. My name is Shubham Raksheed. I'm a senior streaming solution architect in AWS and I'm based in the UK. And today I'm joined with my colleague Masido Saim, another senior streaming solution architect in AWS who has come all the way from Sydney. Well, we will start with the motivation, talk about the Q seman Q capabilities, deep dive into the mechanics, and then show you a demo and wrap up with some limitations, roadblocks, and the guidance for the next step. So let's get started. What is Apache Kafka? Apache Kafka is a distributed partition replicated commit log service. Think of it as a fault tolerant, ordered journal of events to which producers happen to and consumers read from. Each topic is implemented as one or more petitions. Each partition is like an immutable app only log, which is addressed by a sequential offset. Here, the consumer topic has got three partitions P1, P2, and P3. Two different producer applications are writing events to this topic by sending the events over the network to the topic's partition. Each producer is independent in nature. And what happens is, even through the same key denoted by the color ends up into the same partition. Note that both the publisher can write to the same petition if appropriate. The individual consumer clients form a group called consumer group. And it reads data from the same partition in the sequential manner, the same way the data was written in the petition. Each petition is consumed by one and single one consumer only. However, one consumer can consume multiple petitions. While CA can handle high throughput of data and can scale massively for hundreds of applications, let's understand some of the scenarios where it hits the real limitations. In most real-time systems, ingress traffic isn't constant. It spikes during business hours, campaign launches, or any kind of seasonal events. So this can impact the processing of data by the consumers, as the consumers may not be able to keep up with the high ingress traffic. So the consumer lag keeps rising. Additionally, in typical Kafka deployment, what you see is the same data is being consumed by many applications. So you'll see that your ingress throughput is often higher than the ingress throughput. So to allow, uh ensure that the outbound consumption can scale along with your increased throughput is critical to have a good system performance. Well, one way to solve the problem is to add more consumers to the same group. But with Kafka, consumer parallelism is bound to number of partitions in the topic. That means if you want to scale the number of consumer, you also need to increase the number of partitions in the topic. And this becomes a challenge. For example, the problem is that partitioning for peak load when you are deploying the system for the first time and you do not know what the peak could be, you are probably over petitioning for that particular scenario, and this means that you end up having more number of partitions. Even if you do not need that at that point, this leads to operational overhead and cost. Well, also, if you try to repetition the topic later on, it can also pose a challenge because the changing partition count can disrupt the consistent hashing within the topic partition, so it breaks the power key ordering guarantee within the partition, and it depends if you're using a key when writing the data to the topic petition. Now one way to solve that is teams find that it is better to create a new topic with a higher number of petitions, but then again you have to migrate all your application to the new topic, which increasingly increases the complexity and overhead. The second problem is head offline blocking. What's that? Well, Kafka guarantees ordering within a petition. The consumer reading from the petition reads the data in the same order that the data is written to. Now if a single message is slow to process, maybe it requires a downstream API call. Maybe it is doing some kind of extra validation check, or maybe it's some malformed data. So all the records after that particular record will be blocked, even if those records could have been processed instantaneously. So teams typically try to work around this with 3 different approaches. One is by increasing the number of partitions. Well, it doesn't solve the head off line blocking problem, but it reduces the pain point because initially if you had 3 partitions, now that you have increased to 12 partitions, your data is distributed across more number of petitions. But then the operational complexity of handling more number of partitions that we discussed still exists. Second is asynchronous or parallel processing by the consumer, and this leads to some kind of state management you have to do and your consumers end up writing a lot of complex code to kind of handle this process. And the third is you can skip the problematic message, but then again you have to write additional code in your consumer application to handle that. Imagine if you are writing hundreds of applications and you kind of need to handle it in a consumer application, it's an extra burden to the consumer applications. The third is Kafka's native model doesn't behave like a queue. Traditional queue systems are optimized for parallel consumption, where ordering is less important than processing speed. With Kafka, however, parallelism is bound to the partition to consumer mapping. Kafka restricts the partition to a single consumer in a consumer group. So achieving a queue-like patterns for which you need power message acknowledgement, you know, retries, solving the head of line blocking problem, or dynamically scaling the workers, all of these goes beyond Kafka's default design. So teams end up building a lot of custom framework and logic to force Kafka to act like a queue. Which increases the complexity. Well, these problems act as a foundational motivation for the open source community to think about how Kafka can be extended beyond streaming. What if we could use Kafka as a cue? And that leads to keep 932 queues for Kafka. The keep implementation has evolved Kafka from a data stream and to also handle work queues. Ques for Kafka introduces share Group, a new type of consumer group that allows multiple consumers to read data from the same partition in parallel. This allows us to paralyze the processing without adding more number of partitions. It also brings in power message acknowledgement and retry, giving a fine-grained control on message processing. And finally, by removing the need for strict in order processing, it eliminates the offline blocking problem, and this makes Kafka more suitable for queue style workloads. So how does share group work? Well, share groups enable multiple consumers to read data from the same partition concurrently, and it enables higher parallelism than the traditional consumer group model. This is particularly useful when you have more number of consumers than the number of partitions in the topic, so you can scale and you can actually distribute your workload across the available number of consumers without needing to over petition the topic. So Share Group forms a key enabler in ensuring that Kafka can handle queue-like consumption. So application teams can efficiently use that and maintain a controlled message processing. A question that comes up is that can we use a consumer group and a share group together? Well, yes, a Kafka broker can support both a consumer group and a share group at the same time. Let's say you have a topic with a single partition. Well, in a real production scenario, I wouldn't recommend you to use a single partition topic. That's an anti-pattern. But let's take this example, and we have a on the right hand side we have a share group with 3 consumers, and these all the 3 consumers start consuming from the same partition, while on the left you can see the consumer group which has got again 3 consumers. Only 1 consumer can process the data from that single partition. It is important to note that the shared group ID and the consumer group ID has to be different because these group ID belong and exist in the same namespace. Let's look into how the record delivery works. A petition consumed by a shared group is called a shared petition. In the regular Kafka consumer, we have a single offset which is managed by the consumer. However, the share group, we have two different types of offset. One is share partition start offset, which refers to the starting offset of records which are eligible for consumption. And second is shared partition and offset, which refers to the last offset of the record that is eligible for consumption. The records between start of set and the end of set are called the in-flight records. Well, consumer fetch records from the start of set and the end of set range and acknowledge once they are processed the data. If a message processing fails, it can also be retried. Another consumer can pick it up and reprocess it. While multiple consumers can process the data from the same partition, it can lead to concurrency issue and cause overload. So to manage concurrency and prevent overload, the share partition leader controls the number of in-flight records at any point in time, and this is done by using a configuration called Group share partition max record locks. And this allows safe parallel consumption without without any kind of conflict. So with this understanding of the shared window between the start of set and the end of set, let's learn the key states the record goes through. So we can see in a shared partition with 10 records in each of these cells there are three different rows over there. The first one refers to the offset, which starts with 0123, and so on. The second one talks about the state of the record. And the third one that is only seen from the offset 2 to 4, it's called the delivery count. This means how many times the record has been delivered to the consumer application for processing. So the first state is the archive state which basically says the data is no longer eligible for consumption. Maybe the data is already processed or it has been skipped because of some kind of errors. The second is the acquired state, which says that the data is locked by a consumer for processing so that any other consumer doesn't pick up the same data. The 3rd is the available state which says the data is available for consumption and to be assigned by any consumer. And the 4th 1 says that the data is successfully processed and has been acknowledged. Well, what does the states mean in this diagram? So if you look at it, all the records earlier than the offset 2 are in archived state. That means they are not in flight. They're no longer going to be processed anymore. Records 2 and 4 are acquired for consumption and their delivery count is increased to 1, so that no other consumer is going to pick up those records. Record 3 has been previously acquired and twice, it was not only once, it was previously acquired twice, but it couldn't be processed, so it's been given back and so it's now in available state which can be picked up by another consumer. Record 5 has been processed successfully and so it is acknowledged. Record 6 has previously been acquired for consumption by a consumer, but it couldn't be processed, so it was skipped and rejected, so it's marked as archived so that no other consumer picks up that same record again. Records 7 and 8 are in the available state, so any consumer can start reading those data. And any records from position 9, offset 9, and beyond are in available state. So how does the ordering of data in shared group look like? So here you have 9 records with start off set starting at 0 and the end of set at 9. We are the first consumer, which is reading the data from 0 to 4 inclusive, and the second consumer reads data from 5 to 8. Consumer one crashes. But consumer to finished processing the data. So the output that comes out from the application, you can see on the right hand side, it's 5678. Now consumer 2 again refreshes, and this time the consumer 2 is assigned 0 to 4. Which the consumer too could actually process it. And it was initially Consumer One failed to process that set of data. So when the consumer to finish processing the data, the output comes at 0 to 4 in the output order you can see on the right hand side of the screen. If you've noticed it, the whole order of the data within that's coming out of the application is in unordered state. There's no ordering on the overall batch of record. However, if you look at each consumer batch, the data is ordered within that batch. So you can see 5 to 8 are ordered and 0 to 4 is ordered within this um batch. So across multiple batches, records may arrive out of order, especially if it is retried. So the model is mostly designed for scalability and not for strict sequencing. So how are the offsets managed? So currently you can see the start off set is at 1 and the end of set is at 6. We have a consumer that reads data from 1 to 3 inclusive, and once this consumer has processed the data and acknowledged it back, the start offset moves to 4 and end offset moves to 9. And the records 1 to 3 are marked as uh um also marked as archived. The start of set and the end of set is a sliding window where start of set moves when the records are acknowledged, whereas the upper bound of the end of set is determined based on. The configuration we talked about, the group share partition max record locks that basically saying that how many records can be locked at any given point in time. By default, the start offset starts with the latest offset. However, we can also, using the admin tool, we can also kind of point it back to the earliest offset so that we can reprocess the data. Now let's look into how share groups work with Kafka log retention and compaction. So log retention in Kafka says that how long the data will be stored within the Kafka brokers before either the time is elapsed or the size exceeds a particular configured settings. Now, for log retention, the start offset is bounded by the log shared offset, which is basically managed by the retention policy in Kafka. Now if the log segments are being retained based on time, The and the inactive segments of the log, it exceeds the configure time. The log start offset, move to the next log segment, and the old segment is going to be deleted. And if the star shared partition startoff set is pointing to a deleted segment, it will automatically advance to the next log segment. I mean, this behavior is very similar to how in other kind of cue system message-based expiration work. And the beaver looks exactly the same if you still use the retention based on size. Next is the log compaction. So lock compaction is Kafka's way to keep only the latest version of each key in the topic, rather than deleting all the data from the topic itself. After the time of size threshold exceed. So share groups can read data from a compacted topic, and there's no problem with that. Now in most real world scenarios, your consumers can read data from the compacted topic and and the compaction process will not have any kind of problem because the compaction process runs periodically and by the time compaction process run, consumer has processed all the data. However, there could be a scenario that a record is cleaned while the consumer is reading from it. Well, the consumer will finish reading the data and it would process it without any issue, but the next time when the consumer asks for a new set of data, it will see the offset gap, and this process is exactly the same how it works with the consumer group model. So you might get batches with missing offsets. That's totally fine, but just that these are holes in the numbers rather than any data loss. So how do you handle bad records? Well, share groups can deal with bad records so that consumer groups can keep processing without being stalled. And there are 3 different scenarios how these bad records are handled. First, if a record is delivered successfully but it fails during processing, you get an application or a transient error. The consumer can explicitly acknowledge the record and mark it as release if the error is transient in nature, or it can reject it if the error is permanent. Second, if the record couldn't be digitalized. It will throw record desalization exception. So while Kafka will automatically release the record and allow the processing to continue. However, consumers can provide an explicit acknowledge call. If required. And the third is that if the record batch fails the cyclic redundancy check, it throws a corrupt record exception. This happens when the record is corrupted at the byte level. So Kafka at this point rejects the entire batch and ensures no other consumers kind of pick up the same batch and ensures that it can move forward. So the share groups provide this kind of built-in mechanism so that consumer can avoid stalling on the record, on the problematic records, and still provide a good throughput and and resilience. Well, now that you have talked about the cue capabilities for Kafka, let's look into some of the use cases why this feature would be relevant. And what we see is that there are 3 types of patterns where this particular capability will be useful. The first is a task worker processing pattern. This pattern is a classic worker pool pattern. Every message is a discrete piece of work. So this can be an image transformation or an ML inference job or some sort of data enrichment jobs. So let's take an example of an image and video processing. So when a user uploads an image using the UI. Rather than application process the image, it simply puts a message in the Kafka topic says that the image needs to be processed. Another microservice picks up that particular image, particular file, um, and then does some transformation. It can do some resizing, transcoding, generating thumbnail, and so on. If that worker crashes in the middle of the operation, the same task could also be distributed to other worker pool. Making sure no image file is getting stuck in the whole process. So therefore, it decouples the upload latency of the image from the heavy message processing and enables you to do the scale the application horizontally. The second is a microservice job queues. Well, these are typical asynchronous workloads inside microservices like sending emails, generating PDFs, or billing or invoice generation. These workloads don't require any strict auditing of data, but it relies on parallelism and retries. So let's take an example of email and notification service. So instead of sending the emails by the main microservice application, it just writes to the Kafka topic and message says that the email needs to be sent. The, the notification microservice picks it up, formats the email, sends the email via SMTP or SES, and acknowledges the the process. But if that notification service fails to send an email, maybe there is some sort of network delay, some retry, some rate limiting or so. Another service can pick it up and process it. Ensuring that it's a very fault tolerant email delivery process without slowing down the user facing API. And the third one is asynchronous request execution. Well, this category covers operation that should run asynchronously and makes the while still the application responsive in nature. You do not need to rely on the ordering part, but the more focus is given on the power message acknowledgement and worker rebalancing. So let's take an example of fraud checkout. Um, what happens is that during the checkout path rather than blocking the, the whole, uh, UI of the user for doing the fraud to while running the fraud detection models, it emits a fraud check request to a Kafka topic. A fraud detection microservice picks it up, uses an ML inference model or ML model or any kind of rule-based approach to figure out if that particular transaction is fraudulent or not, and then publishes the result. Here also, if that particular microservice dies, that a task is assigned to some other workers. And therefore it eliminates the latency from the checkout path and ensures that the process remains fault tolerant. Well, with that, I'll hand it over to my colleague Sa to take you to the next part of the presentation. Thank you. Hello, everyone. My name is uh Massuti Rahman Sam. You can call me Sam as well. I'm a streaming data architect. I've been working with Amazon for more than 10 years, heavily work on distributed systems like Kafka. So, in the second half of this discussion, I will dive deep into the internal implementation of CAFCA. Of course, Kafkash Group only. Um, so for traditional Kafka consumer, as you know, uh, there's an internal topic called consumer offset topic. For, um, consumer state management, right? Uh, but in, uh, this, uh, new capability, uh, in Kafka Share Group, there is a new internal topic called um Share Group Offset topic. This is a highly partisan topic which is 50 by default. Uh, it uses a cleanup policy of delete, um, and also, um, it is not a compacted topic. So, the share coordinator periodically delete the record to optimize the, um, space of this topic. So, when a share coordinator become a leader of this internal topic, um, um, it, uh, leader of, uh, the shared group, uh, partition and then it can scan the share group state, build the in-memory state, and then start, um, serving the request. And like Kafka Consumer Group, we also know the concept of partisan leader. As you know, when the producer and consumer request comes, uh, usually partition leaders have those requests. When I say usually because from the consumer side, you can specify that to read from a nearest replica as well, uh, with the settings called nearest replica feature. So in KP 932, there is a new component introduceS as well, um, which is shared partition leader. So, shared partition leader, uh, is, is a component in the broker. It reside, uh, with the uh topic partition leader. So, which means, uh, the shared partition leader follow the leadership where your topic partition is on the same broker. It resides on the same broker as well. And the shared partition leader manages the, uh, shared group state, uh, interact with the topic partition, and as I said earlier, it fetches the record from the local replica, uh, where the topic partition leader is. And then it tracks the message state as well, whether it is available, as Suva mentioned earlier, all those definition of available means it is available, uh, to, uh, send to a consumer, um, whether it is acquired, which means the consumer has acquired the record, um, and then the next is my clicker is not working. Um, acknowledge, which means consumer has processed that record, or it can be in archived state, which means it has been delivered multiple times and, um, uh, it is now reached the delivery count limit, so it's not gonna be sent to another consumer again. It is archived now. So, the share partition leader manages the record state. How about the delivery semantic? Well, here's how the flow works. Consumer pulled the record, um, um, when the share coordinator gave, sent the record and the broker, um, Mark the record as a lock record, uh, mark it as an in-flight record where it has been given to a consumer and the consumer process the record, it could be, let's say consumer is acknowledging it, which means in that case, it will be a successful acknowledgment. Broker will mark the record as acknowledged. And during that time, there could be a scenario where there's a transient failure, um, or, um, long duration expire because the consumer has timed out. So in that case, message will be again available. I will discuss more about how this happened as well. And there could be a scenario where there is a problematic message or a poison message. So in that case, Kaf also keep track um track of how many times it has been sent to a consumer, and there is a delivery count limit as well, and based on that, it will decide whether it will send it again or not. So this approach balances reliability and the resiliency, guaranteeing that each record will be delivered at least once, no matter whether something is failing along the way. Now, let's take a look at the lock renewal semantic. So, what I mean by lock is the record has been locked for a specific consumer. It is not available to any other consumer. So let's now take a look how it works. So when the consumer fetch a record, the broker applies the acquisition lock uh for that record that prevents other consumer to read the same message at the same time. Once it is locked, The record is stayed held for a certain period of time. You typically this is 30 seconds by default. So during that time, the consumer has to process that record. And send the acknowledgement ban, uh, back to the broker. So if the, um, if it is a successful acknowledgement, broker mark it as a, uh, acknowledged record or consumer can also decide that I will release the record. So in that case, it will be available for another consumer as well. However, there could be a scenario that consumer does not respond during that lock duration time. So in that case, broker will release the lock automatically. So when it happens, the record stay changed back to available, allowing it for re-delivery to another consumer. So that's how this lock renewal semantic works. And as you know, things can fail. There will be a situation when your consumer is failing or your broker is failing as well. So, let's now understand what happens when a consumer crashes. When a consumer crash, the broker is unaware of that consumer crash. The broker has no clue. So which means the lock remains active during that time as well. And there could be a situation where there is a timeout happening from the consumer, right? So, in that case, the group coordinator will uh remove the member from the group when this is happening and uh when the consumer is stopped sending heartbeat actually. And there is a heartbeat interval setting on the consumer side. During that time, consumer need to send a heartbeat to the group coordinator. And if the consumer is failing for a long time, the broker will automatically release the log based on the log duration settings, as I mentioned in my previous slide. Also, consumer in a share group can, um, of course, it in a heartbeat mechanism to join or um declare that I'm still available or it can leave from the group as well. So when, so when a consumer failure happening, uh, it could be a transient failure. So, that means that consumer can rejoin again and then start um processing, um, um, start processing the record actually during that time. How about the broker failure? So, as you know, the share coordinator is responsible for uh managing the state. Um, and for that, um, the state of the in-flight record persists on internal topic, as I mentioned earlier, which is share group estate topic. So if the broker, uh, acting as a coordinator failing, so during that time, what will happen is there will be a new election happen. So it's like the way a leader election happen, right? So, to take over the responsibility of uh uh of this uh coordinator, this ensures that if the broker is restart or fails, another broker can take that responsibility and keep continue work on it. And during that time, it will read from that highly replicated topic, which is the internal topic as I mentioned earlier. Of course, during that time there will be a process called a state recovery, so which means the new leader will read the state from that internal topic, build the memory state to understand which records are in which state by which consumer. Once it finished building that state, it can solve the request. And that means while there are minor might be a minor pause of data processing during that leader election happening, um, the system can guarantee there will be no data loss. Let's now take a look in the code example to understand how the API works. So, in this example, uh, first, I create a property object to configure my consumer. I set the bootstrap URL to the local host where I'm running Kafka cluster on my local machine, let's say, as an example. And then I give um a group ID which is my share uh provider group ID there. And the next, I create a Kafka consumer specifying a string deserializer for my key and the value. And then I subscribe to my Kafka topic. After that, my topic name is Fu. And then inside the infinite loop. I call consumer. poll method, specifying a duration, which is 100 milliseconds, as you see in this example, so which means that uh consumer will wait for at least 100 milliseconds if there is no record available during that time. And then I run an infinite loop to process my record, the way I want to process. I apply some business logic as well. And if you see in this example, I'm also specifying um the way I want to acknowledge it. So, you can explicitly acknowledge every record when you are processing it as well. So, for example, whether it is, uh, whether I want to accept it, release it, or reject it. And once I finished processing that record, I commit the entire batch, uh, by calling that commit.sync, um, commit as sync method. Of course, you can also call, um, commit, um, sorry, this is a commit sync method. You can also call commit sync method, uh, which means, um, the commit will happen in the background without blocking the process. This will be a synchronous call. And In terms of security and SCL um, um, changes, there is no change actually. So, the way you manage Kafka SCL remains the same. For example, if you're providing some granular access, you keep continue to do the same. Um, uh, so, in that case, keep in mind that if you're providing granular access, the broker needs read and write access to that internal topic called shared group state topic, so you have to provide that as well. And in KP 932, there are some new API operation has been added. Uh, if you are, again, providing explicit granular access, you need to know which operation require which type of access. You can find those in the Kafka documentation as well. There are some additional metrics has been added, uh, for the broker and the client. I will highlight some, uh, some, um, for example, for the broker, uh, some, uh, you can monitor the group status by monitoring the group count metrics and the, uh, group, um, group count, um. Uh, shared group count metrics as well. So, um, and then you can also monitor, uh, rebalancing rate and rebalancing count. Also, to track the activity between the broker and the, and the internal topic, you can monitor partition load time, like how long it takes to load the partition from this internal topic and what is the right latency between the broker and on that internal topic. And for client monitoring, there are some additional metrics available as well. For example, to check the consumer health, you can monitor the heartbeat rate and the last poll, second echo matrix, heartbeat rate will be average number of heartbeats per second, and so on. And to monitor the client throughput, it will expose some metrics like record fetch log and fetch size, A matrix. And to measure the consumer latency, you can monitor fetch latency and the heartbeat response time metrics. So overall with this new capability, Kafka scaling is no longer tightly bound to the number of partitions. You can have more consumer than the number of partitions. So, from the consumption side, uh, it will be more on doing capacity planning on whether you have enough compute capacity on the broker and also on the client, um, not just overprovisioning your topic. So that means the peak load handling, especially from the consumption side, will be much more easier without doing partisan assignment in Kafka. As Subham mentioned earlier about um uh some of the key characteristics of when you're using regular consumer and the Kafka consumer, I will highlight that again. So, as you know now that you can run regular Kaf consumer and the share group consumer on the same cluster. Um, uh, but it resides on the same name space. So, which means as you, as you have seen in my example, specifying a group ID, that group ID needs to be unique. You cannot use the same group ID for this, uh, different type like regular consumer and the share group consumer cannot use the same group ID. And also, the group ID set by the first client or person that you are doing. You cannot change it later as well. So, a clear naming convention will be very important to enforce the group type and to prevent any conflict uh on your environment. And as of now, this share group capability is disabled by default. You have to explicitly enable it. Um, um, you have to use that configuration called unstable API version enable equal to true, and then also you need to add a new rebalancing protocol in your configuration called share before, um, uh, you run your Kafka. So, all those needs to be added in the Salvador property file before you start your Kafka process. So let's now switch to the demo to show you how this capability works. I'm running Kafka in my local machine in Dokar, so just to show you the List of available topic on my cluster. I have 4 topics here. As you see, there's 2 internal topics. One is the consumer offset topic, and another one is for share group estate topic. In the test, what I will do, in the demo, what I will do is I will run a producer and then I will run 3 consumers. Before doing that, I will create a topic with only 2 partitions. And as we have discussed so far, if we are creating a topic with 2 partitions, and if I have 3 consumers, in the regular consumer, we will see that 2 consumers is only reading and the consumer is sitting idle. But in a share group consumer, we will see that all those 3 consumers can read from these 2 partitions. Let's see what it happen or not. So Let me create a new topic. So I'm creating a topic, as you have seen here with To partition only. Let's be at the top. OK. I've created the topic, and what I will do right now is I will start my producer. As you have seen so far on our discussion, there is no change on the producer side, right? And if you see my example code as well, I'm just using Kafka Python library to. Build my producer application. There is no change here, right? So let me start my producer. It will just send some random data to this new topic. Awesome. So it is now sending random data. What I will do right now is I will start my consumer as well, and this will be the share group consumer. And you see, I have consumer 1, consumer 2, and consumer 3. I will start my consumer in every console. It has started. Second one, should start processing as well. I can see it is also processing record. And based on our discussion, consumer 3 should also start processing record, right? And you see in this example, consumer 2 and consumer 3 is reading from the same partition. So, see partition 0, and this is also reading from partition 0. No Sorry. Right? So, that's the whole point of using this capability in Kafka Share Group. So, let me now run regular Kafka consumer and see how it works with regular Kafka consumer. So again, I'm running regular car for consumer on consumer one. It is reading, and I started my Consumer 2 as well. It should start to read as well. Nice. And I will start my Consumer 3 and see what happens. Somebody should say that I cannot read any longer. So, you see, consumer 2 has stopped reading, right? So, there is no message coming in consumer 2 any longer. It's just consumer 1 and consumer 2, because in a regular consumer, as you know, it's a 1 to 1 mapping what Shubham has shown you earlier as well. So that's exactly the case we see. Awesome. Let me stop my producer and consumer, and let's switch back to the presentation. So, though Keep 932 added some interesting and queue-like capability as well, you might not use it yet. There's some reason for it. The feature is still in preview in Kafka 4.1 version, and of course this is a preview version. It is not recommended to run in production yet. Dead letter Q is still not supported. And you cannot monitor the uh share group lag when you're using a share group consumer. And if you have a, a workload where, um, workload in exactly once processing and also you need ordering guarantee of your record, then this capability is not for you. Also, here's some roadmap item, uh, to, um, overcome some of the limitation we have discussed earlier. The target is making it production-ready, uh, in Kafka 4.2 version. And with KIPP 1191, there is, this is still in discussion. Um, uh, there, there is a discussion to introduce date letter Q for shared group as well. With KIPP 1206, um, it will allow, uh, strict record fed so that if you have a slow consumer, you can avoid overloading that consumer. With keep 1222, um, you can have a configuration where consumer can extend the acquisition log. So, when there's a slow, slow consumer, instead of redelivering the message again and again when there is a slow consumer, so consumer can, uh, extend that acquisition log with that. With KIPP 12 to 6, it introduced um uh monitoring the um shared group lag. And we keep 1 to 40, it introduced some additional um group level settings for shared group um that gives finer control like delivery limit or uh log time without changing clusterwise config. And for those who are not aware, Amazon MSK provide a fully managed Kaf offering, and then if you want to test your cue capability, um, you can also try it with Amazon MSK as well on version 4.1. Awesome. So here's the key takeaway. KIPP 9C2 is essentially queued done in Kafka way. No more overpartitioning, built on the same scalable backbone of Apache Kafka. It breaks one partition per consumer behavior, enabling parallel consumption for truly parallel workload all within Kafka. It is not replacing regular Kafka consumer, but expanding what Kafka can be. If you want to learn more about this new capability called Kafka Share Group, I would highly recommend to um check this uh keep 932 using this URL. And thank you for your time today. Uh, we hope this session on, um, new capability introduced in Kafka, Kafka Share Group has been useful for you. Uh, before you leave today, please, uh, share your feedback using the Reinvent Mobile app and let us know how we did. Each year we try to create content that you find interesting. So, give us 5 stars if you think we have made it today. And thank you again. Have a great rest of your day.