---
video_id: MBvyZENChk0
video_url: https://www.youtube.com/watch?v=MBvyZENChk0
title: AWS re:Invent 2025 - Databases made effortless so agents and developers can change the world -INV208
author: AWS Events
published_date: 2025-12-02
length_minutes: 55.72
views: 4486
description: "From startups to global enterprises, AWS Databases has been the trusted foundation powering innovative applications for nearly two decades. Join G2 Krishnamoorthy, Vice President of AWS Database Services, as he presents our mission to transform how you manage your data. Discover our vision where database management becomes effortless, delivering robust performance, seamless scale, and comprehensive security. Learn how databases integrate with AI agents and tools to reduce operational complexity ..."
keywords: AWS, Amazon Web Services, AWS Cloud, Amazon Cloud, AWS re:Invent, AWS Summit, AWS re:Inforce, AWS reInforce, AWS reInvent, AWS Events
is_generated: False
is_translatable: True
---

Please welcome to the stage, Vice President of Database Services
at AWS. G2 Krishnamoorthy. Hello and welcome. Thank you for
joining us today. I'm thrilled to be part of your first day at
re:Invent and to talk databases. We have an exciting agenda
today. And I will be tag teaming with my colleague Colin
Lee Kear to spend some time with you on the big picture,
looking out, say, the next 3 to 5 years. We'll address how
database services fit into the broader data and AI mission at
AWS. Our hope is to give you that important context that you
need as you dive deeper. Get hands on with the various
database services throughout this week. We'll share more
details on how we are bringing this vision to life with
innovations we have delivered throughout this year. Now, all
of this is inspired by what we hear from you, our customers,
and what we know you'll need to be successful with this
transformational opportunity offered by Generative AI and
agentic AI. Also joining us today are Tom Aquino from
Warsaw and Tim Ludiker from Robinhood, who will share their
inspiring stories about the art of the possible built on AWS
databases. Let's get started. We at AWS believe that your
data is your differentiator in this era of agentic AI. Our
mission is to be your best partner for the long term, to
enable you to build your platform for agentic AI with
your data at its foundation. We are deeply committed to helping
you achieve your most ambitious goals, helping you transform
the possibilities of agentic AI that you imagine into reality.
And we believe AWS is the best Cloud for your agentic AI
foundation. Let me expand on why. Quick show of hands. How
many of you are using an agentic AI tool? Most of you.
It's amazing how quickly the adoption of this technology has
grown. If you remember, ChatGPT only came out three years ago.
Agentic AI is here and it is transforming the way we build
with these powerful tools. Developers can be an order of
magnitude more productive, and these tools are democratizing
application development, expanding the pool of people
who can build by another order of magnitude. Combined, we
foresee an exponential increase in the number of applications
and AI agents, and the amount of data generated and processed
by them. To handle this scale, you need to be in The Cloud and
your Cloud needs to scale. To run all these databases
securely, reliably, efficiently. It's predicted that there will
be over 1.3 billion AI agents in production by 2028, and that
may be a conservative estimate. Unfathomable, right? Yet just
ten years ago, we were seeing the same thing about managing
fleets of hundreds or even a thousand databases. So our
vision is to make it effortless for you to discover insights,
make better decisions, and drive innovation across your
entire data estate. And we make this possible by giving you
intuitive tools that allows your humans and agents to
tackle the hardest challenges in your business with ease,
regardless of their technical expertise. By integrating best
in class governance and guardrails so that you can
innovate on your data with confidence, and by delivering
on security, reliability, operational excellence, price
performance at unprecedented global scales. Now we recognize
the importance of interoperability. Some of your
most critical applications are built on legacy database
systems. So we will enable you to unlock your entire data
estate, including those in the legacy database systems for
agentic AI innovations. And once you're ready, we'll help
you modernize to open architectures to future proof
your applications. We'll make it effortless for you to build
your data foundation on your highest quality data, be it
structured data like your tables, unstructured data like
your documents and media and synthetic data that you're
creating to train your models no matter where it lives, so
that your AI agents and applications can access them in
real time. We want to enable you to steer into this changing
world of data and agentic AI. We are innovating on these five
core building blocks, enabling you to scale effortlessly and
be ready for that 1000 scale that we foresee to continuously
and automatically optimize your infrastructure for cost and
performance. To build your agentic AI foundation with
trusted data at the heart of it, and to embrace open
architectures. Now let's visualize this transformation
using a real world example that zoom into how things look today
at an online retailer. Now, I happen to work with one very
closely. Today is Cyber Monday, and I would imagine everyone at
this online retailer the DevOps engineer, data engineer, ML
engineer, merchandizing manager, they are focused on bringing
you the best deals or making your shopping experience in
that app snappy.  Or optimizing your shipping times.
To make this possible, data optimizing your shipping times.
To make this possible, data from all these systems, like
supply chain, is connected to Unified Data Catalog for easy
discovery. All this data is going through continuous data
quality checks to ensure that your delivery times are
accurate, or merchandizing managers are only accessing
their category data using SageMaker. AI data scientists
are training the models to provide you with fresh
recommendations, and these recommendations are stored in
S3 Tables that are easily fetched by the app using MCP
servers. Now imagine there is an inventory issue with the
hottest toy this holiday season. Say the merchandise manager
made a fat finger mistake and missed zero in their forecast.
It happens. So today it takes many people and many, many more
hours to understand what happened and to build a plan to
make it right so that thousands of kids are not disappointed
with Santa come Christmas. And the retailer gets to keep the
trust with their parents. So what happens in the future? We
foresee a future where agents and agentic tools transform
this picture. In this world, agents are capable of doing
tasks that we need human hands today, and humans switch to
directing these agents and agent tools to get the job done.
Using prompts using specs. In this future, an AI agent is
monitoring the inventory and order flow and will generate an
alert to kick off the investigation. The merchandise
manager will tap into more agents to come up with a plan
to make it right for the kids to scour the additional
inventory across the vendor base, to find and negotiate
with alternate suppliers, and more. All this can happen in
parallel and will be much faster. And most importantly,
the kids get their toy come Christmas. We recognize the
most critical applications for your enterprise your CRM
systems, your ERP systems, your supply chain systems have their
data in operational databases, and we at AWS are building to
this shared vision across our database, analytics and AI
services to make it effortless for your applications, AI
agents and teams to get value from all their data. And we are
making AWS databases effortless to get started, to manage and
scale, to migrate and modernize to so that you can focus more
of your hours building innovative features, shipping
delightful customer experiences, and move faster. Now this is a
journey that we are on for over 15 years since the launch of
first fully managed databases in The Cloud, we remove the
heavy lifting involved in managing database instances.
Then we launched serverless to eliminate the need to plan,
provision and manage database capacity as your application
scales up and down. And we took it a step further with DynamoDB,
Aurora DSQL and ElastiCache for key, making it effortless for
you to get started. Have zero infrastructure to manage with
virtually unlimited scalability. You never have to worry about
provisioning capacity, scaling limitations, maintenance
windows. This is why our databases are ideal for the era
of agentic AI. With the emergence of powerful agentic
tools like vercel v0 that you would see next. Now you can
bring these ideas to life as production ready applications
with just a few prompts, and you need a database
infrastructure that is just as effortless with the all the
enterprise goodness of security, reliability, and scalability. I
am thrilled to welcome Tom Aquino, Chief Product Officer
at vessel, to share how Vessel Aquino, Chief Product Officer
at Vercel, to share how Vercel and AWS databases is
revolutionizing Developer Experience to enable you to
build at the speed of an idea. Welcome, Tom.
>> Good job. Yeah. Thank you. Thank you so much, G2. Hello
everyone. I'm Tom Aquino and I lead engineering, product and
design at Vercel. For those unfamiliar, Vercel is the AI
Cloud that provides the developer tools and Cloud
infrastructure to scale, build, secure a faster, more
personalized web. More than 11 million developers use Vercel
to deploy their work, and we handle more than a trillion
requests every month. I'm here today, like most of you, I
assume as a huge AWS fan, AWS is Versalles preferred Cloud
provider, and we've built our platform on top of many
incredible AWS services like Aurora, DynamoDB, Lambda, S3,
and more. We've been working closely with the AWS team on
something I'm excited to share today. One of our mottos at
Vercel is you can just ship things. I love this billboard.
We're obsessed with removing friction so developers can move
as quickly as possible. And one source of friction that's all
too common is setting up and configuring databases. AWS
databases are battle tested and ready for massive production
scale, but that scale comes with a lot of critical
configuration. Normally, when I need a database, I log into the
AWS console and I go through the create flow. This flow has
pages of advanced configuration that I'll need once I'm ready
to scale in production. But when I'm just getting started,
I don't really want to think about any of this yet. I just
want to get my app working as quickly as possible. What if
provisioning an AWS database was as simple as installing an
integration right from within the Vercel dashboard? We've
partnered with AWS team to enable just that, unlocking the
delightful developer experience we are so passionate about. Let
me walk you through a quick demo in my Vercel dashboard.
I'll head over to the integrations tab and browse the
marketplace right here, alongside everything else I
need to build my app. I'll now see a brand new AWS integration.
The install flow is simple. If I don't have an AWS account, I
can create 1 in 1 click. This integration gives me direct
access to production ready AWS databases like Aurora SQL,
Amazon DynamoDB and Aurora Postgres. Serverless with more
options coming soon. These are the exact same databases you
can create from within the AWS console, ready to power real
applications at massive scale. I'll go with Aurora Postgres.
I've been using Postgres for like a decade. I love it, it's
reliable, I know the syntax and my team knows how to work with
it, but the serverless part is what makes this particularly
interesting. Traditionally, configuring a Postgres database
meant managing instances, planning for scale, and paying
for capacity. I might not use Aurora. Serverless gives me the
Postgres I want, but with automatic scaling and pay for
only what you use. Pricing. Since I needed to create a new
AWS account here, that's what happens next. I'll accept the
terms and conditions and in seconds I'm taken to a minimal
configuration screen with some defaults selected for me. I'll
be presented with the available plans here. And what's really
exciting is new AWS customers get a free starter plan with
$100 in credits so they can get up and running. Finally, I'll
click create and wait a couple seconds and that's it. When
this is done processing, I'm taken right into the storage
tab of my dashboard where I can connect this database to my app.
I'll click Connect Project, select my project from the list
in the dialog, and then click connect. That's it. I don't
need to do anything else. No managing connection strings, no
secrets, no other configuration. Vercel handles it all
automatically. The setup itself from nothing to a working
database wired into my app, just happened entirely within
the virtual dashboard in less than a minute, and I'm ready to
start writing queries. And if I need it, there's even a button
to open my new database in the AWS console securely in a
single click. It's magical because I don't need to manage
a separate account, and I'm logged in and navigated to the
exact right place automatically. There's one more thing. I've
been using V0 a lot lately to build lots of different
projects, and it's been a lot of fun. V0 turns my ideas into
real working web apps through simple conversation. Let's say
I wanted to build a simple food delivery app for me and my
family that I wanted to customize just for us. I'll
describe it in V0 and submit. It gets started right away,
putting together a plan and scaffolding my app very quickly.
It's going to determine that it needs to store some data,
obviously. So it actually will prompt me to create an Aurora
database. I'll go through the same setup flow we just saw and
let it process, and then I'm taken right back into V0 to
continue building. V0, then writes all of the code for
creating the tables I need in this app, including restaurants,
menus, orders, delivery, tracking, and more. It even
populates sample data so I can start testing right away. We'll
give it the okay to run this database migration, and then
we'll let it cook. And just like that, I have a full app
backed by a real Amazon Aurora database, ready to be
customized and iterated on. And if I need to access Advanced
Configuration, I'm one click away from the storage tab in
the Vercel dashboard where I can view the database that V0
just created, and in one more click, I can open it directly
in the AWS console, just like any other database I set up
myself. I can even start cloudshell and write queries
just like I always have. That's what this integration unlocks.
You just describe your idea and you get AWS databases under the
hood automatically. And again, this isn't just a toy. This is
a production ready database with best in class scaling,
reliability, and performance. You get all the benefits of
Aurora without having to think about any of the database setup.
This is the relationship we're building with AWS. Not just
integrations, but intelligent infrastructure that adapts as
your ideas evolve. And this is the future that we're building
at Vercel, which is part of our broader vision to create
self-driving infrastructure. Zero config framework defined
autonomous operation of your entire stack. Thank you.
Welcome. Now, I'd like to welcome Colin from AWS to tell
us more. Thank you. >> Hello. It's great to be here
with you today, and I'm excited to walk you through a number of
recent innovations and launches that support our data and AWS
vision. Naturally, we'll begin with getting started. Let's
talk about speed. Speed matters, and we want you to be able to
build at the speed of an idea, which means rapid
progress from your idea to your running application. But we've
all been there. You're ready to move fast, and it's frustrating
when you lose your momentum while you're stuck waiting for
your new database to spin up. And it doesn't need to be that
way. If you're using DynamoDB, you can already create and
access a production ready table in just a few seconds, the same
amount of time that it would take to create a basic desktop
database. But when you need a production relational database
with all the resiliency and security features that you'll
need to build something robustly, it takes time.
Sometimes as much as 10 to 20 minutes, depending on your
configuration. So we asked ourselves, how can we bring
this fast creation experience to our relational databases? We
took a hard look at our newest database, Aurora SQL when we
launched D SQL this time last year. You could create a new
database in under two minutes, but we didn't think that was
fast enough. So we kept working on it. And now you can create
an Aurora D SQL database in just a few seconds. And we
didn't stop there. So today we're excited to announce that
coming soon you'll be able to create a Aurora serverless
database in just a few seconds. Let's take a look at that
experience in the Aurora console. You go ahead and click
create. It's preconfigured with an Aurora serverless cluster,
and you have the flexibility to update your configuration
during and after database creation. Let's give it a quick
name. And in just two clicks you have an Aurora Postgres
database. In only a few seconds. Now that it's created, you have
access to all of the features and benefits of Aurora. If you
have I/O intensive applications, just choose Aurora I/O
optimized to improve price performance, saving up to 40%
on costs. If you need replicas for read. For high read
availability or scalability, simply add a reader to your
cluster and gain access to Aurora's automatic failover
capabilities. If you run a multi-region application, you
can enable global database for disaster recovery. It's the
same Aurora you love, but simply way faster to create a
cluster. From there, you can connect to your database and
get building. But of course, fast creation is only part of
the solution. Developers use a wide range of tools and IDE
interfaces running outside of AWS infrastructure. So to
enable smooth connectivity from any tool anywhere in the world,
will soon be launching a new internet gateway that fully
supports the Postgres wire protocol. This layer provides
secure internet connectivity for Aurora Postgres no VPN or
AWS Direct Connect required. It's also automatically
integrated with AWS Identity and Access Management, or IAM,
so you can enforce identity based permissions for
authorized access only and to safeguard your data. So now
you're probably wondering why am I highlighting? IAM in a
talk about making databases effortless. Well, do we make
databases for IAM effortless? Yes, yes we did. We now allow
all IAM users, including the root user, to connect easily at
login, all while keeping the same high degree of security
and the granular controls that you've come to rely on. And we
didn't stop there. We've updated the AWS Management
Console connectivity experience to surface relevant information,
like code snippets, and provide direct access to tools like AWS
Cloud Shell. This means that you can access your database on
AWS as easily as you saw earlier with Vercel, with code
snippet. Simply choose how you'd like to interact with
your database. Select Copy Code Snippet and then run command to
get started. With Cloud Shell. Simply launch Cloud Shell and
click run. From there, you can connect to your AWS database
directly from the console and start building. Finally, as Tom
mentioned earlier, we launched a new AWS free tier earlier
this year that's available across a broad set of eligible
databases, including databases. New customers can get $100 in
AWS credits, and you can earn an additional $100 in credits
for a total of $200. Coming soon, we'll be unlocking this
free tier for the first time ever for Aurora Postgres
serverless. I'd encourage you to try it out and let us know
what you think. Like Versal, we're also collaborating with
AWS partners to bring our databases seamlessly into their
experiences, and we're integrating directly with
widely adopted frameworks. AI assistant coding tools,
environments, developer tools all to unlock your ability to
build at the speed of an idea. These environments support
every phase of the development life cycle, from writing code
to deploying applications at scale. They break down barriers
to entry, meaning that you no longer have to be an expert in
every technology or aspect of that lifecycle to rapidly make
progress. With so many developer tools, Ides, and
agents to choose from, MCP servers give you the
flexibility and ease to build and deploy from the tools and
agents of your choice. There are critical component in
agentic AI architectures, providing standardized
interfaces needed for agents to interact with tools such as
Kiro, AWS Strands agent, and AWS Database services. By using
MCP servers to connect to various data sources, you can
build agents that perform a wide variety of tasks and
interactions. Imagine leveraging agents to automate
operational tasks, or to build an agentic AI system that
orchestrates tasks across tens of thousands of databases. This
is why we created local MCP servers for all of our AWS
database services, making it easy for you to access your
data. MCP provides you with the right connectivity to develop
faster from your trusted data sources. You can connect via
coding or AI assisted coding tools to make it easier and
faster to build applications. You can build operational
efficiencies and agents based on your data patterns to reduce
manual work. Let's start with the MCP basics. They operate in
a standard client server manner. In this scenario, Weâ€™ll connect to the MCP server for
Aurora SQL, our serverless distributed SQL database using
Kiro. But you can use any AI assistant such as Cursor or
Visual Studio Code. We want to understand the shape of our dev
database, so we can just ask Kiro to summarize that for us.
It breaks down that task into pieces. You can look at a task
if you'd like, and then it gives us a clear answer. This
is just a simple example of a starting point. You can go
leverage these tools to build agents, scale data operations,
or really anything else you can imagine. Let's look at an
example of a more advanced genetic tool that's embedded
within the DynamoDB MCP server. It's fairly straightforward to
create a relational data model that can then be queried by SQL.
It's intuitive. There's even normalization rules that come
close to making it a science. On the other hand, people can
struggle with NoSQL data modeling, particularly when
they encounter it for the first time. It's often more art than
science, and you need to work backwards from your access
patterns and application requirements. While all this
becomes more intuitive with practice and you can get huge
cost and performance wins, learning these NoSQL modeling
patterns can be daunting. With With our recently introduced vibe
modeling tool, which lives within Dynamo's MCP server,
that work can now be done by the LMS. Making NoSQL data
modeling an order of magnitude easier. Engineered by our DDB
experts, the tool provides a structured natural language
workflow that translates your application requirements into
an optimized NoSQL data model. It guides you through the
design process, prompting you with questions to understand
who the users are, what the key scenarios are, and the desired
customer experience. Let's take a look at that.
>> In this example, we're using Kiro as the AI assistant with
the DynamoDB MCP server. We're going to automatically invoke
the tool by saying something like, help me design a DynamoDB
data model. The structured workflow begins with gathering
requirements. The tool knows the questions to ask, and if
you don't know the answers, it will help you make reasonable
assumptions for the initial design. The inputs are saved in
markdown format in your local folder for your review. You can
even edit the input directly to add more information before
creating the data model. The tool generates a prototype data
model that follows best practices, and this is also
saved in markdown format in your local folder. The data
model explains which features were chosen, such as global
secondary indexes, design decisions made, and the
rationale behind these decisions. This serves as a
great starting point to build your application, because it
speeds up your ability to create and optimize DynamoDB
data model within minutes. From this data model, you can then
generate your DynamoDB schema and all the application code
you need to interact with your DynamoDB tables.
>> I'm really excited by the way, this direction that this
work is headed, and there's going to be much more in this
space in the coming years. Now let's see how you can use AWS
databases to create a genetic applications that are powered
by your data. Agents inherit powerful reasoning from Llms.
But here's the thing they're only as smart as the data that
you feed them. Llms have a critical limitation. They're
stateless. Every conversation, every decision, every
interaction requires you to reconstruct that context from
scratch. This can be slow and expensive. You need to use
databases to store all of that temporary data that's required
to keep the agent contextualized. Otherwise, it's
as if you made holiday reservations today, but forgot
you'd already made reservations months ago somewhere else. Two
separate actions in time, but not connected without context.
For agents to be effective, they need short term memory
like the current conversation thread. Long term memories,
like user preferences learned over time, and enterprise
knowledge like transaction histories, each type demands
different storage patterns. Some need millisecond retrieval,
others need complex relationships, and all of them
need to scale. This is why agentic memory is important.
AgentCore Memory creates and manages context for agents,
both short term and long term, for greater accuracy and more
personalized responses. And AWS databases are a great
destination for AgentCore Memory. We've enabled
developers to use Bedrock AgentCore with our databases,
and we've partnered with open source frameworks Landgraf,
Menn0, and Lita to create production ready database
connectors that you can use to launch agents that remember
past conversations. Available today, you can store
conversation state in DynamoDB using Landgraf check pointers.
You can build knowledge graphs of your Agentic memories to
improve retrieval by using Neptune Analytics and FM zero.
You can persist long term memories from agents built with
Lita in Aurora Postgres, and for short term memory, you can
achieve millisecond memory retrieval in Landgraf or Memo
zero using ElastiCache for valky. Let's see how this works
in practice. In this example, we're going to create a very
simple agent with memory using strands. An open source
development framework a memory layer which includes a genetic
memory, frameworks like AgentCore and FMs zero. And our
AWS databases. Strands is a great SDK for building and
running AI agents, and MCP focuses on providing tools and
context to the agent. With just a few lines of configuration code. And two lines of code in your agent. You can add the capabilities of FM zero to enable your agent to create,
store, and retrieve valuable genetic memories using AWS
databases like ElastiCache, Aurora, and Neptune. It's that
simple to get going. Today. Caching exists in your browser
phone, in front of databases, and almost everywhere you turn
in technology, your agentic AI stack is no different. Consider
a use case like an agent and all of the variations there are
to ask it how to reset your password. Much like using a
cache in front of a database, you use a semantic cache in
front of an LLM with the semantic cache statements like
how do I reset a password? Or I need to change my password?
Have the same semantic meaning instead of needing to perform
inference for every end user prompt, you can instead cache
the semantic meaning of the question using vectors. And
then when a similar question is asked, the response is just
quickly returned from the cache. Semantic caching with
ElastiCache for Vasu Chari is going to help you reduce costs,
improve performance, and increase the throughput of your
applications. And we're excited to share that Amazon
ElastiCache, for now, delivers the lowest latency, highest
throughput, and best price performance for these workloads,
with over a 95% recall rate. With AI, you can now build at
the speed of an idea. To do so, as we've seen, and we'll see
this week at re:Invent, your database and development
toolchain is evolving to include MCP servers, semantic
caching, AgentCore Memory, Ides, tools and frameworks, just to
name a few. While much is changing, one invariant is that
data will continue to be the foundation and the underpinning
of every application. We're excited to make databases
effortless so that you can focus on building, innovating,
and delivering what matters most for your customers. Thank
you. Joining us on stage to share why they're using AWS
databases to power all their applications, and how they're
approaching AI in their products. I'd like to welcome
Tim Ludiker, Director of Software Engineering at
Robinhood. >> Good job. Thank you.
>> Hi, everyone. I'm Tim Ludiker, director of software
engineering at Robinhood at Robinhood. Our mission is to
democratize finance for all and to achieve that, it starts with
building platforms that make investing more accessible, more
affordable and more available to everyone globally. Today, we
manage over 340 billion in customer assets across 27
million funded accounts. Our users rely on Robinhood for
everything from real time trading to long term investing,
a service they increasingly need around the clock.
Operating at this scale requires radically reliable and
highly performant infrastructure, especially
given our regulated environment, real time demands, and an AI
first approach to building. That's why our platform is
built natively on AWS. At Robinhood. Our infrastructure
philosophy is reliability. First built to support massive
scale and increasingly shaped by agentic AI. This transition
is helping us evolve from AI assisted troubleshooting to
fully AI directed operations, driving both our internal
efficiency and the intelligent experiences our customers rely
on every day. This reliability first mindset drives every
architectural decision we make, especially when it comes to our
data layer. To that end, earlier this year we partnered
with AWS to complete a major migration moving from RDS
Postgres to Aurora, evolving our foundation to power the
next generation of applications and AI workloads. Today, we run
over 24,000 database cores across multiple availability
zones. This entire infrastructure is designed to
elastically scale up handle peak loads during market open,
and then scaling back down efficiently to baseline. We've
been AWS native since day one, and this journey has taught us
a key lesson. There isn't one database to rule them all. Over
time, we've evolved into a multi engine architecture built
with Aurora, DynamoDB and ElastiCache. This variety of
data engines allows us to power every single part of the
business across all of our core verticals. These include core
divisions such as market data brokerage, crypto futures, and
event contracts. Each database has a specialized role. Aurora
Postgres powers our core brokerage applications and our
user authentication. DynamoDB, on the other hand, handles real
time market data, low latency mobile charts, and even
supports AI initiatives like Robinhood, cortex, and our
customer support agents. And finally, ElastiCache is used to
cache request paths, improving the performance and
responsiveness of our core systems. Earlier this year, a
small team of just eight engineers completed the fastest
ever migration from RDS over to Aurora Postgres, moving 4.5PB
of data in under 120 days, a result that boosted our
operational efficiency by a factor of six. What did this
move unlock? We retained all features we had on RDS while
immediately gaining 20% better cost efficiency. And on top of
that, we gained critical features like fully managed
reader auto scaling, significantly improved failover
as well as storage, and I/O auto scaling. This foundation
now runs our most critical and demanding workloads. These tier
one workloads include critical processes like our core trading
platform, tokenization and fraud detection. These all
support everything from Robinhood gold all the way up
to Robinhood legend. This hybrid database architecture is
key. Not only does it provide the added consistency needed
for financial transactions, it also provides the speed and
scale required for all of our live data and Machine Learning
workloads. Here is what that architecture is actively
delivering for us today. For sheer scale and performance, we
are processing millions of read write transactions per second
via Aurora. We also ensure a submillisecond response time
for all market queries, leveraging both DynamoDB and
Aurora. This scale is achieved while sustaining that 20% cost
efficiency and operational reduced operational burden,
enabled by full infrastructure automation. And finally, for
direct customer experience, this architecture has driven a
significant improvement. The peak write throughput for our
core brokerage and crypto systems has increased by a
massive 60% on Aurora. Of course, being in the financial
services means living under the microscope of regulation. And
we embrace that responsibility to deliver on that. We leverage
key AWS features, including DynamoDB and Aurora S3 export.
This allows us to meet our regulatory compliance
commitments. These tools are critical. They allow us to
retain and access customer data within just 24 hours of any
transaction, saving the company millions in potential fines.
Additionally, our Aurora configuration includes
specialized analytical readers, which supports our complex
audit and reporting needs. Crucially, we retain over seven
years of historical data, ensuring we are always prepared
for regulatory access. Now, speed isn't just a user
experience metric. It's also critical to accuracy. To
achieve this, we have built real time data pipelines that
rely on change data capture from Aurora and DynamoDB
streams. This pipeline syncs money transfers across services
with near-zero latency. It also feeds our real time data lake
and meets our 15 minute SLA target for all of our tier zero
data. Ultimately, this provides us with full observability and
control over our entire platform, resulting in a truly
real time experience for our customers. But having the data
is only half the story. The next step is crucial putting
that speed and data freshness to work to maintain absolute
platform reliability. That's why agentic AI isn't just
transforming our products, it's fundamentally changing how we
run our platform. We are now using AI to reduce operational
toil. Our operational AI automates issue detection,
accelerates root cause analysis, and is even capable of
recommending or executing remediations all in real time.
Over time, these systems will evolve into fully AI directed
operations, allowing us to sustain high reliability and
build operational capacity needed for massive future
growth. With the global demand for democratized financial
tools expanding rapidly, we're no longer just thinking locally.
We're actively expanding globally to offer 24 by seven
equity money and crypto trading. As you can see, we plan to
leverage DynamoDB, Global Tables and Aurora global
databases, which are key for providing data replication and
reliable performance across all regions. We are also exploring
Aurora Dcsscl to significantly enhance our infrastructure
control plane and access management resiliency. It's
active. Active architecture Means we are always able to read and write to any regional
endpoint, allowing us to scale out seamlessly as we grow our
business and our applications across more geographies.
Looking ahead to 2026, the pace of innovation at Robinhood
continues to accelerate in line with our mission to democratize
finance. We've already announced several new products
built with AWS, and we plan to announce several more
throughout 2026. A big shout out to the Robinhood Storage
team. Their incredible work provided the critical base for
our platform architecture, including the successful
migration to Aurora. And finally, thank you to the AWS
team. Their deep expertise in complex infrastructure of
financial services has made them a trusted and essential
partner in our journey. We thank them for building the
infrastructure that helps us innovate quickly, scale
globally and deliver reliably even in one of the most
regulated and dynamic industries in the world. Thank
you G2 back to you. >> Thank you.
>> Thank you, Tim, for sharing your inspiring story at
Robinhood, and for trusting us to be your partner to deliver
the world class experience for your customers, to build wealth
and secure their future. So far, we talked about how AWS
databases is making it effortless to get started for
builders and AI agents. Let's switch gears and talk about how
we are making it effortless to migrate and manage your
databases. I'll be highlighting four innovations today. Let's
think back to our discussion on mission. Core to our mission is
database fundamentals, security, availability, scalability,
performance attributes for which you have come to trust
AWS. One of the biggest contributors of application and
availability is database patching and upgrades. I'm
excited to share that Amazon Aurora now applies patches to
your database in a few seconds. Switch over from Bluegreen
deployments with Aurora now takes less than 30s. And we
expanded Blue Green to support Aurora global databases,
enabling you to perform these critical database management
operations that are so important for security with
little to no application impact. And now we are making it even
easier to manage patching and upgrades across your fleet of
databases using upgrade rollout policy feature. You can
centrally and flexibly manage in what order your database
clusters are patched and upgraded across multiple
accounts and clusters. With this feature, you can set up
dev and test clusters to be upgraded first, followed by
your typical production applications, and have your
mission critical applications go last. This allows you to
better manage risk from database upgrades and adopt
automated patching and upgrades with confidence. And we heard
you loud and clear, and we agree that patching dev before
prod just makes sense. You asked for more headroom in a
single cluster for your applications to continue to
scale. It's simpler when you can scale up the database than
to make changes to your applications, to scale out
across many clusters. That's why I'm excited to share that.
Over this year, we doubled the storage scale in Amazon Aurora
to 256 Tebibytes. Now even the most data intensive
applications can simply scale up and continue to scale. You
also told us that the simplicity of zero-ETL
integrations is a game changer to unlock your operational data
for analytics and AI with ease, especially when the number of
applications, agents and hence databases are expected to grow
exponentially in the future. We are continuing to expand
zero-ETL integration. This year. We added additional support
from Postgres, Oracle, and SQL Server, and our vision is to
make it effortless for you to enable these AI innovation
across your entire data estate. Now you can enable zero-ETL
from your databases running not just in our managed services,
but also on EC2, in your data center, or even in another
Cloud. With a few clicks, you can simply connect to your data
no matter where it lives, and to land that data in real time
in Redshift or S3 Tables, and use the breadth of AWS
analytics and AI capabilities to unlock value from all your
data. Lastly, we know that agentic AI is Cloud native. For
many customers, adding agentic AI to their application means
migrating their application to The Cloud first. As they're
migrating their legacy windows .NET and SQL Server
applications to AWS. They're also looking to optimize costs
by modernizing to. Net Core on Linux and Aurora. PostgreSQL.
These customers can cut their costs by simply avoiding
expensive licensing fees. Just minutes ago, we announced a new
set of capabilities in AWS transform to make just that.
With AWS transform, the AI agent would do all the heavy
lifting, including conversion of your schema, your stored
procedures, and migration of your data. Let's see this in
action. Start with pointing AWS transform to your application
code repository. The agent will identify and assess the
associated databases before moving to schema conversion.
It'll create a new Postgres database. If it's needed, it'll
convert your schema using Generative AI powered schema
conversion tool. It'll configure DMs replication to
move your data. And it will also migrate embedded SQL in
your application code, all from within the single simple
experience. Customers are actually completing their
modernization projects up to five times faster with AWS
transform, reducing their cost by up to 70% and unlocking the
security, performance and scalability of Aurora. Before
we close, I want to take a moment and talk about open
source. At AWS. We are committed to open formats and
open standards because we believe it offers customers the
most interoperability and the best value. We have teams in
AWS who are dedicated to contributing to open source
projects like Postgres, Gal Malachi, MariaDB and MySQL.
This year alone, our engineers upstreams over 1000 changes. I
want to go into a little bit more detail with a couple of
features that we upstreamed that I'm really excited about.
At the heart of Valky, or any cash for that matter, is a data
structure hash table. Most of you have run into it in your CS
101. It's the data structure that gives you constant time
lookup and inserts. Let's take a closer look at the
implementation and value 7.2. You see that we are using
object to wrap key and value using pointer chaining to
handle collisions. Nothing fancy. Now, in a modern day CPU
like Graviton, chasing pointers often means a CPU cache miss
and it's expensive. Modern CPUs can run up to 2000 instructions
in the time it takes to fetch data from memory, and in this
design, we are chasing four pointers for every lookup and
no hashing is perfect, so each collision would add three more
pointer chasing. So in 8.1 we have a hash table that is built
from the ground up, optimized for cache efficiency in modern
CPUs. First we Swiss the design for a single object for the
whole entry, so that lookup is only one pointer down from four.
That's a huge improvement. And to handle hash collisions
better, we switch to a 64 byte cache line bucket. With this.
With a clever secondary hashing, this new design can handle up
to seven collisions with no additional overhead. So when
you are looking for that seventh entry, your overhead
remains one instead of 22. Amazing. And the cherry on top
is that the new design reduces the hash table overhead per
entry by 20 to 30 bytes. This means that you can pack up to
20% more objects in the same amount of memory, saving you
money. I want to highlight one more example, this time from
Postgres. So here we have a table in the inventory database
at that online retailer that we talked about earlier. This
table has an index with order status product ID and customer
ID. And this index makes it easy for you to query customers
who are waiting for a given product. The optimizer will use
the index and process only the rows for the given status and
product. Now let's go back to that earlier example from Cyber
Monday. What if you want to find all the customers who
ordered a toy? Since the query is not filtering on the leading
column status? Postgres 17 or earlier cannot use this index
and the query will have to scan the whole table, which is slow
and expensive. Now what if we can teach the optimizer to look
at the index as a set of MINI indices, one per order status
value? Now you can actually quickly find customers for each
status by filtering on status and product, and move on to the
next processing only the few rows each time. And this is
exactly what Skip scan does in Postgres 18, and the feature
will make some of your query run much faster and cheaper
with your existing set of indices. Just upgrade to
Postgres 18. No app changes are required. So let's recap. We
have shared how we are making databases effortless to migrate
your existing applications to AWS, to operate them far more
efficiently, and to build at the speed of an idea through
integrations with AI builder tools like Vercel v0, Agentic
frameworks like Bedrock, AgentCore, and MCP servers, all
with the security, availability, scalability, and performance to
meet all your application needs. And we are continuing to
innovate at a fast pace. We have a few more exciting
announcements, so make sure to watch Matt's keynote tomorrow
morning. We can't wait to see what you will build next, powered by AWS databases. Thank you and have a fantastic Re:invent