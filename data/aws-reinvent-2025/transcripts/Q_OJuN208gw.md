---
video_id: Q_OJuN208gw
video_url: https://www.youtube.com/watch?v=Q_OJuN208gw
is_generated: False
is_translatable: True
---

Hello everyone. It's great to be here at Adobe Reinvent. I'm Manao, a principal software engineer from Samsung Cloud. Today, I'm going to share a great story about how my team solved massive technical challenge on our platform. First, Let me give a quick introduction to Samsung Cloud. Our platform is key for billions of Galaxy users around the world. We provide core cloud services like Sync, backup, and Restore for popular Samsung apps such as Samsung Notes, Samsung Heads, and Samsung Internet. Our mission is simple. We make sure all your data is consistent and available across all their devices for smooth experience. Samsung Cloud operates at an enormous scale. We handle over 1 billion non-selective devices. This drives 50 billion requests every single day. Facing this massive traffic, which shows Amazon Dynamo DB as our primary database. We use the Amazon Dynamic DB to store our core synchronization data for applications like Samsung Internet. Let's zoom in on the specific data set. You just open tab data. The tab information is always generated and updated, and the user opens, closes, or moves between tabs. As our service grew, the continuous creation and update of tap data caused our data volume to grow fast. The table size kept getting bigger. And by early 2025, it had reached 1.2 terabytes. This massive table was consuming hundreds of thousands of dollars in aid of a storage cost every month. Our mission was twofold. We had to cut our storage cost by 50%. And second, we have to get back our entire optimization investment within 3 months. This was our cri critical RY target. Achieving this 11, 1.2 petabyte table was a huge challenge. With trillions of records, just analyzing the data was a monumental task. We couldn't analyze all the data. So we take, decided to take a sample of records to understand these characteristics. The research was surprising. 90% of our data table was consisted of tap deletion records or tombstones. When the user closes the tab, the system doesn't immediately delete that data. Instead, it changes its status to delete it and keeps in the table for 6 months. This mechanism is important for inactive devices to receive all sync history when they reconnect. The tombstone mechanism itself wasn't the problem. So we dug deeper and found that 60% of our tombstones were over 6 months. Yes. Focusing on fast development and stability in our early days meant long-term tasks like their life sack management were pushed back. This was our technical debt. Based on this analysis, we we decide to optimize the table by removing all the data over 6 months. Yeah, we consider the two main strategies. Strategy A was just clean up the existing table to find and delete the data directly. Strategy B was create a new table and migrate only the data we wanted to keep. The direct scan and delete approach required scanning trillions of records. Consuming massive RCU and WCU. This process was projected to take over 6 months. The total expense will equal to several months of storage fees. This massive spend made it impossible to hit our 3 months target. So we immediately ruled out strategy A. By migrating only the necessary data, we were processing much smaller data set. We reduced the data volume to process to 40% of the original. By migrating 40% of the total data volume was an improvement. But the RUU and WU still amount to hundreds of thousands of dollars. We knew we had 2 barrel. So we took a step back from technical details and focusing our focus, our core purpose of our service. As I mentioned earlier, our system stores the deletion records for 6 months of period after it is transmitted. But we started to question this fundamental assumption. We asked ourselves, is this 6 month retention window truly necessary? A deletion records purpose is fulfilled once the signal is propagated all of user's devices. To find a reasonable standard, we measured the time to get for deletion records to be fully propagated to all of the users' devices. The results of the investigation was unexpected. Fewer than 0.1% of devices need more, needed more than one week for tab deletion history to propagate. Remaining 99.9% completed propagation within one week. And remaining 99% of devices receive deletion history in 3 days or less. Based on the data we just reviewed, we were faced with a pivotal decision. Should be shorten our traditional history window from 6 months to 1 week. The upside was substantial. By moving to one way. The amount of data to be migrated would shrink by 96%. From 24 weeks to just 1 week. This was a major reduction in storage and processing expenses. The trade-off, however, involved a specific technical limitation. The roughly 0.1% of devices would lose the ability to perform data-only sync. Those users would be forced to get full data set rather than just incremental changes. The problem caused by long-active devices creates the system level risk. The first thing, so. At Samsung cloud scale, even 0.1% is millions of devices. If those devices were it has to sync or full sync at the same time, a massive traffic surge could overload the system and trigger throttling or downtime. When it hit this technical road block, we knew the solution wasn't just as technical, so we had to dig into our core service domain. By collaborating with the Samsung Internet team, we learned the critical operation behavior. 99 tap limit. This policy means no devices can ever have more than 99 open taps. This was our breakthrough. A freezing storm is only dangerous when the load is unlimited and unexpected. The 99 tab limit quantified the risk. This meant that the maximum data for any full sync was kept at a small and predictable size. This neutralized our biggest peer. With data propagation in sight and 99 tap policy in hand, our path was now crystal clear. We officially decided to change our traditional history window to one week. This decision meant that the data, the total data volume to be migrated dropped from our previous plan of 40% and to just 10% of the original. Yes. Massive reduction from trillions of records to just hundreds of billions. With our final data studies set, our path was not, our, our mission was clear. Find 10% of only data that met our criteria in 1.2 terabytes table, migrated to new table. And finish quickly to maximize the cost effectiveness. But for the Samsung cloud team. That's even more important than the mission itself. It's our users. So before building anything, we established the two principles that would guide every single decision in migration process. The first principle was simple, use your experience first. Our project could not cause any negative experience for the user. The second principle was control the execution. Every step has to be predictable, observable, and fully controllable. These were not just the slogans, they were absolutely standard for every piece of our migration architecture. Our goal was not a system level, zero downtime. But 0 downtime experienced by the user. To achieve this, we adopted the per user migration strategy. Users no migrated, continue to use the old table. Once the user data migration is completed, all of their request is redirected to the new table. While the device's data is being transferred, the device is placed in a rocked state. Any request that arrives during this Iraq is temporarily rejected. Until the migration finishes, and the device is switched over. If a user request this temporary error, this client automatically destroyed within seconds, ensuring real seamless experience. When the device sends a request, the sync service need to know which table to use. To solve this routing problem, we introduced the control tower, the migration status Table, MST. The mega status table record is user met status for every sync request, the sync service corridor MST first. Determines the user's magnet status and access to the right table. Sorry. Having seen this architecture, you've rightly identified the key issue. Yeah, our pro-regulator, which we will discuss next, protects the new table. It lamps up the right road steadily and smoothly, but nothing protects the migration status table. It was our single point of failure. To solve this single point of failure problem, we applied AWS constant work principle. We placed Amazon less cash instance in front of the MST, made the sync service call it the cash first. And we, the back status table is pre-warmed so that the underlying dynamic table is ready to serve a trap instant. And we also tuned the provision capacity minimum value to guarantee the table always accept the full request, even during the cash out releases. This is what the AWS calls constant work principle. The goal of any migration is speed. But focusing only on speed hurt stability and control. This is why we adopted control execution principle. We didn't open the fraud gates. We managed the fraud. We limited the maximum number of Conquant users, much likely controlling a dam prevent a fraud downstream. This approach gave us 4 key advantages. It protects the old table from IU problems and ensure the new table scaled predictably. Make sure our infrastructure was stable. And secure the observability by making our data flow thin and constant. To implement this controlled speed control, we create a flow regulator using a combination of a J feeder and walking queue. The sharpedo supplies the user IDs to working in Q. The migration worker pulls the jobs from the working queue and execute the data migration. The queue strictly limited the maximum number of users being migrated. If a queue feeds, a back person mechanism is automatically poses the job feeder. The self-regulating system makes sure our throughput stays at the, exactly at the planned level. Our rest technical hurdle was how to find the 10% of necessary data within 1.2 terabyte table. If we had to read every item, just decide if we needed it. You still in curve massive by CU cost. And immediately stretching our project timeline. The breakthrough Don't come from the new technology. By looking at the fundamental mechanism of our existing sync service, timestamp-based synchronization. Our sync mechanism works by tracking by timestamp, the exact time of next modification. This allows the device to only download the, the changes they missed. Our table contains this local secondary index keyed by timestamp support to support this crucially. The LSI has status as a project attribute. This is the key. This allows us to filter out the 90% of the old data by reading the right weight index without touching the main table. So instead of a heavy table scan, we ran a just filtered query. Treating our migration worker is that new device requesting is first thing. Here's how we do data extraction in two steps. We use the LSI to find the, the data to be migrated. We ran a single LSite query to get our two target groups. First, Or active tests, 2, or traditional records from last week. We use the tab ID from one, the first step. To pull the actual data. Instead of fetching each item one by one, we use the diamond leaves patch together item API. The LSI quarry was cost effective and fast, avoiding full table scan. The VA1 API gathered all the data effectively. This combination dramatically lowered our read capacity unit. The migration was more than just moving the data. It was the ultimate test of our philosophy. We are proud to say our principle worked. We will break down our success in 3 key metrics, technical and financial, and most importantly, customers. Our entire migration was completed in just one week. Thanks to our control restriction principle. The number of concurrent biotin users stayed steady at a few 10s of thousands. The double shield on the old table declined steadily. Showing the gradual decrease in traffic. This graphs shows the new tables RUU and WQU increasing smoothly and steadily with no noticeable spikes. This confirms that our control restriction approach successfully managed the load. On the financial side, the impact was instant and huge. We reduced the table size from 1.2 terabytes. To just 100 terabytes. Distressed our storage bill. Thanks to our strategy, the migration cost was minimal. The total expense was less than one month of savings. We recovered our full investment before the next AWP arrived. We achieved 150% of the original goal. But beyond the technical and financial metrics, there's one that matters most to us. The customer metric During the entire warming migration, we received zero custom inquiries or false report related to migration. This is the number our team is most proud of. This is the ultimate proof of our user experience principle was not a slogan. We are proud of these results, and I want to share 3 keys made that possible. The first lesson is the power of data-driven decision making. It was data sampling that first revealed our problem. 60% of our table was old tombstones. And it was data analysis that gave confidence for our solution. We analyzed the log, found that 99.9% signals propagated within one week. And made our most critical decision to change the lesion history window from 6 months to 1 week. Data removed the guesswork. We apply this data first approach at every critical junction. However, our team also learned the That alone is not enough. Data gives you the numbers. But only the domain analysis gives you the meaning. Without this, you can look at the same data. And still make wrong choice. So our second lesson is deep domain understanding. Our first insight came from asking our fundamental sub question. But the true purpose of Tombstone Records. This was the key. That allows us to challenge our six-month rule in the first place. Our second insight. Wasn't just a discovering domain policy called the 99 tap limit. It was a realization what that policy meant. It naturally kept. The first thing We instantly neutralize our biggest technical risk. True breakthrough don't come from the optimizing the how. They come from the questioning the why. The final and the most important lesson is user-centric mindset. This is the principle that guide every single decision we made. Yeah, technical and financial success means nothing if you compromise the user experience. This mindset is why we choose the safest path instead of the easy one. And in doing so, our team learned what the true measure of success is. It's not a petabyte we migrated. Or the dollars we saved. It's the trust be kept with our users. A 1.2 petabyte migration with zero customer constraints. Thank you