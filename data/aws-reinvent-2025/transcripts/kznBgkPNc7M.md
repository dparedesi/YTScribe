---
video_id: kznBgkPNc7M
video_url: https://www.youtube.com/watch?v=kznBgkPNc7M
is_generated: False
is_translatable: True
---

Thank you for joining our session today. The best session of today of course uh what is it that we will be discussing today, right? So I'm more than happy to introduce yourself and my name is Diego. I'm part of Intel and I'm part of the AWS team responsible for AIML and um our session today and what we thought about what we want actually to speak about and get also our customers and partners to little bit educate everyone in the room is how do we build fast, cost efficient and also important sovereign influence platforms. Using Intel CPUs, um, and to do that I think I'm really excited to announce today that we have a really amazing lineup of speakers, right? So, uh, we will be having a session of 3 different use cases um we will be having Caitlin Anderson, um, who's the VP from Intel and general manager for American sales, and we have Mickey that will be joining us from AWS and who is a director for um enterprise technology specifically for Gen AI in the public sector. Um, we will be then having 3 different customers which will be Deloitte and Bob Simmons with us on stage, uh, Amit Gupta, VP from E and Enterprise, and at last also Renato, uh, who's head of technology from Articulate. Um, what can you expect from our session today, right? So we have 60 minutes, we have a long time. Um, and I think we hope we make it as best and as efficient as possible, and, we will introduce actually 4 different flows, right? So on the one side I think we want to give a little reminder Intel and AWS, right? So what is it exactly that we do? How do we actually want to achieve inference performance on Intel CPUs with AWS? And then secondly, we will deep dive into 3 different use cases. Uh, on the one side we will be doing an analysis actually about how to use EC2 Intel powered instances for inference. On the second side, uh, we will elaborate on how to develop a sovereign influence platform jointly with E and Enterprise, and at last, uh, we'll talk about AI innovation actually with Articulate. And now, having said all of this, um, I'm super happy to announce today actually on stage Caitlin, um, who will be talking a little bit more about the partnership between Intel and AWS. Welcome, Caitlin. Thank you Diego. Wonderful. Well, thank you so much and thank you for joining us today. Um, we're gonna talk a little bit just about the partnership and I'll start there. Um, again, I've been at Intel almost 24 years, so, uh, basically our partnership with AWS, uh, spans my entire career, almost 20 years. Um, a lot of the work that we do, and, and many people don't think about kind of the complexity of the relationship that we have together. Uh, you know, really from an infrastructure perspective, a lot of people would think about Intel and the hardware that we build and design and manufacture and obviously sell into AWS. Uh, we have over 400 instances of EC2 that are running Intel, um, which is really we have this huge breadth, uh, of compute and technology when you talk about the hardware and the infrastructure. But even more important, especially in the world of AI now, um, is a lot of the optimization we do with the software with the third party companies like many of you in the room and that's what we're gonna talk about today to basically optimize those instances, um, to run really well on on, uh, Intel AI workloads, um, as well as general compute I should say. Across all of the services that Amazon delivers, um, and then of course the devices, so the breadth by which we design our chips and our products cover the edge, uh, to the AIPC. Um, to a lot of the cloud and of course, uh, data center on-prem as well. So, um, as we think about AWS and the span of all the devices and the services that they offer across the across the breadth of the ecosystem, Intel participates in every one of those things. So it's a really dynamic and important partnership. It's growing by the minute. Um, we have a lot that we do together strategically across both of our companies, um, and I'm super excited for what's ahead as well. Let's roll the video and talk about it. You want it. I want you to hear it from, uh, our CEO Lipu and Dave Brown, who's the VP of compute for AWS. AWS has a long-standing relationship with Intel, actually dating back to the launch of Amazon 2 all the way in 2006. Intel's relationship with AWS spanned more than 18 years and continue to grow. Our collaboration runs steep. Today, I'm excited to talk about our most recent launch, AWS 8th generation Intel-based instances. Our latest Xeion 6 with Porre technology delivers reliability and performance to cloud providers like AWS and their customers. The new Intel instances offer up to 20% performance improvement over previous generation Intel-based instances across diverse workloads. For real world applications, we believe the performance gains will be even higher. This isn't just about migrating existing workloads to the cloud. It is about unlocking new possibilities through nimble and data-driven development pipelines. With the 8th generation Intel instances, AWS is adding support for instance bandwidth configuration or IDC, a new feature that we launched last year with our 8th generation Graviton instances. IVC will give X86 customers the flexibility to optimize their network and storage performance based on their specific workload needs. Our 8th generation custom Intel Xeon 6 processor CM&R instances offer new Flex variants. That have been designed for customers whose workloads have variable compute patterns. They offer a cost effective alternative to standard instances and provide the easiest way for customers to achieve price performance benefits. The future of cloud computing will require even more powerful, flexible and efficient solutions. And Intel is proud to be driving that transformation alongside AWS. We're excited to see how our customers will use them to drive their businesses forward. Wonderful, um, so thank you very much and also I, I, I mentioned this is Intel's AI strategy, but really a little bit of our philosophy as a company, uh, for those that have worked with Intel for many years, we have always had an open ecosystem approach to innovation. Um, that's a big part of our promise. It's a big part of actually we're spending a lot of money as a company now to invest in the X86 ecosystem. Um, to make sure that it's strong and healthy, and that means that we always, you know, really kind of start with an open ecosystem approach, um, a lot, and we'll talk a lot about the software that we are developing to deliver and contribute to the open source community as well when you talk about AI. Um, because I, at the end of the day, uh, we've been through many cycles of innovation, and AI is certainly one, we liken it sometimes to the wireless innovation when things change in the PC dramatically. All of that required the ecosystem to come along, uh. One company cannot deliver uh a movement like that and that's what AI is all the entire ecosystem, um, you know, when we really have this open approach and not these verticalized kind of locked in, uh, systems, that's when innovation will thrive and go faster and scale, um, and that's what we believe will kind of at the beginning stages of this when you talk about enterprise AI. Of course we want it to be efficient. uh, we want the TCO to be very good for your companies, um, and I know that you're facing that as you're making your own decisions on how to develop AI for yourselves but also to deploy it to your customers and of course we want it to be secure. We have a long history in the enterprise market. Uh, we know how to drive security and how to deliver that for enterprise customers and that's a bedrock of what we do across both the hardware and the software. OK, and then many people may or may not know we've been working a lot, um, the last we've always had a strong ISV ecosystem so part of that ecosystem work that we do has been to invest in a lot of the software providers, um, for many years to develop on Intel, uh, but as AI has grown both on the PC and in our data center business we have, uh, strengthened and really tried to invest a lot more time and energy and money into. Uh, enabling and and supporting our ISV customers, um, across to be not only just enabled and developed, um, on Intel and optimized for Intel instances, um, but also to work with AWS as well and there's a lot of really great joint, uh, three-way partnerships that we have as we think about solutions selling into the market. So Adobe is just one example, um, you know, integrating AI into the creative tools we work very closely with Adobe. Um, on all of their all of their use cases and making sure that it can run the best on Intel, um, SAP is a huge customer of ours as we think about, um, the AI solutions that they're delivering to market. And our kit maybe one you you know or don't know but uh really quantum safe encryption, um, some a lot in security, um, and these are just a few examples of companies where we've been partnering to create programs through our channel, uh, programs to make sure that they can. Get access to tools and technology but also to programs and marketing so they can scale their um efforts and many of these are also AWS customers so a lot of work in the ISV ecosystem you're gonna hear from a few of them today as well um and so we can bring a little of those stories to life. And then lastly I know uh it probably sounds really convenient that we would say CPUs can be used for AI uh because we obviously developed CPUs, but. Um, really at the end of the day, and I've been, uh, you know, tracking this, I started out really kind of thinking about the AIPC space and moving over into data center as well, um, you know, the AI revolution right now has been heavy on training. Uh, there's a lot of, uh, you know, a handful of companies really developing these amazing models, um, you know, training and doing some of this hardcore AI work to build the foundation. Um, but we are at this point now where enterprise scale really hasn't happened yet. Enterprise AI in the way that I think that we all envisioned for the future, and it's just beginning, and a lot of that is gonna be on inference workloads. Um, when you think about the next kind of phase or chapter of AI, we believe a few things. We believe that, um, inference workloads will definitely be growing at a significant rate, um. We certainly believe that agentic AI is gonna be a really big piece of the theme of the next couple of years and that we also believe that it will run across a variety of compute, um, that it's not just a GPU workload at the end of the day. Um, that you're gonna need all sorts of types of hardware, uh, to drive the right TCO and ROI to run these inference workloads, and there is a lot of great examples of how inference workloads can run best on Xeion, and we've just launched our Xion 6, processor, which is our latest generation, and we'll show you actually how you can run a very simple inference workload directly on Xion. I know I can say that, but we wanted to show you so you have the visual. So just behind me this is just an example of uh you know, just a simple chat, um, kind of use case, um, where they're searching for information and this is all running directly on Zion, um, oh. If we go back. I think it's a video. No Well, it is running a video in the background, but we'll, uh, we'll have to keep moving there. Um, but simply I think you just wanna, you know, from a lot of the use cases that you're thinking about for your company we're gonna talk to you about how they can run on Zion as well, um, and for that I wanted to invite Mickey on stage from AWS to share a little bit of his perspective on the partnership. Welcome. Thank you, Caitlin. Thank you. The Intel and AWS. Partnership has been strong since 2006. And we've been collaborating. To develop transformative technology and innovation that supports our customers' enterprise workloads. AWS has launched 400+ Intel-based instances. To serve the diverse needs of its customers across all of its regions. And this partnership continues to grow and to scale. Back in September 2024. We signed a collaborative. Multi-million, multi-billion, multi-year strategic agreement to invest. In chip making And that partnership is delivering results. I'm very excited to talk about the custom Xeion 6 on Intel 3 that is now generally available on Amazon EC28I instances. This is across the C, the R, and the M family of instances. And I really want to talk about the fact that this is custom. There is great engineering collaboration that's gone into developing a purposeful, purpose-built chip. That allows us to bring unique advantages to our customers. Giving them the highest performance as well as the fastest memory bandwidth and delivering value to them. Notably, this is 20%. Faster from a compute performance perspective. And it, and when it comes to inferencing based recommendation models, it's showing 40% better performance to the older generations. This is truly fascinating. We have the fastest Intel chip on the cloud running in AWS. But I also wanna talk about how. There are AI capabilities and acceleration capabilities in this new technology. The. AMX Zion 6 Intel chip. Uses advanced matrix extension. And what this does is it allows you to do. Optimization of the matrix multiplication that provides much, much faster inferencing and much, much faster training and it also opens up the array a wide array of workloads through which inferencing can be done. And this really opens up. The business to a large set of industries. It leads to faster training and faster inferencing that I mentioned, but it also helps conserve energy and scales beautifully. Now. What we have done with this EC28I instances that we have introduced it over these different families which are the M, the C, and the R families, but as you saw in Dave Brown's video, we also have the flex option that's available and with this flex option you can have customers match the pricing to their demand. So another thing that we've done is that we've launched these in the US and the Spain region. And we are now working on launching these to additional AWS regions as well. A great example of our collaboration with Intel is what Technology Innovation Institute has done in UAE by using this technology to develop their Falcon large language model. We also have many other partners and customers that are benefiting from these technologies. Next, to hear stories of one of our partners who've done fascinating work in this area, I would like to invite Bob Simmons from Deloitte to the stage. Bob. Thanks. Hi, my name's Bob Simmons from Deloitte. I'm in the government and, uh, public service sector. So what I'm here to talk about is some work that we've been doing with Intel on running LLMs on LLMs and SLMs on a CPU. So the Deloitte AI strategy really comes down to we invest heavily into our practitioners, training them and heavily into developing new capabilities, which is this going to be one of. Um, what we're looking at is from two different perspectives is how can AI win in the market and then how can clients win with AI. So what does the market want? They want a cost efficient way of doing inferencing. They need to be able to scale so most of the projects don't deliver on the ROI whether it be cost savings, whether it be reduction in labor, whether it be an increased customer satisfaction. When it goes into a prototype, it doesn't. Go into excuse me, it doesn't go into production and from production sometimes it's hard to scale, especially if you're looking at trying to use GPUs and there's not enough out there, maybe you don't have the funds to cover a GPU 24/7, but you can a CPU. So what we're looking at is running on a CPU and um being able to reduce the cost and able to scale. So how can the clients win with AI? One way is to use CPUs with SLMs and compressed LLMs, and we'll get into that in a little bit. And most clients have a, a large infrastructure of CPU-based EC2s, which makes it easier to scale. It decreases the cost by about 50%, 50% plus if you're looking at the OEM, the, the, the, uh, excuse me, the ONM. And then, um, it enables scaling and also increases security. You know what I mean by increase of security is a in the GPS they will have a full up AWS cloud on the unclassified government network. They have a full up. Um, AWS cloud on the secret network and the full up AWS cloud on the top secret network, 3 full clouds not connected to the Internet. So the security is when you're on when you're uh on premise you can have different organizations that don't want to share data within the same agency thinks of um human resources and benefits. I don't wanna send PHI to the overall chat GPT that the agency uses. I don't want if I'm in contracts, I don't wanna send. You know, Deloitte's, you know, rates to check GPT to say fill out this form for me. So what they want, what this makes possible is to within your own VPC to have your own G AI to do your work that never leaves and that for a lot of my customers that's like top priority. On this, the current landscape is, um. AI data centers are estimated to like really grow. I think they've already talked about this coming up, but it's growing fast and GPUs aren't out there, at least for my um client, getting the GPUs onto their networks. Um, so that's a problem. They have a large infrastructure of EC-2s, uh, TPU based. So going down to CPUs such as the Intel Z Zion, it offers a cheaper localized alternative to GPUs, all right, for the, uh, GPUs costs are surging because the high demand for them. Um, there's massive large language models on GPUs. There's a lot of times overkill for what you need, but that may be the only thing that's available. The CPU powered models offer a budget friendly way to a broader adoption and what I'm gonna say democratizing it across the agencies. So, and going forward, the small language model we're saying is like 10 billion parameters or less. Um, the model compression tools such as the Intel's Open Veno, and this is kind of crucial, and some new technologies for compression which can compress models down without losing much accuracy and it helps expand the use cases that you can, uh, automate. All right, so, Where are we on this? So we are test, we're working with uh the Intel Alliance. We're testing out, um, running models on GPUs and CPUs. We had a, a phase one where we were running an LLM and an SLM both on GPUs and CPUs and looking at the performance. And then we're saying, OK, we found a company that does compression in a way, they call it uh quantum-inspired compression. That allows them to compress it down, uh, say a 70 billion model or, um, larger to fit on a CPU and we wanted to know what was the decrease in accuracy. That's phase two. And then, um, we went to a phase 3 when we said, OK, that compressed model didn't have a large enough context window. So when we're running it from the GPU to the CPU on phase one, you know, of course we saw and we got um SLMs to work uncompressed, but not the LLMs, it's just too large, right? But you can get a 56% reduction in ONM infrastructure costs as I mentioned earlier. And it outperformed in terms of efficiency, not necessarily speed, but when you're looking at energy and cost and availability for uh scaling, it outperformed it. On phase two. The compressed models run more than, uh, almost twice as fast as the uncompressed models, which also increased. So if you don't want to uh buy more hardware, you can, uh, license a compressed model and do twice the work. We went to phase 3A and we're on the phase two on this chatbot. We were getting 71.3% accuracy and the client wanted 75%. We went from just a larger context window to 80.7%. And note that the uncompressed model had 81.7%. So we lost 1%, which was, I think it was like a, so you're keeping like 98.5% of the accuracy of the uncompressed model. So this is kind of we saw this and said all right we've now shown that you can run large language models and um on CPUs and small language models and that you can run them faster and um actually I'm going on to the next slide actually. So, on the GPUs, Amazon, we're using Amazon AC28Is. This is the Xeion 4. And then the monthly cost for the GPU and the monthly cost for the uh CPU you're looking at about 50 56% reduction in cost. What you're seeing on the lower left and the lower right is the uh um tokens per second on the left and then the latency on the right. The Open Veno model compa uh model server is optimized for the Xion. So we, we're using a ZM that has 48 CPUs. When it loads it, it actually does parallel processing between those CPUs. So on the bottom scale you'll see 1 user, 2 users going on up. These are simultaneous users, and you see the CPU is flat. And actually when you get up to a certain point, the CPU outperforms the GPUs, but then the, you know, the latency is getting up there. And the, uh, tokens per second, you can also see where it's flat on the tokens per second and then it finally got around 40, it, uh, starts to drop off but so this also depends on your use case. If you're not really time dependent, you can use, uh, use this or a GPU if you're really time dependent on need speed. All right, the target use case areas for the SLM is basically when we need security. When we need infrastructure for scaling. When we need to be on-premise, where CPUs are prevalent and GPUs aren't as pre uh prevalent. Um, multi-identric use cases. One of the next phases we're gonna be doing is a more complex use case where there's gonna be three different agents running on an EC-2, um, CPU based EC-2. Um, and that works very well actually. And then on edge devices, say satellites, drones, let's say you're in manufacturing and you want uh L&M out on the manufacturing floor on different machines. So these are the use cases we're looking at. So one of the use cases we're gonna be doing next is that you have a sensitive document. I'm just gonna call it it's an, it's an intelligence report, right? If we want to send that to some allies we've gotta remove some information. So one agent is gonna find the information and then show it to the intel analyst and say, is this correct? Then it's gonna remove the information and rewrite it as a second agent and then show it to the analyst and say, Is this right? And then it's gonna go back and it's gonna reclassify the document based off of um what information it pulled out. So this is just a multi-genic system doing a more complex now granted this is gonna be using a 70 billion parameter that's been compressed down to fit on a uh AI instance. All right, well, actually I need to hand this off. OK. I, yeah, here you go. I think so. OK, so we talked a little bit about thank you Bob um I what I love about this is it's nice to have partners like Deloitte who are really coming alongside to bring these use cases to life to see how they come. You know, to fruition with real customers, um, I said that I'll talk a little bit about software too. Many people don't know that we have thousands of software engineers at Intel, and we've been working in this ecosystem for many years, um, but we, we talked about this open approach, um, certainly with any large framework providers, any of the large language model providers, we're working with all of them, um. You know, we have, you know, for ourselves we wanna make sure is any version drops of any of these models as well we have a very short turnaround that they're up and running on Intel, um, so that's been, uh, really successful and we're gonna continue this work. There's a, a depth of those relationships that it's quite strong. We also have our own software we talked about one API, um, that really is optimized for Intel and it's available, um, to all of you and we've been actually working with one API in our Edge business for many, many years across all the verticals when you talk about, um, optimizing that's where that, uh, you know, started, but it's expanded dramatically with, um, with AI so open, you know, one API is really a great tool, um, encourage you all to take a look at. That and then of course we have um you know open standards and protocols that we also develop uh within the ecosystem so UCIE CXL some some of those as well so a lot of work in terms of how we contribute to this open ecosystem software approach um both with companies and also our own contributions as well. And then lastly we actually just announced um Intel AI for enterprise inference. Try to say that a couple times fast, um, but really the intent there is simple, easy to use, uh, software deployment for this inference workload that we've been discussing. The beauty of having, uh, the AWS instance in the cloud is that this is just available when you choose the EC28I instance. You get this software as well, um, as part of that. Experience, um, but really it goes all the way down the stack to really think about how you can leverage and optimize for inference workloads on Intel and we're gonna continue to build on this, um, and continue that development. So, uh, as an example of someone that's using or used, um, you know, this, this software I wanted to invite Amit up, um, from EA, uh, to talk about a little bit of how you use the software for development for the UAE. So welcome Amit. Thank you, Catally, for the kind introduction. Hi, I'm Amit uh Gupta. I'm the VP for EN Enterprise. I lead the data and AI practice for Asia, Middle East, Africa, and Europe. And today I'm going to talk about how we are collaborating with Intel. Uh, to build the entrance, uh, enterprise AI entrance stack and how we are uniquely solving a problem whereby we create something which is open, transparent, cost effective, and sovereign for our UAE customers. But before that, let me talk a little bit about what we offer as, uh, E and data AI practice. So we offer a full stack, uh, practice whereby we are providing. AI connectivity, AI infrastructure, AI solutions and services, but also AI devices. So we provide, we provide the full stack. We've already delivered a lot of these AI solutions, more than 200+ AI cases and 200 use cases across sectors and industries. We are tech and platform agnostic, which means we work across, uh, you know, AI and data platforms as well as hyper scales. We have a large team of AI resources, uh, located globally and we have developed in-house AI accelerators, uh, you know, that give value within weeks instead of months and last but not the least, we also have our own AI academy where we train practitioners, uh, and a lot of other leaders on AI. We have our latest, uh, flagship program, the Chief AI Officer program, which we are running very successfully. OK, so. I'm sure all of you have seen some of these statistics. The key point here is the inference market is going to grow. The AI inference market is going to grow exponentially, you know, you can see that from 100 billion this year to more than 250 billion in the next 5 years. And through 2028 what we have seen is the cost of AI inference is at least 70% of the total model lifetime, which is far more than the training costs, uh, that are required to do to train a model. So as you can see, the inferencing costs are very, very high, yeah, and some of these stats talk about that at least 50% of GAI projects will overrun the budgeted costs, you know, as per Gartner. Now, how do we build the solution? Yeah. Now the question is how do we build the inference AI platform to meet the customer needs, yeah. Before that, let's address some of the challenges, and I'm sure most of you in this room are also facing these challenges in your enterprises. The first one is about costs. Organizations implementing AI at enterprise scale today face a lot of cost constraints, you know, often leading to more than 300 to 500% exceeding whatever we had planned. The second is about infrastructure management. And if you know that AI workloads today, Uh, have unprecedented infrastructure demands and our traditional IT systems cannot handle that. Not only that, they also have a lot of parallel processing, as you, as you know, with, with all the kind of heterogeneous requirements like CPUs, GPUs, TPUs, NEI chips working in tandem. And lastly, data sovereignty we are building AI for UAE from UAE, which means all our data has to reside within the country, which is cost effective, which is, uh, you know, doesn't go out, which is generated, protected and preserved within the country and meets the local norms. So how do we tackle these 3 challenges? Um, so it is through our solution that we call that SLM in a box or some small language model in a box, and more precisely we are leveraging both Intel AI for enterprise inference and Intel AI accelerated instances to power our sovereign inference as a service platform, you know, and there are our goal is twofold. One is of course keeping everything in the region, which means that we are looking to build a solution. Where it is independent from all other AWS regions, you know, meaning that we wanted to serve only our UAE customers through the UAE region, UAAWS region second. Is about being open and cost effective, so we wanted to address sovereignty through automation, through cost, through security, and through transparency and to do so we are using Intel 7 AI instances as inference at point and which is allowing our customers to not only build use cases over the platform but also offer use cases which are ready to be made or ready to deploy. OK. Maybe we can go a little bit more into, uh, you know, deep dive how our solution works and as Catally mentioned in the introduction, the uses of GPUs is only for inference is a myth, right? We have come ourselves to a conclusion that when running specifically small language models. Models with below 10 billion parameters. Intel instances are a perfect fit, yeah, and it's great to see a variety of partnerships in UAE as Mickey also mentioned about how TII built through the Falcon model, right? Now let's deep dive a little bit more into the solution, specifically two takeaways that I want to highlight. One is we win with open, as you can see on the right hand side. You know, our architecture is only leveraging open source components. Not only this, it's also helping in becoming cost effective. Yeah, we are advancing using what the community is building today, so it, it remains cost effective. Two is about flexibility, you know, that's one thing I really wanted to point out is flexibility. So in today's world where AI is changing everything so quickly, there's so many new models getting built in. This platform helps you choose the model that you want, and it can be deployed uh out of the box. So that's one of the key advantages of this. OK, just to bring it home now, I think most of the slides have spoken about the benefits that we have received, but let me summarize it for you. 1, it reduces complexity, which means introducing a full automated solution. 2, it's quick time to value with automated deployment and management of AI and LLM models. 3, is scalable, you know, with embedded auto scaling. 4, we are talking about being cost effective, which is very, very critical in the overall scheme of things. We are leveraging Intel CPUs as inference head point. 5, it's secure by design, you know, we are deploying it with built-in authentication and APIs. And lastly, it's sovereign by design, you know, we're deploying it only for our UA customers, leveraging UA region only and owing to our infrastack infrastructure orchestration. Everything is happening in the AWS region in UA. So that's it from my side. Thank you very much and over to you, Caitlin. It's. OK, um, we are just bringing it home here, so, uh, the last we talked a lot obviously about our AWS partnership we talked about customers that are leveraging inference workloads on Zion. I'm gonna shift a little bit to a new company, uh, called Articulate. Um, and really it's an amazing company. It actually started at Intel, um, and we kind of spun it off. It was a great, uh, you know, start up, uh, story, but I'm really impressed with their use case. I'm impressed with their platform. It's a lot more about, you know, using your data and enterprise data and insights inside your company to unlock, uh, those insights, and they're gonna, I have, uh, Roberto, uh, Renato here to tell us more about Articulate. So welcome on the stage. Thank you. All right, thank you, Casey. Hello everyone, I'm Renato Nascimento, head of technology of Articulate. Um, thrilled to be here talking about what we are building at, uh, Articulate. So Articulate is a platform that's a smart layer that helps enterprise to convert complex uninstructured or structured data into hyper personalized outcomes as you can see here on the right. On this vertical integrated platform we help you to run um. Uh, across different agents and models, so it's a vertical integrated platform that, uh, works, uh, either on your own premise or can be deployed on your own VPC within your AWS account. So at the left as your data comes into your platform that data gets break down into its components and that's components runs through our model mesh um in house uh orchestrator that will uh use a library of different models, a combination between. Uh, traditional machine learning models, uh, small, uh, language models, and also large language models to understand your data, and most important domain specific models and also task specific models and we'll talk a little bit more about that. Uh, later on that gets break down and understood where we build a knowledge graph and this knowledge graph here is a little bit different than we, um, is done out there. We also augmented the content from your data using the domain specificity from your own enterprise, creating and help you to see, uh, connections that are not, uh, directly, um, implied on the data as well. Um, So what are the um key differentiations of our platform? One is our uh intelligent model routing right where we uh build an in-house uh model orchestrator that helps you at run time autonomously uh decide which agent or model will be used for the task that you're trying to achieve at hand. We do that by um evaluating every single of those models the agents across the tasks that are important for your use case and uh I'll show a couple examples as well. All of that with uh a platform that gives you observability you can audit every single decision that those actions are taken uh on your behalf on the platform as well and that can be run on the scales we have used cases that have been run with hundreds of thousands of files generating knowledge graph with millions of entities uh running in production. And more importantly our platform comes with a combination of in the library of domain specific and task specific models that helps you to achieve a complex use case and um. I, I brought here an example to help you visualize why, uh, domain specific models are important, right? If you're trying to do something very specific or a use case, uh, the general, um. Purpose models out there they might uh fall short of achieving your goal in this example here we have an image. This image is is um it's a grid, uh, is a testing system for uh power cables, um, and if you send that to, uh, for instance in this case to cloud, you see that the image gets, uh, poorly, uh, classified. And then if you use one of our domain specific models in this case it's an energy specific domain specific model uh that one can not only understand what this image is about but also all the um muts and details uh underline of that image as well. So if you have an expert use case, um. Uh, that, um, data might not be out there on the public or even if your data is out there in the public, you might have, uh, to heavily prompt engineer, uh, general purpose models to achieve that goal and might be rule based which might also fall short in, um, uh, corner use cases. Also, um, we brought here we that was an example, but we also don't believe in benchmarks internally we articulate what we do as part of of a model orchestrator as well. We test every single, uh, model in in genies that are part of our platform across, uh, a different variety of uh dimensions. On this example here, um, we have our, uh, domain specific, um. Uh, model being tested across 10 different, uh, domains, um, uh, sorry, uh, dimensions created by experts of the area. Um, each of those tasks contain, um, uh, very curated, um, uh, tasks that those experts on that domain wants to achieve, and, uh, on this, uh, spider plot, um, closer to 100% mean how accurate you are, um, on that task. Here we can see across all the 10 domains or, uh, domain specific model. Uh, overperforms, um, with more than 25% high accuracy accuracy than ever, uh, other model available there either private models, state of the arts like GPT 5, or even large, uh, open source models as well. Not only that, uh, towards, you know, uh, running optimized inference costs as well, uh, this model is also a fraction of the size of every other model there too, so you can not only achieve, uh, better results across, uh, uh, different tasks that are specialized, but also reduce your overall cost of inference. And um I brought one example here with the domain expert model but we have uh data and uh uh experiments like that for all of uh domain specific models out there for instance um uh semiconductor uh manufacturing um supply chain um financial services. As well, and that's all embedded and indeed on our platform, um, it comes when as we deploy the platform as well. So Our platform also brings with it all the optimizations and expertise the team has built over the years helping optimize what model uh works best with what what harder so we have a library of models from all different sizes so we need to understand that the real time uh which uh model which harder is gonna be used to execute that. Uh, either for the small ones that will be, uh, CPU optimized or even from, uh, very large general purpose models as well that will run on GPU. Um We also are uh AWS partners, um, um, um, uh, certified AI, uh, on, on the AI competency that means, uh, we have a very tried and true solution that deploys into AWS optimizing the resources and follow all the best practices there as well. Um, And um just to to to close out uh um I also wanna talk about how you can um start to see our products. So our product is a subscription either subscribe to our software or you can subscribe to our, uh, manage solution as well if you wanna start before you get your own deployment on your own. Uh, AWS account or your own VPC, but also we, uh, uh, have our, uh, agent, uh, a domain specific models and agents, uh, available into the AWS marketplace for you to try. Um, I'm also proud to announce that today, not, not today, this week we are, um, launching a new, uh, agent on AWS marketplace that, uh, does table understanding. And uh with that uh agent you can um um upload uh very complex uh uninstructed files that have data uh on the top of the tables either images or um normal table that it doesn't matter. That agent will understand that table and send that back to you and then you only pay the uh users across saints so it's very easy to start using us either through our um AWS marketplace offering or um getting directed with us using our manage solution as well. Um, if you wanna know more about us, please, please check out our website articulate. AI, and thank you very much. Back to you, Katie. OK, thank you, thank you, Renato. OK, we will wrap it up. So thank you so much for, um, being with us today. Just a few key takeaways to take with you. Um, certainly, you know, check out the EC28I instance with our Zion 6. we know now after this session that you can use these for this instance, not only for your inference workloads but obviously general compute as well. Uh, we will lead and continue to lead with an open and trusted AI approach. We will invest in the ecosystem. We'd love to partner as well when we talk about this third part. Um, if you're a software provider, you wanna be part of some of the programs, if you have interest in how to get involved, um, we'd love to talk to you after the session in the hall as well. And then of course the more you can do there's an intel booth in the showcase as well uh we have round tables on different topics throughout the next couple of days um and you're welcome to meet the team and contact us just through this QR code so thank you so much for taking your time, uh, and investing in this session and we appreciate it. Have a great day.