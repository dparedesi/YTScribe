---
video_id: pjLYOw17bPA
video_url: https://www.youtube.com/watch?v=pjLYOw17bPA
is_generated: False
is_translatable: True
---

Hope everyone had a great Thanksgiving. Thanks for joining us today. Hope your, uh, AWS reinvent is off to a great start. Uh, I have the pleasure today of having, uh, Sawick from ServiceNow and Jamie from SLB. Um, they're formerly called Slumber J, uh, with us today. But before we get to the panel session, just wanna go through the agenda. Um, I'm just gonna give a quick overview of what we're seeing in the market today. Then talk about the Nvidia AWS partnership. um, just tell you what DGX Cloud is. Have any of you heard what DJX Cloud is Nvidia DGX Cloud? OK, cool, um, so I'll explain what that is and um. Satwick and Jamie will be here to kind of talk about their experience with DJ Cloud Nvidia AI on AWS and we'll have a little bit of maybe time to do some informal Q&A. This isn't quite the, the right setup for that, but um we'll see what we could do. OK, so, um, AI adoption is everywhere. Is everyone already doing some sort of AI like either developing or already implementing AI today? All right, cool, um, looks like the majority, um. So this is really a new industrial revolution where it's powered by AI factories and what we're here to do is talk about AI factories in the cloud and AWS really is one of the biggest AI factories that you could use and you can see it spans across all industries and today we're going to be talking about two very distinct industries and they'll talk about their exciting use cases on on how they're using AI. And how um the, the AI factory in DGX cloud is helping them develop their, their AI. So there's we're what we're seeing is we're seeing the emergence of of three scaling uh laws for AI. We really started with pre-training, um, scaling, which is really, uh, teaching a model from the Internet, right? So if you think about, uh, surfing the Internet, you could get a lot of information that way, but then you want to take it to the, the next level. That's just consuming the data that that you're seeing. But then you wanna think about it so like um when we go to post training uh scaling, this is really where the model goes to school and it learns thinking, right? And then kind of the, the, the last part is really the idea of reasoning and so we call this test time scaling. So this is long thinking. It's not just thinking of like what answer it's going to give back, but it's thinking before it gives back the answer and reasons to see if that's the best response to give back to you. And as the intelligence of the model increases, there's a higher demand for compute. And so this is where the, the AI factory, especially AI factory in the cloud where you could scale out as your AI projects get bigger, um, you, you could scale that very easily. And I, I think everyone is when they think of Nvidia, they think about um infrastructure and Nvidia GPUs. It's actually the full stack that um really makes, makes all this happen. It's the, the software layer on up. I'll show you in the next slide, but I want to talk about, um, we're integrated with AWS in our partnership so from, um, CUA libraries, if you're familiar with those, um, having those, um. Available in AWS, but it's also the, the full platforms that we have available in AWS like in the, the marketplace. So anywhere from DJ Hub, which I'll talk about in the next slide, Nvidia AI Enterprise, which is our enterprise, um, AI suite of software, um, Omniverse, if you've heard of that, um, uh, we have Nvidia NIM which are inference microservices. Um, and there's other sessions that Nvidia has here that will, uh, go, go a little bit deeper into that. And then not only that, but, um, all our libraries, our software, they're integrated into various, uh, AWS services, so you could consume them throughout, um, uh, AWS and kind of all this is built upon the infrastructure that you know and you're familiar with, which are the Nvidia GPUs and networking. OK, so, um, DJ Cloud, what, what we've done with DJX Cloud is, um. Nvidia looked at what, what are, what's, what's the kind of um AI factory or infrastructure that Nvidia needs to accomplish our own AI goals and so we, we reached out to, um, you know, one of our biggest partners like AWS to say. Let's co-engineer this because Nvidia had some requirements. AW AWS has technology. How do we co-engineer this and marry the best of the technologies together? So we start with the, the infrastructure. Um, if you look at GB 200's, uh, the latest, I know, uh, B300 was, um, announced, um, not too long ago. So, so keep an eye out, um. On what's available and what may be coming next, but there's um high performance networking and also luster storage. So that's at the infrastructure layer. And then we optimize EKS on DJX cloud. So from an infrastructure management standpoint that's optimized as well. We take it to another level and we've included Run AI. Has anyone heard about Run AI? GPU orchestration. So the idea here is, um, you, you have high performance GPUs that, you know, you, you're, you're paying some money for and you wanna make sure that those GPUs are fully optimized. You're getting the, the most out of what you're paying for, um, so R AO will really help you, um, use the GPUs effectively. I know Southwick has a testimony on that, so it'll be exciting to hear from him on how he's used Run AI and enterprise grade software. So a lot of the AI software you may be using is open source, but there may become a time where you're going to roll out your AI application to production. So you want something that's enterprise grade that's. Supported has um very set like production branching and releases and so we we offer that with DJX cloud and then kind of lastly is really access to Nvidia, AI and cloud experts. So um in this DGX cloud service we provide technical account managers and uh 24/7 enterprise support, um, and then all, all this translates into is that. You have a platform that provides the best of Nvidia AI in an optimized full stack that's on co-engineered infrastructure. OK, and so now that you have the background on where Nvidia sees the market is, what DGX Cloud is, um, I want to introduce our spotlight panel. So, um, I'll let you guys introduce yourself and maybe we start with who are you, who's your company, and what do you do for them. So I'll start with Sethite. OK, sounds good. Um, yeah, that's me up on the screen on top of Mount Whitney, uh, but, um, how many of you have got, how, how many of you have heard of ServiceNow, Quick show fans? OK, everybody knows, um, so I am a principal scientist at ServiceNow. I lead the Foundation Model lab, um, and my team, um, is responsible for the mid-training and the post-training of models. Um, we try to train like, or actually we train. Um, really, uh, small, uh, language models that are on par with most of the frontier models, and our goal is to be able to service, uh, all of, uh, the workloads that we see on a day to day basis, um, yeah, so I, I think that's, uh, that's what I do at a high level. Hi everybody, my name's Jamie. I'm going to ask the same question and expect fewer hands. Who's heard of SLB? Well, there's some at least, that's good. So we're a technology company in the energy industry. We're one of the world's largest oil and gas service companies, but we also have a substantial new energy business as well. So we deal with the full spectrum of the energy business, everything from producing oil and gas globally. We operate in over 120 countries, as well as helping people with their new energy needs, and our goal really is to Enable people to produce energy, the growing energy demand of the planet, but safely and cleanly over time. Now our business is divided up into different divisions. Some of those divisions are heavy industrial divisions doing things like drilling wells, managing production, but we also have a digital division, which is where I work, where we produce technology. That can help us and our customers, who are typically the large energy companies, plan, produce models, simulations, as well as operate responsibly and very sort of efficiently. Most of our business is science and engineering, so it's a heavily vertical business, so everybody, you know, a lot of people in our business. You know, are very specialist scientists or engineers in the different divisions, they might have even gone to different departments at universities, and so ours is not really a very transactional business, it's a sort of high science business and so we're using AI to to try and augment the workforce that does this very important work, especially as perhaps there's less people coming into the industry these days. My role as the business line director is to provide our internal data and AI platform. We use that to power our own turnkey applications that we sell to our customers, and we also sell the platform itself directly to our customers to help them do their business. Energy is really in the spotlight, especially with building out all these data centers and AI factories. So yeah, yeah, sometimes they say you know to do AI for energy, which is what we need, we have to have energy for AI. It's a kind of bit of a hopefully a virtuous circle, but it's kind of interesting that many, many of our customers are really in the business of providing energy for AI as well, so it's become an increasingly important concern. All right, so, um. What, what are your goals in developing AI? So you're here, you're using DJX Cloud, but um, before we get into the DJX cloud part, what, like, um, before you needed something like DJX Cloud. What, like what were your goals? Like what, what did you set forth to say, OK, this is what I need to do this is the AI we're developing and because of that then you had to look for the infrastructure so could you just share a little bit of, you know, what, what your goals for developing AI? Yeah, sounds good. um, so I guess everybody's familiar with ServiceNow, but, um, just to recap a little bit, ServiceNow is basically an enterprise company that hosts, uh, most of your. Maybe ITSM ITO ITA CSM HR legal compliance, whatever kind of data. There's tons of processes that perhaps nobody wants to do these days. Everybody's trying to automate it. Maybe 1520 years ago, you know, the trend was that, hey, let's code up some workflows and get it working and then. With traditional machine learning, it kind of changed to like smarter workflows. So at this point it's like gentech AI applications that are running a lot of these use cases, um, a really cool way to think about it, so. So Anthropic introduced this concept about MCP, right? So you just have like this huge list of tools and functions that you can call, and if a person were to sit there calling the right tools and functions in the right order, you could accomplish just about any task. The challenge with models is really twofold. One is will they be able to accomplish the task, and the second one is the cost. So if you try to use a frontier model for this, maybe GPT 5.1 or maybe GPT 6 whenever it comes out, I'm sure it's going to do a really great job, but then it's not really practical because if you start using You know, the most expensive model for every single use case for every single employee like I don't know I'm here in Vegas trying to book a whole trip and then I just go ask the system to do this for me it's it's going to start costing a lot more than actually having people do this job, so you need to really build efficient agentic systems at the same time make sure the models are efficient. So the goal of at least. My team is to build really powerful foundation models that can power most of the use cases across the Service now platform um and so that the customers can enjoy the same level of performance as a frontier model but not really be. Uh, bogged down by the costs, um, and what we try to do is inspired by the use cases, design, some custom post training recipes, but at, at a high level our model is not really any different from a deep seek or a Queen or any other model. We also open source most of our models so that anybody can use it, um, but yeah, that's, that's at a high level what we're trying to accomplish and DGX and AWS makes it a whole lot easier. Maybe we can get into that. And Jamie, how are you guys? Like what were your goals or what are your goals? Yeah, yeah, our business is kind of interesting. How it works is that we very expensively acquire vast amounts of data, and we'll show you later on what some of that data looks like seismic data, geophysical data. So we conduct surveys in the field to explore for data. And seismic is where we shoot very large geographic areas of data to try and characterize what's underground because if we want to lift oil and gas or if we want to sequester carbon, we need to understand what's going on underground. We can't do that, so we have to build models based on very large volumes of data. Similarly, we sometimes want to know a bit more specific about a particular area, and so we'll drill a well bore. And get petrophysical data about the actual structure of the layer cake, the formations that sit underground. And with that information, geologists and geophysicists can build models, if you like, architectural drawings of what's going on underground, and there's a lot of uncertainty in those models. And so for maybe 30 or 40 years we've been using various techniques from complex deterministic physics and mathematics to be able to predict the structure. Of the subsurface and its behavior and simulate the flow of fluids in the porous media that is the rocks, but over time we've been using more data-driven techniques. We've been using specific models, machine learning and things like that and obviously in the last few years we've seen there's a great opportunity. To use more general purpose foundational models, but we're not really in the business of just going from text to text, like coding or even for process automation. We need to be able to synthesize and generate um responses, synthetic responses from from the subsurface. So we use foundation models for seismic and time series data. And for petrophysical data that would allow us to predict scientific responses as if we'd acquired the data directly. So we call these the main foundation models, and that's what we've been using the technology for and ultimately we expect to combine those with the other forms of data-driven modeling. And physics-based modeling and then the orchestration that you're talking about in the agentic world to be able to provide assistance to geologists, geoscientists, engineers. They're a bit like the coding assistance that you would get in your regular world, but we've got to bring all of these pieces together for it to make sense. So kind of two key themes, you know, building models, agentic AI. How many of you in the audience are building models? Like, are you training fine tuning models? OK, cool. And then, um, how many are creating AI agents? Oh, quite a few. OK, cool, that's awesome. And so, um, you know, we, we talked about DJX Cloud as kind of like this, this turnkey, um, you know, full stack platform. How did, um, DJX Cloud on AWS help you achieve those goals that you set out to do that you guys just, you know, described? Yeah, makes sense. Um, I'll, I'll maybe like give a high level first and then we can go into the details more, um. I think prior, prior to us using DGX cloud on AWS we obviously bought a bunch of hardware from Nvidia and then uh installed it in-house, uh, had our own orchestration frameworks, uh, job management, etc. um, I think, I mean it was great, uh, but obviously like maintaining extremely large clusters is a full-time job. Um, and we decided to like switch to something that DGX and AWS offers just because it's like maybe more reliable and also I, I think if, if you buy hardware you're stuck to using it for a while, right? So we can keep switching out newer hardware when it, when it's available, um. And on top of this, I think, I think one of your older slides captured it perfectly. The cluster that we are using right now comes with luster, comes with runny eye, slide just so yeah, so it comes with luster, comes with runny eye. Obviously networking is super important because we don't want to be wasting any time at all, um, so I think, I think these are super important. Uh, one reason at least why we really, really like Run AI to the point where we don't want to use a cluster without it is because um. We want to maximize the cluster usage as much as possible and Earlier we used to like come up with custom job schedulers that try to like use the compute whenever there's downtime. I think this is made super easy with RunnyI because of the way we can prioritize jobs and stuff like that. So we're able to get almost like 100% utilization either with training models or synthesizing data or evaluating, um, so I think, I think, uh, along with this, obviously. We've, we've had like over the last year or so close to like zero downtime, like maybe a few hours on one day when you guys were upgrading the cluster, but, but I, I think, I think that's what is really important because you can't afford to lose any time, yeah, and then um I can't remember if I mentioned, but if you wanna use R AI so all of this we made available through AWS you all have to use this specifically in DJX cloud. So all of this is available through the AWS marketplace so you could kind of mix and match like what you know the stack but know that everything's really optimized um together um so I just kind of summarized it in in this slide what Sewick just um talked about what he just shared but um turning to you Jamie, how, how did, how did you guys use. You know, Nvidia, DJX Cloud, and AWS to accomplish your goals that you just described to us. Yes, yes, so we've got a long relationship and we've often had quite a lot of in-house infrastructure for running workflows for simulation and seismic processing. So we kind of start off with having some infrastructure and when we were building and training these models, we had some A100 infrastructure that we ran locally that kind of got us off the ground, I suppose. But then the world's moving very quickly. And we've just done two major product releases, generative AI and Energene Assistant, in the last 2 years. And so given how quickly the world was moving, we decided to leverage our relationship with AWS and with Nvidia to use DGX Cloud to bring stuff to market much quicker than we would have done if we were just using our own infrastructure. So we were really able to accelerate the development of these models, these domain specific models, by using this turnkey stack where everything worked beautifully out of the box. The team was able to work on what mattered to them, and then we were able to depend on. The performance, quality and support that we got and the ability to not have to purchase new hardware and everything else using the elastic compute capability of AWS along with that beautifully optimized stack to really let us focus on produce. These domain models and we were able to make great progress and we were able to partner. Cloud was able to be optimized through the support that we had to get increases in throughput from the same hardware as we move from a 2D model to a 2.5D to a 3D model for the seismic Foundation model. So really it's been a brilliant support for us as we're trying to bring products to market quickly. And not having to worry about the infrastructure and being able to tap into the AWS and DGX cloud. OK, great, um. Southwick, you, you were talking about your, you know, the, the Aprio models, um. How, how do the models, like the, the models that you're building, um, tell us a little bit more about Ariel, um, you know, family of models and how it plays into Service ServiceNow's overall strategy and kind of portfolio offering. Yeah, um, so we've, we've been training models for about 2, 2.5 years now. Um, and we decided to open source all the models that we train starting, uh, this year, Jan, uh, just because it, it brings more awareness about what we're doing and also at the same time the model by itself can't do much without all the uh agentic orchestrators and the tools and the data and whatnot, right? So, um, and. This year we've done 3 model releases so far. There was a 5 billion parameter model that we trained, and then we trained a 15 billion parameter reasoning model, and then we also upgraded the same 15 billion parameter reasoning model, which is like a multimodal model. It consumes images, text, and outputs text. And our goal here is obviously not to compete with Frontier Labs. It's it's really about how can we build frontier level reasoning performance on a single GPU scale, right, because if I can have a very small model that's able to do like the most complex tasks, it means that I can service a large amount of workloads without spending a lot. And this will really allow us, allow our customers to like pick and choose. Like if they have the most complex use cases they can reserve all of the whatever open AI cloud credits for those kind of tasks and then just be able to default to like our models for most of the other use cases, um, um, so if, if we look at a couple of slides further down, um, yeah, so this is uh from a model that we released like maybe a couple of months ago, uh, maybe 1.5 months ago. It's, it's our model is highlighted in the red box. If you can see, it's kind of on par with Deep Sea Car 1 and you know, Gemini Flash, and for you know just talking about how big our model is, we're at 15 billion parameter scale. Deep Sea car one is about 600 billion, and then Queen is about, you know, you see it's about like 235 billion. I don't know how big Gemini Flash is honestly, but. Our model is like kind of on par with a model that's like 30 times larger than its size, and the open source community also seems to love it because we got like close to 100,000 downloads. People are using it, and we bring the same model into production so people can actually leverage it for whatever use cases they might have. Awesome. Right, um. Jamie, you, you mentioned, um, you know, as, as we're prepping. There's some unique challenges that that you guys faced um as you were, you know, training and developing your AI um do you wanna share a little bit about those challenges because some, some folks in the audience might be facing similar one so maybe if we go forward to the slide that has some of the seismic on there. So it's very important for us, as I said earlier on, that our What we're doing here is introducing generative AI technology into a portfolio of existing technology, a bit like the tools that you have in the MCP tools that you have in service now. So we will have physics-based simulators, algorithms, we'll have machine learning type models for doing things like automating fault and horizon interpretation that can take months and months and months. And so we saw an opportunity here to Be able to build what's effectively a multi-modal model that's specific for our industry, that would really accelerate the work of the people. So you know we do another example where we have data that we want to bring into one of our traditional applications, but there are some gaps in the data because the acquisition wasn't clean or the processing wasn't clean, and we can cool down on this model what would normally have taken. Weeks or days or even months sometimes to reconstruct the missing data with a human we can actually just call on an agent now to reconstruct that missing data within certain constraints and really accelerate a workflow and give a really complete picture of the earth and as you can see here. This is not just generating text. It's text to images, images to text, and ultimately to models. So really this technology for us is very non-transactional. It's an almost creative process that we're trying to support here amongst scientists and really importantly for us this has to sit in their portfolio of tools and we have to. We have to produce something that's good enough for them to trust, with all of their PhD years of experience, this tool has to sit along with them and say, I can use this assistant because I know it's been trained on the data. I know I can trust the people that have trained it. And so we started off training it with public data and that produced a certain level of capability, and now we've started augmenting that with proprietary data from our own acquisitions. And then I think one of the things that's really interesting from us is that we'll be working with our customers for that trust thing. They need to train it and fine tune it with their own data as well. So that's a kind of unique thing in our business is that every one of our major customers will either take the off the shelf model, but more likely. They'll combine that with their own data as well, so they'll have their own fine-tuned model so it represents the basins and the geological settings that they operate within to build that trust that it will provide accurate predictions. So, um, so in both your cases then you are you're offering these models for your customers to kind of fine tune it and kind of include some of their data. To add the intelligence to it, so are you seeing more customers doing fine tuning or doing like a rag? Like, uh, does everyone understand what rag is? What are, are you familiar what rag is retrieval augmented generation. OK, it, it's basically taking a model and then um going out to retrieve additional data from um like a vector database to supplement the knowledge of the foundation model. So are you guys seeing more like. Customers taking your model and then fine tuning it or doing like a rag, so, so for us, we're not in the business, as you said, of building general purpose foundational frontier models. So when it comes to Interacting with the customer's corporate memory, they don't take chat GPT and fine tune it by sticking their documents in it, they'll use a rag type approach for that. So the data and AI platform that we produce will often go back through. Decades of old documents and report from the 50s, 60s, sometimes even back to the 19th century, that shows the early days of oil field exploration. It's kind of funny if you read these documents, right? Because the language has changed a lot over 100 years. And then what we'll do is, when we ingest. That data we have to use fancy techniques for the of the old data to capture the semantic meaning of those old documents. We'll create pipelines that ingest that into the environment and then automatically vectorize it so that we have out of the box rag going along. But then in the case of these specialty domains, petrophysics, geophysics, seismic data, then we will actually fine tune it. So text, reports, documents, rag, proper scientific output models, we'll use the domain foundation models to create an edge. I think, I think it's a little different for us. If you roll back a couple of slides, I think you had a. Nice illustration, yeah, of how the entire stack looks like um so you have like a whole bunch of documents you have like policies, triggers, tools, uh, whatever else, and then. The large language model is just a small part of the whole, you know, uh, whole thing, um, so what ends up happening is for the most part when people start using our system. It works because I mean how different can I don't know like one customer's flight ordering you know like flight booking versus like whatever food ordering system be from another customer, right? So the reason why it gets complex between customers to customer is because they have different ways of implementing their own policies and sometimes the model just does not understand what it's doing. So ServiceNow actually. Allows people to customize the agentic, uh, you know, the gente scaffolding. You can have custom policies defined. You can have guardrails, and for the most part we see that this works and. I mean if, if there are enough customers asking for custom models maybe that's gonna be a thing but uh I, I think it's, it's for the most part not necessary. OK, great, um then like where do you guys go from here? So how will you move forward with your projects after using DJX cloud on AWS? So kind of what's next? Like you have your model, um. How are you gonna kind of deploy? Are you gonna come back to DJX? How are you gonna use AWS infrastructure service, a mix of on-prem? Like what's next for you guys? Yeah, I, I think from what I can definitely say. Um, I mean, you think about Nvidia and then it's like, you know, you guys are a pioneer in AI. Think about AWS, it's, you know, arguably one of the best. Clouds in the world, so you don't want to let go of both. You want to have both of them combined in the right manner, uh, so to say, but, but I think, I think, you know, DGX has been super useful and at the same time DJX has been hosted on AWS and we've seen that um it's uh it's super reliable, scalable, quick, um, and, and I think, I think we'd we'd hope to like continue with the same thing. But, but at the same time, like you said, uh, Rania is on AWS, um, you know, Luster is from AWS, um, there is existing infrastructure that would obviously. Um, encourage us to explore that option too, um, I think, you know, uh, ideally we want both, but yeah, yeah, awesome. I think for us DGX Cloud and our partnership with AWS and Nvidia was fantastic for that kind of early stage product development. It's given us the velocity that we needed. I think when it comes to us scaling out globally, we'll use a variety of techniques. Some customers are so sensitive about their proprietary data that they're going to want to deal with that on-prem, so. We do that. Similarly, we offer a lot of services as a to a global market, so we'll be able to leverage the commodity infrastructure and commit to the services that you get from AWS or other providers to be able to deliver solutions globally. But I think we always remind ourselves that this is really we're really at the early stages of this revolution. We're out there talking to customers, discovering what they need, so something like DGX Cloud is super important so that we can respond to what we learn from the customers in the market and then as we effectively Establish a pattern and we want to develop and mature that, then we can move towards more commodity infrastructure as well, but I think it's going to be very dynamic over the next few years to understand where the world's going with all of this, especially in our industry, because they're not just looking at it for process automation. In our industry, people are saying that we want to use agents to retire uncertainty, to explore more options than they would have been able to do. With their previous resourcing levels they want to, they don't just want to do rag on their existing corporate documents because then their future organization would always behave like their past organization. They want to kind of have a diversity of sources coming in. So I think it's going to be. A really interesting period and we're going to need all kinds of infrastructure support, but definitely with the scale that you get from AWS and the elastic capability and the kind of precision and beautifully integrated stack that you get from Nvidia DGX. Yeah, it's, it's amazing because you're saying how like your industry started like in the 1900s. It seems like AI is really like making it go fast now, even though it's a market. It only took us 100 years to get there. We're celebrating, we're celebrating 100, 100 years. 2026 is SLB's 100th anniversary, so yeah, yeah, and I, I think, um, you know, something we, we touch upon is the Nvidia like full stack ecosystem whether you're using it, you know, on-prem in the cloud, it's, it's really portable so you have that flexibility and again that whole ecosystem is available on AWS through the marketplace embedded their services so um that portability adds a lot of value and flexibility to customers um. We have um some time left. I don't know if anyone has questions, but it's, this isn't the most conducive way of asking questions. Does anyone have a question you could just ask? I could repeat it so everyone could hear it. If not, there's, there's opportunity to come to talk to us after and um I also, oh, did you want to cover this one real quick, um Jamie? I mean, I think this is really just describing our journey with DGX and how it was able to match our needs as we innovated over a two year period and as we moved from rudimentary. Models and moved up from a sort of 2D model to a 3D model to be able to build, and again we're really just at the beginning of this journey, right, but now we're starting to get foundation models that produce really plausible generative results that we can incorporate into workflows alongside our existing machine learning based approaches and our existing physics. And strict mathematical based approaches as well. So these are going to become a very important part of our toolkit going forward and you can see that we've been through quite a steep learning curve and AWS and Nvidia have very much supported our learning curve, the velocity that we needed. Great thank you so much for attending today. Really appreciate you. Enjoy the show.