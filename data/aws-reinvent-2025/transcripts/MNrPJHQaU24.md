---
video_id: MNrPJHQaU24
video_url: https://www.youtube.com/watch?v=MNrPJHQaU24
is_generated: False
is_translatable: True
summary: |
  Mark and Reluka delivered a code-first walkthrough of Aurora DSQL from a developer’s perspective, framing the session as a live baking show while they built a banking app and stressed it in real time. They introduced DSQL as a distributed Postgres implementation meant to blend relational features with DynamoDB-style horizontal scale and Lambda-style zero administration. A cluster can be created in seconds with single- or multi-Region options, default encryption, resource policies, and a web-based query editor that connects over WebSockets; CloudShell support makes the same Postgres connection string available without custom tooling. They emphasized that AWS operators have no path into customer data and that customers can govern access through public endpoints or VPC endpoints with policy controls.
  The starter kit uses CDK to define a Lambda function and DSQL cluster, wiring the cluster endpoint and credentials in through environment variables. Instead of passwords, Lambda generates short-lived authentication tokens via the AWS SDK, similar to presigned S3 URLs. IAM roles are mapped to database roles through a sys metadata table so teams can express least-privilege access with native Postgres semantics, and the presenters fielded questions about dialect flexibility, MySQL support, and the new resource policy capability. They also reminded the audience that connections themselves are free aside from the reads performed when opening them.
  Building the banking app highlighted why transactions matter. A naive transfer can double-spend under concurrency, so they leaned on DSQL’s strong snapshot isolation (slightly stronger than repeatable read, with write skew as the main remaining anomaly) and wrote optimistic-concurrency retry logic in Lambda. Connection pooling was tuned to reuse tokens, reacquire connections after failures, and collect telemetry on retries and durations; bugs like leaked connections were fixed live to show how to keep pools healthy after node-level events. Audience questions covered the trade-offs versus more pessimistic isolation and the need to design schemas that avoid hot write keys.
  They extended the schema with a transaction log and showed how DSQL keeps maintenance online. The create index async command builds indexes without blocking writes, returns a job ID, and is tracked in system tables alongside automatic analyze jobs; explain analyze outputs helped them reason about when the planner chose full scans versus the new index. Including created_at in the index removed the need for extra sorting when ordering by time, and they advised using system statistics to understand planner choices rather than assuming an index will always be used.
  To prove elasticity, they generated a million accounts, fanned out random transfers from multiple hosts, and watched optimistic-concurrency conflicts drop once the keyspace was large enough. A sustained load reached roughly 10,000 TPS without any capacity planning, and they highlighted that DSQL scales connections and throughput automatically. In the console they reviewed DPU (distributed processing unit) and compute metrics, ran a CloudWatch-driven calculator, and showed that a bursty high-TPS run cost only a few dollars with ongoing charges limited to storage once traffic stops. The session closed with roadmap notes (foreign keys and other Postgres features) and the positioning of DSQL as a pay-per-use, Postgres-compatible database that pairs naturally with serverless stacks like Lambda.
keywords: Aurora DSQL, serverless Postgres, Lambda integration, optimistic concurrency, pay-per-use database
---

Hello, and welcome everyone. Good morning. Welcome to Reinvent. Uh, my name is Mark. I'm an engineer. I work on DSQL and I'm joined by my colleague, Reluka. She's an uh database engineer. She works on DSQL with me. And this is a code talk. This is a developer's perspective. We're gonna be building a very simple application in DSQL this morning using Lambda. And I'm gonna show you how to connect to DSQL, uh, how to use the CDK to manage your infrastructure declaratively. We're gonna be writing some banking applications. It's very simple application that, you know, it turns out I owe Reluka money, so I need to settle my debts with her. And, uh, and we're gonna be showing you, uh, how to, how to scale that application up, how to deal with concurrency, what happens when there's conflicts in the application. Um, this is really meant to be a bit of a baking show. And so they're gonna be moments where, where, you know, the pie is in the oven. We, we're gonna be waiting for it. Um, so feel free to ask us questions. Uh, yeah, let's get into it. All right. I'm gonna take over Mark's computer. It's part of his paying his debts to me. So quick show of hands, who here has used DSQL? OK, that's about what I expected. So DSQL was announced at Reinvent this time last year in preview, uh, almost exactly 1 year ago, and became generally, generally available earlier this year. Uh, we're about 6 months old now. Um, So what is DSQL? DSQL is a distributed implementation of Postgress. Um, and so for the longest time, customers have been asking us, hey, we really like relational databases, we like being able to do joins, manage our schema, run complex queries. But we also like systems like Dynamo DB where there's no single point of failure, where the system scales horizontally. And we also like servers like services like Lambda, where I don't have to think about infrastructure or scaling or patching or security, or anything like that. So these three things, I want them in one package. And that's what we set out to do. And that's what DSQL is. Uh DSQL has 4 pillars. You can, you can find these on our website, virtually limited scale, high availability, no infrastructure management, and ease of use. OK, let's hop on over to the DSQL console and create a cluster. This is gonna go by really fast. I challenge you all to count to 10, 54321. I was late. I can't count to 10. There we go. OK, great. So um that's what it looks like to create a DSQL cluster. It's pretty fast. We've just rolled out fast cluster creation, um, about 2 weeks ago. Um, let's, let's do that in slow mo. So Luca, take us back on over. You'll notice that there's, there's actually two options there in, in the dropdown, there's single region or multi-region. For today's talk, we're gonna be focusing on single region clusters. Um There is no scroll bar on the screen. This is something that I'm just like super excited about, because there are, uh, you know, I've made a ton of databases in my career. And every time I make one, I find it just like really awful, because I have to make all these decisions ahead of time. It's really hard to decide what instance type do I want, how many replicas, what subnet should I be in, what security. It's just this endless list of choices, and, and they really matter. Um, and so which one of those is gonna be the one that causes an outage and, and makes me wake up in the middle of the night. And in DC recall, you don't have to worry about any of that stuff. Uh, you can tag your cluster, um, if you want, uh, if you wanna change the tags later, go ahead and do that. Clusters are always encrypted by default. If you want to change to a different key, you can do that here. If you wanna change your mind later and move to a different key, you can do that later. You can toggle deletion protection. And we've recently rolled out some advanced security options uh for some of, for some of our customers. But that's really all there is to it. Um, let, let's, uh, let's connect to DSQL clusterlico. Right on. Let's choose this one. Have you done anything with it? Nope, not yet. OK. So we have two ways to connect the DSQL cluster. Uh, this is the query editor. Um, we, we actually only shipped this about 2 weeks ago. And this is just a Postgress editor running in the web browser, right? What's really cool about this editor is that your browser is connecting to DSQL directly. Um, DSQL supports WebSockets. And so your, your browser's just connecting over and speaking the postres protocol right there. Uh, we also have a, a, uh, a, a, a cloudshell integration. So every, um, Every, uh, page in the AWS console has a little shell at the bottom left. I don't know if you, if you've all seen that. Um, but you can also go to any DSQL cluster and click connect with cloud shell. It will just fill in, uh, the prompt with everything that's needed to connect. OK, so that's getting started. We're gonna swap over here. And we're gonna, we're gonna start to build an application. OK. So before we do that, um, I wanna orientate you. So, uh, I've built a little starter kit for this code talk. Um, it's on GitHub, if you would like to check it out. Uh, uh, we've got a, we've got a link we can share it with you after the talk. And, uh, this is a little starter kit, and it's got two key folders. The first one is the CDK folder, and the second one is the lambda folder. I'm actually gonna start with the lambda one because it's really simple. Um, here we have a function. Actually, a quick show of hands. Who here has built, built a lambda function before? OK, almost all of you. Great. OK. So I, I won't go into too much detail here. We have a request handler. This request handler is taking an event. Um, the input is gonna be a request. It's one of these request interfaces, and this, and then the response. And so we can call this function and say, hello, whatever we want, and then it's gonna return uh a classic greeting to us. Meanwhile, in the CDK folder, we have um We have our lambda function being bundled up. And every time we deploy the stack, we're just gonna take the latest code that we've been working on, send it over to lambda, and then we can invoke it. Uh, the second thing that's happening in the starter kit is uh DSQL cluster. And this cluster you'll see, it just kind of follows what you saw in the UI, right? So we have deletion protection, we have our tags. And if you're using TypeScript, you can get this like nice auto completion to say, uh, what else can I configure here. Now I've gone ahead and created one of these already. And when you run it, this is what you get. You're gonna get two outputs. Uh, these outputs are defined here at the bottom of, of the, of the CDK stack. Let's say what our cluster endpoint is, and what the security, the, the, the IAM role for our function is. Um. DSQL DSQL, uh, clusters have their own endpoint. Um, actually, behind the scenes is a big shared endpoint, which is really important because that's gonna be part of how we provide automatic capacity management. But we present a, a unique endpoint to your cluster. This is just basically a UU ID for your cluster, uh, which makes it just really easy to connecting with postsros clients, because they expect a host name, right. So that's, that's why we do it like this. Um, and what I wanna show you quickly is what it looks like to invoke our lambda function. Um, so here we say, uh, AWS lambda invoke with the CLI is our function name. And we're gonna pass in name equals reinvent and write the response to that file. And then we can take a look at that file, color reinvent. OK, great. Um, What I wanna do now is to show you what it looks like to connect. So we need to export a coup couple of variables to make the posttress shell work. The first one is the PG user. I'm gonna set that to admin. The next one is the PG database, which is going to be Postgress. Um, then we have PG host. And if we try and connect now, we're gonna get uh a prompt for a password, right? So where does my password come from? And I'm gonna get one from the AWS command line tool. That's my password. Feel free to copy it down because you'll notice that it's expired already. Um. So The way this works is through a technique that looks a lot like S3 pre-signed URLs, if you're familiar with those. So the AWS SDK what it usually is doing is making HTTP requests to a server saying, hey, I would like to download this file from my S3 bucket. And it's building an HTTP request that has headers, that has a body. And then the SDK is gonna go and sign that request, right? So it's gonna go and run these cryptographic hash functions of your headers and, and over the request body. And it's gonna put a signature, right? And that signature is gonna give us a tamperproof uh property. So that if anybody intercepted your request, they, um, they can't modify it. Uh, and it's also gonna give us expiry. And so, um, if somebody managed to intercept this request, they would have a time-bound window in which to exploit it. And usually you can't do that because your request is, uh, your, your request has been sent over a TLS connection. Uh, something like S3 pre-signed URLs works a little bit differently. Because what you're saying is, I, who have access to my S3 bucket, I'm gonna make this request. But before I send the request, I'm just gonna take that signed URL. And then I'm gonna intentionally give it to somebody else, right? Um, That's pretty much what's going on here. You'll see that this looks like a URL. Here's the host, it's our cluster. We have an action called DB connect admin, and then we have that signature. And this is gonna take the place of our password. So rather than, um, rather than giving a very, very short-lived password, I can just export this into an environment variable. And now we should be in. Oops, uh. Did I get the host wrong? Ah, I remember what I didn't do. I haven't had coffee this morning. Uh, demo. Did you send the region? OK, there we're in. Um, so I'm using a demo account and I forgot to switch the profile of the shell. But we're in now and we can uh just run kind of standard, uh, Postress commands. And what I'm gonna do is exactly the same thing in our lambda function. So we're gonna go over to this lambda function and you'll notice that there's a file here called DB.TS. And this is essentially the node posttress driver. Um, I've just gone to their Read me and I've copy pasted everything into this file. Um, There's one change, which is this password. And so the password is gonna come from calling a function. And that function is essentially the same thing that we just did on the command line, right? This is, hey, use the SDK to generate one of these tokens. And you'll notice that the cluster endpoint is coming in through an environment variable. So let's just wire this up quickly. We're gonna say, my cluster End point While you do that, we do take questions. So, wave your hand if you want me to grab the mic and get a question. Yeah, we have a question. How about that? All right, I will, thank you, um, I, I, you mentioned that, uh. Uh, you don't need security groups and, and things like that. Does that mean DSQL runs as just like a general service, kind of like an S3? You don't have it in your VPC or something like that. So we do have public endpoints and we do have the option to set up a VPC endpoint if you need that, but it would be through a VPC endpoint. Yes, correctly, another question. Uh, is, is there a, a my sequel flavor of, uh, re-sequel? No, not yet, not yet. But we are a very um Not very specific in terms of SQL dialect. So if you have uh an application that's pretty much sticking to the general dialect, you should be able to use uh DSQL as well. OK, one question. Uh, just, just to touch on the VPC endpoint part, um, I noticed sometimes in some newer services in AWS, even if you do have VPC endpoints, you don't necessarily always support endpoint policies. Uh, can do you support endpoint policies for DSQL? We just added resource based policies, yes, OK, yeah, that's really important for. What's the use case? Is it we don't want. We don't want the potential of, you know, people getting access to our VPC or it, it's security requires it on our side. I can't remember the exact details, but there's always a sticking point for us. Got it. Thank you. Yeah, there's, uh, we've actually got a really detailed blog post on how to do this. Um, if you want to chat to me afterwards, then I can give you the link. Um, but it goes through like there are 5 different ways that you can lock this down. Um, and as Reluka just mentioned, and you saw on the cluster creation pages that advanced TickBox, you can put in, you can put in just any kind of standard AWS policy language stuff there. So you can say, restrict to these IP addresses, restrict to these VPCs, uh, anything like that. OK. Um, uh, I need to quickly update you on what I, what I did in this. Um, so we've changed our lambda function to say, give me a, uh, give me one of these pools, uh, database pools, and then we're gonna run a query. And so the idea is that if we're still able to see our greeting, then we've connected to DSQL, right? Um, and I wanna quickly draw your attention to this function here. This is gonna make sure that we're gonna reuse our connections between lambda invocations, right? So the first time our lambda starts up, we're gonna have to initialize the pool. Um, this pool's gonna maintain 20 connections, up to 20 connections in the background, like our lambda function's gonna get one request at a time. So this is totally overkill. But you can do, you can do kind of whatever you wanna, whatever you wanna do here. Um, but every time a connection's opened, we're gonna go grab one of these authentication tokens. Now this is just a really, really fast operation. It takes something like 20 nanoseconds to run. Um, and if you're using a service like S3 or Dynamo, you're actually running this all the time on every request. With DSQL, you're just running this code once per connection. Um, OK, the second thing we've done here is we've just, uh, asked the SDK to wire in our endpoint into the lambda function. And then down here, we've, we've told the IAM that we want to allow this lambda function to connect to the database. Um, and if I do that and run my function, I'm expecting it to fail. And the reason I'm expecting it to fail is because we've not yet told the database to allow our functioning, right. So we've told, we've told Lambda, hey, um, the role that you have is allowed to connect to this DSQL cluster. But the DSQL cluster, um, is trying to see whether our, whether our application, which is connecting and as a user called my app is allowed to connect. And we haven't actually set that up yet. So let's do that quickly. Uh, we can do uh create role my app with login. And then I gotta do some copy pasting of this arn. Uh, to say that this AWS RN is allowed to be, uh, is allowed permission to grant this, to to use this role. And if we try to run that again, we should hopefully see success. OK. Um, so, so the way this works, uh, we can actually just take a look at this table, this table quickly. Um, You don't have to remember these names. I just use tab completion there. So there's a name space in the database called Sys that contains all our metadata, and you'll see that we've got a mapping that says, this IM role is allowed to become this PostScripts role. And the reason we went this direction in the DSQL design. is that it exposes the full power of post gross permissions. Right. So if you wanna say, here's a role that can only like read these tables in the schema, or a role that has read, write, or only to specific tables, you can do all of that with the standard post gross, uh, the standard post gross role system. The only thing you need to do is sort of bind which IM entities are allowed to become which roles. OK. Um, so I'm gonna get coding, uh, on our banking application. Um, but before we do that, I wanna quickly show you what the schema looks like. So we're gonna have a table called accounts. And, uh, accounts have an ID. That's just an integer, and it's a primary key. And they're gonna have a balance, which is an integer 2. And once we have this setup, we can start to do a balance transfer, right. So we can run a transaction where we have some money in one account, some money in another account, and then we can move it over. So I'm gonna do that the wrong way, and then we're gonna talk about why that's wrong. OK. Can I, can I start talking about transactions? We, you might not make it to the next session, but um, as a database geek I really love transactions and the reason I, I prefer to do, uh, database changes inside the transaction is because, uh, it allows me to specify all the, uh, changes to succeed or fail at once. So you're familiar everything that goes between a begin and then uh commit or rollback will succeed together or fail together. So that's the atomicity of a transaction. Yeah, so I just did it the wrong way, right? Because what happens if after running this statement, my laptop died, right? At this point, like what I'm intending to do is move some money between accounts. But any number of things that can go wrong, right? Like we may lose connection, we may lose connectivity, database may crash. Uh, but there also could be business logic errors, like maybe I don't have enough money. Right. What happens if I only have $10 to my name, and I, I, you know, unfortunately, I get into a little bit of debt. And so I'm trying to pay off all my debt at once. So I'm gonna go to Ruluka. I'm gonna go to somebody else, I'm gonna say, start a transaction, how much money is in my account? $10. Let's go and pay both debts off at the same time, right? And so, without any kind of consistency checks, uh, two transactions could start, they could both determine that I have enough money to pay. We could deduct money from both accounts, and then we're gonna end up with a bad outcome where I've sort of been able to spend $20 despite only having one. And so for these kinds of reasons, we wanna make sure that we're always using transactions. DSQL supports something that's very equivalent to Postgress's repeatable read isolation, except it's just one not stronger than that. It's a strong snapshot isolation mode. And for those of you who, who aren't deeply familiar with isolation modes, really the easiest way to understand them is through this concept of anomalies. And an anomaly is something that you can see in your transaction that doesn't make sense. Right? So like a good example of this one you've probably all run into is a phantom row. Where you run a, you run a select statement, you don't see a row, you run a select statement again, you see the row, you run it again, you don't see the row, right. Um, these kinds of things are happening because there's multiple transactions running on the database the whole time. And so the job of the database is to try and isolate our transaction as much as possible, to give us the illusion that we're the only user on the system. Because that's gonna allow us to write correct code. And because DSQL is slightly stronger than repeatable read, that means that there are fewer anomalies. In fact, DSQL only has one anomaly called RightSKU. And so if anybody would like to uh talk about RightSKU later in this talk, depending on how much time we have, we can do that. OK, so what I wanna do right now is, um, start to take this lambda function that we have and turn it into a little banking application. I'm just gonna add one thing to what Mark just mentioned, um, strong snapshot isolation, um, in, in the sequel means we're, um, we have this property of being strongly consistent so whatever you write, um, will be immediately seen by all the, um, other. Places you read from so you don't have to worry about routing your query to a specific reader or to a specific endpoint um because wherever you read from you're gonna immediately see uh what what has been written and that is a strong consistency as opposed to eventual consistency with some other database flavors would offer um so that's strong snapshot um. Isolation, um, and then there's two more things we we call transactions assets because they're atomic, consistent, insulated, and durable, um, and I'm gonna quickly touch on, uh, consistency while Mark is doing, uh, all the coding there, uh, that means you don't get, um, these, um. Phantom um money transfer where you've sent, uh, you see, uh, the money in one account you don't see it in the other, uh, everything stays consistent the database moves from one valid state to another in a consistent mode and then you have durability which means the database will recover in case of errors for you. So once you've acknowledged that a a right has been made to the database, you'll always continue to see that uh data persisted in the database. You, you don't have to worry about uh machine restarts or uh failures. All right, let's see, what are we doing? hm. I am doing ceremony. So this is kind of a little boring code, but I wanted to, you know, if we have time later on in this talk, we can maybe uh move to like a a bit of a a higher level of abstraction through an ORM. But I wanted to take the time to show you what it looks like to write these transactions at a very low level. So we have our, we sort of have a tri-catch pattern. And then within the tri-catch, we have a begin and commit, right? And that's gonna start our transaction boundary. And then if anything goes wrong, we are just gonna throw the error. Um, and we're gonna have to take care to return this connection to the pool, right? Otherwise, when we run the function again, we're gonna start to go from 20 available connections to 19 until we eventually have an outage. Um, and, uh, here's our pay state, here's our our pay query. Um, so we're gonna be taking money out of the account. And notice how I'm using, um, I'm using bind variables just to make sure that we don't have a, a case of bobby drop tables going on here. And, uh, this is the payee's gonna be receiving, receiving money. And we need to do a little bit of checks here. So, uh, what can go wrong? Think about that for a moment. Let me know if you need me to hand the mic to anyone. OK. So the thing is, um, Let's say we have, we're running two transactions, same transactions, we'redrawing the amount from the bank account. The bank account has a $100 balance. The transaction is also doing the same like it's withdrawing $100 because both, both the transactions running in parallel. How are you gonna handle that situation? That's an excellent question. Thank you, because it, it segues into what I wanted to talk to you next about, which is um. How DSQL handles uh concurrency control, so we, we do optimistic concurrency control which means when two transactions try to update the same row they will both be allowed to try and then the first transaction to issue the commit will win and the second transaction will have to uh retry and that retry logic needs to be embedded in the application code. And so the ideal design pattern is item potent so that transactions can just retry on failure um and um they don't need to check on embed any business logic so that's um optimistic and currency control and if you if you were to use the opposite which is pessimistic concurrency control this used to be my nightmare as an Oracle DBA. So you, you'd get, uh, all these, uh, sessions, all the transactions lining up, waiting for uh a R lock, um, I think it was called the weight event was. Uh, in Que transaction row log contention, something like that, and in pas grass it will be a lock relation. So what happens there is the first transaction, uh, trying to grab, uh, to update the row grabs a lock on that row and then the subsequent ones will just line up uh and say I want to update that row. I want to update that row, and, uh, eventually either the first session needs to be, uh, killed or it just finishes the doing the processing it needed to do. And then the whole thing uh uh unblocks and that particular scenario also requires an abort and retry logic except in the sequel you just get notified sooner and you need to retry. OK. That's fine. I can. So the transaction, uh, the first transaction, basically when it acquires a lock on the row, the row will be released only when the commit happens in this sequel. Pardon? In this code in the idea that they they the variables are. Mark, your variables are all, all wrong. Thank you. Uh, thank you so much, uh. You probably just saved us an hour. So the release is not gonna happen there's no lock. It's lock free. There is no lock in the sequel. OK, so that's why we can um also uh support higher uh transaction throughput because you don't have to wait for another transaction. There's no coordination there. Everybody who wants to update will just issue the update and then the first one to have the commit wins and then the other ones get, um, serialization error. It's, um, pause gross error code 42001. 0, and by the way, I have to tell you this, um. If you go to the AWS booth in the expo in the Venetian and uh they ask you the one unique thing you've learned from this session and you tell them about serialization error 40001, they will hand you a hoodie, an AWS hoodie and a sticker, a DSQL sticker. Um, do you have more questions? I have one more. Thank you. Yeah, we're actually gonna, we're actually gonna go look right at conflicts in a moment because as soon as we start to run this, uh, a bunch of times we're gonna see exactly your question in action. So let's see if we can invoke this. Mark, I have one more question. Can I take it? Oh yeah, let's take that. It's, it's actually not a question, a little bit of a contrarian comment if I may, um, and don't get me wrong, I, I love the sequel and I think this is a marvel of engineering and, and, and brings a lot of value. I was just wanna say that I'm not totally sold on that uh the optimistic locking approach used here is straight away better than classical pores because having to do item potency puts a burden on the. Developer that if you run on repeatable uh uh recommitted mode which is the usual for Postress and most applications are fine with that it's a bur an extra burden adds friction on the application developer side and having to have retries can also uh at point also hamper uh performance because you're doing retries versus just waiting a little bit for a lock to be acquired so just just this comment is not. Um, it's, it's something that makes sense and it's an acceptable compromise, but still a compromise. I don't think it's just straight out better. We, we call it a trade-off. It, it is what you need to obtain a higher throughput, right? So, so we're, we're, we're gonna actually dig into OCC in a minute. So your two points, I think there's, is it better than post gross? Is it ergonomic, and what our. Performance. We, let's make sure we cover all three of those. So our function works, uh, we, uh, we're returning, um, we're returning the balance using the PostScrits returning syntax. Um, and so if we run a bunch of times, we can see our money going down. Um, so I'm gonna quickly run this, uh, setup script, and then we can We have 1000 accounts. And then we can do Let's see if I remember where we are now. We're on tap chapter 2, I think so, maybe. OK, so this is gonna open up to 1000 requests in parallel. And it's gonna call that function. And every time it calls that function, it's gonna say, pick a random account, a random payer, a random payee, move some money around. And then this is gonna keep track of uh the errors we get back. And you'll see that we're getting change conflicts with another transaction, right? And so the reason this is important from a correctness point of view, is that we are running all of these checks here, like, does the payer exist? Does the payee exist? Do I have enough money? And because we have this concurrency going on, DSQL is detecting, and it's detecting here on line 43, that somebody else is amucked with the data that you've been working with, right? And so you need to try again. Um, and the way we, the way we do that is we're gonna handle this error code and just try again. So we'll say, while true. Um, and then I need to indent everything. And then in our catch block, this gets a uh a little funky in TypeScript because of error handling. Um, but what we're gonna say is, if the error is not a PG error, Something else has just gone wrong. Then we're gonna run this old code. Otherwise, we're gonna take a look at the error code. So this function here just lets us like get some type information out of the error. We can just run and continue. Um, And then we'll have a to do over here. Now, it's kind of important, um, I've, I've made this bug a bunch of times to make sure that we are grabbing a new connection out of the pool inside this loop over here. OK, um, And I'm gonna do a little bit more decoration here, um, just so we can, um, keep track of our retries. So we'll say retries is a number. Uh, that's a type of. And we'll say duration is a number 2. And then at the start here, we'll take the current time. And then down here, we will return the duration. We're gonna do that down here too for. A reason that will become apparent. Um, OK, and then we need to return retries too, right? Right, so we just have some telemetry around the OCC retries to your point. You, you can exactly quantify, um, what your hotspots are, how many times you have to retry if, if the application, uh, tends to have these hot deadlocks and so on, so. And uh if you do want, for example, not not to have all the sessions try retry it once, then you would add some uh exponential back off and retries and things like that but that in in this case I don't think that's gonna be required in the simple application. That is um hold on. I want everybody to hear what you say. I Justify me, um, absolutely, uh, I, I, I like the word that it's a trade-off. That's exactly what it is. That's why I just just wanna make sure that it's not presented as a, a best solution because again if I run, for example, re committed mode using regular postpers, I will not need to do this while through loop and check for the errors and handle the retries. I'm not saying this is better or worse. It's again a trade-off, um, but here you're forced versus not being forced and therefore it's a compromise, yeah. Thanks. Does anybody have any other questions? OK, so this um This code is actually still wrong. Um, and the reason is we need to handle our errors a little bit better over here. Um, but we'll see what happens when we run it. So adding this, adding this retry loop is work, for sure. Um, You know, I've gone a little bit above and beyond in terms of adding telemetry, because I want, I want you to see when we run this code, what the error rate is, right. Um, and it's gonna be really bad, right? And, and the reason it's really bad is that we only have 1000 accounts, right. Uh, we're running at 1000 requests per second. And so the chance of there being two concurrent requests the conflict is gonna be pretty high. Timeout exceeded, uh, I think I'm leaking connection somewhere. OK, give me a moment to debug. I'm happy to take more questions in the meantime. I saw that hand first. I'll go there first. Uh, thank you. Yeah, uh, I was just wondering what the problem was with, like you mentioned, you had to push the client acquisition inside the, the, like retryal loop. What's the issue you run into if you don't do that? Yeah, that's a great question. Give me a sec, I can't do two things at once, um. OK, so the, the reason we had a leak here is that I wasn't returning the connection to the pool. Um, the reason we wanna do that is Depending on how much time we get here today, um, I wanna show you just how you can recover from other kinds of errors in DSQL, right. Um, for example, uh, when you're running on a single node system, what happens if your primary fails, right? Like you have to learn about the new primary, you have to shift your connections open. And so in DSQL, because we're a distributed system, each of your connections is actually running on a different machine somewhere. And so when something goes wrong, you're gonna lose just a fraction of your healthy connections, depending on like what failed. And so it's really important that your application can take advantage of that and get it, grab a new connection and keep going. And so the simple, the simplest way to do that is to simply grab a connection out of the pool when you retry. Right? Because if anything goes wrong, and then you can change that catch handler to simply say, Hey, if any of these, you know, if any of these conditions happen, just grab a new connection and try again. Uh, we have one more question here. And then I'll come there. Uh, do we have to pay for each connection? So if I open 1000 connections, do we, do I have to pay something if I don't run any query? No. So the question is, how do you pay for connections? The answer is connections are free, uh, with an asterisk. And the asterisk is that opening a connection does a small number of reads, because it has to do things like check your permissions, right? But the thing you're actually paying for is usage. So you're paying for reads and writes. And so if once you've opened a connection, like you can just leave it there idle, it's fine. OK. OK, so we're done with that. And we'll notice that our min execution times are pretty low. Uh, but our max execution times are pretty high. And the reason for that is we're doing a bunch of retries. OK. Um, so what I'm gonna do now is actually load the system up a little bit more. And what this script is gonna do is just dump in a million accounts. And while we do that, we're gonna quickly add another table, which is our transaction log table. Uh, Mark, I have one more question. OK, um, you, yeah, hey, uh, uh, I know in the DSQL launched business, uh, there was a few unsupported features like foreign keys, partition sequences. Are things like that planned to be out in the future, or is it like the distributed nature kind of makes it so they'd be not able to be implemented if I phrase that, if I phrase that quickly. Um, yeah, so foreign geese are on the roadmap. We, we, we do plan to be closing that gap with the pores, uh, competitive. One more question here. Did, did you have one more question, or oh, I missed, oh sorry. So, uh, in a distributed database kind of thing, how do you manage because, uh, uh, you mentioned that right consistencies are maintained in pessim optimized way and once you get a confirmation about the right commit, uh, it will be read across the cluster in the same way, uh, but. If somebody reads a transaction which is written in somewhere else. Uh, does it, the the session needs to wait? How does it get the committed block from the storage because storage is being shared, not the data. So how does that happen? Yeah, so this is the reason that we, that we do optimistic and currency control in the system, right? Because there's really two ways to do it. With pessimistic currency control, as soon as you do anything in the system, you have to go and sort of like put a lock there, saying like, hey, I'm busy here, right? And that would that would require us pushing down that locking information somewhere. Right? With optimistic and currency control, we don't do that. We just do our work completely independent of everybody else. And then when we commit, The commit's written to a single place, and that single place looks at everything that you're doing, and it checks if it's been, uh, if somebody else has done something that interferes with it. So you need to check, right, if somebody else made a transaction on the blog that somebody else is interested in. Right. And so it looks at the, the, the changes that you're making, right? So in this case, uh, because both transactions are reading and writing the payer and the payee. ID. So if we have one transaction that, that is 123, right? So that's account 1 and account 3, and the other one is account 3 and account 5, then because both of them have 3 in the right set, when they get to the point of commit, that's when the system, uh, called the adjudicator is gonna look at that and say, only 1 of these can go through, and the other one has to retry. And so there's like no, there's no, there there's nothing happening at the storage layer to do this. It's all happening uh at the commit layer. OK. So do you have a memory share concept between the cluster also? Everything is just fully independent. OK, yeah. OK, so, um, we, we have a table of transactions now. Um, And all this is, is a record of everything we've done in the system. And uh there's two columns that are being automatically generated by the database. The first one is the ID which is the UU ID. And the reason for the reason we wanna use a UU ID here is for the same reason that you would want to use one in Dynamo DB, right? That's gonna just give you the best possible fan out, which is maybe a different philosophy than you would have on an instance-based postco system, where you wanna do everything sequentially to get the best IO, right, in DSQL, it's just way better to go wide. And then the, the last column created out is just a timestamp. And so what we're gonna do in our function is just populate this table. See if I get this wrong again. We'll pay her first. OK. Did you assign the the role to did I do that? I don't maybe I, I missed it. Thank you for checking me. OK, so, so what I wanna show you now is this table's gonna start to fill up rapidly, right, when we start to start to run at scale. And we're gonna wanna be able to query that. So imagine this is a real application. Our customers go to our page, and they're clicking on their homepage, and they wanna see what transactions they've that they've run. And so we're gonna wanna run a query, um, select stuff from transactions. Where we can pick out for this account. Select star from transactions where payee ID equals that number and then we see our log, right? And if we run this a few more times, we should see more rows. There we go. OK. The RNG was on my side. Um, but, but I wanna show you quickly how to think about performance here. So if we do explain, select from account, you'll see that we're doing a full scan on this table, right? Because there aren't any indexes. Um, and what we can do here is, um, I'm actually gonna use explain analyze. Which is gonna give us a little bit more information because this actually runs a query. So we can say, create index, I'm intentionally getting this wrong for a moment, um, on transactions. Uh, pay your ID. Uh, on transactions. Payer ID and I'm gonna insert, put created at in here. For a reason. And we get an error, unsupported mode. And you'll see that we have to use the syntax called create index. A sync. And the reason we do this in DSQL is because in Postgress, when you, uh, when you create an index by default, you're gonna be taking a lock on the table, so nobody else can record any transactions, because the system needs to go row by row and make sure that it's building the index correctly. Now, Postress has a an alter an alternative syntax called Create index concurrently, that does less locking. Um, but it still does some locking, because it has to wait for this sort of quiescent phase, uh, before it can start. And it also has to scan the table twice, which can be very expensive and cause performance impact. There's, there's another small issue with create index concurrently, which is that uh there's no way to track it. Right. And so when we were thinking about building the system, we wanted to solve all these problems. And, um, create index async gives you a job ID that you can wait for. So if we do call sys.wait for a job and put this ID in here. It should say it succeeded. And you can actually take a look at all of the jobs that the system has been running. By digging into this table. And you'll notice that there's this index build that we just did here. But there's also these analyze jobs, right? Because the system is constantly looking at stats on your tables to make sure that, um, that it's building the right query plans. But if we go and run our query again, Uh, you'll notice that it's still doing a full scan, right? And this is just a little, a little got you for you to keep in mind. Actually, I probably picked the wrong. The wrong field here. OK, we're using our index here. But um if we picked a different ID maybe one with fewer transactions, or if we had a smaller table, you may still see a full scan because the query plan is actually free to choose, right? It can say, hey, should I scan this whole table if there's only 10 rows in it, uh, then it might just scan the whole table. Or it's gonna take a look at those analyze jobs that are running, it's gonna take a look at the indexes that are running and pick an efficient query plan for you. Um, This, this may look very intimidating. If you're not used to reading query plans, just dump it in your favorite AI tool and we'll do like a really good job telling you what's going on here. But I wanna quickly um call out a couple of things here. This is using an index, and there is no sorting. Um, and, and if we do something like order by um created at Descending, you'll see that there's still no sorting, just your eyes scan for memory, quick sort, anything like that, not in that list. Because we actually put the the timestamp in the index, right? So this is gonna allow like the, the database to traverse the data in the in the right order. Um, and what's really cool about create index async is you can do this fully online, at, you know, any scale you want, and there's never any blocking and there's never gonna be any performance impact. Because it's built completely asynchronously in the background using dedicated resources for your index. Um, Now with that said, I wanna show you how we can scale this application out. So This could get a little messy because, um, You know, demigods. So let's take a run. So this tool. This is a sustained load generator. And we're gonna tell it how many accounts there are in the system. There's a million. And then we're gonna give each host a target number of transactions per second. And um what what they're gonna do is you'll notice this little in-flight counter over here. They're gonna continue to open uh batches of connections until this number on the bottom left gets to their target. Um, and so with this running, we have 4 machines driving 4000 TPS. And if we jump over here, Uh, we can do And this is actually gonna go row by row and count every transaction we have. And if I uh do this again, it's gonna be even slower. It's gonna go row by row of even more data. But this number should start ticking up. Um, Now, That didn't fail, so let's go a little bit higher. At this number, I should hit the account quota, uh, for lander, right, uh, on my system. Um. But pay attention to. These numbers here, error and OCC. We're not getting any errors, um, and we're not getting any OCC errors. And the reason for that is by sort of cheating with the number of accounts, right, we've created more right keys. Um, and so the probability of any of these transactions conflicting is low. Now, this is obviously a little contrived example. I didn't want to spend too much of this talk getting into like some kind of like, um, really complex real world schema. But the takeaway for you is to think about designing your app and schema so that you don't have what we call hot for right keys, right? OK. Um, while that runs, I want, let's take a, a quick look at pricing. Um, if we jump on over to our cluster. Can I, can I just Grab everyone's attention to highlight that we went from 1000 concurrency to what was it? 110,000? How, how much did you put in the system? Yeah, we're, we're running at about 10,000 TPS right now and we didn't need to provision higher instances or um more parameter groups with a higher uh number of connections allowed or any of that so that that's another nice thing to to bear in mind. Nothing to patch, nothing to scale. Just send traffic at the system, it scales for you. And if you're using something like lambda that can scale, then you also don't have to worry about the compute. So this is the, um, so we're on the cluster page. I've clicked on metrics, I clicked on usage. And you'll see that there are um there's really two metrics for you to track. There is total DPU uh a DPU is a distributed Processing unit. And this is like a pay per use metric. If you used to Dynamo DB in Dynamo in Dynamo DB you pay for rights, and you pay for reads, and you pay for storage. And DSQL has all three of these units too. But it also has an additional one, which is called compute. And this is like lambda, you're paying for compute seconds, because we're a SQL engine, right? And so you can run these complex queries that are doing joints, that can be doing aggregates. You can do Fibonacci in it. And so there needs to be some way to track that usage. Um, DPUs are You can monitor them with Cloudwatch. I've also built a little tool, um. Where if we can take this cluster ID. Make sure I'm in the right place. We can paste it in here. That's gonna go fetch those uh Cloudwatch metrics, just sum up all of the all of the data points, run it through our pricing calculator. And you'll see that we spent $2 on DSQL. Um, If I stop this load, and we wait a few minutes and we run this again, it will still be $2. Right? If we wait 5 minutes and leave the load running, it will eventually take up to $3 or $4. I've run this workload for, um, you know, 2030 minutes, and I came back, and it's running at a million TPS and I spent $10. So this is one of the really cool things about DSQL is that not only are you not having to provision hardware, But the database just out of the box is gonna give you this ability to scale up and pay only for what you're using in that moment. Right. So if this load stops, that's it, you're done. Um, frozen in time, the thing that you're gonna continue to pay for is storage. OK, I think we have 5 minutes left, so we should probably wrap it up and take any final question. I have a 10 seconds and then the rest of questions, if there are more, I'll be hanging around outside afterwards. Thanks everyone please remember to fill in the survey for the session.
