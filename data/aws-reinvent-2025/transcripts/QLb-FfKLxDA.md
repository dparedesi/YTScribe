---
video_id: QLb-FfKLxDA
video_url: https://www.youtube.com/watch?v=QLb-FfKLxDA
is_generated: False
is_translatable: True
---

Well, hello everyone, and welcome on the session Dev 308. My name is Alexei Vanchenko. I'm solution architect here in AWES and with me, Vadim Kazulkin. He is head of development in IPL Labs and as well AWES serverless hero. And for me as the solution architect, main goal is to help my customers for scaling their business and help them modernize their workload without them spending too much time and effort on these processes. So today we're going to talk about how to build modern application with the help of Aurora DSQL. For that, we are going to. For that, we are going to dive deep into building blocks of this database and its architecture. We will go through a few code samples because this is a 300 level session, and we will go through the performance and challenges that you can see within the DSQL and of course we will do the wrap up on what we have learned today. And with that, I'm passing the word to Vadim. Vadim. So the question is why do we need another surveillance database. Uh, we have already several offerings on AWS. The list is not complete. You see Amazon Dynamo DB RDS, Aurora Provisions, and Amazon, Aurora SoLSV2. I would like to talk about these databases, but especially in terms of how they feed for the Sorless workloads, and I'm working a lot with AWS Lambda, so just how these databases can be approached and talked to from AWS Lambda. So let's start with Dynamo DB. It's really the ideal fit for serverless workloads. You don't have any infrastructure management here. It's the database. It scales up and down for you. And if it scales down, it scales up without any call start of the database, so it's not noticeable for you. You have single single millisecond digit performance, so that's, that's really cool database. Also, um, in terms of other things, so I, I really prefer it, but there are also challenges around that, so everything is a trade-off. So for example, it's no SQL database, and with that there's another mindset comparing to relational databases, how you design those tables, how you involve the schema afterwards. It's not very easy. And there are also challenges about certain things like by default Dynamo DB is eventually consistent. You can turn various things to be strong, consistent, and pay more for it, but for example, there are restrictions. Secondary global secondary indices cannot be strong, consistent, for example, and there are also limitations in terms of transactions, how many items can be put in one transaction. So there are certain limitations with that. Uh Now moving to other databases, they are all relational databases, and we love them. I personally come to working 10 more years with Postgrass, and I like these joints and many tables and all that stuff, and it has its benefits, but especially in terms of how how fits into Soro growths, there are certain limitations. For example, there is a lot of infrastructure work to do. For example, you need to defile VPC, security groups. And you need to define the size and the family of the instance, and so on. And also with that you cannot scale, compute and storage with this database, so you have a certain size and then you need to increase the size. You can have multi AZ just that you don't have kind of the outage by increasing one side and then the next side will be increased automatically. And there are also challenges around connection management if you talk from database from lambda. Uh, if you, the number of connections to the database is a factor of the database of, of the family size. And if you have a lot of lambda functions, they scale simultaneously, you can run out of connections talking to RDS database. There are solutions how to mitigate that, for example, proxy by providing something in between like for multiplexing. But of course it's another component. It's a managed one, but it also has a cost associated with that. It can be even latency and so on. So there are other challenges with. Now moving to Amazon Aurora provisioned one, it's mainly the same advantages and disadvantages as with RDS. The only one exception you can um scale the storage independently of compute with that database. Now moving to Amazon Aurora. Serverless V2, we are kind of approaching this world where the relational database are more suitable for the serverless world. Here you can scale, compute and storage automatically. You don't need VPC. You might have VPC if you would like to have private traffic, but you are not required to do that. You also not define the instance sizes. This is more aurora capacity units, and you can say, I would like to scale in this SCU and that SCU. You can scale to zero this database, which is not possible for and Aurora provisions, and it's sometimes very important because on your Store testing and testing environments you would like to, if you don't use it, you don't want to pay it. So that's why, especially for D and Aurora, you provision the smaller instances for those test staging environments. For Amazon Aurora Soro V2, you can specify scaling down to zero, but then you have the disadvantages. It might take up to 15 minutes, 15 seconds to scale up. In case you specify to scale down to zero, by default, the scales to 0.5 is used. And also, if you scale down to zero, you will lose the cash. Right near the instance and that by scaling up the performance of the first queries might be slower because the cash needs to build up. So you see here different challenges, and Aurora SoroSV 2, you can use FDS proxy with that, but they also offer data API so you can talk to the Aurora SoroSV 2 like you talk to Dynamo DB via HTTP. This is this kind of solution in parallel to RDS proxy. You can decide, you can use still RDS proxy or um data API, but data API is complete separate API, so you need to rewrite your code in case you would like to adjust, and RDS proxy you can simply use the you can you need only to change the end point. So the question that, that, that is kind of being asked is, can we have AWS database offering which is as serverless as Dynamo DB in terms of not having to, to, to deal with the infrastructure um at all uh scale up and down very quickly without cold starts of the database, but provides the benefits of the relational databases like this AI, um, uh, things. And with that, I will pass the ball to Ole. Thank you. Yeah, so for that to address these questions, we have introduced Aurora DSQL last reinvent, and we made it generally available in the middle of this year. So what is the DSQL? DSQL is the serverless database, uh, which means that you don't need to have any infrastructure to manage. You don't need to bring your database down for patching or updating. You. by us, it's provides 59 of availability, and you get scaling, compute, and storage separately. You scale the reads and writes separately, which provides you virtually endless scale, and through all this scale, with all this scale, you get strong consistency all the time. So when You would like to look for the DSQL. First of all, you would like to look for DSQL if you are searching for a database that supports AC transactions through multi-region. You want to feed the database for your serverless architecture or microservice design, or you have the application that is following event-driven approach, or you don't know how spiky will be your traffic tomorrow or the day after tomorrow. And of course if you want to continue to utilize already existing tooling and continue to work with SQL. And now this all sounds pretty interesting, but how exactly we are achieving all this? For that, let's dive into the design of the DSQL, and we will start with single region cluster. Single region cluster is Active active multi-writer cluster distributed across multiple availability zones. The data is replicated across 3 zones. You have one endpoint for reads and writes. It supports simultaneously and concurrently, and You don't have any instances to provision. You can send just requests to the database, and all everything else is handled by it underneath. With no instances of provisions, you don't have anything to stop for patching or updating. We also do it by ourselves and with the single region set up, you are getting 4/9 of availability at the same time with the. Sorry, but behind the, behind the single endpoint, you have the Not only, not just one or multiple postgra instances that are hidden and running. No, there is instead of that, what's really powered this SQL is the distributed disaggregated architecture. We took. Critical components of monolithic OTP database and separated them into separate services. We separated them into the connection management. We separated the query processing, isolation enforcement, transaction journaling, and storage. Each of these components works independently and are tuned for its specific role. Each of these components is working with the fleet of the computer resources that can scale independently and can adjust to the workloads. Um Yes, you get one end point for both reads and writes, so that means that your reads and rights are always local, and the only thing which is traveling across AZ is transaction commit. And also, also, as I said, you are getting 5/94/9 of availability with the single endpoint. At the same time with multi-regional cluster, you are getting 5/9 of availability and multi-regional consistent rights. It provides two regional endpoints. Both of these endpoints support concurrency reads and writes, and together it represented as Single logical database. There is also the third region there called the witness region. Witness region participates in the right quorum and works as the tiebreaker to decide which region can continue to write in case of the network partitioning. The witness region receives the data from the regional cluster. It also holds a limited amount of the encrypted transaction log, but it doesn't have any cluster or any endpoints. So if we will look on the high level architecture or building blocks of DSQL, you will find their front end, you will find query processor, adjudicator, journal, crossbar, and the storage. Query processor does most of the skill processing job. It acts as the dedicated postgra engine for each transaction, and there can be as many query processors as there are concurrent connections to the database. Adjudicator determines whether the transaction can be committed while following the isolation rules. Each shard key in the database is owned exactly by the one adjudicator at any given moment of time. Journal makes all the transactions durable and replicates the data between availability zones and regions. Each transaction always assigned to the single journal. Crossbar, merge the data streams and direct them into the storage nodes. Storage nodes. Storage nodes provide access to your data. Storage nodes, replicate, hold the replicas, multiple replicas, and each storage node contains the specific range of the data based on the database key. It may look a little bit overwhelming or sounds a little bit overwhelming, but let's take a look at how all these components work together. Let's start with the standard retransaction and basic select statement. Let's imagine that we are the user in the US. US West one, and we are willing to order food from a local pizzery service and um somewhere here in the Las Vegas. So So what's, what's what's happening there? We're deciding that we want to get the restaurants that have the rating for or higher, so we are executing the query for select asterisks from restaurants where the rating equals or higher than 4.0. So what happens behind Since your application making the connection to the DSQL front end. Front end allocates the query processor and passes your statement to the query processor. Query processor reads the local clock and sets the transaction start time tau start. Uh Sorry, now your read-only transaction begins. The storage is automatically, and what's important, transparent to you are partitioned according to the database key. That's why query processor consult with the chart map to understand where your data in which storage node it is located. And because this is a read-only transaction. Query processor doesn't need to follow the writer's path. It doesn't need to go through the adjudicator or journal. It can go straight to the storage to retrieve your data. Storage notes doesn't return pages. It returns rolls. Storage notes. Also can process predicated pushdowns, filtering, aggregation of the rows before sending them back to query processor. Together all this stuff significantly decrease the amount of the data that needs to travel across the networking network and Decrease the amount of work that needs to be done by the query processor, so. And Comparing to traditional database, we don't have any monolithic cache sitting aside. So when a query processor needs to get the data, it goes to the Storage node which is located in the same region in the same availability zone where the query processor is located as well so your reads are always local um so then query processor return us then. Then query protester returned the results from the storage node. They all merge together and return it to you. So what's happening next? Next, at this point, if you continue to interact with the front end, you are building the interactive transaction. You start the transaction. You do some work with the database. You do some work with the client. You maybe go back and forth multiple times, and eventually you commit. How does it look? How does it look, for example, in our case, when you decided to order the pizza, you select, you select the restaurant, you browse the menu, you choose the item, you add the item into your order, you place the order, and basically that's hitting the commit. So what? In traditional database, how it will look in traditional database, most probably you will have to place some logs and do some checks potentially across the region for each statement in your transaction. Add latency to your operation. Query processor acts differently. It acts as the holding tongue for all your statements in your transaction. It reads data from the local storage nodes and saves it into the local memory. When it needs to write the data, it's already writing the data into the. It changes the data of the local saved data in the memory, and it accumulates all the changes within this local memory and waits for the commit and then when it's needs to commit, it just follows the writing path and sends the data to the adjudicator to check the full transactions to the adjudicator to check the data. So as you can see, query processors do a lot of heavy lifting. Uh, it's basically the heart of the DSKL architecture, and query processor is running in Firecracker, uh, lightweight. Virtual machines called also micro VMs and they are running on the bare metal query processor host and we can run thousands of pre-provisioned micro VMs on the single host. So every time when you connect to the DSQL and start using it. We are making sure that there is enough micro VMs running to handle your workloads and we can scale them automatically as it is needed. Each query processor is fully independent and isolated, so they never communicate to each other. And talking about the isolation in DSQL, we are supporting transaction level isolation called snapshot isolation. Each transaction operates on the consistent snapshot of the database as it was at the. Beginning at the start of the transaction, your transaction begins and proceeds through the SQL execution path where all the reads are happening always with the consistent snapshot at the storage level. These reads are implemented using the technique called multi-version versioning concurrency control or multi-versioning. The storage engine. Keeps multiple versions of the same row and allowing access to the older versions of the row, while not blocking or creating newer version. Um And when basically right operation occurs like insert or update, it's not immediately written to the storage, applied to the storage query processor spools all the changes into the local storage, into the local memory, and creates a private workspace for it, private, private workspace for this transaction. And this approach basically enables you to read your rights capabilities. So for all subsequent reads within the same transaction, you are working, you're working with the pending changes. You read transactions, see the pending changes that you already applied through the rights. So when when your transaction issues the commit, the sequel needs to understand whether all the changes that were applied within the transaction and spooled locally can be applied to the storage, can be written to the storage, and that's where the adjudicator comes into play. It sits in the writer path of the sequels and decides will the transaction be committed and written. Its job is to detect and resolve the conflicts that are happening between the transactions to ensure the consistent rights. So to do this, the query processor forms the payload, creates the payload to send it to the adjudicator, and this payload contains all the necessary information to make this decision. We have the right set there which contains all the items modified by transaction. We have the. Image set which contains the copy of the table rows after applying transaction changes and we also have the start time of the transaction, to start, which plays the crucial critical role in deciding which will be transaction committed or aborted. So, and we will talk about this part a little bit later. So, when, um, now, when two transactions come, what's what basically happens? Uh, here we have two transactions, A and B. They started, uh, Almost at the same moment of time, but transaction A was committed slightly before transaction B. So what happened in this case? In this case, adjudicator will analyze the payload of both of the transactions and examining their writer set and start time. It will look for all the. Changes that were made after the start time, tau start and it will look for overlapping or matching roles that was changed and it sees that both of the transactions are trying to change the same role and because they cannot both of them change the same row at the same time. Because transaction A was applied, committed slightly before, transaction B needs to be aborted. At that moment in time, Transaction A is allowed to proceed and decide the commit timestamp, tau commit. And if If 2, if there are 2. Transactions which have different writer sets or this writer sets contains the not overlapping or not collapsing, not conflicting rights or roles. In this case, both transactions can be can be committed, can be allowed to proceed, and both transactions will be assigned the commit. So once the adjudicator decides that your transaction is able to proceed, what exactly makes it durable in traditional database, durability happens on the storage level. Transaction is considered committed only when it's written durably to the storage. The storage layer is then responsible for recovering your. committed transaction in case of failure, but these things come with the price for that. You need to have the checkpointing. You have to have the logging. You need to carefully coordinate between the memory and the storage synchronization to be able to recover in case of the failure, and all this adds lots of latency and lots of complexity to the system. So this DSQL managed this complexity by using the journal. Journal is uh sorry, internal component that was written for the decade and was optimized for. Order ordered triplication of the data between multiple A and between multiple regions. It scales horizontally, so transactions can come into any of the instances of this journal. Journal coordinate to maintain ordered streams of communication of committed transactions and then crossbar, which is pulling the changes, pulling the transactions from the journal routing this transaction to the storage. And using this order to ensure that all the transactions are written to the storage in the proper sequence. So once the adjudicator decides that your transaction is committed, it sends the payload and commits time stamp to the journal. Once the journal acknowledges the right to acknowledges your transaction, your transaction is durable and atomically committed. The journal sends a success code back to the query processor and crossbar at this moment begins to pull the data from the journal and draw it to the storage. The query processor at this moment sends success code back to you, to the user, and for you from this perspective, the transaction is successfully committed. And now I want to draw your attention to two components that are crucial in this all the workflow. Remember that tau start and tau commit. This is the time. This is crucial because you cannot get strong consistency without synchronizing on the time. You cannot coordinate them. If you don't have the reference clock which is which you can trust and you know that you can trust to this tautar and tau commit. For that, for solving this issue, we and achieving this reference clock which we can trust, we have introduced Amazon time sync service. With time sync service, with dedicated timing hardware that are located across AWS and AWS Nitro system, we having the directed GPS dictated reference clock. That are located on the EC2s in DSQL, it effectively allows us to have a globally synchronized clock with the microseconds accuracy time. With all of this, I'm passing the word to Vadim to talk about authentication and authorization, how it's happening. Thank you. Yeah, so let's look into Aurora DSQL more from a developer's perspective. So I don't the slides are not switching. OK, something. So, OK, now, now it's work. So, uh, thanks. So now let's talk about authentication. How we normally authenticate, uh, to, to the relational database. OK, it's now not working once again. OK, now, So normally we use user and password, and with that, um it might happen that somebody can get access to this password and misuse it. So normally we need to rotate this password, which we sometimes forget. And this offers the attack vector there. So the SQL doesn't use any password, but what does it use instead? It uses tokens to authenticate, and the tokens are generated by AWS SDK and it's fast and what's very important, the local cryptography. And this is more or less the same. That is used behind the scenes if we talk to S3 Dynamo DB, but these tokens will be generated for us. We don't deal with that. In case of the sequel, we will probably need to deal with that. What's important is they are very short-lived. If somebody intercepts this token, it's probably already expired, which is a good stuff. So now let's look at how to generate this token. This is one of the examples we use here, AWS request. We send here AWS DSQL generate the BConnect admin of token, give the host name and how much time to expire. As short as possible is also OK. What we will get back here is basically the response, and the response is the whole token, and we can use this token to authenticate instead of the password. So for example, you can use that to authenticate using PSQL. This is how we talk to the Posgras database. PSQL we simply export it to the environment variable called PG password, and then if we do PSQL quiet connect, then this password will be this token will be used instead of password. Uh, in case you use something like the the beaver or some other tool to authenticate, you simply grab the stock and you use it instead of password. Yeah, this is how it works. The cool thing is that DSQL team released their integrated query editor, square editor in the browser several weeks ago, so you can simply go to the DSQL page and then you can simply. Do all the SQL stuff in the browser and this token thing will be done for you basically behind the scenes so you don't need to bother with that. But of course sometimes you would like to have powerful tools like GB Beaver and use that. What I would like to show you is how easy it is to create aurora DSQL cluster, especially single region cluster. You see, we don't have any scroll bar here, which is surprising. If you created RDS, then you probably know how many choices do you need to meet. Here's the only thing more or less that you need to do is just more or less give the cluster the name, and everything else is already there. Yeah, there is enabled deletion protection, which is a good stuff. You can remove it, you can stabize encryption settings, and you can add the text. That's basically it. Of course, for the multi-region cluster, it's a bit more complicated because you need another region, just to select another region, then select the witness region, but basically the settings will be the same, so really, really easy. I would like to show you code examples. Please raise your hands, who can understand the Java code? OK, lots of people, which is good because I'm a Java guy and I feel understood myself. So now let's take this subset of application that Olei showed you. And this will be kind of only the ordering part. So you see API gate when a bunch of lambda functions, how to create order, get order by ID, or get orders by created range, like give me all the orders from the 1st of November until the 31st 13th of November, and we store that in the Aurora DSQL. I will show you an example with AWSM, but you can use SDK. You can use cloud formation or terraform. Basically this is a very generic example. You can reuse it for all applications, especially if you talk from Lambda to Aurora DSQL. First of all, you need to pass the The Aurora D SQL cluster variable environment variable. This is the cluster ID. You create the Aurora cluster, DSQL cluster, and then you can grab the cluster ID. You can provide a default value or environment variable here. And then you can pass it to create Aurora DSQL cluster endpoint, which is kind of, you can use this variable. It always follows the same pattern here to create this, and I export that as an environment variable for all lambda functions of my application. And as you see below this is the Java code. I simply grab it by system, get environment variable, and then I use it to create this GDBC or LGDBC is the protocol for Java to talk to to database, to relational database. So the Aurora DSQL cluster endpoint that I passed as an environment variable is a kind of substring of that. And what I need to decide uh also to give is the authorization stuff. So here I simply allow lambda function to talk to the Aurora DSQL and uh here I need to provide resource IRN in this Aurora uh input cluster and uh uh cluster ID is also kind of used as a variable for that. Um, so pretty generic logic. Now let's look into the example, uh, Java example. So basically it's a good practice to create, uh, to use and instantiate the so-called, um. Server-side connection pooling and just not only for Java. There are solutions for Python for all programming languages. The reason that it still makes sense because the creating of the connection is still not free. And the connection might expire and may be kind of fetched once again, so it's still a good practice to use that, and I use for Java Hikari data source pool and I create that and say how many connections. You can even say one connection in the pool because basically it's a single threaded or single process application and that's why lambda only needs one connection just at a time and then it does the thing sequentially. Uh, so I create this here, and this is the logic to get the connection. Yeah, you see here, um, in both places, what we are doing here, we are fetching the token. And set, set it as a password. And there is Aurora DSQL SDK which does the same what I showed you with AWSCLI. This is basically you need this to create this generate of token request, provide endpoint time to leave, and then this token will be generated for you each time, and you need to pass it. What you already see here, we need to to fetch this token. In two places. First, by creating the connection pooling, and second, each time that we get the connection to talk to the database. And it seems like a boilerplate court, and that's why AWS team improved that. They started with Java, but now also for other languages they release so-called connectors, and these connectors help you to handle this token generation automatically for you, so you don't need that. And if you look in the previous slide, that was the only dependency to AWS SDK. is to fetch this token. All other code is simple GDBC code. So simply you can reuse that what you already have, for example, if you migrate to the database. There was no no no no um dependency to DSQL. And by removing that we can remove all dependencies to DSQL, so there is no password management and now it works since last week also for Python, for Python PG and PG 2, and also for no JS. There is no JSgrass and postgras JS driver. So how does it look? We need to add the dependency to this connector there in our dependency management, but now our code doesn't doesn't have any dependency to DSQL. It's a poor GDPC, poor Java code, how we talk to the application. You see, we simply, we don't have this set get authentic of token, and we don't have set of token. But how our application understands that this connector should be used. The whole magic is in this red line. We we additionally say it's AWS DSQL in the path, and with that this connector will be triggered instead of poor GDBC post grass driver. So this is kind of the only dependency, explicit dependency in the string which says that this connector should do its work and it will generate the fetch the off token and place it. Uh, instead of the password for us, so huge improvement of, uh, I would say readability, uh, of the code. So what are other benefits of DSQL? It works really well with familiar object relation mapping frameworks. You probably all use them. You have some entity like order, you can annotate them, the properties like order ID with the column in the database and simply save the stuff. And it works with all of them. For example, for Java, the Hibernet is a popular one for Python developers SQL, Alchemy, or Django. Yeah, and it all works with them, so we don't need to change our code. Let's talk briefly about performance of this database because Alexi explained the architecture and it could sound like a magic. So the question is just is it performance database or it's all this magic may impact performance. So first of all, my goal was to measure performance from lambda function. Yeah, so I measured the performance of the lambda function which talks to the DSQL, so that the the SQL time is kind of there. I used Java as a programming language. I only focused on measuring warm start times. I didn't focus on the cold start time of the runtime like Java. It has implications, but it wasn't the goal. That's why I only also focused on 99th percentile, just to exclude the impact of the programming language. And I did the performance measurement for single and multi multiregion cluster. What was not the goal. That's very important. I didn't want to compare the performance of the databases that we talked about RDS, Dynamo DB, Aurora Provision, ServalSV2 because it deserves the whole talk, and sometimes you cannot compare Dynamo DB with relational databases. I didn't want to test the performance at scale, just not. Putting 10,000 transactions per second or just queries per second, and I didn't want to test performance when scaling. I don't have an impact on how storage is scaled, query processor adjudicated. It all happens behind the scenes. So my goal was just to do some basic stuff and see how quick that is. So I showed you this application with with ordering. The database structure is very simple. I only have two tables order. And order items and order has order ID as a primary key. It has created a time stamp, values, such a thing, and all items has product ID and so on. And I have two indexes on the items table like order ID because I fetch items by order ID and also I have one index on the order table on the time stamp because I fetch the results like time stamp between. One day and another day. So very simple uh structure. I don't have user table, I don't have product table, only two tables. So the first measurement was, I'm doing like creation of one order containing 2 items. So basically these are 3 inserts in the 1, in one transaction. 1 insert into order and 2 inserts into order item. If you see the results, let's look into this Lila one. It's P90. In 90% of the cases we have that results that is not kind of worse than that. And you see, it's just for P90 for single region cluster, 20 milliseconds or something around and for multi-region cluster 40 milliseconds for 3 selects. So considering a single region cluster, it's kind of 7 milliseconds per per insert, sorry. Which is, I would say, OK. It's comparable to Dynamo DB for multi-region cluster. It needs to commit in another region so the performance is kind of twice as slow, which is normal thing in case you need multi-region. Another measure was get order by ID and then get two items. I can do it with one joint, not a problem. I wanted just to do two joints without joints like getting the order by ID and with that ID then getting two items with one with another select. So there are 2 selects. And you see then P90 lila 1 to 12 milliseconds for two selects by using primary key for getting the order and then the index for getting the order items. You see that the performance is the same for single and multi-region cluster because it's the read operation, and the read operation is always local. It always uses the local endpoints. So the performance of all the reeds is the same for single and multi-region cluster. So I would say 22 selects 12 milliseconds, a good use case. This is a more sophisticated, just get orders by created date range and I intentionally limited the the the query that it returns only 100 orders maximum. And then for each order it gets the items and I have only 2 items, different items for each row. So basically for each order. So basically I have 101 selects, 1 select for order, and then for each 100 orders, I get two items. And you see 100 selects for 90, 200 milliseconds for 250 milliseconds for 101 selects. I think it's a compatible, very compatible result. Another thing I wanted to test because the specification says there is no call start of the database, and you can measure it. I did that, so my goal was I did not, I, I created a single multi-region cluster and didn't did some operations and then I didn't touch it for 3 days. And then I did the first select via the console, just not via the lambda function because it has performance implications. I simply just use PSQL and said select order by ID and some existing order. And I just did it 44 or 5 times, the same, just 3 days pause and then. Select what you see here, the medium latency in this case was for single region class the 165 milliseconds for multi-region cluster, the 169 milliseconds. So of course there is a small latency because the things need to be warmed up if I don't use something. Something needs to be persistent. But it's not noticeable. It's not 15 seconds like Aurora Sorvalesv 2. It's just one time for the first select or first operation 160 milliseconds. So I would say no false starts of the database. It's simply not there. So now let's talk about, there are also constraints, and you need to understand them and and uh because that that might have certain implications. So let's first start with quotas. Each AWS service has its quotas, and there is even AWS service called service quotas where you can go there, select the service, and you can see the quotas or constraints. Yeah, in the SQL there's no exception. So there are some important ones that I would like to raise attention. Maximum number of connections, it's by default 10,000. It's a huge number, and you can even adjust it. All other databases relational that we talked about don't have that value, and you cannot trace it. So it scales beautifully. There is no problem just to have 10,000 lambda functions talking to it in parallel to this database. It's just. Due to the architecture of querying processor QP, it scales beautifully. Maximum transaction time is 5 minutes maximum. Alex will be talking why those limits are as they are, but you need to think of it as just mostly for the for the microservice applications. So in case you have some analytics job that needs to run more than 5 minutes, probably it's not the best fit for this database. But for certain normal microservice type of things, 5 minutes for transactions seems to be OK. Maximum connection duration is 60 minutes. So one hour. In case you design the server side connection pool, you need you define the time to leave of the connection, please define it no longer than 60 60 minutes, because you will be disconnected. And in case you define lower, the pool will take care of this. And the same if you are talking through the database through PSQL. After one hour, you will be disconnected and need another token. Yeah, you need to grab another token and then you are in for one hour. The maximum number of rows changed in the transaction in one transaction is only 3000 rows, and by changing, I mean delete and update. So in case you have such a use case like I would like to delete one time per year all orders and items older than 5 years because I don't need them from the accounting point of view. You need to think about how to divide the job into parts so that no more than 3000 rows will be changed. That brings you just to think from the backend perspective how to divide it, because normally you would say delete from with time stamp. You will get an exception if the number of rows will be more than 3000. The same for update. I'm Maximum size of modified data in one transaction 10 megabytes. So you need to think if it's normal data, then you probably will not hit that limit. There are also other things that are currently not supported, and I will stress that currently there is no reason why it cannot be supported. It might come every day, but Until today it's the case, no support for Jason and Jason B. Yeah, so it's not 100%. It's compatible, but but the sequel doesn't support all features. There are no columns with default values. There are no temporary tables. There are no triggers currently. I personally. I used them a lot previously, but now it just brings the business logic to the trigger. It's difficult to refactor, but I use, for example, triggers to set the time stamps to now as a default value. It was kind of one of my favorite use cases. I can do it in the back end as well, but anyway, there are no sequences. That's unusual. You can use time-based UID version 7 instead of this. This is a 36 digit number, and version 7 in the supports that you can order by time. And sometimes it's OK. For example, you can design all item IDs like your ID because you don't expose them. That's fine. But for the things like order ID that you would like to expose via email, somebody can call the call center and say something is wrong with my order. They probably don't want to say 36 digit number. So you need to think how to map in this rare situation to the normal value, yeah. Maybe there will be supported later. There are no partitions, in case you have a multi-tenant application, it might be interesting, yeah, in case not, probably you don't use it, and you can use only the functions written in SQL, which is probably fine. Yeah, you can use probably C++, but it's not supported. But I think SQL is OK. And other things that are not supported, constraints or commands, foreign keys. That's something really unusual, probably. You can do your joints, but there is no foreign key support which will ensure the data integrity. And that might be crucial because I like foreign keys and I sometimes or nearly always I would say define sampling on delete cascade there. For example, if I delete the order, I would like that all other items will be deleted automatically, and I can do this with foreign key on delete cascade. It's not supported. Another way to do it is to do it via trigger, which is also no support. Yeah, so this is then up to you to think and write the business in the business logic that you delete the order and you delete all order items in the business logic, not relying on foreign key on cascade. Currently, there is no truncate but we can delete the whole data in the table, and there is no vacuum, but the architecture of DSQL just doesn't require a vacuum because of the limitations and constraints that we talked about. There is also no PG vector support in case you would like to use something for embeddings and so on. This database is currently not for this use case. You can use. Other postgrad compatible databases RDS, Aurora, or OpenSearch that support that. So this is for normal microservice applications. And with that we have some final thoughts. Alexi, so once again these features might come all, yes, but now you need to think about them with all of that, let's see what we have learned today on this session. So first thing which is very important that thanks to the query processor running into the firecracker micro VM, we're getting. Virtually endless scale. We are not only able to scale it to whatever size, we can do it fast, and as we saw from the tests, the cold start is almost nothing. And thanks to the architecture we can adjust different components of the system very dynamically, depends on the workload, and clean up resources after us very effectively. And as in any serverless service we currently have any, we currently have some constraints that are needed to get the better performance and get the better results. So here, for example, 5 minutes length of the transaction allows us to get the better right consistency and get the better performance from the. When the adjudicators need to decide and resolve the conflicts, it also helps us to clean up the resources using the garbage collector, which resulting in the fact that now we don't need a vacuum by design because all the references, all the references will be just cleared and with the. 3000 rolls that can be changed within the transaction, we have the stable performance with only one flash of the buffered memory. And to get the best of your usage of DSQL you should create many connections to your front, to your front and parallel concurrent connections, which will result in a lot of query processors which will basically do the horizontal scaling. You should consider to use small rows and small. Transactions, so it will decrease the load to the adjudicator for deciding which transactions are in conflict and to resolve this conflict for the same reason you may consider to use separate clusters for each of your microservices again to have less conflict and to have less. transactions and if you find any bottlenecks or you would like to know like where the money goes, Explain analyze will help you with this. And of course don't forget about client-side connection pooling. This also will help you with your performance and with that I'm passing the mic to Vadim to wrap up. So one benefit of Explain analyze, you can use Explain analyze for both, and it will also display you the price, the, the DPUs now, and you can do it in this browser editor. We didn't touch about pricing, but it helps you just to understand how much does it cost to run that in that query, by the way. So um. OK, I go back. So yeah, to wrap up, the sequel is very easy to set up and get started. Nearly no infrastructure management, multi-region out of support right from the beginning. Aet support is there completely as we like, especially no eventual consistency at any scale, which is very important, at any scale for single and multi-region cluster and high availability is also there. Now I will provide you opinionated comparison of the databases that we talked about Dynamo DBRDS, Aurora, Aurora Soroli V2, and now DSQL, but only for the Soroless workload. Please understand 3 stars doesn't mean it's 3 times better than 1 star, but 3 stars means it's just a better fit for the solarless workload to solve the problem better than 2 stars, and 2 stars solves the problem better than 1 star. So set up experience, Aurora DSL is brilliant, like Dynamo DB. Simply just quick and you are there. Auto scaling experience up and down automatically. You don't need to do anything to decide how much to scale and how much to scale down. It's just no call starts. Beautifully exit support completely there like for all relational databases. No need for connection site pooling. You don't need the proxy. There is no data API. It scales beautifully. You can have tens of thousands lambda functions talking to a database, and it works. And yeah, it works with familiar drivers and object relational mapping framework as we like. But of course, yeah, it looks beautiful like Aurora DSQL is kind of supports every use case. There are challenges and you need to be aware of that. The first one, as we talked, the feature gap to be 100% postgrad compatible is currently quite big. You need to follow, especially if you if you start now from scratch with your application, if you know that you can adapt to this, I think. But if you I would like to say I would like to migrate your application. You will probably run into a situation that you need sequences. You need foreign keys. You run into this 3000 updates in one transaction. You need to rewrite code, and it poses the question, is it the right time to migrate? But if you start from scratch and you know that you can start, and I'm pretty confident that the features will come with time. And of course the service quotas are a bit restrictive for serverless database users, like 5 minutes transaction time, 3000 rolls. We know these, these quotas from Dynamo, and we appreciate them because they are there for a reason to get the best performance. But for relational database users more or less would like to do everything with the database with. With any limitations, which is sometimes also not a good thing because the query performance will be worse if you draw how you would like. It also doesn't work like this, and these limitations also are there to just deliver the best experience. But anyway, you need to know them. You need to understand them when you design or migrate. So this is the core code of of of my GitHub repository with the examples, yeah, with the code that I have shown you with connectors and without connectors so you can scan and if you can write to me, you can find me on LinkedIn and I will give you the the link to the repository if you if you wish. And for that, thank you very much. Don't forget to rate the sessions, the sessions, yeah, to give the, uh, to receive the feedback, and we are here to answer your questions. I think uh it's kind of the silent sessions, but we are here. Thank you very much.