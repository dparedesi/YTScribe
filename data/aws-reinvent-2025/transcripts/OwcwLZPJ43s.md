---
video_id: OwcwLZPJ43s
video_url: https://www.youtube.com/watch?v=OwcwLZPJ43s
is_generated: False
is_translatable: True
summary: "This session, titled \"Customize models for agentic AI at scale with SageMaker AI and Bedrock\" (AIM381), presents a detailed technical roadmap for data scientists and developers looking to overcome the significant hurdles involved in taking agentic AI from proof-of-concept to production at enterprise scale. Amit Modi, Senior Manager for Model Operations and Inference at AWS, begins by identifying the critical friction points stalling the industry's projected 33-fold growth in agentic AI adoption: the lack of standardized customization tools, fragmented observability across models and agents, the complexity of tracking evolving AI assets like reward functions, and the prohibitively high cost of inference. In response, Modi announces the launch of Serverless Model Customization in SageMaker AI, a fully managed service that allows users to fine-tune a vast library of foundation models using advanced techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning (RLHF) without ever provisioning or managing GPU clusters. He further introduces Serverless MLflow, a zero-setup integration that unifies experiment tracking, evaluation metrics, and agent traces, ensuring deep observability allows developers to debug root causes when agent behavior deviates. The presentation heavily emphasizes cost and performance optimization, showcasing new inference capabilities that allow multiple foundation models to share a single endpoint—potentially cutting costs by 50% through intelligent auto-scaling—and the implementation of speculative decoding, which leverages smaller \"draft\" models to verify tokens generated by larger models, achieving a 2.5x throughput increase. Shelby, Worldwide Specialist for Gen AI, then takes the stage for a rigorous end-to-end demonstration involving a medical triage agent designed to distinguish between varying levels of patient urgency. She guides the audience through the new Studio interface, demonstrating how to upload clinical datasets, configure hyper-parameters for serverless training, and execute automated \"LLM as a judge\" evaluations using industry-standard benchmarks like MMLU to quantitatively prove the fine-tuned model's superiority over the base model. Her demo extends into the practicalities of agent orchestration, showing how to wrapper the fine-tuned SageMaker endpoint into an agent using the Strands SDK, register and version system prompts within MLflow to prevent \"prompt drift\" among teams, and deploy the final agent using AWS Agent Core for scalable identity and runtime management. Crucially, she highlights the new asset lineage features, which provide a complete audit trail linking every production inference back to the specific training data, reward functions, and pipeline executions used to create it. The session wraps up by showcasing integrations with partner tools like Comet, Deepchecks, and Fiddler, reinforcing the message that AWS now provides a cohesive, end-to-end ecosystem for building reliable, cost-efficient, and observable agentic workflows."
keywords: SageMaker AI, Serverless Model Customization, Agentic AI, MLflow, Speculative Decoding, Model Evaluation, Agent Observability, Inference Optimization

Welcome everyone. This session is for data scientists and AI developers that want to customize models and deploy at scale to build high quality and cost effective gentic applications. In today's session we'll also cover some of the new launches that were just announced in the AI keynote. So let's dive in. We are seeing two trends emerging in the market. First, there's a rapid adoption of agentic AI in enterprise software apps, and this adoption is expected to go up from 1% in 2024 to 33% in 2028. That's a 33x increase in just 4 years. Also, organizations are expecting 15% of these decisions will be made autonomously by the agents by 2028. Which would require a lot of compute and models that you can use to build out high quality, cost effective and fast inference. Customers are increasingly relying on open source models to build out these applications. However, in spite of the large opportunity. As well as a clear line of sight on how to build these applications, we see the majority of these applications never make it to production. Let's take a look at 4 key challenges. That are blocking these applications from getting deployed to production. First, customers lack standardized tools to customize models, so they end up building and spending time building out these workflows. And when the time comes to take these workflows to production because they've been put together with glue code, they have to rewrite these workflows with productionized tools to build out these pipelines that can help you build repeatable and scalable workloads. Which often delays these projects and leads to a lot of manual effort. Second, customers lack tools that provide a unified view of model as well as the agent observability. With fragmented tools, it becomes much harder to debug any root cause issues of failure or when the behavior of the agent or the model deviates. Third, With model customization, the need for tracking the ML assets has evolved. Earlier, customers could monitor, track, and version the models and make sure they were cataloged in the right place and they could meet the governance and compliance requirements. Now they need to also think about reward functions that are used in reinforcement learning, prompts, and so on. Which often further leads to building out additional tools or integrations and delayed the timelines. And lastly, customers need. Cost effective, high quality inference and for that building out an inference stack can be very complex. You have to find the right instance. You have to benchmark against different instance types to make sure you, you have the right cost price performance ratio. Then you have to do the same thing with the container or the different frameworks that you're using, which often leads to a lot of manual work and delay in getting these applications to production and sometimes the ROI doesn't seem right because the inference is too expensive. So let's take a look at some of the key capabilities that SageMaker offers to address these. I'm Ammit Modi, senior manager for model operations and inference, and I, along with me I have Shelby, who's the worldwide specialist senior manager for Gen AI. And today we are going to cover the key SageMaker capabilities that will help you address some of these challenges, and then Shelby will demo all these capabilities and bring them to life. So let's get started. Today we are announcing the launch of serverless model customization. Serverless model customization offers you the broadest choice of foundation models that you can use to customize and the models based on your domain-specific or proprietary data of your organization. Along with this broad choice of models, you also have access to broad choice of different fine tuning techniques that you can use to customize these models. And, uh, which includes reinforcement learning and lastly this experience is completely civilized so you don't need to worry about reserving capacity or, you know, uh, finding out where the GPUs are. You just kick off the job and we take care of all the infrastructure for you. You can now navigate to StageMaker Studio where you will find models and under models you will see the list of all the public known uh foundation models and you will see three different experiences. You can customize these models through UI, SDK, or through the agent experience. In today's talk we'll focus on the slides will primarily focus on UI, but we'll see a little bit more in the demos. Once you click on the UR, you will be navigated to a managed experience where you can select the base model as well as the fine tuning technique that you want to use for customizing the model. Once you select that, you can upload a data set. You can upload a data set here or if you already have a data set that is tracked with Sage maker, you can just simply select that data set. Then you can choose the. Reward function that you want to customize, you can choose from a different variety of reward functions, and it also allows you to bring in code. You can manually type in the code or you can bring in a lambda that will do the watt function. And then you can simply select one of the lambdas that you have already registered and then you can get started with the fine tuning experience. Behind the scenes, what we uh Sagemaker takes care of checkpointing your jobs regularly. This ensures in case there's a node failure in the cluster where we run these jobs, we will replace that node with a healthy node and resume your job from the last checkpoint so it never overuses the compute, just the right amount. And in case the job fails due to some reason, you will always get a checkpoint at the end that you can use to resume your training jobs. Once you have built out these workflows, you can use SageMaker pipelines to continue customization and deploy these applications into productions. Today we're also announcing new pipeline steps that are purpose built for model customization and deployment, deployment not only to SageMaker endpoints but also to Bedrock if you're using Bedrock for your inference techniques. With this new pipeline steps, you can accelerate your development by simply using the existing purpose-built pipeline steps so that you don't have to write any glue code to leverage integrations with server EMR Serverless for data processing or Bedrock for deployment. Or for training jobs within uh uh within StageMaker, uh, pipelines are if you already have an existing code that you wrote for your experiments in notebook you can simply annotate that code with at the rate step decorator or use our UI and upload this code over there to convert this into a a fully functional pipeline. And pipelines are serverless, so you don't need to worry about managing any infrastructure. We log all the metrics to Cloudwatch where you can go and debug any issues. Next, we also announced yesterday the launch of Sorous ML flow. Several SML flow solves the problem of fragmented observability that we just talked about. When you are customizing your models, you can log experiments, you can also log evaluation metrics as well as the agent traces once they're deployed. Server 7 flow is fully managed. You don't need to spin up any servers. We take care of spinning up the compute for you as the traffic on the inference or training grows. We scale up the server SML flow and scale down when there's no need for the infrastructure. And also there's no additional charge for ML flow. You can simply use Sagemaker and log all the metrics in severalSML flow without worrying about any pricing implications. Surveless ML flow is also deeply integrated into your experience. So last we looked at the fine tuning experience where we kicked off a job. Once the job is complete or started execution, you will see on the model details you will start to see the performance metrics, and ML flow is deeply embedded in here so you can navigate from here into ML flow and see much more rich metrics if you want to take a deeper look. It's easily accessible under applications in case you want to leverage it for other workers as well. So you can now see under a particular run all the experiments that you ran on different fine tuning jobs, you can compare and contrast those metrics and have a deeper look and then choose the right fine tuning model for you after you've chosen the fine tuning model here. You can, uh, you can start to evaluate that model. Today, we are also announcing the launch of severalless model evaluation inside studio. It, uh, we, as part of this experience, we offer popular benchmarks in the industry that you can use to, uh, to evaluate your models. This experience is also completelyurized so you don't need to worry about managing any instance types. You simply kick off an evaluation job and uh the execution is done for you on your behalf. Let's take a look at how this experience works. So on the evaluation experience, you can simply choose one of the techniques. In this case we'll choose a as a judge. It allows you to also define which metrics you want to use, so you can choose the right quality metrics. These are the most commonly used industry benchmarks, and you can, because responsible AI has now become such a critical aspect of shipping any models, we also offer some of these, uh, metrics out of the box that allow you to measure and put guardrails on your content. When you spin up an evaluation metric, you can provide a prompt template as well there so that the model that you chose know exactly how to evaluate these requests. Once you kick off these metrics. You will see the results for not only the fine-tuned models but also for the base model, both for the quality metrics as well as for the responsible AI content. This helps you to make decision on whether your model is actually performing better on both dimensions much more easily. Once you've deployed these models into production and if you are also building out an agenttic application on the top, if you're just building an agentic application, agent observability already uh is integrated into cloudWatch so you can have all the dashboards to monitor the traces there. But if you're also leveraging model customization. You can use agent core observability to emit metrics in open telemetry format into managed ML flow or partner AI apps. We'll take a quick look at how you can use those metrics in manage ML flow and partner AI apps. So here's a screenshot for what the experience would look like with managed ML flow. On the left side you will see a complete trace tree starting from invoking the agent at the top and drilling down through the workflow build process capturing each lang lang chain operation and showing tool calls. And the calls to the multiple assistant interactions, assistant 1, assistant 2, and this hierarchical view gives you the complete visibility into each step of the agent. We also offer partner applications as managed capabilities on SageMaker so you can simply launch them and get started. So some customers that choose to use Comet ML for tracking their experiments, evaluating the models can also log these metrics into Comet ML and then use that information for both agent as well as ML to debunk the root cause. Comet also offers certain key capabilities like. Ability to optimize the prompts by giving the uh the goal and also optimizing the agents. So it takes the end goal and continues to run the evaluations for you and optimize the prompts till you have the best prompt. We also enable deep checks that allows you to test, evaluate, and monitor LLM maps as well as agents. It provides a much more comprehensive view. You can also manually and automatically annotate all the interactions with the LLM and get a much more comprehensive report which allows you to much more easily root cause and debug issues. And lastly, we also have Fiddler that allows you to do agentic observability and then uh uh root cause it back down to like every single model customization experience. StageMaker also offers capabilities to deploy your models and today we're announcing it's easy integration with uh Bedrock so you can simply from your StageMaker studio kick off a job to deploy the models on Bedrock. Along with that you can continue to leverage SageMaker endpoints for deploying multiple uh adapters for your model onto the same endpoint. We'll take a little bit of a deeper dive into this capability in a minute that can help you save on your costs, and you can use different techniques to further optimize the performance of this model. There are various different techniques offered and we'll do a deeper dive in a bit on speculative decoding, which is one of the key techniques to minimize any latency for your applications. So let's take a look at the first key benefit for SageMaker endpoint. You can deploy multiple foundation models on the same endpoint. This ensures you have one endpoint and as your use cases continue to grow, you are just continue to use one endpoint. This can help significantly reduce your cost. And as the traffic grows, Sagemaker offers capability to emit metrics for each foundation model, so you can build out auto scaling policies for every single model, and then as the traffic grows, that particular foundation model scales to other instances, making sure your costs are minimized. This can help you save on the cost up to 50%. And it has recently launched certain capabilities that allow us to cache the model weights, which ensures that the auto scaling is much more faster. Next, let's take a look at one of the key techniques that we recently launched to help you optimize the latency for your applications. So typically customer expectation for inference is that it's fast, however, foundation models generate one token at a time, which leads to a slower inference. The way speculative decoding works, you have a foundation model and a draft model. The draft model is typically a smaller model. Draft model generates some tokens. For foundation model to review, Foundation model then reviews those tokens and assigns probability and then accept certain token and rejects them. So let's take an example here, uh, on how that would work. So let's put a prompt here. The CAT draft model generates the response for that token. Foundation model evaluates these tokens and accepts some of them. This is a simplistic view. You can use the draft model to actually generate multiple variations of that, and the capability that we launched recently allows you to not only just generate these tokens, but also to fine tune the draft model on your traffic, which helps you get higher quality responses from the draft model. So let's take a look at how that capability works. Uh, so this leads to latency reduction to up to 2.5x throughput without losing any accuracy because you're able to fine tune the draft model on your traffic. So let's take a look at how it works. You can bring your own data set or you can use one of the Sagemaker curated data sets. You can kick off the fine tuning job on the draft model. It's an async job that runs in the background. Once the job is complete, it publishes the evaluation metrics. You can review the evaluation metrics and choose to deploy the draft model on the same endpoint, so there's no impact on your cost, and the draft model gets deployed on the same endpoint. And then the draft model generates these tokens that foundation model can evaluate and accept so you don't lose any accuracy and continue to see this latency improvement. Lastly, we also launched new capabilities that allow you to not only track models but also generative AI assets like data sets, reward functions. It similar to models now you can track all the um data sets as well as validators that Shelby will walk through in her demo and you can not only track these assets but you can also track lineage so you can track each version and the lineage for all of these assets. That note, now I'll hand it over to Shelby to walk us through all these capabilities. All right. All right, let me swap over. All right, so, 00, the thing. By the way, they do, um. What happened to my screen? There it went. Gosh, connections. OK. Sorry about that, but thank you all for joining. I think we're probably standing in between you and lunch or maybe you've already had lunch, but thank you for joining anyway. So for this demo, what we're gonna go through is a couple of things, right? It's gonna be an end to end demo. We're gonna look at model customization and then uh integrating that model into an agent. Uh, that being said, we're gonna use a use case, a really simplistic use case of a medical triage agent. Oh goodness. The demo gods hate me today, uh, but we're gonna do a medical triage agent. So we're going to take an open source model basically customize it or adapt it to our specific task, and in this case it's gonna be an agent that is able to triage medical symptoms prior to allowing a patient to book an appointment and or paging an on-call physician. So it's gonna be in two parts. We're gonna start first with model customization, focusing on the newer model customization experience, and then in the second part, we'll look at how to integrate that with agents, specifically Agent Core. And in this specific demo we're gonna focus a little bit more on kind of the end to end capabilities. Keep in mind over the next few days there's gonna be a lot more sessions that dive deeper into different, um, specifics of the new model customization experience from like the different techniques as well as the different evaluation techniques as well. So that being said, let's get started and hope the Wi Fi is going to work out for me. So like Ahmet mentioned, there is assets. So assets is new in being able to track and manage different assets that are part of your experiments. Data sets is a key asset. So you'll see inside if we click on data sets, basically, it'll show all the data sets that you've uploaded that are available for use for different fine tuning workloads. You can also see inside there maintaining multiple versions, right, across different experiments. You may have multiple different versions of the uh process data that you'll utilize as input into your fine tuning jobs. So to upload a data set, you just click upload data set. Enter a descriptive name. And then you can either upload directly from your local computer or from an S3 location, right? If you already have your data in S3 that you've put, you can upload it from S3. In this case, we'll just do a local upload from my computer. And then you just hit stay or save. You can hit save and then hit create. Um, and what that does basically you'll see inside here now that data set is uploaded, you can upload new versions as you iterate through your experiments. Um, so that'll be used as our input into the model customization job that will kick off. The other thing I want to point out prior, I mean it talked about new serverless ML flow. So you can see if you're used to the interface, you'll now see a new app servers tab inside MO Flow, and this is where all of your serverless MO Flow app servers reside. You can see here I have a default one, but I also have a custom one that I made for this demo. So we'll specifically use that one. And what will end up happening and you'll see through the model customization demo is will all of the model performance metrics are gonna automatically log into ML flow as well as all the evaluation metrics that you run against your fine-tuned models will automatically run into ML flow, which makes it really easy for comparison, visualization across experiments to understand which model you eventually want to deploy and test out with your agent integrations or deploy into high level environments. So that being said, let's go ahead into the new customization experience. So you'll see inside here, there's a range of models that you can customize, including the old existing, not old, gosh, the previous existing models, right? Same with MLDL, all those models are all in one spot. All the models that you have customized will end up over here in my models. So you can see all the models that you've done customization with over in my models, get details about them, and we'll go into this later. But let's go ahead and kick off a model customization job. As you can imagine, it takes a little bit of time to run these jobs, so I do have some pre-baked models, pre-baked versions that we'll use in, in cooking show fashion. Um, but to start with, you basically just hit customize so you can customize through the UI, which is what we're gonna do in the demo, but you can also customize through AI agent, which was announced this morning. Uh, it is in preview, but the ability to use natural language to kind of develop a guided workflow for fine tuning customization. Uh, you can also customize through code as well. So all of this does have an SDK available for those that kind of prefer a programmatic experience. That being said, let's go to customize through UI. So basically, you'll just enter a. Uh, descriptive, uh, descriptive name for your model, and I just call it medical triage. I have a couple in here. And here is where you can see the different customization techniques. And like I said, through the different sessions over today, tomorrow, they're gonna go into more detail on a lot of these different customization techniques. But you can see the ones out of the box today are supervised fine tuning. We have DPO, we also have reinforcement learning with verifiable rewards as well as reinforcement learning with AI feedback. In this case, we're just gonna do supervised fine tuning for today. So you click what customization technique you want to use. You can upload your data set here or in this case, we're just gonna point to that data set that we just uploaded before. And then the version that you want to use too. We only have one version in this case. And then here's where you can modify some of the hyperparametters in the configurations across your different training experiments. In this case, I'm gonna just go with the default parameters out of the box, but you can modify the different hyperparametters here. And then we'll also point to the ML Flow app that you want to use. So you can see again, here's the default one that's created, and then here is the one that I created specific for this session today. And then you can of course adjust the experiment name, have more meaningful titles inside there so it's easier to find inside MO flow. Here's also where you can adjust the security settings, um, as usual, right? Although it's serverless training under the covers, you can also specify to run inside your VPC, also specify the type of encryption that you want to use on the volumes. And then just submit. And what that does, basically, it'll kick off a serviceless training job. So you can kind of see one in progress here. Um, and as it kicks off and starts going, you have the logs here. They're not available yet, but the logs there to watch and if you wanna like do things like early stopping and that sort of thing. That being said, let's look at some pre-baked versions. So I'm gonna go over here. I'm gonna look at this one. This one was trained on a full data set. In this case, we're just using an open data set, which is trained on some, uh, medical, basically medical symptoms and then medical, medical kind of, uh, diagnosis. Let's view the latest version. So here's where you can see inside. These are all your different versions. You can move between versions, you know, clearly, as well as different tabs. There's performance, you can see in this case, it went off the rails a little bit, probably overfitting a smidge. Um, if I'd been watching it, I could have done early stopping and, but I can also tweak the hyperparametters a bit too, right, to avoid some of that. Also evaluations. So you can see here this is where the evaluation jobs, and there's a lot of different evaluations available. In this case, we ran some out of the box benchmarks as well as another full evaluation. But if you want to run an evaluation, basically you just click evaluate, set up a descriptive name again, and there are the three evaluation types like Emit talked about. So there's LLM as a judge, and within that you get to specify which model you want to have as your judge model. And then with that you also specify the metrics that you want to evaluate against, as well as the ability to bring your own custom metrics as well, and LM is a judge. The other kind is custom score. So this is where you can bring your own custom scoring code. You can also use a couple of the built-in metrics specifically, they have some around code execution and math answers. Um, in this case we're gonna just use some out of the box, out of the box benchmarks to demo. Specifically, we'll use multitask language understanding and we'll narrow that down more specific to the task that we're trying to fine tune for which is inside like the medical domain. The other thing I'm gonna click to is compare against the base model. So this is important when you want to evaluate against the base model to see if it makes sense even to do fine tuning, right? So you wanna compare it against that base model. In this case we're using Quinn 2.5, uh, so we wanna compare all our evaluation metrics against that base model to make sure that we are actually making progress on having a more fine-tuned model. So in this case, we'll, it does test across 10 subjects. It's not super useful in our use case. So what we'll do is we'll narrow it down. To clinical knowledge. And then you can also specify advanced configuration parameters as well within your evaluation, so things like top P, K or top K, as well as security settings again, right? So the ability to run these evaluation jobs, they are a surplus, they run in the background, but you still have the ability to run those inside your VPC as well as specify the type of encryption. So that being said, I'm just gonna hit submit. So once you submit, you can see what it does is automatically create an evaluation pipeline for you behind the scenes. You don't have to deal with the pipeline that gets created, but it takes care of those steps that are involved with passing data in, passing it out, publishing those metrics into ML flow. So that being said, so we've done our training, right? Let's assume we've done our training. We've done some evaluation. Let's look at some of the metrics that flow over into ML flow automatically. So first we'll look at the model performance metrics. These are just the pre-baked versions that I have, um, but the from the actual training performance themselves. So let's go ahead and just kind of compare against the four versions that we have. And in here you'll see more of like a table format, right, of the different versions and some of the metrics that are captured during your training cycles. So things, um, and as well as your hyperparameter. So like how many epochs you ran with, like the loss, uh, validation loss, test loss, all of those different metrics. But you can also go into. Those same models. And actually do more kind of visual comparisons, which are, are helpful in evaluating against each other. So let me Nope, sorry. It's the Danger of live demos. All right. Oh, sorry. All right. So inside here, here's the visualization. So this is where you can visualize across the different versions, um, to like compare the loss, uh, that you're seeing across all the different iterations of fine tuning. So that being said, so this is the model performance itself, right? And then there's the evaluation metrics that we just saw too where we use the MMLU clinical knowledge benchmark to benchmark against. So you can also compare those metrics inside here as well. So to do that, I'm just gonna hide the model performance metrics. We could keep them on, but just to kind of highlight. Here are the model performance metrics and you'll notice I'm also highlighting the base model too because we wanna be able to understand is this model actually performing better than the base model, the base queen model, um, for the particular task that we're trying to solve for. And in this case we're using that there's clinical custom you'll see inside here there's LLM as a judge, there's, uh, MLU MMLU college knowledge as well. In reality, you're gonna look at this model across a bunch of different performance metrics, right? This is just one example you'll see. Uh, where we're basically doing it across. One of these evaluation metrics. So you can kind of see inside here. Oops. In this particular case, this first model here, the first version is performing best against that particular benchmark, right? The clinical knowledge benchmark, um, so that way you can do that across different metrics, across different evaluations to ultimately decide which model you want to test out a little bit more within your agentic workflows and or move into production. So that being said, now that we have done evaluation, let's go back to our model again. And now we can go into deployment. So assuming you want to deploy this, integrate it with agents, you would go here into deployment. And then just click deploy and here is where Emmit mentioned you can deploy to either Sage Maker or Bedrock, which is a very nice feature. Let's assume in this case, so Bedrock would be through custom model import, right? Um, through SageMaker, let's just assume we're gonna create a new endpoint. So here you just enter a name, uh, you can choose the instance type to still, right? Like you can today, or just accept the default recommendation. Again, the advanced options are available to you in terms of the max instance count, the security settings, all of those different items, and then just hit deploy. And what that'll do is deploy a Sagemaker endpoint behind the scenes that's now available and ready for use and integration into any agent workflows or just direct application integration. The other thing to point out inside here is the lineage tab, which Emmett talked about. So the nice part about all of these different tasks and steps that are leading up to it, because there's a lot of moving pieces as you can see from the training to all the evaluations that have to happen here you're able to track and maintain complete lineage. So, as an example, if we click on the first one, model artifacts, in this case, it is the base Queen model, right? So you can see exactly what version of the Queen model was used for this fine tuning. And then as you kind of progress along through, like you can see the training job that was used, all the metadata is stored and collected that contains the complete lineage. And you'll see it tracks all the way back. Through pipeline execution and deployment in this case, we didn't, the deployment isn't actually finished yet, but it does track all the way through to deployment, which is really nice for traceability and maintaining that end to end lineage. So that being said, let's just assume we have it deployed out and now we're gonna integrate it with our agent workflow. So these are the steps that we just took, right? Doing all the experimentation, ultimately deploying to Sagemaker AI. Again, you could deploy to Bedrock. It depends on what you're looking for. Some of the adapter-based inferences, really nice cost compelling feature, especially for fine tuning, but you can also, uh, import into Bedrock too. So in this case, what we're gonna do is we're going to take that Sagemaker endpoint that is now hosting our fine-tuned model that's specifically adapted for the task inside the medical domain that we're looking for. And we're gonna assume this is experimentation at this point and use the strands SDK. Keep in mind when we're going through this, you could just as easily use any of the other frameworks that are supported by Agent Core, whether it's, you know, Lanegraf or Crew AI. I, I'm just showcasing it with strands. So here what we're gonna do is create an agent and we're gonna create a couple of dummy tools on the back end and see if the agent's able to make smarter decisions about whether those symptoms are urgent and potentially need to page the on call or go to urgent care or if they're gonna go ahead and let them book an appointment that's a less urgent appointment. And we're gonna also look at ML Flow on the back end for agent traces. One thing to keep in mind, uh, so ML Flow does do agent traces, also agent evaluations, and it kind of depends because also Bedrock Agent Corp has great built-in observability features as well as evaluations that were announced yesterday, I think. Um, so it kind of depends. It, it memo flow may be a good option if you are dealing with models and agents, whereas if you're only dealing with agents, uh, agent core using those native built-in capabilities may more, make more sense for your use case. So that being said, there's a bunch of dependencies installations, but one thing I wanted to show is this is the model configuration with strands SDK. So, just like a Bedrock API kind of endpoint, you can, uh, create a model that is based on a SageMaker endpoint as well. So here you'll see, actually, let me blow this up a little bit. So you'll see there's a specific integration with strands called SageMaker AI model where you specify the endpoint name as well as the inference component name in the case of where you're taking advantage of that adapter-based inference. And then we're just gonna create a super simple agent, no tools behind the scenes, just to see how our model performs against the task, uh, and it's the fine-tuned model, right, without tools. So here you'll just say I need to book an appointment, um. It does a reasonable job, right, in responding, but it doesn't necessarily have the, the smarts to actually go and do anything about it, right? To actually look for availability or page an on-call doctor, that sort of thing. So what we'll do, we'll implement some conversation management, right? And this is just, again, part of the strands SDK. So some conversation management, uh, async operators for streaming, so maintaining some of those conversations. And then we'll again. Try to hit our endpoint on the back end and you'll see you just, you know, get a response again, uh, saying go to the nearest emergency room, that sort of thing. But once again, there is no actual tools to be able to take any action on anything. So what we'll do to fix that is add some tools. You'll see these are some dummy implementations of tools. We have a booking availability tool. We have a booking tool and then a page on call tool. But what we really want to see is if our agent is smart enough to select the tool, right? Based on kind of the input that is provided. So you'll see with the addition of tools, some additional prompt engineering around that. We'll then actually build out the agent. One thing we're gonna do before that is with our more kind of advanced prompt is register the system prompt. So one of the common challenges we hear from customers is the ability to kind of share and manage prompts that are used as part of your applications. Uh, MO Flu does have prompt management capabilities within it. So once we've settled on a, a prompt, a reasonable prompt prompt that we're going to use for some testing cycles, we can then configure, um, that system prompt inside ML flow. So this code here, you can see is basically just registering that prompt into ML flow. And you'll see here's the prompt name. And if we go over and look at it inside ML Flow. This one's the test prompt. You can see there's, this only has one version, but what it'll do is maintain multiple versions of that prompt. What's nice about that is, I don't know about you all, but like sometimes when we get in a room and we're like building out something, everyone's just sharing prompts through Slack or whatever the case is, uh, this allows you to actually one version it, but also share it more easily, right? So you can actually share your prompts across your team. Um, in the production case, it's pretty critical for like tracing lineage though, right? In this case, we're just saying it's a test prompt and we're, we're doing some versioning. So now that we've done that, we've created our prompt, our prompt alias will now create the agent using our tools, right? So in this case, we're gonna take those dummy tools that we created and make those available to our agent. And again, our agent is backed by that fine fine-tuned model on the back end. So to do that, we'll just basically make the tools available here. So this is our booking availability, booking, our paging our on call. And then we'll again invoke our agent using tools this time though. So in this case, I need to book an appointment. And here's the agent response. It's directed right in our prompt. We've told it like you need to make sure that you gather the symptoms before actually letting a patient go ahead and book. So now you can see we're getting good responses, but one of the things in ML Flow that you can do is also tracing of agents to be able to verify that those tools are being called, right, which is really important as you're, I'm sure a lot of you are building agents today and being able to trace the thought process, when it's calling a tool, when it's not, when it's pretending to, uh, is really important. So if we look at some of the agent tracing inside ML Flow. You can kind of see, you know, here, it sounds like you're. You know, experiencing it for, uh, for days. It's a, a medical issue. Um, here you can see where this is just the output to that. You can see in different cases. Booking availability somewhere in here. Oh, no wonder. So sorry, I'm in the wrong. Experiment. Is this the one? Yeah, there we go, sorry. Um, so here you can see executing on call. So there is a medical emergency, so you can see the tool was actually called. And here you just see like where it's interacting obviously with your model behind the vaccines, but then where the tool is actually called on the back end. So the tracing is really, uh, very valuable in debugging to know when a tool is being called, when it's not, and what the actual logic flow of your application is. And again, like I mentioned, you can use agent core observability and that sort of thing as well, now that they've kind of released there. This is just nice when you do need to merge together your model experimentation, all the metrics between models and agents. So that being said, so we developed it on strands first, which is pretty common, right, for development, but now we want to deploy it so it's scalable. So using Agent Core runtime to actually host that agent. So in this case, what we're gonna do, it looks very similar to the last one, but with Agent Core, we're gonna use the runtime agent, like I said, to host the agent, but also Agent core identity to manage the user interactions. So to do that, we just write out our agent code, right? So it's Python file of our agent code. And again, we're using the Sagemaker endpoint name, as you saw before. So we're actually pointing to a Sagemaker model on the back end. We're making our dummy tools available as well and setting up some Cognito user pools and authentication. Here is where we'll register the system prompt for production, which is something I definitely recommend, um, for just kind of reliability and knowing what prompts, what prompt versions are attached to any given agent, um, so we'll register the prompt again, but in this case we're gonna register it as a production prompt. So you'll see here again, uh, it's only one version, but it can maintain multiple versions, right? So you see here, this is the, the agent prompt that is tied to that particular agent. Sorry, it's a lot to scroll through. And then we're gonna deploy our agent. So we're deploying our agent again to Agent Corp run time. In this case, I'm using the, the helper tool, right, to launch the agent, but you'll see inside here we're pointing to our entry point in our Python code. We're also pointing to the agent name will be Agent Prod, uh, medical triage agent Prod, and then using the authorization config. And then here's where we're actually launching that. So once it's actually launched, we can then, uh, test out our endpoint. One of the things I would recommend doing in terms of traceability, most of the time this is gonna be deployed on a CD pipeline in the back end, right? In reality, versus deploying this out of a notebook when you're ready to go to production, um, but I would recommend tagging that Sagemaker endpoint with the agent R and the agent ID that comes back so that way you know for any given endpoint out there, here's the agents that are using or depending on that endpoint because those dependencies are tough to maintain if you don't have that level of tracking. So here we're just tagging the endpoints with the agent R and the agent ID. And then here you can see kind of the list tags. So you can see where we've tagged it. And then we're just going to use the agent. So in using the agent, we'll do the same thing, right? We're going to invoke the agent. We have some invoke agent code code invoke endpoint code. And then we're just gonna send prompts in like we did before, but this time it's running on scalable agent core runtime, uh, compute. And again we'll go back it's gonna auto because we set it up for MO flow for agent tracing. You can go back into MO flow and see the production level traces as well. And the same thing basically, right? You can see when it's calling different tools, when it's not, and collecting that automatically. So this is just going through some iterations, making sure that we call it when we should and not when we shouldn't, right? So like a stuffy nose, that really probably shouldn't be an urgent call, right? So in this case, they're basically just saying, you know, uh, it's not available for today, we can book it for another time because it's not a medical emergency. So that's kind of it. That's basically showing kind of an end to end of using a fine-tuned model that's adapted to a specific task within your domain, um, to get better performance as well as integrating it with an agent workflow. One of the things to keep in mind, we talked about lineage, we talked about traceability, right? What you saw on model customization, it has the nice lineage that's automatically captured. Some additional things to keep in mind though with your agent workflows is I, I showed it in a notebook today, right? Uh, which is great for experimentation. In reality, like I said, you're probably gonna include that in like a continuous delivery, continuous deployment pipeline when you go to production deployment for agents. And there are, as you can see inside here, a lot of different versions to consider. The nice part, that left side of the picture is all taken care of with model customization now with lineage, that full end end lineage. On the right hand side, just something to keep in mind with your CD deployment pipelines capturing all those different versions across agents too, and we did a very, very simple agent, right? Um, let alone like multi-agent orchestration workflows, so capturing all those versions and the dependencies so that you know. Like we did tag the endpoint as one example. So you know, with that endpoint, we have 10 number of agents dependent on that endpoint. So that being said, we went through a lot, uh, in a short amount of time, but yeah, thank you all so much. I'm gonna turn it back over to Ammit. Oh, actually, one more thing. If you want the code examples. I will say I could not push those before this meeting because I had a meeting. I will push them today because I had new launch things in it that I couldn't push. So apologies. But yeah, if you just. Get a screenshot of that. I'll, I'll post the code by the end of today. All right, admit. Oh. Thank you Chaubli uh. We'll do a quick recap and then we'll open the floor for like any questions that you may have. So we Oh yeah, sure, OK. OK OK, just to do a quick recap of what we covered today firstageMaker now offers fully managed and serverless experience for customizing your models from training to evaluation and then deploying the models. With AI SageMaker AI inference capabilities, you can host multiple models on the same endpoint, auto-scale easily, and use the optimization techniques to make it cost effective for you. You can get end to end observability for your model customization as well as agents with a managed ML flow, server SML flow, or partner AI apps, and you can build scalable and repeatable workflows with SageMaker AI pipelines using the new steps that we launched today. And lastly, you can also now not only track model uh and uh track and audit the versions of the models, but also of the AI assets that are used in the model customization. Here's the barcode for the same storyboard. Uh, it's not the exact demo, but it's the storyboard. It walks you through the example step by step. Uh, feel free to check it out. And if you have any questions, there are 2 mics over there in the center of the room. Uh, feel free to bring up the questions, Ms. All right, cool, thank you all. Thank you, thank you, folks.