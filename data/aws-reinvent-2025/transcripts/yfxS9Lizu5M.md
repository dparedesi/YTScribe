---
video_id: yfxS9Lizu5M
video_url: https://www.youtube.com/watch?v=yfxS9Lizu5M
is_generated: False
is_translatable: True
summary: "This session, \"Hassle-free multicloud connectivity with AWS Interconnect - Multicloud (NET205),\" introduces AWS Interconnect, a new service launched in partnership with Google Cloud to simplify private, secure, and scalable multi-cloud connectivity. Alex, an AWS Networking Specialist Solutions Architect, along with Santiago Resco (AWS) and Judy (Google Cloud), detail the challenges of existing multi-cloud solutions like VPNs, co-location facilities, and third-party fabrics, which often involve complex routing, management overhead, and scalability issues.\n\nThe core of the presentation is the unveiling of AWS Interconnect, which provides a direct attachment between AWS (via Direct Connect Gateway) and Google Cloud (via Cloud Router), abstracting the complexity of underlying routers and physical connections. The service features built-in resiliency with multiple logical connections across different routers, automated encryption using MACsec, and dynamic scaling of bandwidth. A key highlight is the collaboration between AWS and Google Cloud to co-develop an open-source \"East-West API\" specification, allowing seamless orchestration of resources and maintenance coordination between the two providers.\n\nThe speakers walk through the technical architecture, explaining how the service provisions connections using the \"least utilized interconnect\" to ensure optimal distribution and redundancy. They demonstrate the user experience, which involves a simple create-and-accept flow in both AWS and Google Cloud consoles, eliminating the need for complex BGP configuration by the customer. The session also covers various deployment scenarios, from single-region setups to complex multi-region architectures using AWS Cloud WAN and Transit Gateway, illustrating how routing and route propagation work automatically. The service is currently in public preview in 5 regions, offering 1Gbps connections at no charge during the preview period, with plans to expand globally and add support for Azure in 2026."\nkeywords: AWS Interconnect, Multi-cloud Connectivity, Google Cloud, Direct Connect Gateway, Cloud Router, MACsec, Cross-Cloud Interconnect, AWS Cloud WAN, BGP, Network Resiliency\n---

Line up for the session. Um, hello everyone and welcome to Reinvent 2025 day one plus two, right, because there's no day three. I'm Alex. I'm a networking specialist solutions architect, uh, with AWBS, uh, and I'm joined today on stage by Santiago. You wanna introduce yourself? Sure, my name is Santiago Resco. I'm a senior product manager for AWS Direct Connect and now AWS Interconnect. Awesome. And by Judy. I'm Judy. I'm a senior product manager at Google Cloud, and I work on the Cloud Interconnect product that has been named that before, uh, Santiago chose this name. Cool. So this is a first, and we're gonna have many of those as we do the presentation, yeah, for sure, uh, so yeah, we're gonna be talking about it via Central Connect, uh, a service that we launched together with, uh, with, uh, Google Cloud in preview on Sunday. Uh, we're gonna dive deeper into, uh, the architectures and how the service works, but before we begin. Uh, this is a 200 level session. However, at some point, uh, you'll see that we're gonna dive into like level 400 routing. So for those of you, uh, here who are familiar with BGP and routing protocols, that's gonna be fun. Um, we'll let you know when buildouts are complete for photos, so we'll pause for a second, uh, on a slide if you folks wanna take photos, but just keep in mind that the session is recorded. Um, it is going to be on YouTube, so slides are gonna be published, so, uh, uh, no worries about taking photos of, uh, every single slide, uh, and last but not least, this is a breakout session, so we won't have Q&A during the session, uh, but we can meet you folks outside. We can have Q&A there and we can dive deeper with you all, uh, about all topics. Now, uh, first and foremost, uh, before we dive deeper into, uh, AWS Interconnect and how it's built and what it's, uh, what it's useful for, let's set up the baseline for AWS networking foundations. Uh, first and foremost, you have the Amazon VPC, which is a regional construct, uh, unlike Google Cloud VPC, which is a global construct, so keep that in mind. Uh, this is the network level boundary where you deploy your workloads. VPCs are account level constructs. Within your VPCs you can create, uh, subnets in availability zones. These subnets can be IPV4, can be IPV6, uh, can be dual stack. Uh, subnets are bound to availability zones, right, and subnet sliders are from the Amazon VPC, uh, prefixes in IPV4 and IPV6, right? So that's what you use to create these subnets. Uh, now to connect many VPCs in a region you can use VPC peering, uh, at a smaller scale, right, which is direct, uh, VPC to VPC communication at layer 3. You can also use AWS Transit Gateway, which is the, uh, AWS routing hub in a, uh, in a region. Now I'm sure that some of you are familiar with these constructs, some of you are not. Uh, for those of you who are hearing about all of these for the first time, I would highly recommend, uh, diving a bit deeper into after this session into all of these. So Transit gateway Regional routing hub, right, um, now if you want to connect globally on AWS, uh, and in your hybrid network, uh, we have what we call AWS Cloud One which is a fully managed, uh, global networking service. Cloud One comes with the concept of segments, uh, you see there as a, as an example, production development. These are global, uh, spanning network layer 3 segmentation, uh, constructs. They're very similar to your layer 3 VPNs and MPLS, right? Uh, they're fully managed by AWS Cloud One. Cloud One under the hood in every single region maintains a core network edge which is actually very similar to a transit gateway. It just has more capabilities. It's smarter and knows BGP. Uh, BTP, by the way, Border Gateway Protocol, for those of you, uh, who, uh, are not familiar with the acronym. Now Cloud one supports many attachment types, right? These things that you can connect to cloud one, VPCs on one hand, um, you can have connect attachment types to integrate your SDWAN appliances, um, and you can actually attach transit gateways in region to facilitate integrating existing environments and transit gateway with cloud one, right. But not all workloads live in uh AWS and not all workloads live in the cloud so we have hybrid connectivity options for uh for integrating your hybrid workloads. The first one being AWS site to site VPN. You can terminate the side to side VPN connections, uh, on directly on your VPCs through a construct that's called virtual private gateway, right? If you attach a VPN to it, many of you call it VPN gateway. It's all good, same thing, uh, we will meet again with the virtual private gateway, uh, throughout the session, so keep it, uh, keep it in mind. And if you are using, uh, regional or global routing constructs like transit gateway or cloud one. You would be attaching uh these VPN connections to uh these uh constructs. Now a bit more on uh VPN side to side VPN um supports uh IPEC, so essentially uh it's um uh it's a well known standard protocol. Uh, we've recently launched large bandwidth tunnels, uh, in, uh, side to side VPN so you can have now up to 5 gigabits per second of throughput, and if you attach your VPNs to, uh, AWS Transit gateway or a cloud one, you can actually use equal cost multi-path, right? So you can have many VPNs and you can use all of them to split your traffic. Now another, uh, very, very, uh, well known, uh, hybrid connectivity option is direct connect, right? AWS Direct Connect, which allows you to connect to your on premises workload. Um, these are dedicated connections that you build, um, with the help of, um, either your routers or cus uh partner routers, uh, in direct connect facilities, right. So what you see uh here on the slide is this line between the DX router and the customer router could be customer could be partner router. That is the direct connect connection. It's a physical connection right dedicated to you. Now on top of this direct connect connection you have to have that layer 3, connectivity, right? So routing BGP sessions and so on to help facilitate that. Um, I mentioned we're gonna meet again with the virtual private gateway so you see it up here, but, um, because connectivity is in nature not single region usually so customers come to AWS and span across multiple regions, we have the AWS direct connect gateway. Which is a global construct, uh, it's a highly available construct is as available as the virtual interfaces that you apply or attach to it, um, and talking about virtual interfaces, these are, uh, what you get to attach to either your, uh, virtual private gateway, and that is the private virtual interface, um, to your VPCs, uh, directly or to your transit gateways, uh, or cloud RA through a direct connect gateway. What is the role of this direct connect gateway here? Well, it allows you to attach either VGWs, so, uh, virtual private gateways, or transit gateways or core network edges in cloud one across many regions, right, because it's a global construct. Keep that in mind because that's gonna be the answer to your questions which you probably have asked yourself for the past couple of days of how do I achieve global connectivity with this thing, right, that's now available. Uh, so I mentioned private virtual interfaces. You also have transit virtual interfaces that connect to, uh, the direct net gateway that on the other side has a TGW or cloud one, or public virtual interfaces that allow you to connect directly to AWS public endpoints. Now we've, uh, we've talked about Direct Connect, a couple of, uh, more in depth details about it because it is going to be the foundations based on which AWS Interconnect is built. Um, you can have dedicated connections or hosted connections, uh, in Direct Connect, uh, for virtual interfaces, and you can create highly resilient architectures, either 3 9s or 4.9s of availability depending on your, uh, workload types. Now we are talking uh multi-cloud connectivity, right? So, uh, let's talk a bit about the background and how customers, how you all, uh, used to build that multi-cloud connectivity, uh, so far. So the background is the customers want this, right? You want this, this ability to connect privately securely, um, your workloads deployed in AWS with your workloads deployed in other clouds, and that needs to be scalable, right? Let's see how you used to do this first using site to side VPN, right? We've talked about this, uh, we've seen how it works, uh, now, uh, that site to side VPN, the customer gateway side of it instead of living in your data center being a router, physical router, could be on the other side, the other cloud service provider service, right? So you can have this, uh, uh, natively integrated. There was a nice comment, uh, that, uh, that came as we launched AWS Interconnect. Uh, that somehow, uh, thousands of VPN connections were a disturbance in the force. So I don't know if that person who made the comment is in the room today, but I really loved it, and yes, they're pretty challenging in maintaining and making sure they're highly available and resilient. The next option is routing through a co-location facility using Direct Connect and the other cloud service provider, uh, analogous of that, right? So you would have a, uh, point of presence where you aggregate Direct Connect. You aggregate whatever, uh, the other cloud service provider supports, and you essentially do layer 3 routing by hairpinning through either your router or a partner router, right? Pretty straightforward. How many of you folks are using this? OK, OK, cool. It comes with its own challenges, right? We're gonna talk about the challenges in a second. Um, so one of the most important parts here is that you do have to have a cloud service provider alignment, right, so they need to come in that same point of presence, or you need to have a backbone, right, to route between whatever you aggregate, uh, AWS to whatever you aggregate other cloud service providers. Now the third option is using a third party fabric right through uh Direct Connect so using a partner that provides you with a uh fabric uh that gives you layer 3 connectivity and you don't have to worry about maintaining your own backbone, right? Pretty, uh, standard option. Now last but not least, I wanted to bring up the overlay solution because I've heard this from a lot of you in the past days. Um, it's pretty common for, uh, for customers to build overlay solutions on top of a layer 3 connectivity option so it's not in the same bucket as the other three, right? It's an overlay that runs on top of whatever layer 3 connectivity exists there. Uh, and you can do this with, uh, third party partners, uh, that provide appliance based solutions, right? Reasons for doing this are not in the scope of this, uh, this talk. But we know for a fact that there are challenges, right? And this is what we started working backwards from. We have scalability management overhead, additional, uh, points of failure that you have in the path that you need to monitor and manage, support and troubleshooting who's running multi-cloud networks here knows that if A in one cloud talks to B in a different cloud and all of a sudden. That flow doesn't work usually the network team is called and it's like, oh it's the network, and then you start troubleshooting and trying to figure out where on that path, uh, is, uh, the problem and you start opening support cases to all the cloud providers including to the third party that you're using to connect them in between. Um, and consistent global coverage, uh, you may, uh, you may be using partners, you may be using your own backbone, and you have to always make sure you have that consistent presence in the regions that you want to expand to or connect. So I think it's pretty clear what the challenges are, right Santiago, but um how did we solve for this? Yeah, very good question. Thank you, Alex. So, As Alek mentioned, um, Multi-cloud networking, well, it's hard. Um, on the previous slide we saw some of the challenges. The biggest one is actually geography, um, for folks who are in multiple regions, multiple geographies, um, sometimes you have a great setup with a partner in EMEA and a great partner in APAC. Now you need to even get those two to talk to each other. It's very hard. So, all these problems um were sort of uh pointed out to us through many, many customer conversations, so we work backwards in AWS so that's our problem, we need to solve that, right? So how did we do that? So we had a vision. The vision was, um, this shouldn't be hard. It is very hard, but it shouldn't. This architecture you see in the slide, that's the ideal. It should be an attachment between two things, uh, folks here doing like cross-repairing, you're probably familiar with this. We don't ask you to reason about routers, but I can promise you there could be 100 routers between two regions, but we don't ask you to reason about them, we just You know, say VPC attached to the RVPC and you're done. So, why not, right? So, that's our North Star. Now, Alex walked you through a lot of uh sort of reference materials and foundational materials, because we want you to understand where we're coming from, right? Um, so you understand our thought process here. So, remember this architecture. Your goal is to somehow communicate our cloud to another cloud, right? So, let's say you're using Direct Connect, you need the assured resiliency, the bandwidth, etc. So, OK, you have 2 locations, right? this only gets you, well, 3 9s, but you still need to manage routers. OK, uh, actually you need 4 9s, well, you need to do that twice, right? And you need 2 locations with, we use different vendors, so you need those relationships, right? Uh, and those of you folks who are running a global network. Well, you get the point. You need to do this globally now, so, it's hard. Now, That's our vision, that's our problem statement. So we're very, very happy to introduce AWS Interconnect multi-cloud. And we're also extremely happy to launch this in partnership with Google, and we're gonna talk a little bit about how that played out in a moment. Now, this vision is where we landed. AWS giving you an attachment to Google Cloud, and that's it. No. There we go. On the AWS side, our attachment object is the direct connect gateway, and this has a lot of benefits. It allows you to immediately have compatible networking with all AWS networking services, so it doesn't matter if you're using virtual private gateways, transit gateway, or cloud one. On the Google Cloud side, the attach point is the cloud router. And this is all you see, that's it. You're done. Behind the scenes though, there's quite a bit going on. So, the way we architected the specification for this is you are gonna be having multiple logical redundant connections across at least 2 physical facilities across multiple routers. So, we'll talk about that in a moment. So how it works. I will need to ask some of your patients here. This is gonna be a little bit unusual, but I'm gonna explain to you how it works from the customer side, then I'm going to show you how it works under the hood, because my goal is to give you confidence on why this is a product for you, but also so that you don't need to think again about this. That's the whole point. You don't need to think about routers or PGP or PR IP addresses, nothing. So, we're gonna talk to you sort of once and then you can forget about it. So, how it works, you as a customer. can request to AWS or Google Cloud, it's fully symmetrical, so you can use whatever console you want. You can request a new interconnect. So we receive your request and then pass it on to Google and say, this customer needs 10 gigs per second in North Virginia. So then they ask you to confirm that request. So we have two main actions, create and accept. Those are your flows. You create on one cloud, the cloud of your choice, and you accept on the other one. And then once you accept, basically we start talking to each other and leave you with your one single attachment. Let's talk how that looks like on the AWS site. We're gonna use the AWS console for this example. So, where you're gonna start by selecting your partner, so you want to create a new interconnect, you select your partner, Google Cloud. You're gonna select your EBS region, and that will immediately populate the second option, which is the Goul region. So we're creating massive amounts of capacity and we're basically interconnecting the two regions that are closer to each other because we found through customer conversations that's the most common use case. London, London, Virginia, Virginia, Tokyo, Tokyo, and so on. And this is all you need. You set up your speed for public preview. We're offering 1 gig connections at no charge for direction of preview, so let's go with that. You pick a name, you select your attach point, which in our case is direct a gateway, you can tag. And you get this sort of review screen and you're done. That's it. Now let's see how this looks on our flow. You created a connection. Again, remember, create flow. So you created your connection. And we get in touch with Google and now you need to activate. Let's see how that works, uh, as though you had started the flow on Google. That is a very long key, basically that's all you need to activate. You paste that key and we immediately sort of confirm details. Again, you select your direct connect gateway, confirm the connection. And that is all. You're left with whatever capacity you requested as a single attachment between the two clouds. There's no BGP, no layer 3 connectivity configuration, BLANs, nothing. So, with that, The API that we created for this is very cool. How we got away with this is that we cabled very, very large pools of capacity between these pairs of regions. So then we can just sell slices of that to you via API in a very short time. You don't need to have someone go with a cable, nothing, all that goes away. But the interesting thing is that because now we're on a multiple router configuration and you don't see it, we can move you around pretty easy. So we'll see what that allows us to do. Let's talk about the architecture behind the scenes. This is what your connection looks like to you. It's sort of the pink line in the middle there. But behind the scenes, for every single interconnect, we're Provisioning on multiple different devices, multiple logical connections, and each one of those is a valid path. So essentially you have the maximum resiliency configuration that Alex talked to you about a moment ago by default on every single connection. But you don't see it. Now, Here's where it gets really interesting, and again, the point is to give you confidence on how we're building this so that you never need to think about this again. The API calls for a new logical object between the providers. You don't see this. These neurological objects can expand across buildings. It understands routers, it understands connections, it understands link aggregation groups, lags, these very large pools of capacity. It knows about all of that. So what this affords us is that immediately we can ensure that your connections are placed in 4 different routers. So immediately we get rid of that pesky thing where unfortunately two connections turned out were on the same physical device. Anyone experienced that? I hope not. Um, well, that's pretty bad because if that router goes down for maintenance, you lose both of your connections. So immediately we get out of that situation. But this is also what allows us to create the flow of placing your connections, because when you tell us, give me 10 gig to Google Cloud, we basically now tell Google, hey, we're thinking about using this interconnection point here, what do you think? And they say, yeah, sure, go ahead. And then our new specifications start building your logical connectivity until all checks are clear, and then you're presented with this new abstracted single object that actually is build resiliency. Now, it gets better. When we run out of capacity, because we're doing great, you love the service, you're consuming all of our capacity, so we need some more. So we started with the purple one, right? Now we need to put some more routers there. So we put in this new logical object, so now we have two, we have the green one. The way we're gonna provision your connections is the criteria that we call least utilized interconnect, and yes, we named that logical object interconnect and yes we did run out of names when we were building this thing. So, your connections are gonna start to get spread across multiple routers by default. You don't need to do anything, because we're always gonna use the least utilized interconnect. So, if your marketing department wants a 1 gig connection, and your production workload wants a 10GB connection, and your other critical workload needs a 100 gig connection, those are gonna be start over time getting spread out across sets of 4 different routers. As it turns out, there are very, very few customers who are big enough to afford this level of resiliency. No one has 12 routers in a location. Enterprise routers connected directly to our location. So, the more you use the service, the more resilient it becomes. Another cool thing now, remember that I said that now you can, we can move your connections around. Say you landed your first connection on the purple object, and all of a sudden you need more capacity. You tell us, hey, my workload grew like crazy. I need 50 gigs or something. And we don't have more space on the purple one, well, we just create more logical connections to the green one. Then we move you, all the checks are green, and then we take you out of the purple one. So you can scale these connections up and down as needed, and because we have these very large pools of capacity. We should be able to get you the capacity you need whenever you need it. It gets better though. This new object actually allows us to understand what's going on on both sides. So, there are things that now we can do to ensure that this resiliency stays even through maintenance. So, that situation you see in the slides would be very unfortunate. Say we take down a router for maintenance at the same time that Google Cloud was doing maintenance on their router, so now we lost half of our redundant capacity. So that's bad. So now we can agree with each other. Don't take down that router over there while I'm working on this one over here, please, so that you never lose half of your capacity because we just didn't talk about maintenance in a timely manner. So that's pretty good. Now, here's the thing though. As you probably gathered, We couldn't have done this with our existing APIs and when we're designing this, we looked at sort of all the approaches to build multi-cloud connectivity, and the conclusion was They're they're not enough. We just cannot operate the service the way we want to, and we cannot simplify the service the way we want to with the existing approaches, so we need something new. It was very, very important for us that as we onboarded other clouds, whenever you get an interconnect to Google and an interconnect to whoever comes next, that thing looks and behaves and is equally resilient with everyone. We didn't want to have one approach for cloud A, B, C, it doesn't work the same way, the API doesn't give you the object, the routing, the resiliency, that was unacceptable. So it was very important for us, for AWS to say, when a customer gets 10 gigs to any cloud, that thing needs to act the same always. So we sort of found ourselves in a problem here. We need a new specification, but we need other folks to adopt it. If you come up with this great spec and no one uses it, well, it's a problem. So we thought, wouldn't it be great if we came up with the spec. So that actually everyone uses it. Your customer expectation should not only be that when I get a connection from AWS on our cloud, it should behave in this way, it should be, actually, you know what? I want all clouds to talk like this. Make it easy for everyone, like why am I thinking about routers again? It would be great, right? Yes. Yeah, I see people nodding, thank you. So it would be great You know what would be even better? If the spec allowed folks to essentially connect clouds with each other. In the same way, even if they don't talk to Aravidas yet, maybe they're in a different region or what have you. But it would be awesome that they adopt the same specification so that when sort of cloud pink up there at the top, then becomes ready to talk to AWS. Well, they can use the same spec, and they're done, right? They don't need to build it again, you just build once and use the same specification many times over. That would be great. So that's what we did. We opened the specification. Under Apache too. So there are no patents. You can fork it. You all can go in and fork it right now. So it's open, it's on GitHub. And the idea here is to give now confidence not only to you customers but also to our partners that this specification is not going away. This is not something that is like will deprecate tomorrow or anything, it's open, you can fork it, you can use it with anyone you want. So it's out there. Now, As I've been sort of building toward this, these are the features and you can take pictures of this. Essentially, this is what you are left with. These are connections that can scale up and down dynamically, that have built in resiliency, you can count that all the physical links between the AWS router and the Google Cloud router are encrypted using Macsac. And it's pretty cool because Maxsec has this mode of operation called, well, depends on your vendor, but must and create um like must secure depends on your on your vendor, but the mode is that the link doesn't come up if the max session is not up. Another way of saying that is, if you don't have encryption going, you cannot transmit customer data. So, you can be assured now that any data you are pushing through this new service between AWS and Google will be traveling encrypted links. That's very important. So let's talk a little bit about the preview. For public preview, we are in 5 regions and more to come. And you will be able to create at no charge 1 gig connections to GCP. Now, here's the interesting part. We wanted to give you the tools to start learning about the future because, well, it's gonna take us some time to be everywhere, right? But now you can create your connections and start kicking the tires, learn how to use the service, etc. so that when we go GA you're ready to go. Another thing that we're doing is that For the first time we're sort of creating a service that's Really going out of AWS, out of the AWS global backbone. So monitoring is sort of an area that will quite simply we lose visibility. We handle the traffic to Google Cloud and we don't know what's going on on the other side. So with every new interconnect, we're also including cloudWatch network synthetic monitor. So you will get one synthetic monitor for every new interconnect. Now, you still need an endpoint on the other side that responds to the probe, TCP ACMP, but you will be able to get signal from wherever you're going to be able to tell how your uh connection is doing, it's health. And the last one, and this is new, as I mentioned before, these connections are scalable. You can scale them on demand, up or down as you need them. Maybe your test environment needs a tiny connection, but then it starts to grow, you grow it. Um, you have a seasonal workload, you need traffic, like to serve traffic spikes, you just get more capacity for busy season. That's fine. Now, I do want to talk a little bit about the partnership and I'm going to do that in a minute, but for now I'm gonna hand off this to Judy and I just finished with one thought. As we built this thing, it was very important to find a partner that shared our vision. It should be simple, like we should take care of the infrastructure, the support experience, the whole thing, right? And for us, I'm very thankful of the partnership. We found in Google someone with that vision, and basically that's the product we're launching today. Thank you likewise, uh, thank you for having me. Um, I introduced myself as the cloud interconnect at Google product manager. I'm very happy to speak to an audience of AWS enthusiasts. How many of you here know cloud Interconnect and have worked with cloud interconnect at Google? Wonderful. Awesome. So for the folks that don't know, um, what this does, essentially, maybe we'll, we'll spend a minute level setting on, uh, what this, what this actually is. Essentially cloud interconnect at Google is, uh, our private on-ramp into Google Cloud for customers that want private, uh, and secure connectivity from their on-prem environments into Google Cloud. We deliver this from co-location facilities. We call those interconnect POPs, points of presence, and in many cases those interconnect POPs are places where we co-locate with Amazon or other cloud providers as well to offer that connectivity to customers, and that's been a model that's been adopted in the industry for a while. This is how cloud providers peer publicly as well to to deliver public connectivity at scale to their customers. And as Alex mentioned earlier, we've seen a lot of patterns where our customers want to use this product for connectivity to other clouds. This is not only used for hybrid connectivity into my own premises. I also want to use it to another cloud because I like the fact that it's private. I like the fact that it's secure. I like the fact that it's dedicated to me. I'm not sharing a channel that is over the public internet, essentially. So we recognized those challenges a long time ago and we introduced a product called Cross cloud Interconnect. We added it in 2023 and we've been seeing a lot of adoption on this product. Essentially what we did with that is that we've extended our footprint from our edge router into the edge router of the other cloud provider. So by virtue of the fact that we co-locate with Amazon in many of those cloud uh co-location facilities, we could just extend our port with a cross connect into the port that's on the AWS side, the direct connect, so the customer buys a cross cloud interconnect port from us, a direct connect from AWS. They give us the LOA and we go terminate that cross connect on their behalf. No more on-prem footprint. No more having physical routers in the co-location facilities. Happy days. I can get my 10GB or 100 gig connection between Google and Amazon in almost no time. There's still a tech that's dispatched that go terminate that connection because it's actually built to order and it's dedicated to me, but I can achieve my, uh, cross cloud connectivity goals in terms of privacy, in terms of scale very easily. I can get all the way to 4 lines of availability without having a single relationship with a third party provider like a co-location facility or a partner provider for instance, which means that I can lower my total cost of ownership and I can minimize the amount of vendors that I need to talk to when there's an incident. Cool stuff. Let's build on top of that to to start getting to that resiliency model with um with Interconnect we've invested a lot in uh building resiliency in layers so similar to constructs in AWS where you get maximum resiliency we introduced that at the physical layer in our interconnect facilities by introducing a construct that's called EADs or edge availability domains. Those are essentially separate fate domains within a co-location facility. So essentially you can think of it as a collection of hardware that doesn't share power, that doesn't share uplinks up to the region. They just don't share fate in a way that we know that if you're connected to both of those, you're not likely to have an incident that impacts both. We're never gonna schedule maintenance in both of those, so that's how we kind of guarantee that we have that physical separation. And building those layers up to the regions, adding some virtualization there, we have a construct called VLAN attachments that allow you to virtualize your interconnect into multiple VLANs that have different bandwidth essentially and attach those to cloud regions. Those also talk to a construct called cloud router that orchestrate your routing. It propagates routes from your on-prem or your other cloud into, into Google Cloud, and that engine is also from the constraint control plane perspective redundant. So you're getting that physical redundancy. You're getting that logical redundancy. Awesome, good stuff, right? Story ends. But These challenges, um, we're seeing a lot of adoption with this product. Hundreds of our customers have adopted this to do cross cloud interconnects, not just with AWS, with a number of other providers that we happen to co-locate with and support this product with. There's a few things here. It's a dedicated link, so the minimum link capacity that you can buy is the line weight of the interface. In our case, the minimum is 10 gigs. We're building that to order so we're dispatching someone to go connect that on your behalf. There's some lead times associated with that, which means that you're probably overbuying capacity because you want to make sure that that capacity is there when you need it. So not great, but we do have a solution. Then What happened was that some customers forget that they need to communicate the context of reliability from one side to the other. So while we invested so much time and, uh, engineering efforts in building resiliency in our stack, the customer goes and builds these things into availability domains, but then they forget that they need to do the same in the other cloud, and then they go connect two routers here to the same router on the other side. Not great if you want reliability. Even worse, something that they probably miss oftentimes is that sometimes even though I make sure that I don't have overlapping maintenances between my two edge availability domains, that needs to extend to the other cloud, so the customer needs to notify me if I happen to have a planned maintenance next week that happens to overlap with their Amazon router that's uh that's also planned for maintenance next week. So customers sometimes miss that they need to provide the same context to both clouds. They need to coordinate between both clouds if they want things to work. So that got us to pause and rethink our approach a little bit. How can we do better? What can we do if we wanted to provide that resiliency to customers without them having to be engaged? How can we address some of the limitations that we have in the service? So we start think we started thinking about what's that North Star and how can we get there. What, what, what if essentially we could pre-build large pools of capacity, pre-cable them to a number of providers, and virtualize the connections in a way that you don't have to wait for that tech to go and this and, and connect something essentially? What if we also encrypt it by default so you don't have to worry about what's happening in that segment that's outside of my circle of trust essentially. What if we can deliver reliability out of the box so that you stop thinking about all these things and coordinate maintenance and things that nobody wants, uh, wants to spend time focusing on essentially what if we can make it scalable in a way that you could scale up like like Santiago mentioned or scale out by adding more connections that could be turned up on demand in minutes essentially? What if we could abstract all that BGP stuff that Alex loves and. But also nobody wants to worry about and not have that be your problem essentially so you don't have to replicate policies across 4 routing sessions and make sure that they are symmetrical and all that lovely stuff. And we thought a lot about this and honestly we thought of a number of ways to do that. We could potentially go buy a lot of capacity from Amazon and try to virtualize things. We could, we could do all sorts of stuff, but all of the, all of those approaches have holes and all road, all roads essentially led us to Rome and Rome is we have to collaborate with other cloud providers. The only way this is gonna work is if we actually talk to one another. But that's impossible. Like, why would Google talk to Amazon or why would Amazon want to talk to Google, essentially. But then we're here so it turns out it's actually pretty possible and it's actually here so the announcement that we made on Sunday that we're all extremely excited about is that we have been collaborating with this amazing team to try to come up with a solution to all these challenges that would essentially check all the boxes for for our mutual customers so they can be successful adopting both platforms without worrying about the infrastructure layer that does not add any value to their workloads so. Essentially I'll, I'll talk a little bit about our perspective of uh of, of how this became, became possible, um, we started working together about a year ago maybe on this east west API that we have between one another. And it's essentially being able to encode all those collaborations that we were expecting customers to do to make sure maintenance doesn't overlap, to make sure resiliency is well met across both both places, to be able to orchestrate the creation of all these resources. The best way you can do all of that is that you can put some amazing engineers from Amazon and Google in the same room to come up with a spec that can automate all of that. And that's what we've encoded in that East-West API that we've co-developed and we've opened so any cloud provider can go implement this or any private peering provider can go implement this. It doesn't have to be a cloud provider. And we've made this east-west API available and we've also implemented it in a number of locations. I think we're in about 4 or 5 regions now and we're looking to expand that globally for global coverage and then we've we've also invested in those north-south APIs if we want to call them that, and that's what we expose to our customers to be able to interact with our tools as Santiago mentioned those are symmetrical. So if you come from an AWS. Environment you're familiar with those tools you can go to the AWS console can figure whatever you want, and everything happens behind the scenes. All you need to do is acknowledge on the Google side. If you're more familiar with our APIs on the Google side, you can also use our console and then acknowledge on the AWS side, which makes it very scalable, very adaptable to any cloud provider, to any customer that's coming from any cloud background, essentially. Uh, Santiago popped the hood on the AWS side, so I'm gonna spend some time doing the same on our side. From the physical, uh, from the physical side, we're using two different facilities to be able to map two different regions. So if we take Northern Virginia an example, uh, we use two facilities where we happen to co-locate with Amazon and we cable, uh, different routers within those facilities to be able to meet the maximum resiliency targets that, that we both have. And then from a from a virtual virtual perspective we're building those VLAN attachments that I mentioned earlier to virtualize the connections. We're building cloud routers and we're putting those in a managed project in a managed VPC that Google owns, so we're orchestrating all those resources that we were asking you to create before. And we're doing whatever, whatever you need to be doing to be able to have that end to end connectivity from Google into Amazon. We're doing BGP. We're, uh, negotiating VLANs, IP addresses, all that lovely stuff that everybody wants to spend their time doing, Essentially all of that is put in a Google managed project, and the way you can communicate from that project into your own environment is with, with two approaches essentially. In preview today you can do what we call VPC peering. VPC peering is essentially uh a 1 to 1 relationship between the managed VPC that we own and the VPC is where your, your resources are. This is perfect if you wanna do, you know, plain vanilla. VMs need to talk between AWS and Google. You have that connectivity up and running. Even if you want to use Google APIs for instance and you wanna be able to call them from from AWS, you could do that with the construct that we call private Google Access and advertise those ranges into AWS to be able to access those APIs safely and privately. In addition to that, uh, after, right after preview, we're looking to introduce support for what we call Network connectivity Center, and, uh, you can think of VPCP as a 1 to 1 relationship. Network connectivity center is essentially a hub that allows you to attach multiple spokes to it. So you can think of our managed VPC as one spoke and then you can attach a number of other spokes to that to that hub so that they can all be able to uh leverage that connection and talk to AWS. This is perfect if you also want to use that for plain IP connectivity. Between Google and AWS or if you have some other types of applications that require some type of exposing an endpoint through a producer consumer model, for instance, or other types of private endpoints within Google through a service we call Private Service Connect. All that if I wanna summarize all that to say is that regardless of what you wanna do we're making sure that we have that transport in place we have that underlay in place for you to build any application on top of that. Our goal is to abstract building that transport the same way you get when you publicly peer between providers. We're doing that for private peering, and we look forward to seeing what you will do with uh, with all of those, uh, innovations with that, I'm gonna hand it back to Alex to talk about, uh, what you can do if you wanted to scale that beyond a single region. Thank you. Let's see if we're completely rid of BGP or not. Uh, so let's see how you folks can use these, right? Uh, we've looked under the hood, we see how this works at the end of the day you get an interconnect, right? What can you do with that interconnect? Uh, well, so there's some, there's some reference architectures here we should look at. Uh, first is single region single interconnect, right? Uh, and on the AWS side you would have a single VPC, usually on the Google Cloud side you would also have a single VPC, right? We, we've seen, uh, in the, in the intro how the VPC looks like for those of you who are not familiar with the subnets and the virtual private gateway that's attached to the VPC. Um, of course it supports both IPV4 and IPV6. I'm very grateful to my partners here, uh, for, for that. So you can rob both, uh, stacks across, uh, across the clouds, right? So you get your interconnect, you attach it to your, uh, your direct connect gateway. You attach it to the, uh, Google Cloud router on the, uh, on the right hand side. Let's look at route advertisements. This is going to be the standard is how we are looking at these architectures. So, uh, first, the virtual private gateway picks up the VPC, uh, prefixes both IPV4 and IPV6, and advertises them to the direct connect gateway automatically. You don't have to do anything. Then the direct connect gateway automatically advertises these prefixes to the Google Cloud router, uh, in BGP, BGP that you don't configure, right, it's already there, uh, it's, uh, it exists from the Google Cloud router perspective. Uh, it advertises the, uh, VPC subnets that you have, uh, in your VPC either in, uh, just this region or globally if you have a VPC that has multiple, uh, multiple regions, um. So advertises them to the direct connect gateway. The direct connect gateway, uh, sends them to the virtual private gateway, and if you have, um, um, route propagation enabled on the subnets, uh, route tables in your VPC for that virtual private gateway, uh, uh, routes, those are gonna be automatically populated in your route tables, right. Pretty straightforward so far, right? Let's look at the, uh, route tables. So from the VPC perspective, VPC prefixes are local to the VPC, right? Those are routes that you, uh, are not gonna override. And then all the other routes that were learned dynamically go to, uh, the directing a gateway BGW and then directing a gateway. The direct connect gateway is going to have, as you may expect, uh, routes pointing, uh, to the VGW for the VPCs attached to that, uh, um, DX gateway and towards database interconnect for what is on, uh, Google Cloud. Google Cloud router will have Google routes, um, or cloud routes, uh, that are going to be, uh, sent, uh, onwards to the subnets that you have in the Google Cloud VPC and automatically populated so you don't have to do anything this will work right? pretty straightforward. Now, uh, if you have multiple VPCs on AWS, uh, we've talked about ways to aggregate that, and if you are in a single region, we are still in single region single interconnect, right, uh, you can use an AWS transit gateway that you can of course attach to, uh, your direct connect gateway. Now, um, ideally you would use VPC prefixes that are easy to summarize so you can have a nice, uh, summary, uh, route that is sent from the transit gateway to the direct net gateway. Um And uh this uh works today through a prefix list that you get to configure so the more compact that prefix list is the nicer your uh your round table are gonna uh are gonna look like. Now same process happens from AWS to Google Cloud routes are, uh, advertised through BGP and the other way around. Uh, routes are propagated into the transit gateway route tables, right? Now from a round table perspective, if we look at the VPC route tables here is where you have to configure routing, right? It is uh the VPCs are still the owners of routing decisions, uh, in AWS so there's no direct route propagation from the TGW to the VPC. Um, so here is again where summary prefixes uh are very useful because they keep your VPC round table, uh, small. On the transit gateway, all routes are dynamic, uh, and they point, as you may expect, uh, either through VPC attachments or through the direct connect gateway. The direct connect gateway route table looks very, very similar to, uh, what you folks have, uh, have seen previously. The direct connect gateway is again a global construct. You can attach multiple transit gateways from multiple regions to the direct connect gateway. Uh, keep in mind that cross region communication needs to go through transit gateway peering. So that's why, uh, cloud one is there. Now on the Google Cloud side, uh, again routes are learned, uh, those prefixes that you configure in the prefix list of the transit gateway attachment to the direct connect gateway and routes are propagated all the way to the subnet side. Animation is complete, so you can take a photo. Uh, Now let's get to more, more complex scenarios, right? The first one is multi-region, right? And multi-region on AWS, you could have multi-region on Google Cloud as well. And you still started with a single interconnect, and you may ask yourself, well, how will this work, right? I have this direct connect gateway that's a global construct. I have maybe cloud one or transit gateway northbound of the direct connect gateway, um, that are attached to it in, uh, in all regions. So how will this work? How will routes be advertised? Well, pretty simple. Direon gateway is a global construct, and whatever routes it learns, it advertises to all its global attachments, right? So let's look at routes and route tables. Uh, first of all, the cloud one attachment to the directed gateway is again a global, uh, construct, and under the hood by default, the direct connect gateway gets attached to all core network hedges in cloud one, right? These small bubbles here are core network hedges are like those transit gateways, uh, under the hood that are your routing hubs in AWS. Now these core network edges advertise older prefixes uh to uh the direct connect gateway automatically. You don't have to configure anything. There's BGP that's routing again uh or running under the hood. Uh, keep in mind that every single core metric edge is going to advertise the prefixes that it knows regionally to the direct connect gateway, so it's not going to advertise a summary route unless you tell it to do so, and we're gonna, uh, look at how that happens, uh, with cloud one routing policies, which is a recently launched feature. But by default every single core network hedge advertises it's, uh, attached prefixes, right, it's local prefixes to the DX gateway. The DX gateway will take these prefixes and we'll advertise them over to the interconnect, right? Pretty straightforward so far. From the Google Cloud router side again, uh, the subnet prefixes in Google Cloud, uh, tenant or customer VPC are going to be taken by the Google Cloud router and advertised over to the Interconnect automatically learned by the DX gateway and then sent forward to the, uh, core network edges automatically so it works pretty straightforward. You have to do anything for all of this to happen. Now if we are looking at the route tables um again from a routing perspective the VPC is still the owner of the routing uh in AWS so you have to configure VPC routing and decisions at that level um summary prefixes are amazing again so keep in mind uh that route summarization is is important. Uh, from cloud one perspective, every single core network edge, um, has its, uh, own row table, right? In this case, all coretric edges, round tables are going to look the same. Each core network edge will know it's attached, uh, prefixes for VPCs or other types of attachments that you have in that region and will know the Google Cloud, uh, IPV 4 and IPV6 prefixes from the attached direct connect gateway, right? Pretty straightforward. And the direct connect gateway will will have in its route tables the routes for the Google Cloud um environment in its route table. Same for uh the Google Cloud router uh route table. It will learn the AWS prefixes and populate its routes in its route table, right? Seems even too easy to be true. Um, from the subnet perspective, uh, again, routes are being automatically propagated. You don't have to do anything. Right. Now the most interesting one is when you have multiple regions and you have multiple interconnects. Now option number one here is where you have full region overlap between AWS and Google Cloud. So let's say you deploy an AWS in Virginia, you deploying Google Cloud in Virginia, you deploy an AWS in, uh, Europe. You have, uh, in Google Cloud in Europe. Now this is a scenario that's pretty uh pretty straightforward pretty common um however you may grow into a place where not all regions overlap and we're gonna look at that next. So, uh, recommendation is, um, to have a direct net gateway per interconnect because it allows you to have full flexibility into how routing decisions are being made now you're getting close to, uh, maintaining a global backbone, right, and you have to kind of know a bit route preferences and you have to know how do you route to certain prefixes in the other, uh, cloud provider environment. So, uh, again I was mentioning that by default when you attach a direct in a gateway it gets attached to all, uh, the, uh, core network edges in cloud one so you don't have to overthink this, uh, too much. Uh, now each direct connect gateway hosts an interconnect, right, has its own BGP session that's already running, so let's look at route advertisements. Uh, coretric edges advertise their local routes to all the attached direct connect gateways by default, right? So now, uh, each CNE will send those local routes to both direct connect gateway A and direct net gateway B. And each of these direct net gateways is going to send these prefixes to the Google Cloud router. Now in each region the Google Cloud router is going to advertise all the Google Cloud prefixes across all regions from the customer VPC. Uh, to the, to both direct connect gateways, right, so each direct connect gateway will know about all the routes that you have for all the subnets in Google Cloud. And what will the direct net gateway do or the two direct connect gateways do? Well, exactly what you expect it to do, uh, they will send, uh, the routes to the core network edges automatically without you having to configure anything. So this is how rod propagation looks like. Let's see how raw tables look like. From AVPC perspective, nothing changes from a core network edge perspective, here is where you can start doing the, um, how to say, the, the starting point of traffic engineering on AWS, right, where you can decide which paths your, uh, traffic should take depending on your available egress points. In this example, um, I've configured, uh, using cloud one, advanced routing policies a preference for direct connect gateway A for the Google Cloud prefixes in, uh, US East 4, and, uh, direct connect gateway B for, um, the prefixes in Europe, uh, West 3. You can do that however you want. You can use AS path. You can use local pref. You can prepend. You can play with all the BGP, uh, constructs that you, uh, may or may not be familiar with. You should be probably, uh, on cloud one, and you can decide how cloud one routes. So cloud one is your global, uh, uh, backbone on ABS that you can, uh, you can manage however you, uh, like. Now the direct connect gateways round tables are going to be pretty straightforward. They will know the prefixes attached to every single CNE and Uh, the Google Cloud routes, uh, for the cloud routers are going to look very, uh, very similar to, uh, to before. And then from a subnet perspective, each subnet on the Google Cloud side is going to select, of course, the Google Cloud router in, uh, its region to route to, uh, AWS prefixes, right? So even though they each subnet receives the routes from, uh, multiple Google Cloud routers, it will select the regional one. Pretty straightforward, right? And last but not least, at the end of this, um, it's the probably one of the most common scenarios but also, uh, one of the ones that requires most, uh, routing knowledge from, uh, from all of you, uh, which is not full region overlap, right? You may have regions where you overlap and you can create these interconnects and you may have regions where you don't overlap between clouds, right? But you may still want to say, well, I have this workload here in AppSoft too that needs to talk to an API endpoint or uh whatever data, uh, store that I have in Google Cloud, and it needs to do so across region, right? So can I leverage Cloudo as a global backbone? Yes, I can. Um, so route advertisements again, every single CNE advertises to the direct net gateways. It's attached local routes, so pretty straightforward. Nothing, uh, changes here. The direct net gateways send, uh, the routes to, uh, Google Cloud routers over the interconnect, and, um, that is pretty straightforward, right? I don't know if you folks, uh, have the time to take a photo if you want to take a photo. We're gonna look in the next slide on the other direction, right, so on, um, the, uh, Google Cloud side towards AWS, uh, the Google Cloud router again in each region advertises all the routes from the customer VPC. These routes are uh propagated to the core nitric edges uh and they're of course learned and here's where you can again apply traffic engineering uh to understand. Well, what should the next hops be, right? This one just shows you an example. For example, core network in US East 1, in AWS prefers, uh, direct connect gateway A for prefixes in Google Cloud, uh, North Virginia, and then, uh, prefers direct connect gateway B for prefixes in, uh, Africa South 1, right. It's totally up to you how you want to route those, right? It's up to you depending on how you think about like how your workloads are deployed, what are the connectivity options. You may have, you may, uh, you could also say that, uh, the CNE in US East one routes towards everything in Google Cloud using direct and gateway A. Totally, uh, totally good. Um, just keep in mind, uh, to have that observability set on these interconnects to understand utilization bandwidth when you need to scale up, which is now very easy, uh, or if you need to scale down. From the direct connect gateway perspective, everything stays the same, and on the Google Cloud side again, all routes are being automatically propagated. And now for the last 30 seconds I'll give it back to Santiago. Thank you, Alex. So, This is our product, we're super happy to have launched it. Uh, we think it's gonna be great for your products. Uh, the idea is that you know you take this and build whatever application you want to build that requires connectivity across clouds. We're also working with Azure to bring you the same product uh in 2026 with them. And the last thing is, please bear with us as we roll out this globally, turns out it's a pretty big globe, and sometimes we are in the same facility as are the other providers, sometimes we are in the same campus, sometimes we are in the same city, sometimes we're not in the same city. So, it's gonna take us some time to be everywhere. We're investing heavily in this, we're building new facilities to be able to provide you the best possible experience. So please bear with us. We're gonna be using our documentation to signal to you folks this number of regions are gonna come in this time frame, this other time frame, this other time frame to keep you informed so that you can plan your products around ES interconnect. Thank you.