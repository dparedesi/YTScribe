---
video_id: mSrDZuxWFtw
video_url: https://www.youtube.com/watch?v=mSrDZuxWFtw
is_generated: False
is_translatable: True
---

Thank you for making the trip all the way out to Mandalay Bay. I know this is one of the, the harder venues to kind of get people into, so I appreciate you making the trip. Um, today we're gonna talk about, uh, what's on the screen here, maximizing EC2 performance. So, uh, this is a code talk, uh, and this is a little bit of a different format than what I, I've only given one other code talk, and I was at a Toronto summit and I was on ground level, so I'm not used to being up here on the stage. Um, and I was hoping to make this a little more dynamic, but now given that we're up here and not able to kind of mingle, maybe it's gonna be a little bit weird, but I have stickers. So, uh, if, if, uh, I was, you're gonna be, you know, if you were answering questions or we were getting some engagement, I was gonna give out stickers. I can't really hold them hostage. So if you do want a sticker, uh, please afterwards come and get, talk to me, and I'll be happy to give you one. Um, but yeah, my name's Toby Buckley. I am a specialist solutions architect, uh, on the EC2 team. I focus on EC2 performance and helping customers get the most out of EC2. I'm joined by Jeff. Hi, I'm Jeff, Jeff Blake. I'm principal engineer for, uh, Annapurna Labs at AWS where I work on optimizing performance for graviton instances all the way from user software, which we'll talk about today, all the way down into the hardware, which we'll also touch on. Jeff's super smart guy. Um, so yeah, I think we should have an interesting jam-packed session for you today. Uh, this is what we're gonna talk about. Um, Really just, you know, kind of talking about performance engineering from a, a high level perspective. So not necessarily getting, I know we got one HFT person in the audience, not gonna necessarily get into the high frequency trading like low level performance stuff. This is more of a high level stuff, although some of the tools we have may may bring to bear some, some good signals for you to kind of, um, APRf being that tool. So we'll, we'll introduce AEROf if you're not familiar with that. Um, it's a, it's a great tool for, for understanding the performance of your system. And then we've got a couple of of examples. Uh, we've got a groovy demo, uh, anybody, any groovy users out here, any Java groovy users? OK, so one, it's great, but, uh, I'm, I'm not a groovy expert either, but, uh, kind of put this demo together, um, really to, to try to be. Not so contrived that it seems silly for for those that are in the audience, but one that is, um, more mimics maybe a natural organic growth of an application at a company. It starts off pretty innocent and then it starts getting big and then it gets slow and now you need. Don't understand why, right? So something, something more like that. We also have one on MongoDB and how you can get more performance out of Mongo DB and how the tools can surface, uh, surface the signals to so that you can understand why, uh, and then like any good talk we're gonna send you off with a call to action. Um, I think that's it. I'm gonna turn it over to Jeff. What just level setting here, I'm gonna run some of the slides, he's gonna do the coding, uh, As I said before, we don't have a whole lot of code for you to follow through with, so there's none of this stuff is in a repo necessarily, but I'd be happy to connect with you offline, and we could, uh, you know, maybe offer up whatever we can. We could potentially get some of this in a repo if that's of interest to people, but again, hit me up afterwards, uh, you know, outside or up after the talk, and we'll talk about what logistics look like moving forward. So with that, I'll hand it off to Jeff. OK. So we're gonna introduce performance engineering for those that may not do this day in and day out, like, uh, myself and some of my team, but I guess, show of hands, who does performance engineering as their primary role? OK, one, so you might know some of this already, but for the rest of you, we're gonna give a quick primer on performance engineering. So when we go down to the code and try to figure out what we're gonna try to take advantage of to make things go faster, you have a little bit of a base to, to rely on. So performance engineering very simply is just finding opportunity in your system. Uh, whether that's for efficiency gains, price efficiency, or performance efficiency, which, you know, I like the most, but price performance engineering is also something we want to find opportunities for. And while that sounds simple, it has some big challenges. One of them is abstractions that you rely on to build your software. Everyone uses things like sockets and web frameworks to build their software, so they just concentrate on their business logic. When looking for opportunities for performance, those things can leak, where you have to start looking underneath your abstractions that you rely on to build your logic, but now they're not performing the way you want. You have to go and look underneath the covers. Uh, case in point, we were helping a customer. Optimize some uh, some code that they were uh trying to get performance on Graviton. I'm a graviton engineer, so I'm gonna talk a lot about Graviton is they had this abstraction they built their software on, and we had to actually go into the abstraction they had no knowledge of and actually show them that their optimization wasn't in their code. It was in this abstraction that we had to go and do some away team programming to make faster. And again, Abstractions leak. Everything has a cost. Now you have to start thinking about what you've built, what you're building on, and what those costs are to try to understand how you can find more opportunities, such as you don't just have a VM that's all by itself, it may have different storage, different networking characteristics, and you have to know what those costs are, not just in price, but also what performance it can get or what performance you're not getting from that. Another part with performance engineering is bottlenecks may hide others. You can have a bottleneck that you're trying to remove to get your performance up, and then when you finally fix it, you find you've uncovered a worse problem that was just hiding behind it. We had something like this where we fixed the performance of a network application and we uncovered a synchronization problem that actually made the performance worse by 15x. And we had to go underneath again. We had to dig into the distractions we were relying on find that second bottleneck before we actually got to the performance we expected. So these are things that can finally lead to just, I've already talked about a lot of things that can happen. This is a search space explosion. So lots of things to think about. It's a very Uh, rich environment to go and learn things, be curious about what it is you're building and what you can do to make things faster cause you start understanding the different layers and the different costs of everything you're working with. But let's talk about how we're gonna actually just do performance engineering. Uh, ideally the model is you define what you're going to measure. You're gonna measure it, we're gonna understand it, we're gonna tune it, we're gonna get some performance, we're gonna get some, uh, Some more efficiency and we're gonna go around this loop a few times and finally come out the other side and say yay, we're done. And performance engineering, for those who are wondering, it's never done, it's just something you stop. You say my return on investment's good enough, I'm gonna stop. I'm not gonna look for that extra 0.5% or extra 0.10%. It's not worth it. But as I've, but you may find that again, the reality is gonna be a little different. With performance engineering, there's lots of tools out there. So these are just a handful. You can use things like STrace, IoT, EBPF, ASP prof. These are all tools that can help you go very deep into parts of your program. And This becomes a problem because now you're taking meandering paths, it's no longer this nice concise loop, straight line path. You may have to look really deep, take a couple of false turns, take a couple of dead ends before you find your tuning opportunities, and then come back to. Remeasure what you've done, see if it's helped. Uh, this turns out has been somewhat of a problem even on my team where we try to use things like intuition to know what tools to use. And we said, let's take a step back and figure out, and this is the things we want to share with you from AWS we said let's take a step back because we're doing these performance engineering problems and we're not getting as far and as fast as we want. And we found our intuition is actually something that gets in our way. Uh, intuition can be misleading. Doing these depth-first searches can be very inefficient, especially if you pick the wrong path first, before you come back out and say, oops, that didn't do anything, let's go somewhere else. And it's simply, if we said, let's try going wide before we go deep, and that's just what it sounds like, breadth-first search. Prioritize your opportunities by looking at the full system first. And when I say full system, things that you may not even. Uh, think about when it first comes to finding performance. Some people will get, will say, well, it's my code. I should know what to go fix in my code, but maybe you need to look at something more system-wide that's not even related to your code. And that's where we found that sometimes big gains hide in plain sight. Uh, we had another customer that said, we need to optimize the compression library. It's absolutely the compression library, and That's where our intuition was telling us to go, and we took the step back and said, let's look at the whole system first, and it turned out compression wasn't even the bottleneck, it was just the machine was running out of memory and then swapping to disk. We fixed that opportunity and then the performance lifted up by 50% and we were done. So putting that into graph form, we say, let's define our measurement, look at all of our signals, get the system-wide understanding, get our tuning candidates, and then we finally go deep after we pick the ones that seem the most profitable to go look at. And it's all well and good to talk about it, but we decided, or we should say we want to share with you a tool that we've developed that helps take away the I have to remember 15 different tools to go use to get my wide view before I go deep. And we started developing this tool, and now it's on GitHub and version 1.0 as of last week called APRof, and it's a wide and deep focus tool. It's not meant to go deep in just code performance tuning. It's meant to look at everything very, very wide. Before we go deep, and Toby can share a couple of anecdotes as how he's used it in his role. Yeah, so, uh, as an essay, I don't know, am I still on. Um, as I say, you know, oftentimes customers call us, we're, we're in an account, we have to have some problem we have to deal with. Uh, the, the beautiful thing about AR, how many people have heard of APR before today? No, OK, one, OK, the performance engineer has, um, great, so that's good, uh, but, but the beautiful thing about it is that it lets us, you know, oftentimes if I have a problem that I can't solve and I need to bring it to Jeff and his team, we need some kind of common set of language, some kind of common data to do that, right? A proof is that tool. It lets us, uh, we, we ask. The customer to if they're having a particular problem, go deploy it, record some sample some run, let us know what's happening, send it to us. It packages up a nice tar ball with all the reports in it, and we could then visualize it. We could send it downstream if we're, if we're stumped. So it's a nice, it's a nice thing for, uh, just making, uh, shared reality for all, all parties. OK. So before we go any further, I wanna say it's not the only tool. Uh, it's a tool for your toolbox. It's a good wide and deep tool. As I said, there's plenty of other tools out there that go much deeper. Uh, I could talk forever about some of those deeper tools if you catch me after. But APERF, as I said, we want to measure hundreds of system-wide statistics, uh, just Things from very high level, CPU utilization, memory utilization, all the way down to the, the deep low level metrics that tell you how the CPU itself is performing and all of these signals can be taken together to try and find those opportunities. Uh, it's meant to be simple to use, it's a self-contained binary. You put it on the system under test, it's a point tool. You don't have to onboard a huge service or infrastructure to get it going. You can use a test box, put it on there, take a recording. Pull the recording off and then it generates a static set of web pages which will show as we get into the code portion of this talk, and it's very low overhead, we've measured it as less than 5% of one CPU when everything is turned on, which we find to be an acceptable trade-off to get the hundreds of statistics we, we want to measure. And how about deployment? Uh, deployment can be, uh, you can just SCP it onto your VM, onto your bare metal lab machine, or it has the ability to be packaged up in a container, and we can put it into something like a Kubernetes pod to measure uh Kubernetes node in a privileged, in a privileged container, which we know is a fairly, uh, common use case for deploying web services, and that's a new capability we've been working on in the last month or so, and we'll show that here today as well. OK, I'm gonna hand it back to Toby, get on my laptop to get things set for the demos, and he'll explain what we're gonna talk about first. Cool, uh, thank you. So as, as we mentioned, Groovy is gonna be the first demo, not because we love groovy or anything else, but because it, it has some of the stuff we're talking about. Anybody heard of an aspect oriented programming? OK, ALP, it's been around forever. It's basically the idea that you've got some, some cross cutting concern you wanna apply it to a bunch of different methods. Maybe we want to do logging or we wanna do whatever the case may be. You could define that as an aspect and you could apply that aspect multiple places. Beautiful for. For maintainability, right, for testability, for readability, all of those things, uh, not without cost, just like everything in engineering, um, there is always a trade-off and so the really the goal, one of the goals of this talk is to help you understand what those costs are so you know where to make those trade-offs, um. Yeah, so, again, the example is, uh, is, is not terribly contrived. It's, it's, it's fairly full featured, we think. Uh, here's the setup, the topology of it. It's a simple cluster. We've got 3 node groups, a, uh, one dedicated to our load generator, and we're using WRK 2, which is just a, a, a nice load generator, gives you nice tail latency numbers and, and tells you request per second and, and all that stuff. Uh, an M7G is. Is using a few different flavors, one for an unoptimized version, and then another one we'll just restart the pod with some optimizations and see what, what effects we have, uh, and then we have an MAG, which is also, uh, a potential option for you if you're trying to get performance out of. System you'd always make the hardware bigger, right? You could always make it faster by potentially throwing more hardware out so, uh, we, we walk through all those scenarios and kind of do a price performance analysis we're kind of verbally give that to you, but we, we have some, uh, some numbers we can give you at the end of that, uh, and with that I'll let Jeff switch over, uh, and we'll, we'll get to the demo. OK, so I'm gonna be showing everything in my VS code window. So can everyone read this that are in the back row? Do I need to go a little bigger? Back, back row, guys, everybody's good? OK. So, so, uh, let me know if I need to go one option bigger. It's already a little cramped, but, uh, so I'm just gonna show really quick. This is the container we're building our groovy app into. Uh, it's based on Amazon Linux, uh, 2023. We put Coretto 21 on there for our Java engine and then we install our Groovy app by building it with Cradle and then we transfer it onto a server or a container that's using Tomcat for our web server and we just throw this on here and we use some very, very basic, uh, Catalina options for the, for the JVM some. 8 gigabytes of heat, G1 GC, very basic stuff. And now we're gonna go take a look at the groovy code itself. So what we did here is we just started building up a very simple. Uh, web application. As Toby said, it's not completely contrived, but it's meant to show examples, not so much be a full-fledged, uh, enterprise service, but we've seen this help with other customers running full-fledged services, so it's a relatively good example. And What you might do in a JVM type language like Groovy is you're gonna start defining endpoints like the hello endpoint, the process endpoint with some argument to it. And as we go along, We're gonna say we wanna do some things in our endpoints, but we primarily wanna have separation of concerns. We want our business service logic and its own, own package. We wanna call that from our endpoint. Our endpoint just handles HTTP requests incoming and responding to them. But as you might build up an application and if we go through here we can see we have other endpoints like a message endpoint, a set up endpoint for authorizing users, a health check endpoint because we're in Kubernetes. And also, you know, we want to be able to pull metrics, say our uh. Another health check in the Kubernes cluster wants to pull some metrics to keep an eye on the health of the. Of the deployment And but as we go we might say, well, we really want to take a look at all the people accessing our our system so we want to log every client IP to get security auditing going. We want to check that we have everything is authorized and the way we're applying these aspects that we can look in here aspects are in the aspects directory. Uh, and all the code for doing authorization and stuff is in here. And we can do that either as annotations or if we say we want metrics to apply to everything, we can do that with Spring boot that we built upon with cut point filters. So we're really taking and leveraging the aspect oriented programming to have this everything, all our concerns separated, cross-cuttings, uh, cross-cutting code can be applied to all the endpoints we want, whether it's explicitly with annotations or using the cut point filters to apply it more broadly. And this builds up a nice web application. It's very clean code. And the thing that we will start noticing is that we want to keep the performance at a breaking SLA or breaking SLO service level objective. Everybody familiar with SLOs? With service level objectives this is a, but this is something I hear customers they're, they that we don't really do either a good job of conveying or it's just not out in the wild for consumption. SLO think of it as. What, what you need for your business to operate, right? What is it? It's kind of like an SLA, but it's also, it's more about it's from the business perspective versus the end user perspective. So what do you want to run, you know, from a, from your business perspective, how quickly do you want to run it? Maybe a P99 of 100 milliseconds of a throughput of 5K. Or something like that, right? You got, you, you pick that, and then that's what really is kind of sets the anchor for your performance when you, when you're making changes, you know if you're going up or down from that SLO and or staying within it. If you're outside your SLO, you're breaching your own kind of contract, right? So that's, that's really the kind of the what underpins a lot of what the, the work that we're doing here. Yeah, and the SLO should be, you know, ideally something you said at business time, not necessarily a comparison to something you've done before, but in this case, we're, our SLO is we want to stay under P99 of 100 milliseconds, and we found the breaking latency, if we went past 4000 requests per second, which is what our workload generator was giving us, is. We got to about 50 milliseconds, but then if we went any further, we went well past 100 milliseconds, past 4000 RPS. And, and you've probably seen that curve before, right? The, the braking latency curve, it's going up. I'm at 2000, I'm at 3000, I'm at 4000, I'm at 5000, starts to plateau and then diminishes. That knee, uh, the curve is the braking latency. We're, we're staying right at that latency. So. We're gonna say for the sake of this code talk, we're gonna say this isn't fast enough. We wanna go faster, we wanna get, you know, above 5000, we wanna get, you know, maybe up to 10,000, double, more than double the, the performance, and how will we go about doing that? So the first thing you might say is, well, we can maybe optimize the code, but there's, if you look through, you know, because this is an example for a code talk, there's not a whole lot here. There's not a lot of logic, you know, metrics aspects is inserting something into a concurrent hash map. Our business logic isn't terribly complex right now, but we're still not able to push all that much throughput for 100 milliseconds at P99. So we're gonna, we've already gathered an a proof report that we'll go into next. Uh, as we're looking at that a proof report, I'm going to kick off an optimized run. That we'll come back to in about 3 or 4 minutes. And if you notice here we're gonna patch our pod and I'm gonna put in a bunch of different uh Java optimizations and these optimizations we'll talk about in turn as to why we're doing that and why we would do something using the signals we got from AProof. And I'll just add, so we're, we're doing a lot of this through a script, so we, we've done it just, just to make our lives easier. If anybody's interested in what that script's doing, come, come up afterwards, we're happy to show you. It's basically just deploying the pod, starting the run, recording and stopping, making sure that everything's kind of got good timing and everything. OK, so we already gathered the APROF report for the base program, and this is on an M7G XL with 4 CPUs. And when you open up an APROf report from the index HTML, it's this type of website, web page. Before we continue, sorry to interrupt you, Jeff, this is an unplanned interruption. How many have heard of or using Graviton? That's so like a show of hands of who's heard of it and knows what it is. OK, so probably about 6-70% of the room. How many are using it? OK, great. For those who aren't, come to, I wanna talk to you afterwards and find out why. Um, but OK, continue. OK. And another disclaimer is that everything we talk here, most of it will apply to X86 based instances as well. Uh, a lot of things that we talk about for graviton aren't just special to graviton, but we're using graviton because, um, you know, I'm a, I know a lot about how to make it run faster, so it seemed a logical choice. So APERF, as you can see here, has a homepage. It just tells you some very basic, uh, statistics about the, the recording that you did. Things like check that your AMI ID is what you thought it was, uh, check your instance type is what you thought it was, uh, check the kernel version is at the version that you expected to measure. Uh, these are all things that are very wide, seem very simple, but I've personally run into cases where we've had people tell us like, well, we ran. The comparison on a 24 x large system and we're comparing against an 8 X large and the 8X large is slower. I'm like, well. OK, that seems expected, but you know, let's, you know, these types of things pop up and sometimes it's good to have that right in your face, that's the first thing. Uh, and then along the left-hand side, we have a bunch of different statistics. As I said, we collect hundreds of different statistics and we've grouped them into, you know, logical units like CPU utilization, all the different CPU utilizations, and we can click on that and we get, uh, aggregate CPU utilization with time series along the X axis and utilization along the Y. And we can see we're pushing around 60-75% CPU utilization for this application. We're really riding along the top edge of how much we can get out of this pod because we're told that you only get to use 3 CPUs and it's a 4 CPU node. Uh, nothing here looks out of the ordinary, and this is where the wide part comes in. And we wanna start looking at different things, you know, maybe we check our memory utilization. Is, is it going up and down? Is it doing something odd, and we see, no, we're staying pretty constant, our heap is doing what we expect. Uh, there's a lot of other things in here. I encourage everyone to go out and take a for a quick spin and see if it's measuring things that you might not already be measuring or not already thinking about. So we get things like, you know, more detailed virtual memory stats, interrupts, disk stats, kernel config, sys cuddle config, where you can go and tune to fine tune things like the TCP networking stack or your scheduler. If it's not behaving like you expect, and then we also get code profiles like flame graphs, standard system flame graphs from PERF. We're running a Java application, so it doesn't look like much here, but we also have the ability to interface with a sync profiler, and we're able to do CPU utilization profiling. Have, have folks seen flame graphs before? So, OK, so for those who don't know, a flame graph, uh, it really is showing you a population of function calls along the X axis and stacked depth along the Y. So if you see, if you see a wider bar, that's not necessarily how much time was spent in that function, it's how many times that function was called. So it's just a pure, pure alphabetic numerical, or sorry, alphabetic number, so population of function calls would determined width, along with stack depth. Yeah. So it gives you a proportion of how much time each function is run. Uh, it's not, not necessarily telling you the, you know, like, the order is only applicable when you're going up and down the stack depth, and if you're going from left to right, that's, there's no information there. You only have to look at the widths of the bars. And, uh, a sync profiler is used here to gather these plots. Uh, there's a little legend here for those who are curious, you know. Bright green is JIT compiled, so that means the Javajit has actually compiled it into native code. Uh, light scan is inlined. And dark blue is things that you're matching against, but the first thing we want to notice here is. In this flame graph, if I was gonna say I wanna optimize my AOP code, I wanna make the code faster, is that this stack trace has lots of functions that I personally don't ever remember writing. I don't remember writing an internal do filter or uh invoke invoke exact underscoreMT at 20. And these are all of the things that implements the aspects in the object oriented programming in this JVM language that we wrote this in. And it goes on for almost seemingly forever. It's over 100 stack frames deep, and if I tried to look for code with this little magnifying glass, ALP heavy code. It's found some, but we have to go and we find a handful of things. That are things that we wrote. So there's really not much we can do to optimize this code if we just want to go in and hack on our code. We probably need to go look at another signal, another opportunity. And one of those opportunities might be, let's go look at why the CPU is not performing as fast as we think it should be. And before I go deeper into the PMU view of things, let's just give uh how many people here are micro architects that have looked at how a CPU works. One person. OK, I'm gonna talk to you after this. I work at we used to be on the same team. Oh, Julio, OK, now I recognize the voice. Definitely talk with you. No, it's the lights in my eyes, man. OK. All right, let's talk about how a CPU works cause I'm gonna go into lots of CPU metrics, not, not lots but quite a few. So for those that don't know, a CPU is literally the simplest loop state machine you've ever heard of. It gets instructions, we do some math, we go back to one. And this is the You know, abstraction that we all kind of depend on, and it goes around the sloop billions of times a second for any modern CPU. You know, graviton CPU is almost 3 gigahertz, 3 billion instructions per second. Some of the high-end desktops can get almost 6 billion instructions, 6 billion cycles a second, but it's really just getting instructions and executing them. It's just doing some math. But to go a little bit further to look at what ARRF can tell you is we want to look at a little bit more detail. A modern CPU because it's trying to get as much performance as possible, has two halves. We'll talk about a front half and a back half. The front half gets memory, gets instructions from memory and feeds it into a queue that the back half executes. This is almost like a microservice where you have separations of concerns again, where one half is doing some work, pushing it to a queue and the back half is doing some more. But the front half can't wait for the back half to tell it if it's going down the right path in your loops or in your if conditionals, so it's constantly predicting where to go next from the previous history. So if your loop is is doing 1000 iterations, this this front half will try to predict like I saw this loop previously, I'm gonna do 1000 iterations of this loop and feed it to the back end, and hopefully that's correct. And the back end will then tell it whether or not those were correct. And every time you're wrong, or the front end got the wrong instructions, you basically have to flush the entire thing and start over, and that's a very painful, painful thing to have in your code. You want these two halves to operate at full speed at all times, and then you'll get the maximum performance. And so let's go back to the demo. All right, so the very first thing we're gonna look at is just what's the throughput of our CPU, and this is this IPC metric, which is, uh, APERF now has some annotations you can click on things to get some help. This is instructions per cycle. Instructions per cycle is just how fast the CPU is processing, and you really wanna have something greater than one. And if we look at the average here, it's actually under one. So we're spending a lot of time doing not a lot of work. Uh, we want to drive this as high as possible actually for most of our code. Uh, modern CPUs can get anywhere upwards of 8 to 12 instructions per cycle through their pipeline if you get everything aligned. It's very hard to do this, but I'm just saying that CPUs have this capability. They're, they're really fast. Uh, so we wanna go a little bit further. An AERF opt, it puts our PMU stats in ways that we can understand just like we had in that slide. So we can look at front-end stuff first, and we can see that front ends is installs per 1000 instruction or 1000 cycles, uh, so anything above zero here means that things are stalling and not doing any work, and we're at almost 60% of the time, or 600 cycles out of 1000. The front end can't actually feed anything into the back end. So we really should probably look at here first, but since we said let's go wide and deep, let's see if our back end is actually worse, it's not. The back end's actually processing things pretty fast. It's at 2, it's only stalling 200 times out of 1000. So we can put that to the side and say our opportunity is really on the front end. So what are things that we can, can look at there. Some things are branch misses, how many times we're wrong in predicting the previous behavior as future behavior, and this is another thing we want to go and make as close to zero as possible. Something like 10 per 1000 is actually not great. It's meaning the CPU is stalling and flushing things pretty often. Uh, the same thing for is the instruction memory we're fetching from being well utilized. Uh, there's caches along the CPU microarchitecture before you get to main memory, and you want to be able to put all your instructions in the closest memory as possible, so it's really fast and easy to fetch. And in this case, we're also not doing a very good job here. We're missing out of that cache a lot 60 times out of every 1000 instructions. Uh And we can see the same thing translating virtual addresses, physical addresses is also fairly high at 4. So all of these stats I'm going through, you wanna drive as close to 0 as possible. So, I wanna go back to those JVM ops options that we've, we've put in. And we'll talk about the actual results from that script, but the things that we did were We said everything in the front end. We wanna, uh, so things you wanna do to make things front end bound happier is you wanna put things close together, squeeze the instructions as close together as possible, and you want to, uh, and those things help you put things in closer memories. It helps the branch predictor track all the history because it is also a cache, and if things are spread out too far, it'll start missing. And we also wanna put, uh, things in. Contiguous parts of memory as well, so we take pressure off the things that do the translation. And these are all options that we actually put into our Java application. We said, I want to turn off tiered compilation, which means tiered compilation is a way for the JVM to get faster startup times. It compiles methods twice. Once in a very simple, uh, assembly code, and then after it sees it run a couple of times, then it actually does the full optimization. So it actually keeps two copies around, and that takes up space and pollutes your caches. But we can turn that off if we're not worried about startup time. Some people might be, so maybe you wanna keep this on. It's a, a thing to experiment with. Uh, same thing with reserved code cache size and initial code cache size. The default is 256 megabytes and the JVM in this case is not very smart about where it places methods. It'll just find a gap and put the methods there. But if you constrict the space it can be in, it'll actually start packing methods better for you. And then finally we say use transparent huge pages. And this is a way to force the machine or force the JVM to put things in contiguous memory addresses, so you have to use less entries to translate virtual or physical memory. And all these options were just options on the command line. We didn't have to change code. We only had to redeploy our pod, and the final results of that is, uh, we got, now we got almost 20% more throughput at 4750 requests per second. And our P99 is still under that 100 milliseconds. So that's actually pretty good, uh, pretty cheap tunings for a 20%, almost 20% return on, on that. And we can take a look at the report. Did any of the things I just talked about making the front end better actually work? And, and real quick, uh, I'll just point out, you know, we're, we're talking a lot about Java and groovy right now and, and the tunings that you could do in a JVM. There's corollary tunings for basically all languages and all platforms, right, so we do have some of the stuff, uh, documented. We'll, we'll touch on that in a second, but just pointing out, I not, not to, I don't wanna go too far down the groovy thing and let lose people, but letting you know that. APR is gonna be the thing that surfaces those signals, and if you do get those signals, how do you fix them, right? Like that's what we're really trying to teach you here. Yeah, we're, we're trying to focus on, use the data, use all the data to try to guide where your, you know, best return on investment and optimization can be. And so here's a comparison report. So this is another feature that A proof that Aproof has that we find very useful is you can put two reports side by side. So now we can go and do a compare and contrast, an AB comparison. So this is super interesting, but what happens if we have two different microarchitectures in our, in our ARV? So if we've got, say, an M7G on the left and M6I on the right, so the report does the exact same thing. It'll put them side by side, and as much as possible, the metrics that we collect will be named the same and be. Uh, comparable between each other, so the PMU events will be named the same, and they'll map to the same basic, uh, basic ideas. So cache misses are still cache misses. Instructions per cycle is still instructions per cycle, so you can look at them and compare 1 to 1. So that's pretty huge from if you've ever done any kind of performance stuff, understanding the different, you know, the different nuances between say Intel and AMD and, and AR are is, is a big thing that we've already done for you. So that's good. OK, so let's jump straight to the PMU things and so we can show some other features here. So we had not only the summary in the original report of, you know, this time series thing, if we're not wanting to squint too hard, is this line actually 10% higher than the other line? We've now put a summarization on the reports under comparison, and we can see pretty small, that's pretty small, but I'll blow it up just a little bit. We can see that instructions per cycle did get 12% better on average, so we did do what we, we said we could do. We actually improved the performance of the CPU by about 13%, even though the score went up by 17%. Uh, stall front ends actually went down by about 6%. It may not seem big, but every time you can reduce, uh, front end stalls, they're far more expensive than anything on the back end because the front end works pretty much in order. It has to get one set of instructions before it can get the next from memory, but the back end can do things way out of order and in parallel. And if we look at stall back ends, this is kind of the interesting thing you might notice is that when you remove one bottleneck, you start pushing on another, and if you removed all the stall front ends, you'd actually see this would blow up to be a huge difference in the negative in the red, because now you've moved the bottleneck from one part of the CPU in this case to another, but as we talked about this can happen in code as well. Uh, other things that happened is branch mispredicts went down by 20%, so that's good. We're doing exactly what we said we would do and all the things that we thought would happen because we constricted the code cache, we reduced the number of times we recompile methods and uh I'll show you in just a second when we put things in contiguous memory, things get faster, so. Uh, instructions got packed tighter, so we're missing less in the instruction L1 cache. And then we put things in contiguous memory and we got a big jump in the, the amount of times we miss in the cache that translates from virtual addresses to physical addresses. So the core doesn't always have to fault and go look through a page table. It doesn't take page faults all the time. It actually tries to cache those translations. And we can see this went down by 60%, so a big, big decrease. But if 20% is not enough, what can we do then? Um, we can continue down this path of, well, let's get the hardware just to operate, execute our code faster. And we can go to M8G. We've already run this in the background. M8G is getting 7000 requests per second for the same code. With the same optimizations, and we're actually running at a little bit better of a P99 latency, but again, if we push past 7000, it blows up pretty quick, so we're already at our breaking latency here. Uh, 7000. OK, we got 60 or 70% more performance, 10% higher cost. It's a 60% price performance benefit from going from 7G to 8G. That's actually a pretty big win. And so far we haven't actually touched any code if you've noticed if we open that report. I'll go through that really quick. And this is comparing back to the same one that we just looked at, the optimized code path against an optimized code path in 8G. And if we increase that one, we get 38% more IPC. And this is just, just to, to jump in here, this is a function of, of the processor getting better, right? Like, so the processor got more efficient and is able to do, give us these gains, right? And so you'd see that you'd see that, um, year over year probably or gen over gen, uh, uh, not only graviton but basically any processor that everybody's endeavoring to do this kind of stuff, right? Everyone's trying to get the processor faster so you get those free upgrades. So, and graviton is no different. Uh, every generation we've been trying to shoot for 20 to 25%. Intel and AMD try to do the same. If we've got plenty of different levers to make things faster, but you can see even in APRRF that this is what's happening. We are, the cores are getting faster going from 7th gen to 8th gen, and we're getting a lot of performance for a fairly small bump in price if that's a trade-off you're willing to make. Uh, as we can see, back-end stalls actually got higher again because we've moved the bottleneck from front end to back end, and branch misses went down by almost 50%, so everything got better. But so we could say, no, I don't wanna move to 8G. What can we do then? I like maybe there's some, there's some constraint. You're maybe we haven't dropped 8G in the region that you have to run in or something like that, right? So it's a, it's a real problem that we've heard customers talk about. OK. So at that point, what's, what's one of the answers? And really the, as we said, everything has a cost. And aspect-oriented programming or leveraging object-oriented programming interfaces, abstract classes, lots of derived classes, all of these have costs. And a lot of times the cost is hidden in extra code to make all of these nice software abstractions and um. Constructs work in that they run extra code. And if we're really set on we need to use 7G, then we have to go and actually change the code. And in this case, we went the extreme version, we removed all of the aspects, except for the, the very handful of ones we absolutely need, like using the aspects to turn these into HTTP endpoints, and we started inlining a lot of the aspects that we showed earlier, like tracking authorization stats is now inlined, uh, logging is now inlined. And uh if we go into here, we can also see uh this is my, this is the hello or the yeah. The other things in here is we also inline metrics, we also inlined uh rate limiting. So a lot of things are, we've taken away, we've made the trade-off now is we're gonna make the code less maintainable and maybe this is not a trade-off you wanna make, but if we did. The question now becomes, well, did this do anything? Did we get, you know, do we spend a lot of effort and a lot of tech, you know, potentially tech debt to get a small modest gain. And in this case, the answer for that was And, and just, just to jump in here again, like I hope I don't see a review or somebody, a survey that says Jeff and Toby told us not to use object oriented programming. Like that's not the goal, right? Was it the, the goal is to show that there is a cost to all this stuff and it's not for free, although, although in your organization or whatever else you may value, uh, maintainability and readability more than performance, and that is perfectly fine, but that's your decision to make. Yeah, and the, the point is that you can use all the data to try to come to these trade-offs, you know, you're seeing the CPU going slow. You have flame graphs that are 2, 300, 400 stack frames deep. Is, you know, that's all telling you that the code might have gotten too complex or the things you're using are adding lots of extra overhead that wasn't immediately obvious just by looking at the code that you wrote. And if we wind things back and take away some of these aspects, we actually get a pretty large healthy return for our application. We're up to 11,000 requests per second. And still under our SLO of 100 milliseconds we're at 28. And that's that's on a 7G, right? That's on 7G. So 7G to 7G, we can increase performance by almost 3X just by rearranging parts of our code. We still have the same functionality, but how it's implemented and exposed and how it's implemented, compiled, and then run has changed. And we can go and take a look at the report and These are all scripts just so I can open these up fast. And we'll just go straight to, to the Java heat maps. We can put these also side by side. And the thing to notice here is ALP optimizes on the left, so it's the same code with all of our aspects. And then if we run the clean version on the. On the right, one thing we noticed is the aspects we still have these very, very deep flame graphs that are lots and lots of calls that we have to make just to make the code work and if we start backing those out, I don't have to scroll down nearly as much to get to. The end of the code. So really what we did here is instead of, you know, I didn't go and make redo concurrent hash map. I didn't optimize anything with assembly. All we did was take away the overheads that we saw from the signals we got from APERF, which was we're running a lot of code we didn't expect. Can I take away some of that code, run less instructions, and that leads to, you know, some pretty sizable performance gains. It's not always about I came up with a clever new algorithm. It may just be things like this that are just hiding in plain sight. And so that's the groovy demo. Uh, we do have the getting started guide for all of these optimizations we talked about. Yeah, so that, that was the thing I wanted to, sorry if I blinded somebody with the laser there, uh. That's one thing we wanna talk about. All, so there is a getting started guide, uh, that's the, it's a graviton getting started guide. Don't let the name fool you. All the stuff that we suggested in there is applicable across architecture, so it's not just a graviton thing. Uh, and it's all for a bunch of different languages in there too, so it's not just gonna be Java or JVM based languages that you'll see C++ and and some other stuff in there too if you're interested. Uh, Cool. Any questions on, on the groovy stuff? Any comments, any. I have stickers I can give away, yeah. That's correct. You can run it on, on Intel, AMD, whatever you want. OK. So next demo, and we, and we're coming up on 15 minutes, so we're gonna probably buzz through this fairly quickly. Uh, we have Mongo DB. The, the whole goal of this was to show you that APRf is going to surface signals, not just of your own code, which, um, but of other, other things too, right? It's an ambient collector. It just sits on the box and it just collects all the stuff and serves them to you and gives you nice visuals. So what if one of those things that we needed to do was Mongo? We, we had Mongo and it was running poorly. Uh, is there any way to, without rewriting Mongo, is there any way to kind of figure out why it is running poorly? Uh, so this is the, this is our next, our next set up here again. The topology is very, very simple, same, same three node groups. First one being our, our load generator where we're. Running YCSP to kind of load this up. Uh, another one running two different types. This is actually, there's a typo in this. It's an M7G and an MAG. That is not the case. We have an M7G and an M7GD. Does anybody know what the D on the end of the G is? It's attached NVME, so no surprise which one's gonna probably run faster here, right? So we've got EBS back storage and NVME storage for the GD. So the two node groups in uh M7G extra large, M7 GD extra large, both running Mongo again that little A is indicating that we have an ARF pod running on there alongside it. After, after we start it up and load it up. So I'll, uh, I'll let Jeff, uh, do his thing and we'll look at that. OK, so because this is an application that we didn't write, we're not gonna show any code of Mongo. We're just gonna say we deployed a pod with this pod spec where we took Mongo 8. And said, OK, we wanna run Mongo 8 on 3 cores and we wanna get, you know, say 6000 requests per second on Mongo 8, and we're gonna attach some storage to it and we just attach 128 gigabytes of storage and we use a uh a uh volume claim, persistent volume claim on GP 2. And we think this should be good enough. And if we run YCSP against it just to load test and see if we're correct. We come out with a score of only 4000 requests per second. And so this is a case where, well, what can we do to make this faster, and we're not gonna optimize code. The point of this talk is to show you can optimize by optimizing EC2. How do you do it with all of EC2, all the instance types available. And so again we went and recorded APRf. What will APRf show us? So we go and start looking at all the signals first before we just dive in and say, well, maybe I shouldn't use Mongo. That is an option, but can I make Mongo go faster because I really want to use Mongo for some features it has. So we'll open up the EBS report again, now we should be very familiar with the AR start page. We made sure we're running an M7G and we don't have to go very far before we see a problem. Uh, we see that our CPU utilization total is pegged at 100% or very close to 100%. And along the legend, we're looking for, if we look for a user and system, it's way down here. It's hardly using any like active user code at all. And because we're using EBS we're actually sitting a lot of times just waiting for the disk. We're in aisle 8 most of the time, 65, 80% of the time. IO8 is just a signal telling you that, hey, I have threads ready to run, they could do something if my disc came back fast enough. And so in this case, we're just looking at uh. You know, we're disk bound. I don't have to change the code. Maybe if I just throw in something with faster storage, we can, uh, do better. And we have a little thing, this is where we have to say, what are the costs? Uh, OK, yeah, here you go. So. This is another thing we're thinking about with performance engineering. We look at it holistically with all our data is what are the costs to access data. This graph is just all the various things you can access on the system that's some form of memory or storage from a CPU register all the way up to S3, and along the Y axis is latency and nanoseconds on a log scale. So as we go, we're increasing the amount of time it takes by 10x. It's a log 10 scale. And it's not even a linear chart, it's actually more than linear, so it's more than exponential as we get out to disk, where we're taking tens of thousands of nanoseconds, hundreds of thousands, and if you're using S3 as your storage, it's, you know, tens of millions to hundreds of millions of nanoseconds to just. Gather some data to use for your compute and in the CPU terms that's an eternity. If we put this into human time, uh, this chart is just that same graph, but in a table form. The CPU times are in the actual time, but if we scaled it to you're doing the math, you're the math engine, and you can do a math problem once a second. If we're going out to local EBS or or remote EBS because EBS is its own distributed storage system that's spread across an entire region. You, if you have to get a piece of data before you can complete your, your, your calculation, it could take anywhere from 20 of your days or my days to half a year of sitting there waiting for someone to run and get you the paper and give it to you. And that's where we say, well, maybe we should try something like local SSD, the GD. It would only take it 1 day, that's a, you know, order of magnitude faster. So we'll go back and we'll see if that's actually true, if the data that we collected with APERF is actually. Uh, going to give us some advantages if we went to say the 7 GD and we ran this in the background while we've been talking. And if we run YCSB with 7 GD, uh, we see a 3X performance increase. We're up to 12,000 requests per second, and it does exactly if I opened up the AROf report, uh, you'll see exactly that IO weight goes down to almost zero and our CPU time is now the dominant factor. We're actually computing stuff on our document database instead of just waiting for the disk to get back to us. And so that ends the code demo. OK, yep. Cool. So back to the PowerPoint. So takeaways. So takeaways here are just, you know, performance engineering is not just about hacking up great new algorithms and writing in low-level languages, Assembly or C, but it's about finding opportunities wherever they are. So it may not even be code, it may just be opportunities of optimizing the EC2 instances and their setup that you use, and we did that plenty during this demo where we weren't hacking on code so much as we were, let's try 8G or 7GD instead of 7G. And as we said, those opportunities could be anywhere. Um, they're not necessarily always, you know, in the algorithm. It could be how you set up the networking on your devices, the instance families that you choose, the sizes that you choose, cores and memory ratios. And so that's where we've found a lot of great takeaways to share with you, which is using AR or other tools to get that whole view before you go and start hacking away to make things faster. Yeah, part of it is, is understanding what your system wants more of, right? And how do you, how do you identify that? And then once you've identified it, just then you can make the right decision, maybe whether that's a new instance, whether that's faster discs, or whatever, right? So, uh, that's it. Hope, uh, we've got 7 minutes left or we let you go early if you've got questions, we're happy to answer them, but if not, uh, please fill out a survey, tell us you loved it, hated it, uh, or whatever so we can get better next time, uh, but I do have stickers. I have.