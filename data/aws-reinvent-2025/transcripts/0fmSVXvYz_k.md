---
video_id: 0fmSVXvYz_k
video_url: https://www.youtube.com/watch?v=0fmSVXvYz_k
summary: "In this insightful session, \"Building smarter AI coding assistants: real-world implementation guide\" (SPS322), Aitor and Stan from AWS Professional Services share a battle-tested framework for successfully adopting AI coding agents in enterprise environments. They begin by identifying a critical failure mode: the \"Blindfolded Coding Agent.\" They argue that while agents can generate code at \"lightning speed,\" without deep organizational context, they often produce \"fast code\" that is worthless—for instance, implementing a generic authentication scheme that violates strict corporate Single Sign-O (SSO) policies. To prevent agents from \"going rogue,\" the speakers introduce a robust governance structure built on four pillars: Specs (a defined route plan for tasks), Hooks (automated \"GPS rerouting\" triggers that correct deviations, such as ensuring documentation updates with every code commit), Steering Docs (persistent \"traffic laws\" or design principles, like \"always prefer internal blueprints over public tutorials\"), and MCP (Model Context Protocol) servers for on-demand access to dynamic lookups like AWS pricing or API specs. Stan delves into the \"Intelligence Gap\"—the disconnect between an agent's general training data and a company's specific reality. He presents a sophisticated Deep Research Architecture designed to bridge this gap. This system splits knowledge retrieval into two streams. First, for Internal Knowledge, a \"Panel of Expert Agents\" (specialized in Security, Operations, and Functional Fit) evaluates every proposed component against internal wikis and Architectural Decision Records (ADRs), assigning a compatibility score (0-100). Second, for External Knowledge, a \"Deep Research Framework\" employs a Coordinator Agent that breaks complex questions into sub-tasks for browser-equipped sub-agents to scour the web for the absolute latest updates (e.g., features released at the current re:Invent) that the base model doesn't know. The session culminates in a live demonstration of a \"Design Agent\" workflow. Starting with nothing but messy unstructured inputs—scattered meeting notes, email threads, and photos of whiteboard sketches—the agent executes a multi-stage process. It first distills these inputs into clear Functional and Non-Functional Requirements. Next, it queries the internal \"Intelligence Hub\" via MCP to identify approved solution patterns. It then generates a full System Architecture Document, complete with a self-administered Quality Assurance score to verify completeness. Finally, it proactively generates a Security Assessment Package (including threat models and control lists) to fast-track the mandatory security review process. This \"Design Agent\" ultimately produces a rigorous \"Specification Package,\" effectively translating human ambiguity into a precise blueprint that a downstream \"Build Agent\" can execute safely, ensuring that the \"launch experience\" of AI adoption lands with precision rather than crashing at speed."
keywords: AI Coding Assistants, Agentic AI, Model Context Protocol (MCP), Deep Research, Design Agent, Internal Developer Platform, AWS Professional Services, Governance, Steering Docs
is_generated: False
is_translatable: True
---

Hello, thank you for joining us in this session. Uh, my name is Aitor, and I'm here with Esten. We are both from the, uh, AWS professional service team, and we are here today to talk about our experience helping the AWS professional service organization. Uh, adopt coding agents and lessons learned that we, we accumulated so far. So you're gonna see that we're gonna start with this problem statement to help set the station understand a common trap that we have, this blindfolded coordination uh problem. Then we're gonna go and talk about how can we mitigate and avoid this problem. And To start, like, have you ever experienced this situation like you are assigned to a project and all that coordination, it's available, everyone expecting to deploy things in lightning speed. And now you have the um experience saying can this coordination generate authentication system for my e-commerce platform? And right after a few minutes you're gonna get a an uh output. First time it looks OK. Like everything is using the latest and greatest um spec or documentation and when you look deeper, the documentation is there, so it's pretty well structured and so on, but then reality uh hits the the ground so you're gonna dive deeper and see that the agent's not following the session manager that you have in your front end environment. Basically, ignore all those um requirements that you have for single sign-on in your enterprise. And finally, just to keep it simple, like it's volunteer itself to generate more features that you didn't ask. And this is exactly what we mean by fast code isn't necessarily good code. And even good code is not, eventually not the right code for you. So, cause there are many ways to implement things, but you need to implement things following the compliance of, of the, the rules that you have in your organization. Just to put this into a different perspective to illustrate this, if you think on your right-hand side, you have the target, like your desired solution, and you are using the code agent to implement that feature for you. If you don't provide enough context about how you're gonna build this system and give the guidance how to to do that, eventually it's basically using the coordination blindfolded and it's trying to guess navigating through this ambiguity to get to the, the target. It may or may not deliver what you need and uh the session that it's gonna be about how can we prevent that. The good news is Quiro has a lot of features, built-in features that can help you. Minimize the possibility of going. Off of your target. So one example is Spe different development that we talked so much. It's basically how can you break down large complex problem into define the success criteria, the requirements that you have, how you design the solution break down into tasks so you can think if you're planning to drive to a specific location, how you plan your route to get there. Then we have hooks. The uh analogy that I put here is like using a GPS because eventually you may get a wrong turn and the GPS in your car basically is rerouting to help you go back to the ideal course. So you can do this in your environment to uh define non-negotiable requirements that you have and if something is going off the plan, you can use this event to go back to the, the original uh design. Eventually you have the steering dogs, and this is one feature that you can leverage in keyword to define what are intrinsic knowledge or design principles that you have in your organization. To basically teach Quiro to behave like best in class derived consultant. So in this component we are trying to define to Kiro, share our lessons learned how to do trade-off analysis and how to behave when we need to make a decision. And finally, the most common one is MCP. So if you have a a tool that you need to access to execute a task, it's available there for you. The main difference between steering docks and MCP is how frequent those things change. So what I would say is steering docks is more design principle. So if you think about what acted, uh, framework that we have this that's true over time and persistent. Most likely this knowledge should be represented as a steering dog because you use this to understand what good looks like in your environment. And MCP is more for dynamic data, so you don't need to throw all AWS uh documentation inside the session window and flood the context window of the keyro to understand what the service feature set is available. You can access those things on demand when needed. But the main question that we have when we are, uh, working in, in preserve specifically is how can we get to this level of precision like I can't afford to have this margin of um errors or the coordding agent going rogue in different direction. I need to be exactly grounded on the on the customer requirements and that's when we, we start this conversation. So because. Most of the time we are trying to rush into code phase and asking the coord agent to go fast, even if we don't know what we are trying to build first and what are the possible paths. So if you want to get this level of precision, you need to go way back to the notes that you have about what you are trying to build, translate the context from uh window, uh, the human context into a context that I can share with the agent, so agent can be more effective executing the task. So, as you're gonna see, this is all about how can we find a balance between. What's the, what we should put in the context window? Cause if we put too little, The, the agent's gonna need to navigate through the ambiguity and probably this is when they can make the wrong call. If you put too much, it's gonna flood and they basically can ignore some important uh notes that you have about the project. And worse than that, if you have conflicting requirements, like one specific document is saying something and another is saying the opposite, how the coordinator is gonna decide which one to, to pursue. So let's do, do a thing like we're gonna start seeing how we, we implement those things inside professional service to give her a sense how to avoid those problems that we are discussing so far. I know this this is a 300 sessions so um I'm gonna use just a few minutes to level set everyone like I don't know if how many of you have been using Kiro so far, but just to uh level set and show those uh components that we discussed before. So I just opened my. Environment here And on the left hand side I have my project and right hand side, I can start a session using keyword to help me, uh, do things. So if you open the keyword tab, you're gonna see that you, we have all those components that I was discussing before. So SA, the routing plan, hooks, the GPS, um, adjustment. Then we have the steaming dogs, the design principle like I mentioned before, and MCP, the tools that are available. Just to give you a sense what those things means, I'm gonna open here and ask Kiiro to to describe those things for me. So first question that I would ask is. What can you do for me? So, what key role is available? How can they help us build project? And you can basically chat with the agent right in the session, and it's gonna say that I'm here to assist you code development. I can create CLI command and so on and so forth. As I mentioned before, We have a few specs, so if you don't want to dive too deep what spec is, I can basically ask Kiro to say, can you explain to me what's inside this SEC document? And it's gonna try to make sense of what you define in this uh Spike document and define to you, uh, define for you what's in there. So let me see if I can, I need to. Do this. It's back Design agent And So it's thinking, analyzing what's um what's inside this design document, but as we wait, I can show you actually this in the, OK, so it's it's explaining to me exactly the problem that I was describing at the beginning, like how can we translate human context into context that's defined for a coordination to execute the plan. So everything that I'm describing here is trying to go through raw notes, distill this information in a way that I can hand over to our to our agent. So if I go and expand this to see how IPCC is structured, we have 3 main tabs here, requirements just describing why, what we are trying to build in this spec, like this task list if you will. In this case I'm trying to say. I want to be uh navigate through raw data and create what I'm calling a pack package, that's this uh context for agents. Then this is how we're gonna do that. So I've created those many stages, but if I go to the task list, you're gonna see that basically I'm instruction to Kiiro there's just one second, I'm gonna get to the list. I'm starting to kira say this is how you're gonna navigate through this, uh, execute the task and so on, and we're gonna see this in action in in a minute. The other concept that we have here is. Uh, hooks. So the base, the easiest way to, to see this, how it works is actually trying to create one so I can open, um, keyboard here and ask to create a new hooks. The one that I, I like that resonate a lot with uh users that are using this for the first time is update my documentation. So basically what I'm describing here is if I'm trying to build something. And as soon as I save a a code base, a new file in my my my ripple, it's gonna go to my, uh, project rhyme file and update so the readme file is always reflecting what's actually implemented. So if I do this key role behind the scenes is evaluating my prompt here and I wanna create this rule for hooks defining what's the event that's gonna trigger this, uh, action, and then what we're gonna do since whenever we get this uh action being triggered. So as you're gonna see here, this is documentation sync the descriptions every time I'm saving a file with this extension. We're gonna prompt the code to say, is there any change or significant change in my report now that I need to represent in the Rhythmic file? If yes, make the relevant change in the Rhythmic file and keep this updated. All right. Spec, how we decompose complex problems into task lists, hooks, how can we keep this following uh non-negotiable uh criteria that we have. Steering docs, I think the easiest way is again ask Kiro. So I have a lot of knowledge that we put here in the steering doc folder, but the one that resonates a lot with users is this collaboration workflow. And just to give you an example of how this works. I'm gonna ask Kira to describe what's in there. And this is basically. Telling Kiiro how I want the coal agent to behave so. At the very top, I'm saying we are working on this together. If you'll see that I'm asking you to do something that's not aligned with best practices, I prefer you to push back and see why this is not a good idea and give me, keep me honest when I should execute things. If we are trying to do a problem solving, I don't want to, uh, try to get the quickest solution. I need to go deeper and see the root causes on those things. And another thing in our situation is every time we are building stuff, I don't want the agent to take critical actions, and this is things that I'm describing here. Uh, please don't automatically do with commit, get push, and so on. Uh, ask me if you want to do this, uh, take those actions, and if I ask you to do so, confirm before executing. So this is just an example of things that we, we described how I want keyword to behave. We have many other objects here that we usually load as needed, but this one is specifically. If you see at the very top, keyboard has this option to define the inclusion, uh, mode that we want. So when we say always, every single session that we load, and one example is the very top, everything that I put as always, is gonna be loaded for every single session. And if I put manual, let's say, I think this one is manual, so. If I open this one, I'm only loading this steaming do when needed. And MCP is the most popular one. So let me see if I can quickly show one example here. Um I'm gonna ask you to describe what's available in this Knowledge MCP server. And as you're gonna see, it's gonna go through the definition of the tools that's inside this MCP and give me a sense what's available there. And by the way, this MCP is available to everyone. So if you want to leverage this in your project, you, you can include, you know, development environment. And the idea is Um, I can search updated documentation of AWS. It's gonna list, uh, the ripples that we, we usually, um, consume. So it's AWS documentation, blogs, GitHub sample, constructs, and all those things. And I can basically interact with this MCP when I need to get access to your latest data. All right, now that I know Spark. Hooks, Asian steering dogs, and MCP. How can I make sense in a real scenario? So I'm gonna close everything. I'm gonna start with this sample project. In this situation here, I'm trying to simulate what usually happens in projects. Imagine a situation that a person went to a uh um. Reinvent summit or um uh AWS summit and they have an idea like always that I go to those uh summits, I have these notes that I never consolidate well enough when I go back to my company and I need to do this knowledge transfer. I always struggle to remember what I took notes and so on. So what if we have a system that I basically take a screenshot, add, uh, notes in my um DXT or markdown 5 and so on, and I share this with an agent, and the agent is gonna make sense of this. If I'm asking open questions, not answering my notes, the agent's gonna be responsible for searching for the answer and help me consolidate this so I can go back to my company and do a a knowledge transfer effectively. The problem is, it's never well structured like that, a person describing what it needs to be done and how we're gonna execute. It starts with someone. Having this idea via email, then this move to uh Slack thread that people are brainstorming about what we should do, then someone else replied to the original email saying, what if we use the knowledge MCP that we saw to answer those unanswered questions that we have in our notes and then they have a meeting, they transcribe this meeting, then they have the another session is like the whiteboard solution. And now I have no clue what I'm trying to build. If I just ask keyboard to do that and implement this software for me, most likely it's gonna try to help me somehow, but if I don't structure this context in a way that's gonna be effective for code agent to execute the plan, most likely you, you don't have what you are expecting. So how can we Do about that? Like, how can we save, uh, improve this process? Remember that I mentioned in the diagram that we have this design agent? Design agent is is this spec doc that I I created in my keyboard environment here and the way that I can call it is basically saying can you execute. The tasks that I define in design agent and when I go to a new session. And say I'm gonna start this process. It's gonna understand that I put all those notes about this project in that input folder. It's gonna read all those things and execute the task that I defined in this workflow. So first thing is. I need to read all the input data, so all those files that's in our uh input folder. Try to understand what are the requirements, non-functional requirements, risk, and anything that we, we captured from there. And then share this back with the human so they can validate if this is exactly what you're trying to build. So when you go to the build phase, you're most likely gonna be successful because you know exactly what are the requirements, successful criteria, and so on. So instead of waiting this to the end to complete, I'm gonna skip ahead. I have another one here that I executed and the output of this. Processing this input data is in this generated folder. So if I open here and go to functional requirements, for example. I'm consolidating this in a in a way that I know exactly what I'm trying to build in a consolidated way so I'm trying to create a content upload interface. I need to have the content storage. I have an AI power summarization feature and so on and every single. Requirements that I'm listing here I'm trying to, to put clarity for Quiro, understand what I'm actually trying or how I'm gonna assess the completeness of this task. So for this one I specifically, I'm saying this is a must have based on the context and I'm showing where I got this information from, from, from, from the original email and the mid notes that I, I got originally. And this is how I captured the successful criteria of this uh task. And this is not only for the functional requirements, we have for non-functional requirements as well. So we have, uh, I captured the response, uh, response time. Uh, scalability criteria, how we're gonna, um, Evaluate the the storage requirements. And then Finally, one thing that I would like to highlight here is. When I was reading all this context from the wrong notes, I'm trying to break it down in two levels. One is. Business objective, like what are the goals that my leadership or whoever started this project shared at the beginning. So as you can tell here we have 6 main objectives of this project. So I'm trying to organize conference learning, improve knowledge, answer a question that I don't have, and help me like create linkeded posts when I go back to my components so when I can make sense of my notes. And this is how I broke down this into requirements. And I, if I'm trying to think about this from the project management perspective, I also instructed in my workflow to create user stories so we know how can break down this into small modules or work stream and in the end of the day. What I'm trying to do here is. Uh, understand the what, like what we are trying to build. And one thing that I, I have here just to, to consolidate all those notes that we described so far. We have many ways to, to represent knowledge to a coordination to, to leverage those things when we're executing a project. In my, uh, during our experience and to understand how you can select which bucket you're gonna use, I would use this approach. Like if you're trying to represent a workflow, like I was describing, saying you need to read the files, you need to extract the requirements, you need to understand that you need to map those things. So if it's a very structured workflow, Spe may be the answer for that. And important to to understand is you need to. To guarantee that you are tracing uh the criteria to the requirements. And then, when you create a complex workflow, you need to keep this uh handoff material. So when you go to the next step or the next session. The next step in your workflow, they're gonna know what to execute next or what you did before. So this is basically the routing planning that you have in your, uh, car metaphor. But if I'm trying to do event-driven automation, like when I save something, hooks is important, highlight here and based on our experience, try to match this in a time needed. So when you save a file, when you create a file, another important thing is to avoid this loop. If you're trying to edit the same file that generated or triggered the, the event. Try to just list what you should do instead of editing and saving to avoid this infinite loop in your environment, because this is your GPS routing. So if you're making the wrong turn, turn, it's gonna help you go back uh to the original plan. Then When should I understand if it's a steering dog or um MMCP? One thing that I, I wouldn't highlight here is steering dogs more for design principles. So if I need to understand what good looks like to make a decision between trade-off, this is when steering dogs, it's a good approach. And the steering dogs, the most important part is this is, uh, basically when you're trying to drive, what are the minimum amount of information that you keep, so the traffic laws that you, you need to memorize to understand how to drive in a, in a street. So this is what you put in steering dock. It's important to, important to remember the proper cruising pattern to avoid flooding the, the context. So if you are saving everything for always be loaded, probably you're gonna waste tokens in your environment. And finally, MCPs is for dynamic content. So if you have a, um. Um, tools that you need to interact to take action. This is actual the real-time traffic information. So if you need to remember something as needed, like the real traffic information, this is, uh, the bucket that you use to represent this knowledge. All right, so this is just a level set that now we understand what we are trying to build, uh, translating the human context to a context that the coordination can behave. But before we go to the execute, there is another important part that to understand among all the possible paths that we have to execute the plan, how can we pick the right one? And for that I'm gonna, uh, invite Stan to join the stage. oops, to. Share this next session, next part of the session. I think you Good afternoon, everyone. I'm Stan and I will take it from here. So now we have our agent equipped with steering dog specifications, hooks, and MCPs, but is it enough for our agent to be truly useful in real work. Will this agent be able to write to a code which is not only good, but the code which is right? Imagine the situation. You have your advanced agent who is capable to write a sophisticated quote and solve complex technical problems, and one day you're asking your agent to provide a recommendation for your containerized application. Authentication mechanism. And uh your agent just returns your generic of 2.2 authentication instead of referencing you to approved integration patterns for your IDP provider. Or you are asking for the recent AWS pressing information and the agent just apologizes that it doesn't have this information. Doesn't sound familiar? This is an intelligence gap problem. That can affect even the most sophisticated AI agents. The development team Often reports that the agent can provide uh technically correct but contextually relevant recommendation due to lack of access to organizational knowledge. And it's not just a minor problem, it's significant. barrier in the adoption in enterprise environments where context matters as much as capabilities. When developers first deploy their agents in production environment, we quickly realized its limitations. Agent may be excellent in general program, problem solving skills, but uh it lacks a significant context which makes it really useful in real work. Let's take a look into this problem closer. Most of organizations have available information stored in the system which are locked from AI agents. With the information such as architecture decision records, internal wiki pages, team standards. Uh, without accessing this information, agents just use general knowledge and cannot build on top of experience of the organization. From another side of this problem, we have uh External knowledge problem. Uh, knowledge of your agents is limited by training data cutoff time, and that means that we don't have access to information about recent series capabilities, new functions such as functions, uh, and services announced that reinvent. We might not have access to recent pricing information, which is important for cost estimation and evaluation. And we don't have access to uh security vulnerabilities and purchase information that can affect selection of technical stacks and technologies in your system. And in order to solve this problem, we need to find a structured approach which will convert your AI agents from static. Knowledge repositories into intelligent system which can connect, collect, process, and deploy information from both internal knowledge and external current information. How we address this issue in our intelligent agent architecture. On the left-hand side of the diagram, you may see components responsible for internal knowledge processing. And on the right hand side, there is a part of architecture which processes external data. Let's start from internal knowledge processing. And the first component of our architecture. Is a layer of data agents. You may think about data agents as specialized agents, and that has tools that allows them to connect to your internal systems and we have very detailed prompts. System prompts, which explains how this agent should use these tools to effectively extract information from your internal systems. This agent might, might be connected to a system which stores reusable artifacts, um. Various code repositories with blueprints and reusable components that you can use in your architectures. Or we can have access to internal wiki pages and documentation, which stores information about services capabilities and limitations and various best practices and industry approved use cases. When that agent extracted this information from your internal system, we can pass this set of reusable assets into the next layer. On the next layer we have a, Panel of expert agents, you may think about these agents. As a specialized experts, one of them, uh, each of them is specialized in a certain aspect of architecture. We can have functional fit experts. Which determines if a component solves core business problem, we can, uh, or Uh, implements all the required features. We can have architecture fit expert which determines if the component. Follows the established best practices such as well architected framework and how well it's integrated with your internal applications. We can have security experts who can evaluate if this component follows your security best practices. For example, does it have proper authentication, industrialization implemented and how it stores the data, does it have. Uh, encryption at rest. Or we can have operational experts who can determine if a specific component. Is production ready. Each of the experts of this panel review the components and produce the score from 0 to 100 on how well this component fits the target architecture. A part of a Uh, score, each agent also produces summary of where this component fits the target architecture and where the gaps are identified. After all the components are evaluated by expert panels, we can move to the next stage of this process where we perform a ranking and scoring of assets. We aggregate with uh scores based on each individual component and using this aggregated score to form a short list of components we are going to use in our target architecture. Now let's take a look into uh part of architecture which is responsible for uh external knowledge aggregation and processing. In order to collect and process external data, we are using implementation of deep research framework. If you will search over internet for deep research framework implementation, most likely you'll find white papers from companies such as Enthropic, Lang Chain, or Mirorflow. All these white papers are different in details. However, the core idea of all these, uh, work, uh, systems is pretty similar. All of them are using multi-agent architectures to implement deep research functionality over three major steps. First we do job planning, then execute subtasks, and finally, we produces aggregated results. Let's take a look into this architecture in more details. So a core component of this architecture is the coordinator or main agent. Uh, the role of this agent is to get complex user query. And split it into List of individual tasks. Often coordinator is using special tools, a reasoning tool, which helps uh Coordinator to plan the activities and perform the split of complex tasks into a sequence of independent subtasks. And once, uh, coordinators split this complex task into set of individual task subtasks, it spins up multiple subagents and passes these individual subtasks into each of subagents. Subversions are responsible for processing over the subtasks. And in order to process this information, agents are equipped with a set of tools we are using to execute their subtasks. Once each subagent finish their part of the job, they form intermediate result and with result headed back to coordinator. Coordinator aggregates results from multiple subagents, performs cleaning aggregations, and produces the final result. Let's briefly talk about the tools we are using in our architecture. First of all, we are dealing with external data, so we need an instrument to source data over the internet. So we have internet search tool. Once we identified sources of information and we know where exactly this information is located, we need to. To provide functionality for our agents to read this information. So we need some type of browser. You may use one of open source implementation of browsers such as Chromium. Uh, headless or playwright, or you may choose one of specialized, uh, components such as Agent Core browser. Our agents also have access to file system and reading and writing operation, and usually we use this operation for intermediate results processing and estate management. Often, uh, deep research requires accessing specialized sources of data such as code repository on GitHub, and if this is the case, you may put MCP server and for example, uh, strength agents provide the functionality to wrap any MCP server into a tool which you can use in your agent. What are the lessons learned uh when we build with deep research implementation? First of all, uh, deep research framework is quite a complex system with a lot of moving parts and components, and observability is a key component for successful implementation of deep research functionality. Using proper Uh, Observability, you can quickly answer the questions what exactly each subagent is doing, what tools are available for subagents, how we use these tools and which results every subagent produces. And another lesson learned for us was, um, We need to understand what exact, which exact tools are available for each subagents and how subagents are using these tools. We found that sometimes uh agents are using tools in a completely different way from what we expected. So it's important to make sure that agents are using the tools the way you're expecting. And the final lessons learned. Was that um optimization of deep research framework is about finding proper balance between resources you are going to spend for your deep research analysis and Quality of output. It means that you can technically spend hours and millions of tokens without significant improvement of quality of output results. So it's really important to have in place a configuration which you can quickly, where we, you can quickly define. Depth and breadth of a deep research functionality. And you can Uh, flexibility configure with balance to find proper balance between, uh, resources you are going to spend and, uh, the quality of outgoing result. OK, with that, we will switch to the demo. OK, I will start at the same stage where Hector Aar showed before. So we have our demo project. And we passed the first two stages of this project. Just a reminder, in stage one, Uh, we analyzed input documents and we extracted information, uh, what is the scope of the project and if this project is POC or it's fully functional application. So all these details that we can derive from source informa, uh, from project documentation. And on stage two, we generated functional and non-functional requirements, as well as, uh, traceability matrix and user stories. So now we can continue to the next stage. Uh, so we'll type continue. Our agent maintains the state of the process so it knows exactly where we are right now and it will continue with the first task of the next stage 3. the next stage in our process is the generation of, uh, system architecture documents and, uh, at the beginning of this process, our agent checks if we have access to our back end. Uh, our back end that we discussed before is available to our agent through MCP server. So this one intelligence hub here provides access for our keyro agent to our back end which takes care about this internal and external information. When VMCPCR is available, uh, agents submit uh functional and non-functional requirements to our backend and backend, uh. Produces the outgoing uh results. So it will take some time, so I'll switch forward. To show How it looks like. So when the agent submitted the information to our back end, it receives a job ID that the agent can use to first of all, check the status of the job, and the second it can use this job ID to get the results of the processing. You can see that the agent is instructed to check status periodically time to time and it keeps it rating until Let's see uh that uh the status of job is changed to completed. It means that we can't finish the processing of the job and, uh, our agent can receive the results using a separate get results call. Let's fast forward and see uh what did we get from our back end. Uh, so our backend intelligence hub produces, uh, the result which consists of two main parts. As we discussed before, uh, we have two different workflows for internal and internal data. So our, uh, outgoing documents are structured the same way. We have the first part of the document which lists all the components, our experts evaluated and, uh, produced a shortlist. We can see that. Uh, top asset identifier is agent core runtime blueprint, which is our internal blueprint which uh guides agent on how to build and deploy agents on gent core. Our sample project is involving, uh, development of agent and that's why this blueprint is determined as a strong fit for this application. If I scroll down, I can see. Sorry, what is it? Uh, we can see the whole, uh, short list of the components, and for each component, uh, we can see, uh, breakdown of expert evaluations as well as, uh, summaries of all the experts where certain components fits the target architecture. And at the end of this document, we have a block of data. Associated with deep research analysis. So once deep research analysis is completed, it adds the results into the same document. So now our, our agent have the following documents and it has access to. Functional requirements documents to non-functional requirements documents and the results of our intelligence hub evaluation and on the next stage, uh, agent will generate system architecture. Uh, I forwarded to the end of, uh, stage 3, so now I have system architecture generated, so we can see it's quite detailed document which describes all the layer of architecture with, uh, recommended, uh, services. In our sample project we asked to generate. Uh, erless applications, so most of the servers, uh, services are represented by a serials AWS components. When we have our architecture generated, our next step. Uh, is to check what is the quality of uh architecture generated. We have quality assurance step. Uh, where our agent instructed to, uh, perform, uh, quality analysis of a package of the documents to determine if we can consider this packages are finalized or we need to do one more iteration over this package. So our agent evaluates the package across 5 weighted categories like requirements, architecture, security, implementation, and so on, and finally produces a final score. If the score of the package is below the bar that we predefined, it means that the agent will need to do one more iteration of requirements, refinement and regenerate the requirements. If the package exceeds the bar in terms of quality, we can consider that architecture is successfully generated and we can move to the next stage. Next and final stage of uh our process is security assessment. You might ask why we are doing security assessment after architecture generation. Uh, this security assessment produces a package of documents for. Our delivery security review, which is a step in project delivery when AWS professional services provides this set of documents for security assessment by our application security specialists and uh our agent generates this package of a document, uh, documents that is significantly simplifers work for our. Consultants. So you can see that uh we have generated threat analysis document. Uh Which identifies risks associated with certain components we identified for implementation. We have security controls. There's a set of controls we need to implement in order to make sure that application is secured and the set of tests with. are required to implement to make sure that security controls are properly configured and Uh, apply it. Uh, that's pretty much it for, uh, this, um, agent. Uh, today we are demonstrating 1st, 1st agent from a group of agents we are building for professional services. This one is a design agent that, uh, processes, uh, project requirements and generates a specification package. Uh, the next stage in this process could be, first of all, this package can be handed over to the delivery team to implement the functionality. Or this package can be a source of the data for the next agent, the build agent, which can use this information to automatically build the solution. If you're interested to see this build agent in action, please join us at AWS Professional Services kiosk, uh, where we have demonstrations for this agent and uh other agents. And with that, I will hand it over back to Hater. Mr. A Few final comments is, as you're gonna see like those coding agencies helping us build things at lightning speed and it's changing uh the dynamics. It's not a driving exercise anymore it's a launch experience because if you don't plan this properly, most likely you're gonna go very fast in the wrong direction and that's why now it's more. Important than ever that you plan your project properly. So when you launch with vision, like the agent gonna land, uh, where you need like with precision, uh, with that we're gonna open for for questions. Please, uh, fill out the form at the end and if you need to dive deeper on the topics that we discussed here, this is the CR code that you have some learning there, but we're gonna, we have some around 10 minutes for questions.