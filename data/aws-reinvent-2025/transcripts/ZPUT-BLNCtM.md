---
video_id: ZPUT-BLNCtM
video_url: https://www.youtube.com/watch?v=ZPUT-BLNCtM
is_generated: False
is_translatable: True
---

Uh, we're gonna be talking about building .NET applications with AI, uh, and, uh, it's funny when we're putting this talk together we were going to do a semantic kernel talk and then as many of you in the audience probably are following along, Microsoft said goodbye to Semantic Kernel and brought in. Asian framework. Asian framework. So we will have some semantic kernel in here, but we are also going to bring everybody into the future with agent framework as well. So you're gonna get a little smattering of everything, right? You're gonna get everything, everything that's available today as well as everything that's still in preview or beta just so that if you're building something in the future, you can take advantage of uh of the next generation of frameworks from Microsoft. With that there's a giant caveat obviously all of this is still in preview. Things break, things don't. Uh, stay the same, right? So all of this code is experimental, the best kind of code, right? The stuff that you deploy out and you go, ah, maybe this will work one day. Uh, anyway, I'm Am Gravelni. I'm a, uh, principal developer advocate here at AWS. I've worked at AWS collectively for about the past 7 years, uh. I've been doing .NET ever since I, uh, started programming. C was my first language. It's how I learned how to code. I'm Nikki. I'm a principal engineer. I actually am working on the Amazon side now. I, I used to work on the AWS side. Uh, .NET was also very early on in my career. It's very nostalgic. I don't currently work on .NET. I, I actually work on the mobile shopping app for Amazon, um, but C was also my first language and, uh, how I learned how to code as well. So, uh, you know, I thought I'd do a little throwback here, join AM, um, and, and talk about .NET for y'all. Nicky and I have done a lot of talks over the years together, but we have not done a talk together in at least 5 years, uh, 4, maybe 4 years, a long time, basically, uh, but. Dot net never leaves your blood, right? He didn't miss me though, because most of our talks usually is just me making fun of him the entire time, so that's true, she will be heckling me, so you don't have to. Don't worry about it. She'll, she'll cover that. Alright, um, we're gonna get through some PowerPoint. We're gonna get into code for most of the rest of the session because this is a code talk after all. But we do want to set the scene with some of the tools that we're gonna be using, right? Uh, so let's talk about the problem space first though. Uh, AI and chatbots and all of the beautiful LLL LLM things that are going on. Uh, first off, we're talking agentic AI, right? This is. We're rife with all of these different phrases, these philosophies, what is an agent? Yeah, what is an agent, what do you tell us? Well, uh, I won't tell you because I will speak in the words of somebody much smarter than me. Uh, ABS has a distinguished engineer. His name is Mark Brooker. Big fanboy of Mark Brooker over here. Uh, please don't tell him I said that, even though this is recorded. Anyway, uh, I got to interview Mark yesterday, just happenstance, but, uh, I follow Mark's blog. Mark talks a lot about AI, Agenic AI. Mark is, uh, one of the engineers that brought Aurora DSequel to life last year, if you, uh, were following the reinvent announcements last year. He worked on a lot of really, uh, interesting projects. He co-opted a definition from, uh, Simon Willison of what an agent is. And I think this is a good basis to start any time that you're starting to work on agentic workloads. What is an agent? At its core, it's very simple. It's an LLM that exists in a loop that is able to call on tools to do additional work. That's it, right? But this is more of a philosophical thing than it is a technology thing. Uh, and Mark actually had a little quibble with the use of LLM. He said that's limiting. There are other model types. You might have an SLM in that, right? But an agent needs those two core things to be quote unquote, an agent. A loop in which, uh, the actions occur, prompts come in. And then tools to call to extend the functionality of that LLM, right? That's it. And a lot of these frameworks, a lot of the tools that we're going to be talking today achieve that. uh, but Nikki, you work on like real problem solving. Uh, with AI and I, I do, so what are some actual problems that you might want to use an LLM application to build? Uh, a good example of one is like you can use it to summarize meeting notes and then extract the action items from that meeting and then assign them to various team members that were in the meeting or you can use it for personalized. I know we do a lot of personalization in the Amazon app itself. Um, you can also use it to, um, basically create a support chatbot so you can have it give it access to your documentation and have it interact with, uh, customers of yours and use the API documentation it has access to to answer questions from that customer. All right, we've broken this talk into, uh, I'd say two categories primarily. We're going to be talking about tools built by Microsoft that you can use and tools built by AWS that you can use at the intersection of the two, but we're also going to frame this in some of the basics of the tools that you'll use, the framework that you might use. Those are semantic kernel and agent framework these days. And then finally we've also thrown in, uh, a discussion of agent core, right? So this is AWS or Amazon Bedrock Agent Core. This is an AWS product for running and providing, you know, more of the. Operational side, the infrastructure really the infrastructure to run your LLL map so the Microsoft tools that we're gonna show you today are useful to program your LLL map and then you're thinking where am I gonna deploy it and that's where the AWS tools come in. All right, we gotta start at the very, very basics. Uh, if you've done any programming against AI or LLMs with .NET, I hope you know this already. If you're brand new, welcome. We're excited to try this stuff out. This is the foundation of pretty much everything else we're gonna be talking today. This library is very simple to understand. It is just the abstraction layer because look, this space, uh, I know we say this about all tech, but this space in particular is moving extremely fast, right? So, instead of having to learn each individual. Method of invocation or sending a prompt or setting settings, Microsoft has built a library that abstracts that away, right? Because you're just trying to typically build something like a chat client, right? So you can have an abstract chat client that then is. Built into a concrete class by a provider like Anthropic or in our case, AWS. We have built extensions, which we're going to talk about. And I will just give you a little bit. This is one of our first code snippets here. It's not really, uh, that complex. It's not even that compelling, but it does a lot and it does a lot for you. Uh, so this is using the actual direct Microsoft extensions themselves, adding a chat client, uh, and then asking for a response from the client. Simplest use case there is on the planet. Yeah. All right. Now, building on that foundation, Microsoft has built a framework called Semantic Kernel. So the, basically the abstraction layer he was just talking about are interfaces and semantic kernel is the framework that implements them. So it uses Microsoft Extensions AI. Um, and, and it's composed of a few parts, so we have plug-ins, um, and plug-ins are gonna be like your function calls and your tools, so you're gonna give the LLM access to be able to tell the date or the weather or you might give it, um, access to search the web, um, rag essentially, uh, and then it has AI models that you can access or you can give it an AI. Model that you'd like to access and then hooks and filters and hooks and hooks are middleware to basically intercept before the prompt gets sent to the LLM or after the response comes back from the LLM. Maybe you wanna transform the data a little bit or maybe you wanna transform the prompt before it gets sent, uh, maybe you wanna observe it, maybe you wanna log it, um, and then filter. Is basically a type of hook, a specific type of hook, where you can give permission to that LLM. So it's basically a permission-based hook where you can say, yes, I want this prompt to go to the LM or no, I don't. So providing security there or just, you know, not it doesn't have to be security. Maybe you just decide I don't want that LLM called right now or I want this other LLM called based on this specific prompt. So as you can tell, maybe, as you listen, uh, and I'll use a place near and dear to your heart, I hope, to, to maybe help establish what these types of frameworks do, right? Um. So this software is building what we talked about already. It's building that loop. It's building that function calling, right? The tool calls, uh, for you. You can do all that. You can build that in software yourself, or you can use a framework like Semantic Kernel that achieves that for you. To bring this into a different area of technology, think about the, uh, glory days of JavaScript. Anybody, uh, remember, you know, having to make. Uh, Ajax calls. Do you remember Ajax? Oh yeah, fondly, right? Yeah, everybody loves Ajax. So again, a lot of these, uh, prior to things like React and Angular, you would make the Ajax calls, you would redraw the UI, right? You would write that all in JavaScript code. Then they started building frameworks that did this for you, right? So it's exactly the same idea. You could build these agents, you can make them do what these frameworks are doing, or you could just use something that somebody else is maintaining, somebody else has built, exact same idea. Yep, totally, yeah. And you'll see from the code snippet how easy it is to, to use. Use the framework but also to add we're gonna show you how to add plug-ins in in real time we're gonna show you in the code, um, and so it just makes it super easy to basically create your LM application of course you could write all the code yourself, but why would you when you when it's just basically like 10 lines of code here versus thousands if you were gonna write it yourself, um, so this code snippet is also very basic, um, basically just getting credentials. I'm stating the model ID of the model I want to call this example is using Bedrock and semantic kernel. So I'm establishing the Bedrock runtime client with a region and my credentials, and then I'm building Semantic Kernel, and I'm adding Bedrock as my completion service. So you can obviously add multi different completion services, but Semantic Kernel has, as you can see here, native integration with Bedrock today. Um, and then you can basically call the, uh, the chat completion service. So you can see here I'm sending hi, how are you, which is really a dumb lame prompt, and then I'm writing the response to console, which would probably the LM probably would respond, hi, happy to see you. How can I help you or whatever the heck it's gonna write. Yeah, and as Nikki stated, right, uh, semantic kernel, and this is going to be a key difference that we see when we move into Microsoft Agent framework. Semantic Kernel has built-in native support for Bedrock, and it also has built-in native support for Bedrock agents, which if you're building new these days, we would advise you to not start with Bedrock agents. But instead moved to Agent core. Uh, again though, there is not, uh, this native support like in Semantic colonel for agent core, right, for and for agent framework either because it's in preview. Well, that's gonna be an interesting thing that we're gonna see here in just a minute when we get to. Microsoft Agent framework. There is no native support for any AWS services at this time in agent framework. Uh, this is my ask to you all. Number one, this is brand new. Uh, actually, this altered the foundations of this talk a little behind the scenes. Nikki and I, uh, were, were messaging each other as we're preparing back in October or so. Hey, wait a minute. What just happened, right? Because Microsoft made a very, uh, exciting announcement, admittedly, that they were taking Autogluon and Semantic kernel, uh, moving them aside, kind of combining what these two were good at into this new Microsoft Agent framework. Uh, but as of right now, it's still in preview. So I gave you the caveat up front. I'm going to give it to you again. This is all experimental. All the code you see may or may not work. It's just to show you how to use these things and hopefully my ask to you here is that we as a community can go in. This is an open source. Project, we can request, hey, we want bedrock support. We want this support, we want that support. That's what I want you all to be thinking, uh, as we continue in this talk. Like, how would you want to use agent framework with AWS and what should we take as feedback from you all? Again, Nikki and I are both resources that you could reach out to. We'll. Have contact information as well. We're in direct connection with the dotnet, uh, AWS team as well. They are looking for feedback too on how you are going to want to use this directly with AWS. And as I said, there's a GitHub repo that you can go open issues yourself if you want to do that, or you can even open a PR if you really feel so inclined. That's, that's, uh, aspirational, but yes, please do that. Uh, anyway, so, uh, what is the difference between agent framework and semantic kernel? I think that's yet to be fully realized. Uh, I think from my usage of it so far, I can say personally, it's a lot simpler. Yeah, much simpler. So you saw semantic kernel. The code snippet had about 10 lines of code. Well, we're going to show you a Microsoft agent code snippet and you'll see it's a lot less, but it offers a lot of the same functionality that we just covered in semantic kernel as well. Obviously it has tool calling as well. Again, we said at the beginning of this. That's a pretty foundational part of building agents. You have to be able to call up to tools. It also has the loop. They're still adding in more and more capabilities. They've got capabilities for, uh, memory, both short term and long term. They've got chat history, I believe, so far. They've got quite a bit of multi-turn conversations. Yes, multi-turn conversations. They do not have 1 to 1 parity with all of the features in semantic kernel yet that I've seen. So, again, This is maybe not something that you're deploying today, but it is probably something you'll be deploying soon. So it's good to get ahead and learn about what is coming. Uh, let's take a look at that code snippet. As we, uh, could mention. Nikki, this is much, much simpler, yeah, you see, it's much, much less lines of code in general, um. Yeah, definitely less, uh, but here we're doing the same thing. We're establishing the model ID, obviously getting AWS credentials, establishing our bedrock runtime client, and then, uh, passing it the model ID, and then we're creating a new chat client agent. So that's different from Samantha Colonel, which is a type of AI agent, as you can see. And then I have my message, and then I just call agent.run a sync with my message, and that's it. Um, and it's, and it's really easy to integrate features into, uh, Microsoft Agent framework from here. It, it just, it's less complicated than semantic kernel. So for me I can't speak on behalf of Microsoft, but it feels like semantic Kernel was basically like, let's get something out the door really fast because this, uh, you know, this space is moving really quickly. And then it feels like this one they really thought out and they thought how would a customer be using this? What, what easy code what's ease of use basically for writing code for an LLM app and so they kind of put it together here which obviously it is using concepts from semantic kernel, um, but it just feels, it feels smoother, feels very easy. Yes, yeah. Uh, I will point out though, you don't see any mention of bedrock directly, uh. Right? So this is not a native integration with Bedrock like in Semantic Kernel. This is just a standard interface you can see as iChat client, right? So this would work with anything that could implement iChat client, which As you can guess, if you've been following along, comes from that Microsoft Extensions. AI library, right? So, while there's not native support like there is in Semantic Kernel, you can still use Bedrock very easily. Uh, speaking of, let's get into some of the AWS tools and see why you can use it so easily. Uh, we will, uh, Will, will speak on our implementation of that library now. So, again, like I said at the beginning, The Microsoft.extensions. AI are just abstractions, not concrete classes. They don't have concrete, uh, implementations. You have to build those as a provider, and we have. We've built, uh, the bedrock. MEA, which stands for Microsoft Extensions AI. Very creative, right? Uh, it says what it does on the tin. Anyway, so you get a few things out of that, right? You can use the embedding generator. You don't have to set that up yourself. An image generator, uh, that also, uh, does not take extra setup. Uh, so. Inside of our project that is using Agent Framework, Microsoft Agent Framework, we are also pulling in as a dependency package, this library, right? Because again, I said none of the, uh, Microsoft Agent framework. Code itself has implemented any of the Bedrock features yet. If you're using semantic kernel, you don't necessarily have to pull in this package. Semantic kernel has built-in support for Bedrock in the code itself. Uh, and here's the code if you wanted to use just this, right? Just the MEAI package itself. Uh, this is just doing the exact same thing we've been doing in all the other examples, connecting to an LLM, sending in a prompt, getting a response. Key thing to note here though is that this example is actually not using Microsoft Semantic Kernel or Microsoft Agent framework. It is. Just using the extensions AI interfaces, um, as implemented by, uh, MEAI, our, our extension basically. That's right, no loop, no function calling. This is at its basic. I just want to talk to an LLM. That's it. All right, let's talk about the, uh, AWS tools that you might use. In addition to these frameworks, so this is where we kind of shift, I'd say into talking less about I'm building the software that's gonna run an agent into I need infrastructure to run that agent. Nick, you wanna, you use Amazon Bedrock quite a bit, yes, so we've been talking about Bedrock, you know, for how many slides now, but we haven't actually told you what it is. And if you, if you're not familiar, uh, Amazon Bedrock is our AWS cloud service that provides. Uh, you know, fully functioning models and, um, optimizations to app developers. So, you know, we probably are familiar with SageMaker. SageMaker is our traditional, um, ML service where you can train a model and you can do data transformation and all kinds of things. But if I just want an LLM today, I don't wanna train anything. I wanna use one of Anthropic's models or maybe I wanna use Nova, Amazon's model, uh, where am I gonna go? I'm gonna go to Amazon Bedrock. So I feel like it's really for the app developer and in that same vein, we launched Agent Core. What is Agent Core? Well, Agent Core just takes that even one step further and is offering a ton of serverless infrastructure for you to basically run your LLM app. So we have runtime. What is runtime? RunT is going to run my LLM application inside of a container for me. I don't have to really do anything. I just have to basically provide an ECR image. For my, for my, um, for my container and then it will basically handle the rest so I'm not doing any of the maintenance. I'm not doing any of the scaling. I'm doing nothing. Runtime's doing it all for me. Um, Gateway Gateway is providing my LLM application access to functions inside of AWS, so lambda functions, um, or other APIs that you might have, um, and then there's memory, so we have short term memory and long term memory, um, and it'll basically stand up a another container. Conserve memory to your LLM application. We have observability default, basically functionality built into CloudWatch. So when you're running your runtime, you will have your container logs in CloudWatch. Identity is going to be, uh, access to authorization and authentication via Cognito. And then code interpreter, that's a really cool one, that's basically running code in, um, an isolated sandbox for you and then browser tool is giving your LLM access to a web browser. And today we've got a project that we've built that we're going to show off specifically runtime, gateway, and code interpreter. So I wanted to give you a little bit more information about these three aspects of gent core. Nikki gave a great overview. Run time is where your actual agent code will get deployed. Um, so, again, this is another thing that I'm going to implore you all, uh, and there is no GitHub repo this time to open an issue. So you will have to talk to me or Nikki. Uh, we would love to hear how you want to use agent court, because right now, Most of the support for using Agent Core without setting up a lot of things yourself, as you will see in our code, is done, uh, in Python. So Python has a lot of decorators, some libraries. I'm sure I see some heads nodding out there. Maybe you've already looked into this and gone, wait a minute. I have to build all this. Hold on. OK, so we're gonna see, uh, I'll walk you through. I did some discovery. Uh, there's some things that I can point out about if you wanted to run .net code in agent core runtime today, you can do it. I'll show you how, but I want to make that easier for you and I want to hear what you want from that too as well. So please feel free to share that, but. This is a very interesting service. It runs on Firecracker, which we're going to see. So it is an isolated micro VM, uh, it gets invoked. In two ways. It is, uh, when you deploy your code to runtime, there is an API endpoint that you need to implement called Ping, which, any guesses what that does? There you go. Uh, yeah, it will ping the service to make sure it's healthy. Then there is a slash invocations. So, you are not actually going to interact directly with your agent running in runtime. You are going to have the prompt sent through another SDK call in the. AWSSDK called invoke agent, right? And then that in turn passes on to your agent code running in runtime, which then invokes this slash invocation. So it'll look a lot clearer in the code. So if you have like a UI basically with a chatbot and you're interacting, your UI is going to use the SDK to call invoke agent. It's not going to call like an API endpoint directly. And this is just a, uh, architecture slide of how runtime is, is operating, the things that I just described to you. Um, so it is a container-based service. You will build a, uh, an A container specifically, and, uh, it gets uploaded into ECR which in turn goes and deploys on runtime and it goes through this cycle of if I get a prompt through invoke agent. I send it to slash invocations in runtime. And whatever you do inside how you implement that endpoint, it's up to you. That's where those other frameworks so that we talked about can really help you out develop that application. So that's what we're going to show you today with our code is how to use semantic kernel or Microsoft Agent behind that invocations endpoint when you deployed it to Agent Core runtime. Yeah. And you might be asking, well, why would I need this? Why don't I just deploy it on my own container? Obviously, you can do that as well, uh, but this handles, uh, scaling for you. It handles session isolation. So this is a very fun problem to have to solve, uh, if you have to solve it yourself. You have to somehow isolate sessions from one another when you have users interacting with your chatbot, right? You don't want questions from user Nikki leaking into user AM session, uh, even though I would love to see what Nikki thought. Well, maybe you do. But so some of the things you get, uh, are things like session isolation by using runtime and in conjunction, uh, identity, right? Nikki told us about identity earlier too. All right. Let's talk about Agent Core Gateway. This is one of, uh, my favorite things in Agent Core. It's really, really neat. Uh, you don't have to build an MP NCP server, right? That's the, the premise of this. If you already have a REST API. And you've got an open API spec, you got an MCP server now if you want to upload this to Gateway. All you have to do, get that open API spec, uh, create a target within the gateway. This is another really interesting thing too. You can have a single gateway endpoint serving up multiple MCP servers. Uh, and an API is not the only way you can implement it too. You can also choose a lambda function, right, to fulfill what you need. So, uh, you get an endpoint for your gateway. You can manage things like API keys that are needed to invoke your rest API, for example, uh, and you just provided targets. That's it. You got MCP servers. It's, it's really, really, it's really easy. We're, we actually used it in our example and we're going to show you how it works. All right, and there you go. Same thing. Uh, as I said, you're gonna have an API endpoint target that's just gonna manifest then as tools within, uh, The gateway, and same with a lambda target. You just implement it and then it is exposed as MCP server tools that your LLM running in one of these frameworks can utilize as a tool call. Not much. Not much to it. It's pretty simple. You don't have to build an MCP server, which I love to hear. You don't have to build in any kind of sentence, um, agent core code interpreter. So I kind of mentioned this briefly about code running in a, in an isolated sandbox. And basically if you can think about a use case where the LLM needs to write some code and run it to give you a response, so let's say I tell the LLM, hey, I want you to read the CSV file and I want you to calculate the highest revenue in XY. Y Z category which is relevant to the CSV file. Well, my LLM needs to go write some code to extract the contents of that CSV and then run the calculation. And so where is it gonna do that? So it's gonna do that in agent core code interpreter. An agent core code interpreter is literally just another container that the LLM can spin up. It's an isolated sandbox, so it does not have outbound Internet access, so everything is secure. It can basically write the code, execute it. And spit out a response and then send you back that response. Um, you can also give that container you will, you don't have to set it up, which is nice. You don't have to do anything normally you might have to do that. So this is another way to like, you know, do something for you because Asian Core is always about doing things for you and you don't have to do it. But also you can give it IIM permissions so you can have that container be able to access something else in your AWS account that you needed to access. So here if you look at this diagram. The user is sending a query. The LLM is invoked, and the LLM goes, Wait, I need to run some code, and it's going to spin up this other container, the code interpreter, where it's just a container. It has a file system and a shell. It's going to run the code that it, uh, wrote, execute it, spit out a response. Obviously, you get observability on this container that is running via cloudwatch, and then it's going to send the result back to the agent to then give back to you. And some of these choices of what you use when come down to, you know, trade-offs, as is everything in software is trade-offs, what you want to do, how you want to do it, etc. You may be thinking, why don't I just implement, you know, in C, a tool that does the thing that I'm going, I'm, I'm trying to do in code interpreter. Well, number one, code interpreter, if you noticed on the slide previous, supports Python, JavaScript, TypeScript currently. Um, so maybe you need to do something with a Python library, right? And you need the LLM to do that on a regular basis. We've got an example that we're using in our project. One of the other neat things of code interpreter is you can include files alongside the code that runs. So, maybe you're following me here. You put a CSV alongside this, uh, code interpreter, and then you use statistical libraries from Python, which Python has a lot of built-in math and statistics and data science, uh, libraries that are very commonly used. You want to do statistical analysis for a data set and give permissions to the LLM to do that as people are prompting the LLM. You would probably put this in code interpreter, right, instead of building a tool to do this. Not saying you couldn't do that. You could probably build a tool like that as well, but code interpreter is built for things like what I'm just describing, right? OK, let's show them what we built. Is it code time? It's code time. All right, let me log in before you, uh, There we go. OK. Let's see. Did that not work? OK. Hold on. Connected. Try it again. Of course, technical difficulties, love that. Oh, there it is. You had to threaten it. Thank you. Yeah, that worked. Thank you. It got scared. Decided to work. All right, we're gonna start slow. Uh, not for you all, for me. So we're gonna build up to what we build. He always needs it slower, so that's, that's normal. You know, this old brand doesn't work, uh, as much as it used to. All right, so. I'm gonna start off with an example here. This is using, it's so weird, you stash. I know this is bizarre. Do you want me to sit next to you? Would you feel better? Oh, but that chair is taller. Anyway. All right, moving on. Sorry. Uh, all right, so we're gonna take a look at, uh, I'll show you here, nothing up my sleeve. Uh, I'm using the bedrock. M A M E A I. That's tough to Jesus, I'm done with the chair. Uh, you know what, I'm not, I'm not that tall, so you sitting and me standing, it's fine. Uh, so I'm using the MEAI package here. That's it, right? And I wanted to show you, uh, we saw this in, uh, PowerPoint, but you know, it's not as fun coding in PowerPoint. Uh, so, all that we're doing is instantiating. This is from the AWS SDK, uh, for .net. We're sending in a model. We're using Sonnet 4, anthropic model, uh, and then literally just sending a prompt is this thing on. And of course, it wouldn't be complete if I did not .net run it. So I know you're all gonna be like, where are the, where are the gasps? Where are the oohs where the ah there we go. All right, yeah, uh, I don't know if this one deserves that to be honest, not yet. All right, OK, sorry, a little, a little too early for that, I guess. Uh, so that's connecting up to Bedrock, which in turn is sending our prompt to this anthropic model for inference, and we got a response. I know. Uh, it's, it's, it's amazing, so magical it is. Uh, we'll take questions in the back at the end. Uh, all right. This is the next demo. We are going to start slow again here with our two Microsoft frameworks. So we've got an example here in Semantic Kernel and we've got an example in agent framework. So this is your typical hello world. Like if you just wanted to get things connected up, get a tool call going, and, and see it working. Do you want to talk through semantic kernel or agent framework? Let's go to semantic kernel first, OK. All right, so I mean we saw, we saw, we saw this in the PowerPoint, but you know it's always better live because we're actually gonna see it run. Um, the only thing different here from the code snippet you saw earlier is I'm actually adding a plug-in which I mentioned were functions that I can call, um, to give the LLM more, more tools in its toolbox, and this one is a date plug-in that we wrote because LLM. Can't tell time, we all know. And, uh, all it does is just give the LLM the date. So if, if the prompt is what is the date, the LLM can then invoke the date plug-in, um, you know, well, semantic kernel helps the LLM then use that tool. Yeah, I want to point out a few things here too. Um, so Kernel, right? This, this class Kernel, this is, uh, at the heart of Semantic Kernel. This is setting up the actual agent loop, the function calls. This is how you start any, uh, Usage of semantic kernel in any code you write. If you are building this into a web service, for example, uh, Microsoft in its documentation recommends using a transient service for each invocation, uh, because they, they built it in a way that's very lightweight, you know, standing up a new one is not, uh, intensive on resources. Uh, this is a native. Support built into semantic kernel. So you see in this example we're not invoking the bedrock runtime client because we don't actually need to because Semantic Kernel is doing it for us with this native support. All we have to do is give it the model ID. Yes, and as Nikki mentioned, uh, we have a date plug-in. Um, this will be another divergence that we see when we look at the agent framework example. Uh, this uses these annotations, right? um. You don't see this in agent framework. We'll take a look at that example next, uh, but that's just a little implementation detail, uh, something I thought was very interesting. The next thing I wanted to call out to was this function choice behavior. This is registering those plug-ins with the kernel, right? So the actual framework itself needs to know. How do I use these tools, right? That's why we have a description gets the current date. Uh, that's up to you to write, by the way. If you don't write a good description for your tool, it's not gonna work very well. So, uh, you need that as well so that the LM can figure out which tool does what, uh, and finally. We've got chat history. We're adding a user message. Uh, that's a distinct difference between Samantha kernel agent framework. You have to create a chat history array, add a message to it, and then pass that in, um, when you make the call. Let's see my program.cs. OK, so we've got Samantha Colonel. You all want to see it running, I'm sure, right? Did you pray to the demo gods today? No, I didn't. That was a mistake. It was a huge mistake. 00, it worked. Alright, that's look. LMs are really good at a lot of stuff. Uh, it's funny that it says I don't have access to a function. It does. Uh, LMs are good at a lot. It's I gave you the date. It's not good at telling time. It does know, it, it does not know what day it exists in, I promise you. Uh, if you just ask an LLM what day it is, it won't know. But now it does because we allow it to. So, let's move over to agent framework and see the differences, shall we? So remember, no native bedrock support, no AWS support really at all, because it's brand new. Um, and so you see here we have to establish the Bedrock runtime client ourselves. Then we're gonna use our MEAI extensions. We're gonna turn this into an iChat client. So now the Bedrock client is going to implement that interface, uh, for us using those lovely extensions written by the .NET team at AWS. And then we are going to add the date tool. You see how it's very different how we're adding a tool here. It's not considered a plug-in. It's called a tool, and we're adding a list of AI tools and then we. Establish our agent, we pass it our, uh, implementation of iChat client and our tools, and then we have a message, so we don't have a chat history array, and then we can just run our, our message asynchronously. And if you wanted to have multi-turn conversation, you would have to create a thread object and pass that thread with the, with the message. So it's it's a kind of more isolated in this way and that you can just have a single prompt and response and not everything has to be a conversation. Another thing I wanted to point out that's very interesting to me in this new implementation is, uh, you actually can register an individual function from a class as well. Uh, so that you're seeing this AI function factory. create instead of registering a type, for example, in semantic kernel. Semantic kernel, you can also do things like, uh, add a plug-in by object if you need to instantiate the, uh, object yourself, maybe pass in some, some parameters in the constructor, you know, whatever you need to do. This actually lets you. Just do individual functions as well. So it's kind of, uh, a little more flexible, I'd say, uh. And that flexibility is kind of goes to what I was saying before how Semantic Kernel feels like they just got something out the door as things were moving quickly and Agent Framework, they really put more thought into how you would use this. I have to use our incredibly high tech solution of commenting out code. Look at that, wow. This is not the main event, by the way. This is a precursor. We just wanted to make sure you really understood the code before we jumped into our, the application that we built. OK, so. You can see also from this response, a little more aware of the tools for some reason, uh, I wish I could tell you why, uh, the difference between these frameworks, I can't, uh, but. I like this designation as well. We asked for the time, right? The LLM using the, the, uh, actual function that we gave it as a tool recognizes I can't actually tell you the time. I can only tell you the date. So it's very specific, you know, uh, it, it's, it's telling you exactly what it can do. It's still got December 1st. Feels a little bit smarter. Wonderful. OK, all right. So, uh, I'm gonna move over now to our, uh. Favorite genic ID hero, and we're gonna look at a much larger project now. So this project has, like, as you can see from the, from the, um, File Explorer, 4 things going on. So the horoscope API is a lambda function we're using gateway to communicate, uh, with this function to our, um, LLM applications. Horoscope UI is really just an MVC app that's running a nice beautiful UI and interacting with Agent Core via the AWSS. As we mentioned, that's how you invoke your, uh, LLM apps deployed to Agent Corre runtime. And then the other two things that were in there were two different LLM apps. One was Semantic Kernel and the other one was Microsoft Agent, um, and they're both deployed to Agent Core runtime. So if you look inside of those, which we will, there will be docker files there and there will be a deployment. Script that sends that image up to ECR and then it is deployed and there's very specific differences that make it an agent core runtime application and he kind of mentioned briefly how you needed a ping in point and an invocations end point. You also needed to run on port 8080. There's very specific things that we will make sure to show you of how to actually get it running in Agent Corre runtime. You all wanna see it working first though, don't you? Of course, yeah, yeah, uh, and Nikki, you're very proud of this UI, aren't you? I am very proud of this UI. You can switch between the LLM applications in the UI. Yeah, look at this. Look at this feature. That's incredible. Uh, OK, let's start with Semantic kernel. Semantic Kernel in this project has more limited capabilities that we built into it than we spent time building the agent framework side. So we'll see that. Uh, we thought it would be more important because if you were starting to build an LLM application today, you're not going to start building it with semantic kernel. You're probably gonna want to start building it with Agent Framework. Uh, but I also wanted to demonstrate, like, you know, if you build something with different capabilities, what does it look like, etc. etc. So, uh, let's start. This is a horoscope, agent. That's all it does. It gives us a horoscope. Unfamiliar with that? We all have a zodiac sign, uh, dictated to us by when we were born. Uh, it just so happens. And you can argue with me in the back of the room at the end that Pisces is the best sign. I, I, it's, it's just the data. I'm sorry. And it also just so happens that we are both Pisces. I only took a sample size of me and Nikki, uh, so that's where the data came from. OK, so go, give us our horoscope. I wanna know. Are things looking up for me today? OK, so we'll use it. Let's see. OK, today brings a wave of creative inspiration, dear Pisces. Wow, I feel it. I feel it too. I feel it in the air. Your intuitive nature is particularly strong. OK, so I wanna show you, uh, one of the diff differences that we built into our app here. Uh, can I get a horoscope for Pisces? For the week. And I didn't spell Pisces right, it probably won't care, but. Not gonna tempt any fate here. Let's see, this one might be challenging for it. Ah, as you can see, it's the same. It's exactly the same. It does not have capabilities that we built into it for a weekly one. However, we will see in, uh, agent framework that we actually gave tools for that. So first off, why we showed you this date plug-in, uh, when you're asking for a horoscope on a certain day. You gotta know what day it is, right? You don't want your user to do that. So that was the first plug-in that we gave our semantic kernel demo was, how do I tell what day it is when a, a user asks for a horoscope. Uh, the next plug-in we gave, and this is, uh, This is actually calling our API directly. So as Nikki mentioned, and we can look at this depending on time, it's not the focus of this talk, but I do think it's kind of an interesting project. We built a, uh, AWBS SAM application, not familiar with SAM, that's our serverless application framework, uh, for deploying serverless applications, uh, a little bit more quickly. Uh, anyway, we've got, uh, this is just a lambda function. That's all a horoscope API is. Yes, uh, it's API gateway, a lambda function, and a dynamo DB table. Uh, it also is using bedrock to generate the horoscopes in real time. If somebody has asked for a horoscope already for that day, for that sign, it stores it in Dynamo, retrieves it from Dynamo instead of going to Bedrock to generate a new one. so we actually implemented in Get Horoscope as our tool, our plug-in, as they call it in semantle, semantic kernel, uh, an API call directly. So again, a tool can be anything that .NET code can do. You don't have to use MPC or MCP. I do that every time. MCP servers, you don't have to use that necessarily as your tool definitions. Anything like we saw our, our very simple date plug-in just gets the date. That's all it does. Uh, so this is actually calling our horoscope API that's deployed. Uh, we've got. Uh, we didn't go through the API endpoints yet, but we do have a daily, a weekly, and a monthly. But you can see maybe why an MCP server in this instance would be a little bit more convenient to use, because I'm gonna have to build a tool for each of these endpoints, right? I'm gonna have to build a tool for daily. I'm going to build a tool for weekly, and build a tool for monthly. That's not true. If I'm using an MCP server, for example, uh, I mean, you could, you could potentially build a tool in a way that makes it more configurable. There's, there's other options as well, but MCP as we're going to see in, uh, agent framework example, makes it a lot easier. All right. So this is why. It can't do weekly horoscopes. It literally can't. It does not have to. It only has access to daily. That's right. OK, um. Let's take a look and compare. So again, we've got a date tool, exactly the same. We've already seen this. I'm gonna skip it. Let's look at the horoscope tool. So, if you notice, I'm doing the same thing I just mentioned, uh, what about an MCP server, right? I'm not setting up the MCP server here because Agent Framework, uh, if I go to my agent.cs class that I've built here, uh, it will actually let you. Take an MCP client. Uh, use the MCP client to list the available tools based off the input that you give it and automatically register those MCP tools to the LLM as function calls. Right, that's pretty freaking cool. All that's doing this tools. a range. So we saw this already. AI function factory. You see he's adding a bunch of tools, state tool, analysis tool, horoscope tool. And, and then the other one, so he's adding specific functions in the horoscope tool, there were two functions, one to get your horoscope, the other one to get the different zodiac signs. That's right. But with my MCP tools, like we can keep, uh, looking at this, uh, I'm actually doing a list tools async, and I've set up, OK, you have to trust me. I, I can show you if you want later, but you have to trust me in this demo. I've set up Agent Core gateway with an open API spec. That points to that horoscope API and takes all of those endpoints weekly, daily, monthly, and provides them as MCP tools in that MCP server. So, as I connect up to that MCP server, I can use, and this is just a standard MCP client library in .NET. It's the one that everybody uses. I make a call to the MP NCP server. Do it every time, every time. Make a call to it to list the available tools. It has my daily, my weekly, and my monthly available to me. We can actually go look. Shall we switch? Let's do it. You gave me this beautiful UI that lets me switch and show people. Thank you. You're welcome. All right, so I'll ask the same question. I even copy paste. Wow, how lazy of you. Exactly. That developers are lazy developers. All right. Can I get a horoscope for Pisces for the week? Now we're on our agent framework implementation. It should work. Well, you didn't pray to any of gods today, so I don't know. Oh, here's your weekly horoscope. There is bingo. Uh, this week brings a powerful wave of intuitive energy your way, dear Pisces. I feel like that. Feel the wave. I feel the wave. It's really strong. You can see it also, uh, is calculating the week for us. Uh, that's part of the API, uh, capabilities as well. I can even ask, uh, Should we do another sign? Yeah, sure. What's, what's your second favorite sign? Oh, Scorpio. Do Scorpio. Why? Because it's another water sign. It's another water. I was gonna do Cancer. Oh, do Cancer. OK, do Cancer. Uh, give me. A horoscope for cancer for the month. Uh, as this, as this loads, I'll, I'll, as an aside, say I'm very aggressive with LLMs versus Nick Nicky puts like please and thank you and things in and I'm more aggressive. He's like very demanding with his LLM commands. All right, what's, what's going on in the month of December for cancers? This month brings you a wave of emotional clarity and intuitive insights for you. More waves. A lot of weight. Do you feel like that's coming from Taylor right now? Yeah, Taylor, I think, uh, is having emotional clarity. Uh, she does have natural nurturing abilities. So, OK, we're right on. It's great. Works. OK. What else should we talk about, Nikki? Um, I don't know if you just want to show quickly in the NBC app how we're invoking the runtime. Agent Corre run time in the NBC app. Yes, I did want to show that. Thank you. Um, so we mentioned, right, in Agent Corp run time. Uh, or do you wanna take this, uh, we mentioned that we have to, if we deploy an LLM application to Agent core runtime, we do not call it like the normal API call. We actually use the AWS SDK, and this is very specific. Um, there are multiple SDKs for Bedrock, so I made this mistake, but it's not the bedrock SDK or the bedrock runtime SDK. It is literally Bedrock runtime agent core, I think is what it's called here we are. Or is it Bedrock agent core? It's super specific. You can easily get it wrong, um, and so that's how you invoke the runtime. Um, you might wanna also show the container running on port 8080 and having the invocations on point because that's very specific to runtime. So here's the actual AWS SDK. 4.NET code that invokes your agent within a runtime. Uh, so we have a runtime request object here we're giving it the runtime ARN. You don't know ARN, Amazon resource name, right? This is the actual, uh, identifier of your resource running in your AWS account. We give it a session ID. Remember when I said run time allows you to do session isolation? There you go. So if you give it a new session ID that's gonna create a new session for every invocation, you can reuse the same session ID to invoke a prompt in the same session. So you would connect this perhaps with agent core identity or you could do, uh, your own roll your own, right? Uh, manage the session IDs yourself up to you. Uh, and finally, the payload is literally the prompt. OK, what am I going to ask Agent Core, and we just do invoke agent runtime as sync. That's it. And then it sends us back a message. We, uh, do some string reading, uh, and, and Jason serialization and deserialization. Yes, uh, deserialization, sorry, uh, and, uh, and that's it. And then the container is very specific. So for that container to actually work, it needs to be running on port 8080 and you need to have your slash invocations endpoint and your slash ping one. So there you go. There it is, um, so you can see that invocations endpoint is actually just call it's we, so we can do anything we want inside of there, right? But we're, what we're doing is we're actually calling into one of our frameworks. So, uh, in this case, uh, Microsoft Agent framework to then, um. Implement our request. That's right. Yeah, um, something I wanted to, to mention as well, um, if you're using the Python libraries, for example, to work with Agent Core, there's like a starter, uh, tool kit, and some decorators and things like that. Uh, it will set up a lot of this for you. Oh, there you go. I just want to make sure you guys had access to the code before the session ends because I know they're going to kick us out in 3 minutes, so. This QR code will get you a link. Uh, we did not find a lot of dotnet examples with these tools online, so, um, feel free to use the code that we wrote, uh, because we put a lot of time into making it work. Yes, uh, you may have to make the repo public also. Oh yeah, let me do that. I'll do that. Uh, yeah, you can do that. Uh, all right, uh. Word of warning though, as I said, all of this code is very experimental, very, uh, brittle. I would not deploy this in production. This is definitely code for learning and seeing what's possible, uh, and hopefully giving us feedback as well to bring back to the .NET team here at. WS to improve the experience. As I said, uh, there are some things that are handled for you when you're using the Python tools for Agent Core that are not handled in the .NET side that I've had to build. For example, can you go back to the code real quick? I know we have 3 minutes, um. For example, there is a decorator in the Python version that will set up this, this type of entry point for you. You don't have to implement invocations yourself, you don't have to know about that. It abstracts that away from you. Uh, you can use our code as an example if you want to get started today running things in Agent core. Uh, the other thing that happens is a lot of these GI session IDs, these things like that are sent to you in a header, so you would have to inspect the headers coming in, not always. The most fun thing to do. And a third thing that I learned while building this, uh, so Firecracker, I mentioned earlier, is the underlying, uh, way that these containers are getting spun up. Firecracker uses IMDSV1. It doesn't actually use IMDSV1. It mimics IMV IMDSV1, uh, which has been removed from version 4 of the AWS.NET SDK. So, uh, I have in here an. They call this MMDS. You can look this up. Firecracker uses MMDS. It's a micro VM version of, uh, and this is, if you're unfamiliar with IM IMDES, it is how you retrieve credentials, uh, from a role assigned to a compute runtime. It's on EC2, ECS, etc. uh. You can use this, this MMDS credentials, uh, class that I built here for you if you want, uh, to retrieve the role, uh, credentials for the container running in Agent court run time. All right, we are right at time. We're yeah, related sessions if you want more about.net and, uh, Jen AI throw those up there and then just our, our information. At the end. If you want to reach us, these are our email addresses and our usernames for GitHub. Yes, uh, we'll leave that up. We will go out, stand right in the hallway if you want to ask us questions. Uh, thank you. Yeah, apparently questions have to be done outside the room.