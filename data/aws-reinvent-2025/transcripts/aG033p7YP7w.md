---
video_id: aG033p7YP7w
video_url: https://www.youtube.com/watch?v=aG033p7YP7w
is_generated: False
is_translatable: True
---

Um, let's start off by a scenario. It's 2:37 a.m. You get waken up by your phone and you hear your CEO's voice in a panic, and it tells you basically that the company's application has been down for about 3 hours now. Costing you guys millions in revenue and as you frantically try to restore these services, something is like in the back of your mind right? Could this scenario been prevented. You know what could you use AI to identify and test these failure points before they even began, you know, before this 3 a.m. crisis. I'm Narita Wu and this is Hans Is it, and today, in the next 60 minutes we are gonna show you how combining generative AI. And AWS fault injection service will save you time and effort into testing in today's thank you, not just by helping you discover unknown risks but also turning past incidents into automated tests to prevent history from repeating itself. In the past year, 50% of companies suffered through more than 10% or 10 hours of downtime in the cloud. While systems fail, something far more valuable. Begins to crumble, right? Customers don't just get frustrated, they left. Leadership didn't get concerned, they lost confidence. Investors didn't just ask questions, they lost trust. Revenue didn't just dip. It disappeared Here's the reality everyone talks about resiliency until it's that 2 a.m. call while everything breaks. The question isn't whether you can afford to implement or invest in resiliency, it's whether your business can survive from it. Everything fails all the time. We need to build systems that embrace failure as a natural occurrence. When Doctor Vorner Vogel, VP and CTO of Amazon.com, said those words, he was defining a fundamental truth of modern day cloud, right? We've heard of this saying all the time, but this wisdom drives our shared responsibility model. AWS ensures the reliability of the cloud while you architect the reliability in the cloud. It's a partnership built on reality, not wishful thinking. The most resilient organizations and customers aren't the ones who believe that they can prevent every failure. They are the ones who embrace this quote by Vonnervogel and then weaving resiliency into every layer of their architecture transforming this shared responsibility into their competitive advantage. So let's talk about how our journey is gonna unfold today. First we're gonna talk about how gendered AI can be used to be your early on detection. Discovering failure scenarios that you haven't even imagined yet and think of this as architecting who's constantly asking what if right? that little mind in the back of your head saying what if could you do this, could you do that next we'll go back. Behind the curtain and see exactly how this works, no magic, no buzzwords, no, oh let's see this, just practical implementing technology that transforms how we're approaching resiliency testing. Then we're gonna tackle how equally crucial, challenging or critical challenge within it and using AI to validate these failures that are already known. Because let's be honest, how many times. Do we think about when we test or when something happens, could have this been prevented? And then finally this is where where it gets a little bit more exciting and the actual thing that we're waiting for is like we'll show you how to put this into action, not next quarter, not next year, but by tomorrow or bringing it back to your teams you'll be able to implement this all together. These are gonna be through real world scenarios, practical coding that'll be provided and then also give you everything you need to get started. And then by the time you're done with this, you'll have a new approach to resiliency and it'll get testing and get the power AI into your resiliency testing altogether turning months of work into weeks and then we're gonna show you how this all works. So Hans, thank you. Alright, uh, here is the resilience life cycle framework. Has anyone seen this before? It was released in October of 2023, so it's been around for a a reinventor too, and there's a QR code at the bottom if you wanna check it out as well. But this is our North Star when we think about building resilient applications. And today we're gonna spend some time talking about where does Gen AI or Agenta capabilities come into play in this life cycle. So you may see it's not pointed at the top or set objectives. Set objectives typically is, uh, you know, when we talk about recovery time objective, recovery point objective, RPO, RTO, and our SLAs. These are typically business decisions that are made, and then those requirements are then given to teams to execute on, right? So those are business decisions for set objectives. However, then we move to design implement where we take the RPO, RTO, SLAs that we must provide to our end users, and this is probably the most common place where I've seen agentic capabilities so far. We saw announcements this morning or more talk of Quiro and Quiro CLI. How can we use these agentic capabilities to one, create our infrastructure as code, right? Make sure it's across your availability zones, make sure your database is configured in a highly available fashion. But also when it comes to building our applications, right, when our application code, uh, we're writing that, can we make sure that we're adding backoffs, retries, baby jitter, and having proper instrumentation for observability we wanna be able to answer the question, where is the challenge? What instance is it coming from? what service is it supporting? what availability zone is it living in, right? Where is the pain? Let me find out very quickly. But once we've designed our system and implemented it, we can then begin to evaluate and test it. Here's what'll be spending a good amount of our time today. Once I have that infrastructure, how can I use Agenta capabilities to help me discover what fault modes it may have? And how can I build tests to help me validate my application is resilient to certain scenarios. Um, from there we operate our application. Uh, we saw the, uh, if you guys watched, uh, Matt's keynote this morning, we saw the announcement of the, the DevOps agent as well, right? I think of the DevOps agent when I think of operate or there's cloud Watch investigator as well to help us determine what's going on in the environment and shorten our mean time to recovery to bring an application back into service. And uh as Nareda was talking, like 50% of organizations had incidents and commonly after incidents we produce root cause analysis, right? Your teams probably do something similar. What was the challenge encountered? What was the timeline associated with that? What events came together to make this, uh, impairment happen? And then how can we. Use that data to recreate a scenario to validate all the checks and balances that we put into place after an impairment did what we want them to do, right? We want, we don't wanna just add retries if that was what caused the error. We want to validate that that made the impairment, uh, not happen again, right? We are resilient to that impairment in the future. OK, before we jump into our demo, let's talk about what application we'll be reviewing here. We have the, has everyone dealt with a three-tier application before in some form or fashion, whether they were on-prem and you migrated to the cloud or just have built a monolith, but we have an application load balancer. In our case, we have a Windows EC2 instance or a set of EC2 instances backed by a relational database, and this one, a, uh, MS uh MySQL database. And so when we start to uh. Do our discovery, we have two agents that are built and backed by Bedrock in this case. So we have our inventory analysis agent. Our inventory analysis agent will go help us discover what is on the box and help us determine what failure modes or scenarios are associated with that failure. And from there we have our document generator. Document generator, uh, there is an integration between fault injection service and systems manager, uh, documents or automation. That for the actions that are not native to fault injection service we can write automation documents to facilitate these, right? For example, uh, impairing IIS is not a default action. However, we can write a document to go do that for us, right? And so this document will help us, uh, create those impairments. And one thing I'll also call out here, you may be thinking, Hans, isn't there a service called Resilience Hub, and what is the difference between what you're saying right now and what resilience Hub provides. Resilience hub is focused on your infrastructure and so it can determine, you know, you have auto scaling groups associated associated with your instance. You have, uh, multi AZs and all that stuff from an infrastructure infrastructure perspective, but it doesn't have insight into what's running on your EC2 instance. And so take pairing this capability with resilience Sub gives us that powerful, you know, how can we take this to the next step. OK, in our demo today we're going to show a, uh, the agents running. It's going to review what's installed on the EC2 instance. It's going to help create a hypothesis of what impairments that we could facilitate on that instance based on what it finds, and then we're going to see a document created, uh, based on one of those impairments. OK, here we have our agent. Uh, so, going to start running the agent now. It's going to start to, uh, identifying what's installed in the environment. It's using the use AWS tooling to configure what is the network settings. We start to see there's one instance now that it's online. It's uh associating with what's running on the server. We see that it's installed with a IIS application. It's understanding what the database connectivity looks like based on what it's finding. And uh we'll go we we review the prompts for these in a little bit for how we steered them but then it's able to start creating uh chaos engineering and operate uh hypothesis based on what it found. And you see at the bottom we're excluding some things as well we don't wanna test Microsoft Edge that's not part of our application, right? We wanna make sure we're focused on applications specific things. Now you'll see the data from that was discovered is being passed into our document writer agent and it's going to start creating a document to facilitate an impairment of IIS on that box. So the uh systems manager document will run on that EC2 instance. There is an agent that is uh on the EC2 instance that facilitates communication between, uh, the EC2 instance and systems manager, and that's how we will invoke this document on that box. And so here we can expand the capabilities of what's native to fault injection service by utilizing that combination to be very application specific when we're testing things in our environment and not just utilizing the built-in fist actions which are very advantageous as well. OK. So I know that was a little bit of scrolling, so let's go back to our presentation and take a a a a look at that and what the output was and talk about some of what it found and how it deduced these things. But you may have also asked yourself what, how did you do all that, what lines of code that must have been terribly many lines of code that you wrote to go run those agents. So here you can see the couple lines of code here we're using the AWS strands framework to facilitate the building and running of the agent. Uh, the AWS strands is a. Simple yet powerful SDK that takes a model driven approach when building and running agents. So you can see we specify a couple things here the model, which model we want to use, uh, the tools associated that we wanna give it, right, uh, to arm it with the actions to go discover things in the environment. We have it a prompt and a callback handle so we can view what's going on from a logging perspective. Uh, one thing I'd like to call it here is the use AWS tool. This tool was built by AWS, but it allows you to just then discover and allows your, your agent to have context of the AWS APIs and services. So when you're asking it, go to systems manage your inventory and discover these things, it understands what you're telling it. And then we have another tool here that allows it to understand tags across an environment, right? Two powerful things to go give me context in there and then understand the ecosystem as well. So not, not very many lines of codes to build and create that agent. OK. Now let's. Take a step back and look at uh the scrolling text from the agent but maybe a little bit slower. Uh, so here we see that it's found a web application server running with a couple services. So I like to think of this agent as the infrastructure detective. It's able to go run on the box, discover what's running, and start building context of based on what's running, what failures can happen, right? Uh, again, when we're thinking of large language models and agenda capabilities, we want to think about how can we arm it with the context it needs to be successful, right. Um, so another helpful thing to add at this stage is not just allowing it to inventory what's on the agent, on the, the EC2 instance, but also maybe at providing the application code as well. But by by providing the application code we can understand how does it then retry and back off, and I can have better hypothesis based on what will actually happen when it's impaired. How does it act right by by pairing those two things, we have much better context on how the application handles scenarios. So Discovered IS it's running. It also begins to discover that there is a ODBC driver for a SQL server, uh, installed. And there is no SQL server installed locally. So we can then begin to deduce, OK, this has external dependencies on a database, and if we, uh, use that tooling, that, uh, tag tool as well, we can then begin to, uh, learn, OK, what, uh, if we're tagging our applications or if we have them added to a resiliency hub, what actual database is associated with that application, because ideally I want to probably go do some impairments and play with that connectivity when we start thinking when we, uh. Throughout the scrolling there some of the interesting things it started suggesting also that our native actions to vault injection service are like the uh AZ latency which was an announcement a couple weeks ago that we reduce or uh release rather. Um, so it begins to discover all these things and put them together with context and so with understanding how the application functions, uh, how the application code functions, and then knowing about, about the environment, is it part of an auto scaling group? How are the health checks configured? We can begin to put that picture together and understand that this instance is actively serving traffic and if they're, uh, if these services are disrupted. The web server would stop serving this purpose. However, with the way auto scaling is configured, once it begins to stop, uh, serving the traffic, the instance will be replaced. So we will have a healthy instance take over this instance and begin to serve traffic again. And I know when we're, we're thinking about, uh. Having been on, uh, infrastructure teams in previous lives that weren't the infrastructure teams that were supporting an application, sometimes you're in scenarios where the people supporting the application may not have full context of what the application code does, right, or SREs that support applications, but they. be the ones that build it as well and so having capabilities like this to build hypothesis based on what that service actually will do and how it runs is very advantageous, right, especially if we don't have all the context or familiar with it for a long time. It helps speed up the failure modes that I'm aware of and help me execute tests on those. OK, let's talk about some of the, uh, documents that it would then write based on what it found, and these are the non-native actions that we're talking about here. There were other native actions like AZ latency or AZ impairment, the power interruption that it would suggest as well, but let's talk a bit again about the non-native actions that that. But it didn't suggest. So here we have a database SQL port blocking. So it would go and manipulate the security group on these, uh, database servers to possibly block, uh, all the security groups from the web application servers, saying I'm not gonna let any servers communicate between these two layers. How does my application handle? Or we can also have the scenario where maybe we're going to block a specific instances IP and then determine that our observability platform allows us to observe what instance is having the, the blockage, right? We should be able to determine it's one instance within this AZ serving this service and now I can go take action and bring this back into um. Service faster, so some good hypotheses we could observe and learn about our application and how it handles based on the scenario. And then again, how do, what happens if we impair this, the IIS service or the application pool that's serving traffic? Will it continue to try to serve traffic? Will clients retry? How will this happen? What will I observe in my platform? And based on what the agent hypothesized and discovered, the, the healthy web servers will be replaced, um, and the service would resume normal operations. And again, having it have context of the auto scaling. Group and health checks we could then also infer how long that would take. Would it take 5 minutes based on how everything is set up? And so this helps you build those hypothesis much faster without having to do a full discovery of what all the failure modes are for your application and making sure all of the stakeholders who have the knowledge of that application are there as well so you can have a meaningful conversation on how does the service function, how does it retry, how do we believe it will act in these scenarios. All right. And back to Narita So let's talk about some of the prompt engineering and what helps us kind of guide our agent into understanding what we want to do. Now think of these prompts as a detailed job description for what the agents are gonna do. So within that scenario that, uh, Hans showed us, this is where we're guiding that agent to do that. Now for our inventory agent we are being very clear on what we wanted to focus on and that is really thinking about the applications and your servers on what patch or to avoid what is patches and updates and also focus on business applications altogether. Now why do we say ignore patches and updates? It's not really relevant to what we're gonna test, right? That is maintenance that is routine that's not gonna show us what is, um, focusing on resiliency altogether. Um, with our automation agent that we had showed earlier, that's the same mindset that we're going with the same focus being very specific, you know, never touch protected services. We wanna test resiliency, we don't wanna break it, we wanna be able to recover from it. And so in each prompt that we have guided our agent, which will show you it's kind of defining one what do we want to do, what can you do, what is their role, and then secondly we're focusing on more of what can it not do so we have to focus more on what it can and cannot do so you can guide that agent and then formatting what is that response that we want it to look like. And so there's a balance between um our agent being intelligent enough or effective enough to be able to give us what we want within the scenarios but also constrain it so that it doesn't do everything that it wants by itself so we're putting in those different guard rails for the different agents. But before we even get started into understanding what our agent is gonna do, we gotta understand what is what is on our actual nodes or instances if you would call them and so we we're working with an idea of if you're an architect and you're looking at building blueprints, you gotta understand all the different components, right? The systems, the connections, everything that it has encompassing so that you can understand what the integrity of that, uh, building is, right? Same concept here. So we're using AWS Systems Manage inventory that's gonna be our blueprint reader. It's gonna tell us what is on the node, what is installed, what it needs to be updated in the sense of what is already on there. And so our first step is understanding what's it being on the node. Really understanding what is installed from the software, um, network configurations, you know, what database connections that node has, that traffic that it's pulling, all the critical components of the application that it makes up, and this is where it gets really interesting by combining systems manager inventory with fault injection service FIS we're really bridging, uh, a solution together to kind of. Experiment with expertise or understand that instead of throwing chaos we're experimenting with precise um controlled environments to understand the resiliency. And the difference really is thinking about when you're testing you want to understand what you're testing for, so being controlled and concise is what our end goal is and by combining these two we're creating that solution and making things that are actionable afterwards. Wrong clicker. Um, so let's talk about our first agent, the inventory agents. Now our primary directive to this agent is looking at, if you look at it, it's saying running an online easy to manage instances. This isn't by accident. We're being very precise on what we want because one, we want reliable and actionable data that can we can make information based on this. We don't want to do anything with offline instances. And the two main key phrases that you can see is uh running online and manage instances only online EC2 instances we only care if they're online like I mentioned and actively reporting to systems manager will allow us to get that reliable up to-date data from our notes. Now think of this as setting up the guard rails for our agent altogether. We're not telling it um. We're not just telling it what to do, we are focusing more on where to look, what to see, and what to look at. And so by using our prompt engineering we're basically saying that this enables us to be more consistent with what we're targeting so that we're removing that noise and only focusing on what is our end goal. Now, here's where we're telling our agent what not to do, right? We said that you have to focus on what it can or cannot do. This is saying what we don't want it to focus on, so we're very adamant about not looking at patches and updates. Why? It's very simple, right? Like I was mentioning earlier, you're not gonna really wanna focus on what KB entries you update and install last week. It's irrelevant to your resiliency testing. You wanna remove all that additional noise to get more precise and control experiments and so what we really care about and from a business critical standpoint is what web servers are serving traffic, what application is processing all those different transactions, what databases are. The different connections handling that in the customer data all together. By really telling our agent to ignore the rest of that information and being very clear and precise, we're turning that into actionable insights and we're not just being wishful thinking that it's gonna understand what we're doing. And so focusing on this approach we're really letting our agent analyze the different systems and look at all the components that matter that are gonna impact your business and not getting distracted by everything else that might be installed in the node. Now let's kind of look into the construction side of things, what our agent actually does on the note that you're telling it to do. Think of this more as a 5 step discovery process looking at everything that it's building, giving you the full picture of what is on your system. We first catalog, you know, what is the software installed on those different nodes, the application, you know, this is that foundation where we, where we wanna get that information from. Secondly, we're examining the different configurations and the different packages installed. Now this is very much where we wanna focus to because it's kind of setting up everything that it's working with, giving those different components. And then third is the more crucial component that we also wanna focus on is really documenting everything that the service and. The actual component of the application and their versions, so we're looking at the services and their versions why? Because sometimes you might think something is installed on your note and then you realize actually that version is not the right time or the run time is different than what is actually installed on the node, so you wanna make sure that those two different things are aligned to what you're looking for because it might create a disruption, might be subtle, but it could cause a little bit of problems down the road when you're testing for resilience. And then now we're also doing my #5 is packaging all these different inventory reports for that other agents can be able to look at the inventory report and use it as actual data. It's always useful to be able to repackage everything and so if you remember. We're only looking for online EC2 instances and that are reporting so through all this process that we're only working with online and not offline because they're not relevant to our testing and they're not in production. Or within our task dev and all in the life cycle. So looking at. What inventory does. Think of this as a way for you what really matters within your resiliency testing. So starting off with business business critical applications, think about your environments and what are you hosting if it's Java, if it's Python, no JS, if it is .NET frameworks, you know, you're looking at everything that is within the data layer within it all together. So when we think about database and the clients and document everything that is on there, that's also a big component into understanding what is happening, right? You wanna understand if it's whether, uh, Pache IS or if it is Tomcat. They're always the front line of your customers, so paying attention to what is on that layer, you wanna make sure that you understand that completely. And now when you think of this, you don't wanna forget the business, the customer facing applications, right? Those are the specific tools that if you either built in-house or if it's, you know, a third party that you actually take the time to, uh, look at the custom applications altogether. Now the key thing here is really focusing on is it a direct business impact? Yes or no? It's not just creating that one list for you to understand, but it's giving you that map and building out what are the different components that you see within your application that you might have not known that it was tied to your application. Now one thing you should think about is the thought of four questions that build up the backbone of this analysis, right? First is what is the server's primary role? This isn't just thinking about labeling that server, it's going beyond that. It's thinking about. What is the purpose that this server has if it's beyond those different tags if it it's managing managing the database, is it running application logic or is it a fundamental this is like the fundamental understanding of what the purpose of this database is gonna be or that server. The second one is what services are actively running and. It's providing business function altogether. Now this is really that emphasis and I I'll keep saying this throughout the time is it actively running is it reporting assistance manager that's where we wanna focus our time. Now this really tells us we should be focusing more on. How these failure impacts are gonna affect our business right when we talked about earlier about how. We don't just lose customer trust, we lose different components that we might not see something more valuable in the beginning. And so when we talk about dependencies within the different services, this is where we think about failures failures don't happen in isolation we think about that domino effect one thing happens and then a bunch of other things happen that we might have not known about so that is really thinking about what dependencies are within your application while we're testing and what would happen. And now we're moving towards what do these questions help us by providing inventory of our instances altogether and we're bridging the inventory with resiliency intelligence to give us more of an actionable plan for what we're doing that might fail. This slide represents the guard rails of our agent, what it can do in a sense of protect the different services that you want to be able to either test or not test, right? We're creating an uh uh isolation boundary for different resources. Think of this as more of a way to really maintain control, observe, and if needed stop as a reaction to not being able to go into a resource. The key message when it comes to protective services is really chaos engineering. Isn't about random testing. It's not about just going in and testing everything within your environment it's being selective on the different resources that you want to test and make sure the application can withstand it. This is really meant to restrict you or restrict the agent altogether it's really to empower you as a tester or building that resiliency into your environment so that you do it in a safely manner. Now this isn't an end all be all. It could also be changed. This list can be modified to your use case and so it, it continues as we go into what are those key analysis. Now thinking about what kind of questions you are you're asking the agent to solve for you, the first one is gonna go back to what is the primary role of this. Um, so when we go into what is the primary role of this agent altogether we're thinking about. That core understanding of what is happening within your environment, what it can or cannot do. And really under and then once it comes into which services are active and serving business functions as you could tell the agent does a lot of similarities between each agent um from what we're going from so when it comes to the protective and versus it coming into uh reporting and so thinking about the dependencies is like kind of the same concept as we're thinking moving into like understanding the appli application what is happening with the application. Oh, duplicate slides, I think it does have, sorry, this does a little bit have duplicate slides. That's the part I was looking at um. Going back to beyond just protective services. Think about as a way as remembering so when we talk about our protective services that is services that we don't want to affect, but in reality like using the systems manager, that is a way for us to recover our different notes if something were to happen and so that remember statement is more not just a summary but more of a design choice that we implemented on there. So it has a purpose for reinforcing those mission boundaries of what it can or cannot do and only focusing on what it should be doing. Right. Thank you. Alright, so Narita spoke on how we can, we can discover things with our agent. Now let's talk about how we can actually take action on those resources. And so we're using systems manager documents to facilitate those. So before we jump into those agents specifically, let's talk about what is it, what makes a good systems manager document. I think one of the most important things that is native to the FIS actions if you use the uh EC2 CPU stress action, the EC2, uh, memory stress action that FIS provides behind the scenes, these are systems systems manager documents managed by the service team that go into your instance to facilitate those actions, and they are, if you look at those, they're, I think they're prefixed with AWS FIS in the console. If you go look at them, they take care of putting. The things back in order when they are done, we have to restore the state if we're stressing the CPU, we have to stop that stress when the experiment is canceled, it fails, or when the duration we specify runs out. And so service state restoration is an important thing that we need to take care. Care of ourselves if we are writing documents that are outside of what this Ains provide, right, we need to put the resources back into shape that we found, right? And one way to do this is by making our documents very modular. On the right hand side you'll see a cloud front impairment that validates a multi-region cloud front implementation and each, uh, portion on failure to cancel we're rolling back and reverting our changes to make sure that we are our services how we found it, right? We're cleaning up our house. Wanna make sure we're ID potent. So for example, the CPU stress action that's native, we don't want multiple actions of those running at the same time. We want one. So how can we do that? Make sure that we do that, and adding preconditions. I think if you saw the tech scrolling quickly for the, uh, automation document, there were preconditions saying this runs on Windows boxes. We're running PowerShell, right, making sure we're running the right resources on the right instances. And so this is what our prompt will take into mind when uh we're building this uh document. So again, the same concept we're starting with what is the agent's persona. The agent is a specialized agent focused on writing systems manager documents to facilitate chaos engineering or resilience tests, and it will generate systems manager documents to help create those stress conditions. And again we're continuing to steer it. We want you to be focused on the business applications you're being passed. You want to be, uh, creating very specific things using these documents and automation, and we want to be focused on doing things that are not native to FIS. If the FIS native action is there, I don't wanna reinvent the wheel. Use that. It's good. I don't wanna handle all those things. Use native actions and only create things for what FIS can't do natively or to go into our box to create impairments. We wanna be able to do this on the OS level if needed and implement all those safety mechanisms and specifically for this one we, there was a rag database behind this prompt that had the blog for how to create best uh what are the best practices for writing documents in the database as well as all the fault injection service native actions in the database. So we gave it context of what is good, what good looks like, as well as here's what is already invented, don't go do this, we already have it. OK, and. Before we run a resilience experiment, we want to make sure there are certain things done. For example, for our IIS, uh, instance, um, we may not, we don't want to run an experiment to impair IIS if IS isn't installed on the box. Is it currently running right? We wanna make sure is it online? Is it running? Is my application stable state healthy at this time. Let's validate that. And then after that is if I'm dropping a file locally to make sure I'm being ID potent, is there enough space for me to drop that file without, you know, filling up the, the block drive that I'm dropping that on? And perhaps are the PowerShell modules that I need to facilitate this already installed on the block, so we're making sure is everything there for me to be successful to create the impairment. So the agent will take this into account based on what was in the database's best practices. But again, reiterating into it, follow these preconditions. And then once we have uh the experiment that was successful and gone through, we wanna make sure that the house is put back in order. I have a toddler at home, so I'm used to telling him, hey, you're done playing. Let's pick up your toys. Let's put our toys away before we move on to the next thing. So the same thing is true here for IS experiment. Is the service going from a stopped to a running state or started state and then a running state, right? Is or is everything that we did undone now then once all of that is true, remove that local file that you put there for ID potent reasons, right? Let's clean up our house after we're done with an experiment and the agent will take this into account. Um, and as Narita was saying that we wanna set ourselves up for success with these experiments guiding it along the way, we don't want it to impair the systems manager agent. I can't restore the state of the system if I can't communicate with its for this one. So leave systems manager agents. Alone if it's installed in the box, don't impair that. Don't just do something to critically impair the OS itself and don't impair impair the network connectivity between the instance and systems manager, right? We wanna make sure that we're able to communicate with it and take action as we see, right? So again, steering it of what to do, but what also to not do along the way. And again reiterating, you focus on the business application you are past. You focus, you can impair non-critical temporary files or cash directories. You can impair application specific services whether these are a third party or custom and user level preferences. So we're guiding it along the way of again your path to hypothesis, but remember all these things are true when you're writing your, your experiment and your documents to make sure that we're having, uh, the best practices implemented in along the way and setting ourselves up for success. OK. Back to you. uh, looks earlier Hans mentioned how the agent created an automation doc for us and we're gonna take a look on how that actually looks like. So the agent made 3 main critical things where it talks about a validation step. This is where we're running tests and not focusing on that already impaired systems, right? Going back to what we wanna focus on and making our objectives pretty clear. And then the second one is more of a controlled execution phase on the dock, thinking about being a little bit precise on what it's doing for within the different steps that an automation dock does for you and then an automation restoration step. So like Hans was saying, if something were to stop, to bring it back online so that it goes into the regular state it was when we first started testing it all together. Now, The idea behind creating or it creating this automation doc is more of a defining the different states and maintaining that state within all the different levels of that, uh, phase that we're going within our testing now this is providing more of a structure within our automation so the different steps that it's gonna orchestrate in a flow that goes within the life cycle of your testing. Now we're looking at the FIS template template guide that it is creating before that the automation doc is going to execute all together. Now it's passing specific values towards the different targets that it is testing residency with. It is thinking about, uh, the 5 or we're focusing more where it says the 502nd. Uh, pool in that area and it's thinking about the duration and targeting that default at pool for our testing altogether. Now these are parameters that are defined in our dock earlier. So when this is targeting those nodes, it's targeting the different nodes altogether in the application and then it'll execute within the automation dock itself. This is kind of combining those two different services and leveraging what could be native and what is not built in and then adding the different different configurations that might not be supported so from blogging IM permissions, different components that could not be actively supported and it's supporting it through automation altogether. Now we do have a demo, so Hans, let's see it in in action. So let's take a look at all the things we built previously. So here we'll go into our fault injection service and the resilience testing here in the console, and we'll see the service that the experiment that it created. You can see here that this the template ID, and we're gonna start the experiment. We're using the console now, but this can be done via APIs or CLI to invoke the experiment. Here is our web host. It's still up. It's connected to the database currently, so the service is still online. You can see here that the action is pending, so it's about to start and impair our web service, and you can see here's the action and how it is connected to the SSM, uh, document, as Narita showed in, in that as well when we exported the JSON template. That's what it looks like in the console and the same thing here you see the experiment and all the the PowerShell that was written for us as part of that document generator agent implementing best practices logging failure fail back along the way. Doing all that heavy lifting for us. Now if we refresh it, the instance is taken down. So we had it we had the agent discover what was happening and then we plus it created the document for us as well as then impaired the system. So, so far we've talked a lot about what if I don't know if what I want to test, right? Discover the system for me and help me figure out what are the likely failure modes. Now let's move to the uh learn and respond that last phase of our resilience life cycle framework. What if I know what I want to test? So. In this context is we had an impairment and we produced a root cause analysis of what the timeline was, what actions were taken, and we feed this document of our root cause analysis to our agentic or large language models. We can provide it good, better context by giving it good known good experiments. This could be the fault injection service. Ones owned by the service team and systems manager, uh, accompanied by the best practice blog or some samples from our open sourced phys, uh, template library. So what are, what are, what do good experiments looks like and what are the best practice associated with it. And then uh we can also give it an AWS environment to go review as well that's production like to have good context of here's the impairment, here's what the what good experiments look like, and here's the environment go help me create this recreate an impairment that will help me validate what happened and all the checks and balances I put in place that don't, these don't happen again. Then ideally once we have all that built we want to test in a like production environment, right? There's many differences at times between a dev environment and a pre-prod environment in production so we wanna test as like prod and validate that we're able to observe the impairment and either in terms of DR and in terms because resilience is. High availability and then disaster recovery if we're doing something, did all our playbooks that we updated to help recover from that impairment, are they all valid? Do all those work and do our teams, our teams able to validate that so we can take all these things into context to help validate, uh, recover and, and learn and respond from something faster. OK, and we'll talk about what could that agent possibly look like again steering it, giving it the direction of you are AWS resilience expert. You will discover what is in the root cause analysis document. You will comprehend it. You will create failure scenarios with fault injection service and systems manager based on what you find. Uh, and again you're going to have some inputs into your decisions. You'll be given a root cause analysis document that has timelines that has details of root cause of what's going on, and from there you're supposed to look, you will look at your target environment and the RTO and RPO of the application that you were provided to know all the requirements of how should the application function and what happened previously. And from there consume this RCA document or root cause analysis and instruct the failure modes. Make sure you map them to the dependencies and sing single points of failure that could be in the environment and formulate testable hypothesis based on these and then go build them for us. So again, saving time to help us build tests to validate what happened and does it actually stop it from happening again. And working with many customers, if we, we have no context of the application, it may take us a, a, a week or two to actually go sit with the application teams or sit with the, the SREs to, to have a good conversation on what are all the failure modes for this application and document those and how then to test and prevent those in the future. And then we create hypothesis from those that could take a couple hours or days depending on the amount of effort and what people's knowledge of the application is and once we have those things then we have to go build our systems manager documents which could take a couple days to build and test. If we're writing them ourselves and then we want to spend time validating that, right? We have to test the automation we're going to trust but validate that was another thing from Matt's slide this morning we trust the system to do things, but we want to validate it before we start experimenting so we wanna look at the automation document. We wanna make sure everything looks good and then begin to test this in our environment to make sure that it's doing what we want and not just running code. And so from time from doing our root cause, uh, you know, our discovery of the system to having the experiment ready to run in an environment that could take a couple weeks possibly depending on teams and setting up meetings and having our day jobs as well, right? And so how does AI speed this up for us doing all the things we talked about. So we can now discover the system and inventory it to produce failure modes within a couple of minutes. That was a single instance, but it took, you know, 2 minutes to fully discover what was on the box and provide me some hypothesis of where I could start testing. It then designed the experiments and the hypothesis within a couple of seconds and created those documents, implementing best practices along the way. And then I can spend my time as an engineer validating those and giving feedback to those documents of what needs to change, but I'm not spending my time doing all those things. I'm being very effective with how I'm spending my time go build it for me. I'll test and I'll validate it, and that allows me to get things done much quicker and spending my time efficiently as an engineer. OK. Now let's talk about a little bit more on the actual time savings, what it means for your team and your different organizations, right? Disclaimer, it's always gonna be based on your use case, but it's up to 90% reduction in experiment time. It's gonna go from day we're going from weeks to days from what Hans explained earlier, so it's not just about speed, it's also about. Enabling resilience. So when we're talking about the automatic discovery, it eliminates more than just manual systems or improves those different systems, right? We're thinking about removing the extra fluff if it's going through wikis, if it's going through. Our internal tools we're kind of automating and making that life cycle a little more faster for us, you know the agent is generating contextually all those relevant scenarios that we will likely like need to test and so it's giving you that actual. Or it's reading all those different information that it really needs and you're because you're providing all the different context from the RCA's, uh, the best practices, and it's also defining what is happening within the environment if you're meeting those requirements, you know, and it's also doing it within a safely mode so not breaking your monitoring if you're having different applications you're it's allowing you to do it in a safe environment. And so what does this really mean all together for us it's kind of taking you into a little bit of acceleration when it comes to your resilience testing. So think of this as more of a way to instead of you doing everything manually you're having someone do it for you but you're also validating, right? It's kind of bringing those two different components, AI and human into oneself to kind of give you a way to safely and an accelerated way for your teams to bridge that gap as a working, uh, team instead of it being individual. So how can we implement AI into test resilience kind of talked about this, but Hans will bring it back into the whole life cycle. Yeah, so we, we talked, we showed a bit of how we test and evaluate, right? Discover the system for me, create these things, save me some time. And then we talked about learning and respond, but these were isolated use cases in themselves. So how could we take this to the next step? How can we take this, as we like to say, at AWS? What is the art of the possible to this system? We can create these into a multi-agent system responsible for different things. Here we're going to start on the left and move to the right on the top. We have a hypothesis generator, possibly the systems manager, uh, agent that we have. is a subagent of that hypothesis generator. It will not just take in the account of what's installed on an EC2 instance, but look at the application code, look at the environment holistically tell me everything that we could hypothesize for this given scenario. From there this could create many hypotheses, but then this will feed it to the next agent, which is a prioritization agent. Which go tell me which one of these is the most likely to actually happen, right? Let's test the most likely ones. Let's spend our time where we should prioritize these examples for me. And then from there once we have the hypothesis who have been prioritized, then we need to design the experiments, right, giving context of what are the native phys actions again, what do I then need to go create myself and so the systems manager document agent would most likely be a sub agent under the experiment design, right? Go design these experiments, have context of what is native, and go beyond that where need be with safety guard rails in place, and the experiment agent would been able to. And to actually go execute these experiments for us, we can have other agents that are possibly even, even evaluating each of the systems manager documents we write for their efficacy. Are these following best practice using LLM as a judge, as you may have heard that before. We can implement these other agents along the way to make this more of a solution to say go discover my environment, hypothesize, build experiments, and then test them for me, and then also learn. And iterate, have another agent monitoring the experiments running. Are they having the desired impairment? Are we seeing the logs come through and being able to observe what's happening? Or do I need to go reimagine my hypothesis and can you continue to iterate on that? And so we can put all these agents together to help us have that holistic solution to evaluate and begin along the way, we still want to trust but validate, so we would want, we would probably still be a. Looking at the system manager documents that we're producing and just validating things as we move them from a deep environments, but at least it gives us a head start to go create these things and let me spend my time efficiently rather than toiling with creating documents, inventorying systems. Give me a head start. So thinking about takeaways because if you could take anything from a 60 minute conversation from everything you've learned it's kind of a lot to digest but when we think about how we're using AI with resiliency is thinking about how it can help us detect vulnerabilities in our different complex systems, right? You know, we're thinking more towards how that can accelerate us into being able to test faster, remediate, mitigate as much as we can, and without it just being us. And so it's helping us go beyond what what we already know and helping us map out those different dependencies that we might be unknown, so testing that unknown. Now, Talking about that collaboration between AI and ourselves, so thinking how we can really think about all the different possible scenarios that we have not thought about or that we have not tested about or we have not tested, right? when something happens and you get that 3 a.m. call, you already know what you're supposed to do or it could mitigate that risk or reduce that risk of that possibility of being woken up at 3 a.m. And so by using the fault injection uh service framework with AI we're really having that controlled environment being concise with our chaos engineering and testing with a purpose. And so this is kinda accelerating bringing that collaboration between AI and the different teams that you might have. So the real truth and the power that I like to say is that AI partnership with us it's not replacing one component of your team it's building a team so that the AI becomes more knowledgeable through every test, every failure every success, but also getting that feedback from your engineers to say is it right? is it not correct, it's validating between each other so that you're building that. Application that can withstand any failure at any component wherever it might be altogether. And so this is kind of the evolsion that we've seen when it comes to resiliency testing it's building that bridge, understanding what is happening, what is not happening, and then being able to kind of learn from each other and validate and progress as the company grows over time. So what's next? Earlier I mentioned that we have something that you can implement right offhand, right? In AWS we like to say we don't, we like to repurpose already things so multi-agent chaos engineering is where you'll find the solution that we demonstrated today. It's all the different coding that comes with all the different with the agents. What is the agents doing so that you can repurpose as well, test at your own risk, right, validate it, make sure it's meeting your guidelines, and then we have the AWS resiliency, uh, analyst framework that is kind of giving you an action plan to understand your app define your application, move towards what are possible scenarios, failures, and also plan for mitigation altogether. And then the AWS fault Isolation boundaries is kind of talking about our infrastructure and how can you plan for what could be a disaster and how do you implement that and design your own application. Wanna know more beyond just resiliency. We do have a booth at the village. We have multiple use cases from observability, cloud governance, uh, uh, cloud operations in general. So come find us at the booth. We have demos. We have swag. We all love swag. So come visit us at the kiosk. And if you have any questions we could take questions in the back, but thank you so much for being here and I hope you have a good reinvent.