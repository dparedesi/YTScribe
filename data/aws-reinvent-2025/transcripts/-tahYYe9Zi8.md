---
video_id: -tahYYe9Zi8
video_url: https://www.youtube.com/watch?v=-tahYYe9Zi8
is_generated: False
is_translatable: True
summary: "This session reviews the latest advancements in Amazon OpenSearch Service, highlighting its evolution from a traditional search engine to a comprehensive platform for observability, high-scale vector databases, and agentic AI. Carl Meadows begins by detailing the project's massive growth under the Linux Foundation, citing 1.3 billion downloads and over 100,000 active customers processing 10 trillion requests per month. He emphasizes significant performance gains, including an 11x improvement in search speed, and the service's role as a trusted backend for diverse analytics workloads. The platform has expanded its ingestion capabilities with OpenSearch Ingestion Service and \"Data Prepper,\" which now supports a wider range of sources like Jira, Confluent, and direct S3 pulls, alongside new features for batch AI inference that enable cost-effective embedding generation. McCool Karnick demonstrates the enhanced observability features, presenting OpenSearch UI as a \"single pane of glass\" that unifies data from CloudWatch, S3, and Security Lake. He introduces a redesigned \"Discover\" experience powered by PPL (Pipe Processing Language) and natural language querying capabilities, which allow users to debug complex log issues—like extracting nested error messages—without needing to master complex query syntax. Shifting to search, Karnick outlines the progression from keyword to semantic and hybrid search, culminating in support for massive scale: one trillion vectors. To manage the costs of such scale, he unveils new storage tiers like \"disk-optimized mode,\" which uses quantization to keep compressed vectors in memory while storing high-precision data on disk, and a new S3-backed vector engine for even greater economy. He also introduces an \"auto-optimize\" workflow that automatically tunes hyperparameters to balance latency and recall based on the user's specific data. A key segment features Corey Nolette from Nvidia, who details the strategic partnership designed to overcome the bottlenecks of vector indexing. He explains how traditional indexing algorithms like HNSW weren't built for the parallelism of GPUs, leading Nvidia to develop the \"CAGR\" algorithm and the open-source CuVS library. This collaboration allows users to build indexes on GPUs 20x faster than on CPUs and then seamlessly \"hand off\" the finished index to CPUs for standard searching via the FAISS backend. This \"build on GPU, search on CPU\" model, combined with OpenSearch Serverless's ability to provision and then shut down compute resources, solves the \"idle GPU\" problem, delivering 14x end-to-end speedups while maintaining significant cost efficiency. The session concludes with a focus on Agentic AI. The team announces the availability of an OpenSearch Model Context Protocol (MCP) server, allowing AI agents to directly query logs and data with standard tooling. New \"agentic memory\" capabilities enable the storage and retrieval of short-term and long-term context, which is crucial for maintaining state and personalization in multi-turn interactions. They also introduce specialized, pre-built agent types—including \"Flow\" for sequential tasks, \"Conversational\" for dialogue, and deeply reasoning \"Plan/Execute/Reflect\" agents—cementing OpenSearch's position as a critical infrastructure component for the next generation of intelligent applications."
keywords: OpenSearch, Vector Search, Observability, Agentic AI, Model Context Protocol, Nvidia, PPL (Pipe Processing Language)
---

Thanks, uh, thanks everybody for coming out, um. Uh, we're here to talk about what's new in OpenSearch and the Open Search service. I'm Carl Meadows, director of product here at AWS. With me, I have McCool, who will be coming up, McCool Karnick, who, uh, who's our director, and Corey from Nvidia. Uh, Nolette is gonna, uh, talk to us about, uh, how they've helped us with OpenSearch as well. So before we jump in, uh. It's important to talk about like hey what do I do with this open search thing, uh, so open search is a very popular platform for a number of use cases. The first and foremost, as the name implies, it's a search engine and so it is super popular for building search applications and more and more search applications are becoming the foundation for Gen AI and AI powered applications that leverage Open search as a search back end. Additionally, it's very popular with analytics use cases where, uh, it, since it's a very flexible engine, the data is highly, is highly indexed and is fast, and you can find the needle on the haystack. Uh, it's very popular for observability, security analytics, and general real-time analytical use cases where, uh, I can on the fly do aggregations, do searching, and you know, build, get deep insights into my data. When we talk about open search, uh, there's, it's actually, you know, there's a set of open source technologies that power open search, and then there are the AWS services that we provide to, uh, for those open source projects. So the the primary open source pieces were, when we, when we talk are data prepper, which is an ingestion pipeline that sits in front of open search that we deliver as open source and open search ingestion service. Then there's open search itself which is the engine, uh, uh, the core data engine analysis engine which you know is delivered via open you know Amazon open search service and our serverless offering. And then the front end uh for data visualization and exploration is powered by open search dashboards in the service we deliver this as open search UI, which is a centralized dashboard that you can use to connect to any of your open search service or collections which we'll talk about. So before we jump into the surface, I wanna talk a little about the open source so. Open search, uh, you know, is an open source project. Last year we transitioned it to AWS being the primary steward to transitioning it to the Linux Foundation, and it's been a really great year. We launched the Open Search Software Foundation to support the project in the Linux Foundation. Uh, we are a premier member. Recently, IBM joined us as a premier member. We have another, a number of other community members also helping us support this project. The folks supporting the project are actually independent from the people contributing to the project and the actual maintainers of the project, uh. Anyone can participate in the technical merit drives the governance of the project, but, uh, you know, these folks are the ones that are helping us put together events, do trainings, really spread the word about OpenSearch to help make sure that we've got a broad thriving community. And in the last year. Been really thrilled with the progress that we've had with OpenSearch. So since we, uh, since the initial fork, we're now up to 1.3 billion downloads, which is a lot in four years. Uh, we've got in the last year we had over 3000 active, uh, contributions from 400 different organizations, um, and, uh. It's not just the number of contributions, it's the deep meaningful contributions that we're starting to get into the project that's really driving a lot of excitement and momentum around the project. As evidenced, user communities have spread up, you know, across the globe. So if you're in any of these dots, you know, you can look up and find the user community, and you can go to local events and, you know, uh, participate with the other folks that are interested in open search. If there's not a dot there, like, talk to us, we can help you set up a user conference with, you know, it's really exciting to see this community growing. And when you think about like, so why are people gravitating to open search? Well, you know, choosing a platform that's gonna be as important to your business as like your core search engine and your core analytics engine, these are multi-year bets. You're making a large investment and so it's really important that you feel like that's innovating, that's sustainable, and it's really, I think that innovation, as you can see by the progressive more and more meaningful features we've been able to land an open search every year that I think that gives people confidence that like, hey, this is the horse I wanna bet on. This is the project I wanna be involved in because look at all this innovation I'm getting for free by participating in this open source project so just, you know, really excited about, you know, where open search is headed and all the things we're doing there. And, and, you know, another reason is, you know, since we've, you know, started the fork, we've taken a deep look into the engine on how we can really improve performance, you know, no data engine ever went wrong by being, you know, cheaper, faster, or more resilient. And so we've, uh, you know, since the one, you know, the 1.X line, which is essentially, uh, when we started the fork to 3.3, which is the the most current release on the service. Uh, actually, most current release 3.4 doesn't release for another couple weeks. So most current release we've had 11x improvement in overall search, overall, you know, search and aggregations. And you know 2.5% increase in our vector search functionality and We're not done yet, so we're going to continue to drive greater efficiency, better performance, and lower costs on the engine as we increase, which we'll talk more about as we go. So really excited what's going on upstream. And then here at AWS we take that project and we deliver it to you as Amazon open search service which uh may be a little biased but I think is absolutely the best way to consume the open source project um because we wrap it, we wrap the service up provide you, you know, operational simplicity, all of the all of the features and benefits you get in the open source with none of the pain of managing it. Cost efficiency and it's deeply integrated into the AWS ecosystem which makes it, you know, super easy for you to integrate into the rest of your AWS operations. And You know, as evidenced, over 100,000 active customers use the service today. We process more than 10 trillion requests per month on the service. Um, you know, it gives you a, you know, self-managed patching, one click upgrades, 24/7 monitoring, self-healing, no downtime. You can, you know, completely change your topology from, you know, with a click of a button and no downtime, we'll do a, you know, blue-green and move you to a completely new environment. Provides deep security features like fine grain access control, encryption keys, audit logging, all the things that you need to run an enterprise grade application. We support, uh, you know, uh, multi AZ deployments, SLA up to 4 9s on a single cluster. And we built a lot of innovation of the product as well on top of the open source for, you know, uh, delivering, you know, great economics, you know, such as our ultra warm features, specialized instance types like the open search instance that we'll talk about that give you, uh, um, a, you know, full resilient, uh, environment. Yeah, so it definitely if you're running open search, I believe we're the place to do it. So as we jump in, When you think about the, you know, the landscape, most simplistically, you've got a bunch of data that could be in the form of documents, logs, in a transactional system. It could be embeddings, it could be, you know, pictures and videos and other things we could, you know, put on here now too. I wanna take that data, process it. And then eventually land on open search service and then uh vend out the use cases that I'm I'm uh you know from open search service to my end customers. So when we talk about that first part, the data ingestion part, I mentioned data prepper and the open search ingestion service. And what this is is a very simple serverless capability that allows you to have, you know, select a source. It provides a buffer and processing and transformation on that data, and then we'll sync that data into open search into a managed cluster or a serverless collection, or and also can sync data into S3 as well if you wanna do, uh, you know, routing such that like, hey, I want all my raw data in S3. I just wanna put aggregations into OpenSearch or things like that. You can do that in open search ingestion very easily. Oops, wrong button. There we go. So some of the things we've done in the last year with open search ingestion, uh, we increased the, so the, uh, uh, I mentioned it's a serverless product, so it's powered by open search compute units. You don't have to provision anything. These automatically provision for you and scale up and down based off of the traffic that they're processing. Well, we increased the memory of those at no additional cost to 15 gigs so that even the largest aggregations or complex computing jobs we could do on those pipelines without having to scale out additional pipelines. We also enhance the auto scaling so that it's more it's getting more signals to be more responsive to different types of changes in demand. And when you look across the capabilities of the open search ingestion, You'll see there's a wide range of sources that we support, um, you know, it supports both push and pull sources so you can push to it through like an HTTP endpoint or a hotel endpoint, but it can also pull from systems like Dynamo, DocDB, RDS Aurora, RDS Postgrass, you know, Elastic Search itself, open search itself, um, and most popular, one of the most popular is actually pull data out of S3, uh, and process it in open search. We've added, I mentioned RDS and Aurora, we added Jira and Confluent, and we're gonna keep adding connectors just make it really easy to get data into OpenSearch. Then when it's once it's in the pipe. You can do a large amount of processing on that data to enrich select entries, do conditional routing. Uh, drop aggregate, um, uh, do hotel processing. We added this year a lambda processor which there you can call out and anything you can write in a lambda you could then do on that data stream. So if none of these processors work or you wanna do something different, you need leverage that lambda to say maybe enrich that data from another data source outside the service. You could write a custom connector. We also added batch AI inference to make it really cost effective to build embeddings off this pipeline. Um And then like I said you'd sync that data into a managed cluster or serverless collection or into S3. Another feature that I think is uh really improved the usability we launched this year. Is an improved UI or user experience that makes it gives you a guided visual workflow turns out that um. Not everybody just wants to write YAML, so making this easier to write these processors so that you can have high confidence that the permissions are set up right, the processors are configured properly, and a few clicks and that the data is gonna be delivered in the format that you, that you want, uh, while building these pipelines. Alright, and so with that I'm gonna turn it over to McCool who is going to walk us through logs. Yeah. Thanks Carl. Folks can hear me, right? Yep, cool. So one of the impressive things, uh, in Carl's, uh, talk was just the, uh, sheer momentum we're seeing in open search. So getting to 1.3 billion downloads in four years is an achievement. So the project is really growing fast and we're seeing, you know, uh, just the momentum even grow faster. So really excited to see that. Uh, before I get into logs and after that search, I wanted to just get a sense of how many folks, uh, have used OpenSearch, uh, for logs use cases. OK, so a few out there. And how about search vectors? OK. So, kind of more on the search vector side, uh, good to see that. So, um, Yeah, so, as, as we know, you know, log data continues to increase, and with G AI and agents, it's, you know, even, uh, increasing even more. So why do we collect all this data? We collect all this data because You know, downtime happens in software, and when there is downtime, you want to reduce the MTTR or the meantime to respond and come back online and to do that you need to debug and get to the root cause quickly. And, and, and that's why you need all of the telemetry to get, get to that root cause. I think Gartner did a survey a couple of years back. Um, they found that on an average a company has 87 hours of downtime and spends about $3.6 million in lost revenue or sales. And so it's, it's really important to reduce that, uh, downtime and, and get to the root cause quickly. And to do that you typically need a single pane of glass. The log data can be in different places, you know, some data can be in CloudWatch. Some of the data is in OpenSearch. Uh, some of the data can be in S3 because it's just too expensive to index it. And what we launched last year is OpenSearch UI, which takes the Open search dashboards, open source project, and is a fully managed SAS experience. That now connects to not only just open search so you can connect to multiple open search clusters, but you can also connect to data in cloudWatch. You can connect to data in S3 in Security Lake and have the singular view from this one end point where you can kind of visualize the data in a single dashboard. So really powerful to be able to do that because data exists in different places and it's expensive to kind of bring it into one place, so. Um, this is kind of a set of new things that we've launched like the cloudWatch integration and the Security lake integration. We've also been improving the experience of the open search dashboard UI. So, how many folks here are familiar with, you know, the discovered experience, right? Not many, I guess, but we've been really improving this experience and, and the key area where we've improved is be able to type in a query, which is a PPL query, and I'll jump into what the PPL is. And get, get visualizations as well as logs so you can get the raw data which is the logs or you can get aggregations and visualizations and you can kinda get trends within your logs pretty quickly with this kind of interface. And the key power here is the PPL, uh, queries that we've, uh, implemented. So PPL is a pipe programming language. Uh, it supports different kinds of commands, search, uh, D dupe, and all kinds of commands to basically extract, filter, and transform log data. And so with this you're able to, uh, deep dive into logs and get kind of the insight that you're looking for. And over the last year we've spent a lot of uh uh foc we focused a lot on improving the PPL capabilities, adding a lot of new analytical capabilities to uh PPL. So we've added the ability to join indices. So if you have, you know, log data in two different indices, you can now join them using PPL. You can, uh. And do additional kind of lookups and filters and so it's a pretty powerful set of capabilities. Just this year we've launched about 39 different capabilities and in the Open Search 3.3 version that was released last week, uh, you'll find that all of these capabilities are available and uh would be really uh good time to try this out. And I'm gonna jump into a demo of uh how the new dashboard experience uh along with the Discover and like these PPL capabilities can help you get to what you're looking for in your logs. So here's the new open search dashboards experience we launched this uh uh like late last year and since then we've been improving on it. Uh, you have this concept of workspaces. Think of workspaces as a unit of collaboration. So if you have a team. You want to have, uh, you know, collaborate on uh say on observability use case, then you can create an observability workspace. We have other workspaces like security analytics and search workspaces, but let's click into an observability workspace and see how things have changed. So when you click on it, you land on a new, newly designed discover experience where at the top you have a place where you can enter the PPL commands and then you can quickly uh look at the logs and visualize the trends of the data in those logs. So that's kind of at a high level how it works. Now let's say you have an issue going on, right, uh, then you can quickly type in a PPL query where you're searching for, you know, error in your logs. That's effectively what you're doing. You type in that query. And then, uh, you're able to, uh, visualize, OK, here are some of the errors in my logs, and then you, uh, can, uh, click on the AI summary and understand actually, uh, uh, LLM summarized view of what is going on in your logs as you read through that, you quickly realize, uh, that the errors in your logs are actually nested into a body message and so it's becoming kind of hard for you to debug. What's going on and you want to maybe extract the error message from the body so you can go and update your PPL command. To extract fields from your JSON object that that is there in your logs and with that updated command now you're able to better pinpoint all the errors in your logs. So initially it looked like there are many different uh log messages, but it boils down to this target error message that is coming and so now you want to figure out. Uh, OK, looks like there's an error that's happening. Is it a big problem or not? Like, so you wanna be able to do stats on it and so you can quickly do stats and find out, OK, like how often is this happening and looks like it's happened, you know, 70,000 times, so it's a, um, significant issue and you're gonna probably wanna know what's going on. So now, what, what do you do? You want to ideally find out, you know, who's, which service is kind of causing this issue and how can you get to, you know, the owner of the service. So you can, you can join this logs data with maybe, you know, service metadata that you have. and with that you're able to find out, OK, these actually error messages are coming from the load generator service, so maybe it's not as critical as you know, a production system, but still, you know, you can get to uh quickly, OK, find out what's causing these errors. And then um uh you're able to uh kind of take those uh error messages and find out, OK, when did they start and so you can have a time span, uh, and, uh, find out, you know, these error messages start at a particular time and you can use that to engage the owner of the service and, and get kind of it resolved. You can also take these the visualizations that you've generated here and add them to your dashboard so that way the next time you go you can just directly go to your dashboard and look at the errors so it's a pretty powerful way to get to root cause of um the errors that are happening in the system, uh, debug it using PPL. Now, now we may not know PPL like many users are not familiar with PPL, so we also support natural language which is pretty intuitive where you can just type in, you know, natural language query and, uh, it generates the PPL and of course gives you back the response. One of the things that we've also been doing is, uh. Adding MCP capabilities, and I'll talk about it a little bit later to open search. So if you have, uh, you know, open search back end and let's say you have an agent that you build that's, you know, uh, looking through all your logs and different systems, you can use the MCP server of OpenSearch and pass in these natural language questions or the PPL questions and. The rich analytical capabilities, all the functions, the joints, all of that are available through that MCP integration and so you are able to get again to that root cause pretty quickly without having to even maybe interact with open search dashboard. So pretty powerful kind of core capabilities that we all can leverage for debugging logs. So that's kind of the logs overview. A lot of improvements and many coming over the next year as well. So if you have not looked at Open Search for logs, you should definitely look at it. Now search and search and AI. A lot of the, uh, like there's a lot of innovation going on in Gen AI. A lot of excitement, a lot of new models coming out, and search is changing, you know, um. And in this world of GEI, uh, if you think about it, at the core, like this problem of search is core problem of information retrieval. And like you have, you know, different kinds of information you have, you know, documents, you have images, you have videos like you wanna like the goal of search is to really be able to extract that information so that you can get your insights quickly and to do that there's several techniques, right? And if you looked at about 20 years back, keyword search was a big thing and typing in the keywords and finding relevant information. was really useful. And then as uh machine learning models came along you're able to do semantic search and with with that you're able to you know find not only the exact keywords but like similar uh sentences that uh had semantically similar meaning so it was very helpful to kind of get semantically um relevant information from your data. Then came what we realized is actually keyword works really well for 70 to 80% of the use cases. So combining keyword search along with semantic search and developing hybrid search was the next kind of innovation that happened in search. And these days we see actually hybrid deployments, uh, hybrid search deployments across many customers. Many customers are using hybrid search for all kinds of use cases, whether, you know, they're, uh, searching for. Uh, data in their internal systems, whether they're powering their external facing user applications, so a lot of use of hybrid search across different systems. Uh, we also see customers using OpenSearch for new US cases, including agentic Search where they want to not only leverage that data that's available in OpenSearch but also leverage data in other systems outside OpenSearch. And, and so we'll go into details of all of this, uh, but that's how search is evolving and a lot of innovation happening in a very short time frame. Oh yeah. So let's look at a typical search workflow these days, right? You have your data or information. Images, documents, you take this and you typically ingest that into index that into open search. You also use a machine learning model to generate vector embeddings and you store those vector embeddings in OpenSearch. So you'll have a lexical index, you'll have a vector index. And you store that information open search. Then when you get a query, you're able to kind of use both these indices uh to retrieve the relevant documents and then uh do re-ranking if necessary and surface that those documents or the results to your uh user. So that's a typical kind of search workflow that we see. The challenge is it is pretty complicated, right? You have to figure out how to which model to use, how to host the model, then you have to generate all these embeddings, store them in a separate index along with the lexical index, and then figure out how to combine these results in an intuitive way. So it's a pretty complicated set up. So one of the things we launched, uh, uh, like pretty recently was automatic semantic enrichment. Think of it as an out of the box, semantic search and hybrid search capability for your use case. In this case, you're simply sending your documents to OpenSearch. In the background we have, we are using a sparse neural model that we host and manage and we generate the vector embeddings and the uh semantically enriched uh documents that we store along with your data. And so you don't have to worry about hosting, you don't have to worry about uh paying for, uh, you know, the, like the GPUs when, when you're not indexing. So it's, it's pay as you go, uh, but it kind of enriches your document. And then when you're querying you're able to leverage um the semantically enriched index uh and and you know get better search results. What we've seen in our testing and we've used different benchmarks is that this technique actually does really well compared to even like the other embedding models that are out there and so. It's a really easy way to get started with semantic search and hybrid search for your application. So if you're not using any of these techniques, there's a very simple way to get started. Um Let's kind of dive deep and understand how this semantic enrichment works. So let's say you have a text document. Uh, in this case we have picked, you know, uh, the Cricket World Cup, uh, which happened last year, and, uh, we've got a bunch of documents related to that. And what during ingestion when you ingest those documents. We call a machine learning model to expand the vocabulary and generate uh semantically similar words to what is there in the document and store that along with your uh regular data and index and and when so when the query comes we are able to leverage this semantically enhanced, um, think of it as synonym dictionary to be able to give really relevant results for use cases so it's a pretty powerful technique and. A pretty simple way to improve the accuracy of your search results. So let's talk about OpenSearch as a vector engine, right? Like all these machine learning models generate vector embeddings and you want to store these vector embeddings in some place and OpenSearch is a pretty popular, um, vector database these days. The reason is we started building vector capabilities in OpenSearch in 2019, uh, and at that time, you know, there are very few machine learning use cases, uh, for things like personalization for similarity search and so we needed maybe, you know, 10 million vectors were sufficient and so, uh, open search kind of started with that and as years progress in, um. 2122, uh, that grew to a billion vectors and we're supporting, you know, more of the, uh, bird kind of models that were out there, uh, still a reasonably small size. Then in 2023, 202024 as Gen AI became really popular, uh, we started seeing a lot of use cases for larger workloads and OpenSearch was now supporting up to 100 billion vectors, um, uh, like. Amazon's fraud detection system. Uses OpenSearch for up to 70 billion vectors and so it's a pretty large use case where OpenSearch scales really well. And more recently, more like many customers we see are wanting to index and create vector embeddings for all their documents, and, and that makes sense because you know all these models are pretty powerful and you want to leverage, you want to get most out of your data and to do that you want to generate these vector embeddings and. Now we support up to 1 trillion scale on OpenSearch, which is really useful for the large use cases for using vector embeddings. But Doing that at that kind of scale can be expensive, right? And, and so, uh, you got to be able to do it in a cost effective manner and quickly. To do it cost effectively, um, I mean, of course, if you put all of your data in memory and use exact cane and search, you're gonna get the best results, but it's the most expensive. The next thing you can do is use an approximate technique, keep the data still in memory, and that approximate techniques will help you, you know, reduce, uh, uh, the amount of data and amount of compute you wanna use and, and, and kind of reduce cost. Then we introduced the disk mode uh again uh early this year that lets you keep the vector data on disk and only kind of quantize data in memory and that reduces your cost even further. And just uh yesterday we announced the general availability of S3 vectors and the open search integration with S3 vectors and that with that you can keep your data all in S3, the vector data, and reduce your cost even more. So as we get to this trillion scale, uh, you know, there's different ways to reduce cost and get to that scale. So let's look at the disk optimized vector mode, right? The way this works is you take the high dimensional vectors that you have and you use different bitte quantization techniques. To generate, uh, you know, uh, vectors of lower fidelity, so you get up to, you know, 16 or 32 X compression and you keep those vectors in memory and you keep the high precision vectors on disk. And then when you get a query. You first do a like a canine search or approximate search in for in memory for the byte quantized vectors, get maybe 1000 results instead of the 10 results, and for those 1000 results you, you kind of get the retrieve the exact, you know, the high precision vectors and do an exact KNN. So with that it's a two pass kind of method. Uh, but you still get the higher recall and the that you're looking for, but, and you don't have to keep all of your data in memory. Uh, the only thing kind of caveat is that it adds a little bit more latency. So if your workload can tolerate a little higher latency, this works out pretty well. The other kind of way to do this is to use S3 vectors, uh, which we just announced, and there's two kinds of integrations we have with S3 vectors. One is you provision a cluster and you can now pick S3 engine as a vector engine in OpenSearch. Once you pick S3. We will, when we, when you send us the vector embeddings, we'll store them in S3 instead of storing them in OpenSearch. We can, uh, you can still keep the lexical index in open search that way you have the vector embeddings in S3, the lexical, uh, index in OpenSearch, and when you get a query you're able to do the vector search using S3. You're doing the regular search using OpenSearch, and you can do kind of combine those results and still do hybrid search and get all of the rich open search functionality while getting the kind of low cost of S3 vectors. This works really well for use cases where you don't have high TPS. Like, uh, at higher TPS you may want to keep all of your vector data in S3 as well, sorry, in open search as well, but for a low to mid kind of QPS use cases, the S3 vectors will give you an improvement in cost. So, so that's one way to use S3 vectors if you want to. The other way to use S3 vectors is to keep, you know, keep, take the data uh in S3. And bring that into open search so you could choose to, for example, you know, bring only the latest data or like, you know, uh, data for a particular category or a segment into open search and use that for vector search that way you don't, you're not bringing in all of this information into open search and that's another way to kinda use open search in combination with S3 vectors. So these are the two kind of integrations we have available, um. With that, uh, I want to bring, uh, Corey on. Uh, uh, he's gonna talk about how, uh, open, how and media has helped open such scale to the Australian, uh, vectors. Awesome, thank you, Muko. Hey everybody, can you raise your hand if you can hear me? Just a quick sound check. Awesome. Nice, kept these seats full. Yeah, Corey Nolette. I'm a principal architect for vector search, uh, various machine learning libraries at, uh, N Nvidia, um. This is no surprise that vectors are the language of AI today, right, unstructured data, as Muku pointed out, um, 1 trillion vector scale is becoming more commonplace, you know, even just as recently as 6 months ago, I would hear trillion scale thrown around by maybe 2 or 3 different organizations, and I'd hear it maybe once every 3 or 4 months. Now I'm hearing it literally weekly. People are are really paying close attention to this. A lot of organizations are in the process of kind of dipping their toe in the water, and we're really seeing this trending up. If we can get to 100 billion scale, we can get to 1 trillion scale, right? If we can get to 10 billion scale, we can get to 100 billion scale. So growth has been exponential. We've been seeing since about 2017. What a lot of people don't realize, and Muko kind of alluded to this as well, is that vector search indexes are not like traditional database indexes, right? They're approximate, and when you make something approximate, that now means that you have to model it, right, which now means that it's a machine learning model, right, of some sort, um. So, you know, traditional database indexes, if we, if we don't tune them properly, right, we still get the correct results, we just may not get them back quite as fast, right? Vector search models, we have to consider the trade-offs, right? For a more accurate model, we may not necessarily get the best indexing throughput, right? We may not necessarily get the best search throughput. And we have to make these trade-offs. Well, these trade-offs can be fairly expensive. They can have a huge impact on both cost and performance. That's, that's the big deal here, right? And so by utilizing GPUs we can try to work with some of those trade-offs and make them more manageable. We can make them more cost efficient. We can make them, uh, higher performance, right, especially at trillion scale. And so our partnership with AWS Open Search Team kind of um kind of formed around these 4 challenges that we're finding, right? And I'm gonna walk through in in the next 8 minutes or so, uh, how we constructed our solution to address these 4 challenges. The first challenge being index builds, right? Indexing 1 trillion vectors, building machine learning models for 1 trillion vectors can take a long time, as you can imagine, right? It doesn't necessarily scale linearly in all, in all cases. Right, interoperability is a big deal too. Something a lot of folks don't realize is in agentic AI and rag workloads, most of the time, at least at the present moment with our current technology, um, the, uh, the actual vector search lookup is not often the bottleneck, right? When you have an LLM in the mix and you might be reaching out to a different remote service to, to inference with that LLM or even if the LLM is local and it takes several 100 milliseconds. Uh, that's, that's usually orders of magnitude longer than, uh, the vector search. If I could do the vector search in zero time, I'm gaining nothing, right? So having a GPU to do that, uh, allocated for that period of time and sitting idle for most of the time doesn't make sense, right? So it's important that we're able to inter-opt, that we can build indexes really, really fast on the GPU and not lock you into GPU for search. Uh, as Mukel also pointed out, mixed types are a big deal, right? We're, we're not often just doing a semantic search. We might need to do a structured search, combine that with a semantic search to improve our results or even just to make our results, right? We might need to query from, um, you know, from geo coordinates within a certain bounding box and, and, and do a filter by a certain age group before we do our semantic search. And then of course last being cost efficiency, we don't wanna have to spend a lot more money to be able to do this on the GPU, right? We wanna be able to really have a mix of both. So we've been working for the past couple of years with our AWS open search team uh to solve these 4 challenges. So I'm going to put the solution together into a little layered stack like this. I'm gonna start with the index build, uh, which you probably recognize as green to signify Nvidia, so it probably means that it's being done on the GPU. A couple years ago we created a library called QVS, uh, using our, our standard coup prefix for CUA CUA vector search. Uh, why another library? Well, CUDA can be challenging to write. It can be expensive to write because it's challenging. It can be very low level. Uh, you can spend a lot of time optimizing an algorithm in CUDA, and then a new architecture comes out that introduces several new instructions. A new library comes out with a nice abstraction. Uh, now you've got to rewrite or refactor your code, so you're constantly playing this catch up game. So one of the big benefits of having a library that you can just pull off the shelf that can provide the building blocks for implementing vector search, whether that's directly in an application or inside of a database is you allow us then the the the manufacturers of the GPU, the, the creators of the CUDA versions to uh to maintain this going forward, making sure that you're always getting the best performance, the best cost efficiency out of the hardware and the software. And so it's fully open source. It's Apache 2 licensed. Um, the goal is to provide both the building blocks and end to end algorithms. So again, can be used in applications, but also integrated into databases. We build on top of all the foundational libraries that, that, um, that you would probably be used to if you were, um, doing CUDA development. Right, so most of the algorithms that folks are using today, especially off the shelf in a lot of the databases, we're working to change this, right? But most of the algorithms that you're using today are on CPU, um, one of the standard algorithms being, uh, the H&SW algorithm, graph-based algorithm, really fast search, not so fast build, um. It's an SW is not foundationally a really uh GPU centric algorithm, right? It utilizes multiple threads locking on a centralized data structure to to try to have low latency for insertions into a graph, um, so we kind kind of had to go back to the drawing board and we built an algorithm that we call CAGR from the ground up for the GPU to be able to do the construction of this graph in one big batch, uh, minimal to no locking. You know, CAGR's proving to be a pretty useful algorithm. It's a little bit different fundamentally, right? The H in H and SW stands for hierarchical. CAGR is not hierarchical. It's a flat graph, um, right? The, uh, SW stands for small world. CAGR is not a small world graph, right? But it turns out that it is navigable enough that we can convert that into a, um, H and SW graph. Now we find that as the number of dimensions increase, like today's embedding. Uh, models are, are getting kind of out of control right with the number of dimensions that we have to use. Uh, we can compress them down, we can kind of make it a little bit better. They're still kind of out of control though, so we noticed that the gap increases when we're building indexes, um, as the dimensionality increases, as the scale increases. Increases the more vectors that we need to index, uh, but also as the quality of the model increases, right, so as I want a, a, a, a model that can give me 99% recall versus something that gives me like 80 or 85%, then we're gonna notice that gap increasing even more. So, um, unsurprisingly, the lowest, uh, rung on our, on our stack diagram here would be the CVS library that we've provided to solve the challenge of index builds. So for the challenge of interoperability now we can convert this Cgo graph into an H&SW Cgo graph so we can search, sorry, into an H&SW graph on the CPU so that we can search on the CPU without losing any quality, without losing any latency, right? This bottom chart here can show you you can actually benefit. Um, in, in, in some ways on, on getting, uh, better latency, uh, by doing this conversion, but this is a big deal, so we can build indexes 20x faster or more on the GPU, and then we can convert them so you're not really losing anything, right? And that still prevents, uh, provides a little bit of a challenge, right, um. The phase library here has for several years now they've kind of pioneered vector search on the GPU before we called it vector search. It was approximate nearest neighbors, and this was an approximate nearest neighbors library that came out of meta. They've done this stuff for a long time. They've done a great job at it. Their algorithms are really well optimized for both CPU. They also have GPU algorithms. We have been collaborating with the Phase folks for, uh, about the past 3 or 4 years. Uh, we will eventually swap. The phase classical GPU back end out for uh the QVS back end we're we're kind of working towards that but at the present time we have a back end for QVS for phase, and this provides that seamless interoperability so you can build an index on the GPU, then you can search the index on the CPU. This is another one of those missing pieces here, this operability piece, right, that we can put in our rung right here. So the phase library kind of became our solution to that. Um, and it was kind of delightful to find out that AWS OpenSearch had already invested in a phase back end for OpenSearch, so they were able to reap the benefits of that interoperability mostly by flipping a switch to now use the QVS backend on GPU for building indexes. And so mixed workloads is a big deal, right? It's not enough to say that I can do a semantic search now. I have to have a solution that's going to let me do that hybrid in between, doing the structure, doing the semantic, doing the, the sparse lexical search along with the semantic search. So, uh, the Amazon. Open search service has enabled this. They have pulled out the standard, you know, let me build the index and then let me search the index in the same process. They've pulled this out into separate processes now so we can offload index building to a different instance if we need to, and that's really a big deal, right? It's not good to have a GPU that I have to pay for all the time if it's going to sit idle most of the time. Right, there are many reasons why I might need to build and rebuild an index, especially in the, the foundational architecture here, which is Lucene, right? I might want to adopt a new model. Well, now I need to rebuild my indexes. I might wanna do some level of tuning of the parameters so I can find the recall and latency trade-off that I need. Well, that means I need to build new models, right? I need to reindex. So for the mixed types we've adopted the open search service. And now this is kind of the, the, the big cinematic climax here, right? In order for me to be able to extract a cost benefit out of this, I need to be able to give the GPU back when I'm done, right? I'm not building indexes constantly. I, I might be building indexes constantly, and at that point that's great. I can have my index service running. Um, most people are not building indexes constantly. They might have a continuous ingest going at a fairly low volume. Right, but the ability to give that hardware back when I'm done is, is what really makes this, this cost efficient, and this is, this is a big deal, right? This is, um, this is something that, uh, our open search friends, our collaboration with the open search service developers has, has done completely novel. This, this hasn't been done, um, up till this point, um, and we're finding an extreme benefit here. So there's some benchmarks here showing cost and speed over on the right so we can end to end, right? If I'm just building the index, I can get a 2020x speed up, right? And that's, that's not including having to ship the data to a serverless infrastructure, having to ship the, the, the model back afterwards, right? Everything's said and done and to end I still see 12x speed up. Um, sorry, I'm, I'm, I'm seeing 14x speed up. I'm seeing 12x cost benefits out of this. It's this, this is a big deal. So kind of putting this all together, right, I can get faster index builds with the QVS library on the GPU. I can build on the GPU and I can search on the CPU with the phase library. Putting this all into the open search service allows me to do this with mixed types with an end to end database, and then having the new open search serverless GPU allows me to give back the GPU computing when I'm done so I can reap the benefits, not have to pay for it running all the time. And so summarizing this again, the 4 challenges that we've solved here. What we're noticing with cost efficiency too, I think was just announced at Matt Garman's keynote was that we're seeing up to 10x faster. That's a that's that's an average here at about 375% lower cost. So thank you everybody. Thanks Corey. Um, really exciting to partner with NMedia on this. Uh, we are seeing some amazing, um, outcomes. Like one metric, uh, now is that we can build a billion scale vector index in under an hour. So previously it would take, you know, maybe even sometimes days to build. Now we can build it under an hour. So really exciting to see that. Let's continue on some of the search innovations. So as you all probably know, building a vector index can be complicated. Uh, Corey talked about the trade-offs between latency and recall, and then you want also factor in costs. You have all these different parameters, different modes to configure, and so it can become challenging. And usually you have to, you know, build to build it, evaluate, and then again, OK, this is not working, so you go back to the drawing board and, you know, it's just kind of rinse and repeat of, uh, different parameters and that can take time. And typically what you're looking for is a trade-off between latency and recall, and cost is a factor as well, so. And the, the thing, the challenge is these parameters behave differently on your data. So if your workload and your data has certain characteristics, it will behave differently than other workloads. So you cannot even generalize it. And so you really need to use your data to figure out what parameters work well to give you the recall that you're looking for and the latency profile that you're looking for. To kind of address some of these challenges, what we launched yesterday is auto optimize. So auto optimize is a workflow where you upload your data. We take that data and then run through a bunch of different uh like experiments to find out which combination of latency and um recall that you've specified. What is the lowest cost option for that. So you know if you're looking, if you're OK with higher latency, we'll recommend a disk optimized mode. If you want really high recall, we'll recommend some uh some particular hyperparameter configuration so. We'll run all the different configurations and this job will provide you an output that you can directly apply to your cluster and uh and get the best outcome that you're looking for for your vector tuning without having to spend, you know, days trying to figure out what to do. So this will really help accelerate your POC to production uh for vector use cases um. And as I was saying, we paralyze all, all the different uh combinations that uh you want to evaluate with the GPU acceleration capabilities we are able to really do that quickly and give you an output uh uh within an hour. So that's kind of uh pretty uh like you know, important launch. Looking back at the overall open side stack. The stack has come a long way from traditional search. At the bottom of the stack you have 5, you have Lucine, different ways to get different engines for, you know, open search, and now S 3 vectors as a new engine. Then you have different use cases, right? You have all the search use cases, uh, hybrid search, multimodal search, semantic search. And then of course you have the vector database capabilities of open search, uh, and you can do, you know, uh, exact cane and approximate gain and the steered storage in there and then, uh, we are also building, um, many of the AI powered use cases, so agentic, uh, capabilities in open search MCP server, and, and I'll talk about some of that. And we are also building tools to be able to do, uh, you know, uh, like improve your experience, so being able to, uh, have, you know, connectors that connect out to different services, uh, AI workbench where you can, uh, you know, build different workflows. So a lot of this is now part of OpenSearch. So, uh, you all should try out for your next application, uh, and like OpenSearch has come a long way from just a traditional search engine. What we the one of the areas we are seeing a lot of uh interesting innovation is in the agent world. so search um is playing a pretty critical role as uh as you all build agents. What we are seeing is agents need context and agents are very iterative. Like, uh, you, you ask a question and they're pretty iterative in how they kind of decide what, uh, plan to use and execute. And agent like the agentic search is very different from a rag kind of use case. A traditional rag use case is you have a query, you, uh, go to a vector database, get additional context, and you take that context and pass it to, uh, an LLM, right? But with agentic search, what you're able to do is given a query, you take that query, you do some amount of reasoning. You use different MCP tools to get additional context. Then you use some, uh, data from short-term memory, long term memory to get more context. Maybe call an LLM, uh, get a plan back, refine that plan based on the additional context, and so it's an agentic loop that, uh, you have to kind of execute and this requires a lot more tools than, uh, what we have. So I'm excited to announce that we have many of these tools available. So we have an MCP server, we have uh agentic memory capabilities, and we also have some specialized agents that we built in OpenSearch. The MCP server is pretty standard, I think. Most of, uh, almost everyone probably knows, uh, how an MCP server works, but OpenSearch has a, uh, you know, uh, MCP server capability, uh, that you all can use. We have different set of tools like list index, search index that you can directly call using the MCP protocol, and your agents can directly access your data in Open search. Uh, so, uh, and we integrate, uh, we provide authentication and integrate with, uh, you know, popular frameworks out there. We also launched agentic memory capabilities in OpenSearch. So let's say you're using OpenSearch for vector capabilities and you want to store either a short term or long term context for our agents. You can now do that with OpenSearch and you're able to search through that memory using the Open searches search capabilities. So it's a pretty powerful way. To show store both the short term as well as long term memory, and we have different ways to manage the temporal capabilities. So if you are time sensitive data like you can delete that, so it helps you kind of manage that as well. And finally we've also launched 3 different agents. We've launched the flow agent with OpenSearch, which gives you a sequential kind of execution capability. So if you have like a single turn workflow for search, you can use the flow agent. You can use a conversational agent for more of a multi-turn capability and so it, the LLM in this case reasons about which tools to use and gives you kind of that ability. And we've also launched a uh plan, execute and reflect agent that leverages the LLM to plan a kind of execution flow, get additional context, and do the deep research work that, you know, some agents do so you can build a deep research agent using that kind of capability. So some pretty powerful capabilities in the latest version of OpenSearch, uh, for building agentic search capabilities. With that, uh, Carl can talk about the infrastructure enhancements. Thank you, Nicole. You guys can still hear me? Right? Um So we've talked a lot about the capabilities in open search. Before we close, I'm going to talk a little bit more about the benefits of these on the service. So mention the benefit of Amazon Open Search Service, the scale, you know, these 1 trillion, uh, 1 trillion vector indexes. We do support up to 1000 nodes in a single cluster, up to 25 petabytes in a single cluster, um, with high availability 49 SLA on a single cluster. Durability of 11 9s when you use open search optimized instances which are S3 backed and all of the performance benefits that uh we highlighted earlier. Another feature I'm really excited about is we launched Cluster Insights very recently, so this cluster insights feature. When you're managing a cluster, uh, it monitors the cluster. It can give you insights into how nodes are performing, how shards are performing, give you recommendations for better tuning and optimizing your cluster, queries, hot queries, hot shards, um, and all this just makes it much easier for you to, you know, uh, make sure your cluster is operating at peak performance, um, with detailed recommendations and insights. This is now available for any cluster running 217 or greater. You'll see a cluster insights link that takes you to the open search UI where there's a this panel will show up automatically. Few other things I wanted to call out uh. The open search optimized instances, I mentioned this before, these are high indexing high throughput instances that are backed by S3. We launched the OR1 last year. This year we launched our second generation with the OR2, and we also launched our first M series of the Open search optimize the OM2. Uh, the OR2 has a 70% improvement in indexing throughput over an R7G. The OM2 has a 66% improvement over an M7G. And we'll, you know, continue to see, uh, improvements on those generations so definitely check those out, um, if you have not already and you have got high indexing workloads. We also added a derived source feature. What this does is it can cut your storage in your cluster by up to 40%. So open source traditionally keeps source, which is the JSON data around. It needed that JSON data in case, uh, whenever they needed to do segment merges and shard, uh, uh, um. Re-indexing other sort of cluster operations with this we drop that we actually don't use the JSON data for any of those operations anymore we actually uh perform those operations based off of the pre-index data or the derived source. This not only means I don't have to waste 40% of my cluster on that source data anymore, it also is much faster, 20% improvement in the indexing and merges because, uh, the data is already pre-indexed, so I don't have to index that data again to do those, those operations. So definitely just check that box, turn that on, uh, save you, uh, save you quite a bit of, of, of expense. I also want to call out the custom plug-ins, uh, feature we launched last year. We added scripting plug-ins. If you have custom plug-ins, you can now run them on the, the service, um, and we're continuing to expand the capabilities of the classes we support there. So love to get your feedback on those features. I'm running pretty close on time, so I'll go quickly the uh. On the Amazon Open Search server list, which is the easiest way to get started with OpenSearch, with the serverless, I just, uh, create a collection. I don't have to worry about shards. I don't have to worry about sizing, so it's by far the easiest way. We're continuing to expand and mature the serverless product. We now support up to 100 terabytes in a single collection for time series data. We've expanded regions out to 22 regions. We added some key features like being able to audit data plane calls and cloud trail. And uh Snapshot restore and we're gonna continue to invest in the serus uh platform very excited about where that's going. So to wrap up Uh, if you wanna learn more, there's a link to, uh, our skills, uh, for more deeper learning on, on this topic and lots of open search topics. Also, you know, we're at our booth. There's a booth in D23 in the AWS Village. There's actually also an open search project booth down there in the expo. Come down, say hi. Would love to learn more about what you guys are, what you're doing, any questions that you have, and Last but not least, I really thank everybody for coming and you know if you could please fill out that survey, that data is really valuable for us, you know, we're, you know, wanna make sure that we learn the good, the bad, and, you know, are delivering the best possible content for you guys here. So please take the time to fill out the survey if you could. And with that I, I appreciate everybody sticking around for the hour and really excited to talk about OpenSearch and see what you guys do with it.