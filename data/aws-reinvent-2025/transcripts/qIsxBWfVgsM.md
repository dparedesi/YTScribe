---
video_id: qIsxBWfVgsM
video_url: https://www.youtube.com/watch?v=qIsxBWfVgsM
is_generated: False
is_translatable: True
summary: "This session covers building a unified, open, and secure Lake House architecture on AWS for accelerating analytics and AI/Agentic workloads. Speakers Pavni Bhaipuri and Mahesh Mishra from AWS, along with guest Achal Kumar from Intuit, discuss how a strong data foundation—featuring AWS Glue Data Catalog, Lake Formation, and Apache Iceberg—is essential for enabling AI agents to make data-driven decisions. Key announcements include an MCP (Model Context Protocol) server for Glue Data Catalog (enabling natural language data discovery), expanded Zero-ETL integrations (23 sources including on-prem databases), Catalog Federation (for Snowflake/Polaris), Iceberg V3 support (deletion vectors, row lineage), and Materialized Views for automated data transformation. Mahesh details operational primitives like automatic compaction, snapshot retention, and orphan file cleanup. Achal from Intuit shares their journey implementing a dual native catalog (AWS Glue + Databricks Unity) with federation, unified governance (ABAC/TBAC), and row/column-level security (RLS/CLS) for regulatory compliance (GDPR, CCPA, tax data restrictions) across 300,000+ tables and 70,000+ pipelines."
keywords: Lake House, Apache Iceberg, AWS Glue Data Catalog, Lake Formation, Zero-ETL, Catalog Federation, MCP Server, Data Governance, RLS CLS, Intuit, Unified Data Platform, Materialized Views
---

Hey, welcome everyone and thank you for joining us today. Today we'll explore how to accelerate your analytics and AI using open and secure lake house architecture. I'm Pavni Bhaipuri, accompanied by Manoj, uh, Mahesh Mishra, principal product manager from AWS. We also have a special guest, Aal Kumar, director data and analytics from Intuit, who's going. be discussing his real world implementation of lake house architectures. So whether you're just getting started or you're looking to optimize your existing lake house architectures, you'll walk away with practical insights to help you succeed in your data journey. Agentic AI is the next step beyond generative AI. It doesn't just answer questions, it also takes actions on your behalf. As you embrace AI driven decision making and automation, the quality and accessibility of your data becomes even more important. And that starts with a strong data foundation. For strong foundation for strong data foundation, you really need a strong storage management system. What that means is that you're able to bring data from different systems and formats and make sure that it's securely stored, organized, and is accessible. Then comes data processing and transformation. Here we're talking about raw data being transformed and enriched into this high valuable AI insights that the models can take decisions based on that. Next we're talking about data catalog and governance. Data catalog gives structure and governance gives trust. With data catalog you're able to discover your data in any part of your organization and with governance you ensure it's compliant, consistent and used responsibly. It's more essential than ever to get your data foundation right and rather than building new systems or new silos for AI you basically leverage your existing systems while maintaining security and governance. But building a foundation in the real world isn't easy. In most enterprises, data lives in dozens or even hundreds of different systems, operational databases, SAS applications, data lakes, and data warehouses. Each system has its own access patterns, its own security models and APIs, so for AI agents that need comprehensive context to make decisions, this fragmentation creates massive integration overhead. When agents need to query say multiple systems, they need to each request ads latency, network hops, authentication, handshakes, and format conversions. This fragmentation leads to performance bottlenecks and makes it hard to scale AI workloads efficiently. Data quality and governance becomes inconsistent because you're repeating rules across systems, and when AI agents need access to say sensitive data, they're basically introducing a new attack vector which requires you to have fine-grained access controls to make sure that you have better control on what it's accessing and when. And you also need auditability and to top it all, you need to make sure that you have reliability guarantees so that agents don't make decisions on stale or incomplete data. All these challenges make it difficult for teams to really execute efficiently with speed, agility, adaptability and flexibility that the AI agent systems need. The lake house architecture directly addresses these challenges through 3 through 3 technical, uh, capabilities. First, the discoverability. Your AI systems and developers need to quickly find the right data. The lake house provides rich metadata management, making it easy to discover the available data set. Next, consistent data access policies. Instead of managing security rules across multiple systems, the Lake House implements a centralized access. One set of policies govern all your data, ensuring consistent security policy and compliance across all AI operations. Third, we'll build this on open scalable systems. By supporting open formats, you leverage a broad range of tools, right? Well this means is you can bring the tools of your choice, whether it's Spark, Presto, Athena, Redshift, all working with the same underlying data without having to worry about conversions or duplications. Let's talk about the lake house architecture of Amazon Sagemaker. It starts with Amazon S3 providing the foundation layer, that's your storage layer. S3's object storage model is really ideal for data like architectures because it can scale independently. The storage and compute can scale independently. We also have S3 tables which provides a fully managed layer storage layer for Apache iceberg tables optimized for analytics workloads. Then we have Redshift, which is your data warehouse. It contains your structured, highly curated analytical data. Redshift really works well when you have complex queries and you have to basically join across large tables, and you require consistent performance. Next we have glue data catalog that sits in the center of this as a metadata layer. It maintains schema, partition information, table statistics, and table locations for both S3-based tables, as well as as well as redshift tables. Starting this reinvent, every new redshift cluster automatically registers with Blue data catalog. So what this means is you define your metadata once and all services can discover and use it. Then we have a critical layer that's the iceberg layer. Iceberg basically gives you the standardization. What that means is it gives you a standard way to read and write the data. And that makes it easy for Apache compatible engines to start reading the data from storage without really having to worry about conversions. We also have fine-grained access control. We do that through lake formation that's closely tied with with glue data catalog. When a query engine makes a request, lake formation intercepts that, reads checks permissions, and returns, um, uh, you know, data after applying column and low level filtering. With this, what you get is every query engine basically reads the same data. It gets the same consistent security policies and can benefit from the same metadata. So Glue data catalog, Glue data catalog is truly the backbone of our lake house architecture. It operates at unprecedented scale. Today it's used by hundreds and thousands of users and basically runs billions of requests weekly. It's also basically managing hundreds of millions of tables. These numbers don't just demonstrate scale, they also the reliability that the customers depend on for your mission critical workloads. The Glue data catalog acts as a central metadata. It's highly available, scalable and durable, and offers, and it offers 19 availability in a serverless cost effective fashion. The catalog is Open API compatible, Hive compatible, and supports OpenTable formats, namely Apache Hoodie, Iceberg, and Delta. It also integrates deeply with the AWS analytics services and the governance layout providing fine-grained access controls. It can ferrate with external catalogs. What that means is you can discover your data across your organization using Blue data catalog in one single interface. It also provides you. Uh, you know, strong security auditability capabilities. In a sense, Glue data catalog is basically transforming your lake house into a self-describing trusted data platform where every data set is discoverable, governed and optimized for intelligent use. Now we talked about the metadata. Next is, how do we make this metadata accessible? Typically scientists, analysts, users have to learn APIs, they have to execute SQL queries or navigate consoles. In today's AI driven world though, we need a more intuitive way to be able to interact with our catalog, one that speaks the user's language and not just the systems. We now introduce the Open Standards MCP server for glue data catalog. NCP, which stands for Model Control Protocol Model context protocol, is an open standard that enables AI to directly interact with your data. With this you can now explore your data. You can actually even create update table schemas using the plain English requests. MCP server extends to lake formation policies, so the policies that used to now earlier protect your analytics will also protect AI any access by AI tools. And because we build this on the Open MCP standard, it works with a wide range of MCP compatible agents, not just AWS services. You can use cloud, you can use custom agents built on land chain and any other MCP compatible tool, and they all respect your security policies. So let's walk through this demo. So here what we are looking at is Octank, that's a multi-regional company. And basically what particularly we're examining is the customer feedback table. What we've done is for the MCP we've restricted access and it's not able to access the PII data. So the first thing what we're doing here is listing all the databases. Next, what we'll do is we'll query the feedback table, the customer feedback table. Notice how your English request is converted into a query. And you can see that you get all the results except customer's name and email address. Next, what we'll do is we'll try to drop the table, see if that works. And you can notice that it wouldn't work because it doesn't have, you know, the MCP server doesn't have the permissions to be able to do that. Lastly, now what you can do is you can explore more your customer data itself. So you can watch how your uh natural language request is converted to analyze the customer sentiment across regions. And you can further, you know, explore furthermore. So now that MCP server makes it easy for you to discover your data and interact with your data through AI, we need to ensure that all your enterprise data is available for this lake house architecture regardless of where it originates. Our goal here is to bring the data from wherever it originates in any parts of your organization and make it accessible without you having to create complex pipelines or having to create new data silos. And we have 3 complementary methods to be able to do this. 1st, 0 ATL integrations. 00 ATL is a pat pattern where data is automatically and continuously replicated from the source systems into your lake house with minimal manual intervention. Within seconds of transactions being written into your operational databases, it's available in Redshift. Zero ETL handles both initial full load as well as ongoing change data capture, giving you near-time access to your operational data. We've significantly expanded our ero ATL offering and now support 23 unique sources across three categories AWS databases. This year we added support for RDS for Oracle, RDS for Postress, and Oracle database at AWS. We also added support for self-managed databases. This reinvent, basically you can run zero ETL integrations between MySQL, PostSquare, SQL Server, Oracle that's deployed on-prem or on EC2, and you basically can replicate the data to Redshift. Zero ATL also supports S3 tables as target. What that means is you can ingest data from Dynamo DB. You can ingest data from third party applications into an, uh, into S3 tables in Apache iceberg format. Second, we're talking about query federation. This is ideal for on-demand access with no data movement. Query Federation lets you run SQL queries across multiple source systems that span across different clouds as well without having to move different data. The query engine federates these sources in real time, queries are pushed on the source uh systems and the results are returned. So this is perfect for on demand analytics. We have a wide range of support for these sources. Third is something that we released just this reinvent it's catalog federation. It provides centralized discovery with optimized performance. This is our newest capability. Catalog Federation AWS Glue data Catalog provides direct and secure access to iceberg tables that are stored in S3 but are cataloged in remote catalogs. You can use AWS engines now to directly access this data. So how this works is, first you establish the connectivity with the remote catalog. Once glue data catalog communicates with the remote data catalog, it discovers where your iceberg tables are. It also discovers the metadata information. All and and next what it does is it registers this catalog as a federated catalog in in Blue data catalog. What that means is now your query engines when they're going and reading the data they have the location of that particular you know they already have the location of the data so they can directly read uh the data from S3 rather than having to push it to the source systems. You can also apply security policies with lake formation. So lake formation becomes the centralized place where you can build governance for your data. So let's take a look at the demo. Here we're seeing a demo with snowflake integration. In this scenario what we're doing is we see that Snowflake handles data ingestion and processing with metadata cataloged in Polaris. The customer wants to use AWS services for uh for to be able to query this data. So what we are doing is we are first. Establishing connection connection using your auth and you can see that this IAM role that you see is what is being used by Blue data catalog to connect to the remote catalog that is the Polaris over here. Once you have this established, it's getting a, you know, it gets mounted as a remote as a federated catalog, and now you can see the tables over here. You can also now set up tags on this data. Here what we are doing is we are restricting the query engines to only basically access the public, uh, any, any columns with public, uh, tag. So once we've established this connection, typically this is basically administered by a by an admin. Then we go to Sagemaker uh Sage Maker Studio and now you can directly query the data. And you'll notice that it's only returning the data, the columns with public tags showing late formation column level security in action. So, which one to use? Each approach offers distinct advantage. Many organizations use a combination of these approaches as part of their data strategy. You may use 0 ETL for your core operational data when you need the latest or the fastest query response times. Query Federation really works well for on-demand cross-system analytics. And catalog in uh federation really works well when you want to integrate with remote catalog while you want to leverage the price performance benefits of of AWS analytics engines. Now we brought the data in, and the next question is, how do you ensure that it's stored and managed in a way that supports reliability, performance, and flexibility that modern AI and analytics workloads demand. Open table formats bring you these capabilities. Apache Iceberg has emerged as the leading open table format that transforms the way you're managing your data lakes and lake houses. Let's see why it's so powerful. First, it provides asset compliance, automicity, consistency, isolation, and durability, ensuring your data lake and lake houses can handle concurrent transactions with reliability, which was previously only available in databases. Multiple users can now read and write your data simultaneously without any corruption, without having to worry about any corruption. Second, it provides scalable metadata handling. Iceberg maintains consistent performance even with billions of files and petabytes of data. Third is schema evolution and enforcement. Iceberg allows you to add, drop, rename, reorder columns without downtime or compromising data integrity. Next, last but not the least, is time travel. It lets you maintain a history of table changes, allowing users to query previous versions for data auditing, debugging, and recovering from errors. When integrated with Blue Data catalog, Iceberg provides high consistent performance data access control across all your engines from Athena to Redshift to EMR and SageMaker. At Reinvent, we also announced support for Iceberg V 3. AWS services now support deletion vectors instead of rewriting the entire files when the rows are deleted, which can be extremely costly and time consuming. Deletion vectors maintains a lightweight bitmap that marks the rows that have been removed. Where this helps is it helps with your improved, it improves your query performance because the engines can now skip deleted rows efficiently during scan. Next is our support for low row lineage tracking. This capability gives you complete visibility into where the role originated and how was it transformed. Extremely useful for audit trial compliance and impact analysis. That is to understand how the downstreams, uh, downstream services get affected before you make any changes to your data pipelines. Now that we understand the performance, you know, the powerful capabilities of Apache Iceberg, let, I call upon my colleague Mahesh to discuss critical aspects of managing iceberg tables and security controls at scale. Thanks. Bye. Am I audible? OK. So My name is Mahesh Mishra. I'm a principal product manager with AWS Analytics. Before we start talking about how to operate Iceberg compatible lake house with Iceberg as the OpenTable format as well as Iceberg Ras catalog as the interface. Let's, let's do a poll. How many of you really use iceberg in production? About 50% this is good. So, so basically what we do with with AWS, uh, Blue data catalog and lake formation is we make it incredibly easy for you to operate an iceberg compatible lake house. So the key problems that customers experience uh when they uh they have Iceberg as a, as a, as a table format is. It's when, when you write data into your tables, you, you create new versions of the data and you create a lot of stale data sets in your lake house that increases your storage cost. Your, as your scale increases, you, you see performance issues. And then transformation of data becomes harder. Finding the right data set, finding the data of what changed, and being able to transform it in the right way becomes harder. With AWS Glue data catalog, we have built primitives that makes it easy for you to operate this lake house. So before we go and understand what those primitives are, let's try to understand what happens really underneath the hood, why over a period of time you see performance regression on iceberg tables, and why your cost profile goes up. Oh, think about the time you started the Iceberg Lake House, you created a table, you started writing data to it. As the time progresses, what happens is you accumulate a lot of data files in S3. And if you are actually writing small data in chunks, you are accumulating a lot of small files in S3. Then what happens? Iceberg is basically a metadata layer on top of your S3 files. Your metadata becomes big every time you file a query. You have to process large amount of uh metadata, a lot bigger metadata files. Similarly. When, when you have a large number of files in the storage, you make round trips to the storage, which is S3 to get those data files. Eventually what happens when you have a large number of small files in your iceberg tables, you see the performance going down. Similarly, Iceberg is a model where every time you write data, we, uh, the table format creates new versions of the data, and it marks certain versions of the uh versions of the data as tail, right, as older snapshots. So the data that is sitting in your storage, and if it is, if it is an older version, you are no longer using it, it, you're just paying for the storage, right? So Iceberg has control plan operations where you do compaction, you do snapshot retention, but you still have to manage it all by yourself. So with Blue Data catalog, we think about these as primitives. They just work for you. And you just focus on reading, riding data into a lake house, and all the operations, the muck that you are doing today goes away with a single click a button. Two core capabilities here is compaction becomes a background job. And Snapshot retention or removing the older files or versions of data is also a background operation. Let's understand how it actually works in practice. If you, first of all, it is a background job. You just said it once and forget about it. It now, once you have set it up, it intelligently decides when to run a compaction job and what is the right size of file that it needs to create. And once the compaction is done, You, you will see sudden improvement in performance. This year we added more advanced capability to compaction. We went beyond just small file compaction into something like sorting reordering. These are, these are mechanisms where you can sort your data on the disk, and we do that while compacting your, uh, compacting your tables. In blue data catalog. You should be able to define what is your compaction strategy. Default is bin pack. You should be able to, if you have sort out of columns defined on your tables, you can, you can actually. Um, enable sort compaction. So during the time of compaction we'll sort the data. If you have queries that are running across multiple dimensions, you need multi-dimensional analysis. Zorder helps you to sort data. It'll cluster the data in a way in the storage that you should be, you should be getting better performance. Let's look at snapshot attention. Just as we talked earlier. You every time you write data, some, some, some files become obsolete, some new files get created. And then customers don't want to store all those versions of data. They only care about some versions of the data uh so that they can go back in the history and do time travel on it. What in blue data catalog, we allow customers to define those policies. Those policies can be time-based, number of snapshot to return base, or it's, it's more flexible time-based policy that you can define where you can essentially say, hey, I want last 5 days of snapshot to be returned, or I want last 5 snapshots to be returned. So it's a flexible policy model. You you define that policy and set it once. Once you do that, glue data catalogs will have a paired compute which will go look at the policy and apply that policy on a data set. Now think about a case where you are just reading, writing data into your iceberg tables. And your lake house is fully managed by AWS. How much more value that you can generate from your data? So there is a 3rd type of maintenance procedure that Iceberg has to go through, which is orphan file. Someone who has operated a large scale lake house would have seen that ETL jobs fail. It's, and when ETL jobs fail, what happens is you have created a bunch of files in your S3 storage and those files stay there. So We included a catalog we allow customers to define those policies so that these orphan files which are not referenced by any table, those are still files, we delete them that way your cost remains in check all the time. So if you are building a lake house and iceberg is your format, it's fully managed by AWS, but customers told us that that's not enough. So So they, they said, OK, data lands in my lake house now. How do I transform it? It's incredibly hard to, can we come up with a primitive where transformation becomes a background job? That's interesting, right? So before we started thinking about this, we, we, we looked at the problem. We saw that every customer who, who creates data transformation starts with this vicious cycle, starts with understanding what changed in my sources source tables, then write a business logic, apply it on the change data, then that change data gets persisted in a separate table. The pipeline has to be orchestrated. Infrastructure needs to be scaled. Jobs fail. You monitor, manage everything. This is taking you away from your real value from radar. So this year, We announced Materialized view that our eyes were compatible on Blue data catalog. What is a materialized view? Think about it as an iceberg table that is fully managed by Glue data catalog. You said it once, forget about it. Blue data catalogs manage compute, we'll do the change detection, bring the data over, write it in the. In the materialized view, you don't have to manage pipelines. You just set it once and forget it. So think about the primitives that we have built. You are bringing data into your lake house. Don't think about storage management anymore, just taken care of by the Glue data catalog. You want to transform your data, set it once, forget it. Your data transformation happens automatically underneath the hood. It's, it's compatible with Spark SQL, so you have a declarative SQL language now to define your transformation and deploy it as a materialized view. Your, we did, um, your first party spark engines which includes EMR glue and Athena Spark engines should be able to detect that there is a materialized view and if they see a query that can be run using a materialized view, they'll rewrite the query. So we changed the optimizer of Spark to automatically identify materialized view and our benchmarks was we got up to 8X performance improvement on Spark. Because of materialized views. In summary, Metalized view, make it incredibly simple for you guys to transform your data. We talked about bringing data over, we talked about managing the data in the lake house. We talked about generating insight by transforming the data. Now how do we secure it? The security controls, there are two key pillars in the security controls. The first one is fine-grain access control. You want to be able to control who has access to what at a table level, at a database level, at a column level, at a row level. You should, you should be able to scale your permissions through tax or attribute-based access control. And you should be able to do zero copy data sharing, which means you can share data with your partners, with your customers, with your teams without making a physical copy of the data. All these 3 capabilities are powered by lake formation. So let's look at how fine-grain access control works. Think of a table like this where you can define policies on the tables. Providing a list saying Here are the inclusion columns. Here are the exclusion columns, right? So, so as a customer, you can say. Mahes can have access to everything in customer table except for the customer ID or customer email address. Or you can say Mahes has access to only customer ID column, nothing else. Both inclusion lists and exclusion lists are supported. You have filters for low level access control, and when you combine them together, you can restrict your data access at the cell level. Customers asked us to. These, these permission models are very useful, but they want to scale. They have thousands of users of their lake house now with agentic workload. They want, want every agent to have their own right access controls as well. So to scale your uh permissions, we, we had tag-based access control. Think about tag as a collection. You, you create a collection of data sets and say some a user has access to the collection. That's this, this is called tag ontology. You define your ontology based on your business requirement and give access to your users. Customers said that's not enough. Our people move around the company. With so people change departments. How do you solve that? This year, we announced attribute-based access control. What that does is policy management on dynamic user attributes. The user attributes will be evaluated in the runtime. And the policy will be evaluated on the runtime to decide who has access to what. What that means is. My head was part of Say, sales department. Moves to something like marketing, his policy automatically changes because the my the attribute changed. So your security controls are there, but we also have customers who have regulatory requirements. So, so we provide best in class authentication, access control mechanism, auditability. Compliance, we are satisfied with SOC, PCI, FedRAM, HIPA. So if you want to use, if you are a regulated industry, you should be able to use it out of the box. And we're, uh, and we have encryption for data as well as metadata. So if you're an enterprise, Struggling to build a lake house, AWS is the right place. To start With this, I'm going to invite over Achal Kumar from Intuit who is going to talk about how they're using these capabilities for uh their future lake house. Thank you. Hi, everyone. I'm Machal, and uh I'm part of Intuit Data Analytics Organization. And with me, I have my team here. So if you have tough questions, go out to them, not me. So what's IT's mission? Our mission is to power prosperity around the world. And how do we achieve it? We want to provide more money to our customers' pockets with no work and complete confidence, and our customers are consumers, small businesses, and mid-market companies. And we want to build an air-driven expert platform which sells for them. So if you look at Inter's platform advantage, and luckily for us, we have been focusing on data for the last 2.5 years. We have embraced data mesh concepts, we have embraced data as a product concepts. So all consumable data is easily available and discoverable for our data workers. And if you look at the scale, we are talking about 86 million consumers. 10 million small and mid-market businesses. And look at the data 70,000 tax and financial attributes per consumer. We are talking about 625,000 customer and financial attributes per small business. This is huge And on this data, we can run our machine learning models, our agent to AI experiences on all that. So let's look at into a data landscape, and I've divided into three categories. One is scale. We have 300,000 plus tables serving diverse use cases. 70,000+ data pipelines, ingesting and transforming data as we speak. And this says 2,500+ users, but in this age, everyone uses our data. Software developers, data workers, data analysts, machine learning engineers, everyone. And we have 200 petabytes of data or more. I think every data keeps coming in, fueling ML models and AI agents. And the biggest challenge is the technical heterogeneity we have. I'm sure most of you guys face this, is we have two warehouse platforms, AWS and Data bricks. With distributed engines requiring unified governance and a seamless user experience for our data workers. And because Intuit is a financial company, we have a lot of compliance complexity. Like for example, 3rd party contracts, you can't use the credit report data for x purpose. Regulatory requirements. Biggest example is tax data cannot be accessed outside the United States. Or tax data cannot be used for anything else except improving your tax experience, unless the user has given consent. So a lot of ifs and buts are there. And of course we talk about GDPR, CCPA. And the last but not the least are the consumer rights. Our consumers opt out. They say, Don't use my data for analytics. Don't use my data for machine learning. So we have to comply with all that, and that's where things get interesting. So, uh, let's look at the evolution. So, I know for most of you, but we were, and we, we are still using high metastore for some purposes. And that has hit the scale of cloud limits. So data was stored in S3 buckets. We wanted to apply finding an axis. The only way to do was to use AWS policies, and we hit AWS 20 kilobyte policy limit, so that was scale. The second reason why we had to move beyond HMS was we wanted finer grained access control, not just at the table level. The same table can hold data for 2 users, one has given consent, one has not given consent. So now we have to apply row-level column security, row level security. Now same table can have PII column and non-PII data. So now we have to apply column level security. So that's where RLS and CLS capability is very quiet. And the last one is the policy-based ABAC. So until now we were doing data stewards were approving access requests for consumers. We want to get away from that. We want to make it automated. So based on the policies, data tax, and things like that, we should be automatically approving or denying access. Now look at the constraints. Of course, the business has to run. So no user disruption could happen. Teams who are using AWS should continue using AWS technologies, and same for data bricks. No business impact. We have 70,000 production pipelines running. You know, you cannot stop them. We have to preserve unified experience. And we have to enable RLS, CLS, and dynamic policies while maintaining all this. And the most important thing is there shouldn't be any user disruption, or we can't ask people to migrate from one tech to another. So what we chose, we chose dual native catalog. We have to maintain AWS Glue data catalog as well as Unity, which is Dabrick catalog. With a federation layer between those two, which Pavni has talked about, so that there's no data duplication. And and what we had to build is a unified control pane above both the catalogs, so our data workers don't worry about these two catalogs. And why this works. So, we are using native technologies, GDC and Unity, and both of them provide native RLS CLS for us. And the Federation helps us because the data is not being copied or replicated across these two texts. So if you look at the high level picture of the foundation, And we have adopted Iceberg as the format. Until now we were on parque. You can laugh at it, but So if you see here, database workloads will light data in Delta Lake format. Uniform comes as savior and generates iceberg metadata from delta tables automatically. So AWS workloads interact with that same data in the iceberg format. And in the bottom, the Unity and AWS Blue catalogs are being federated. So each table lives in one native catalog federated to the other. And on top of that is our unified control pin which we have to build, which provides single source of truth from metadata across both. We manage compliance tax, life cycle, access management, all that is done through our control plane. And users see one unified experience despite dual catalogs. That was very important for us. And if you look based on that architecture, All these use cases work the same way as they used to work before. BIA analysts, they query any data via Tina, Redshift, build their dashboards. Data engineers, they have 70,000+ pipelines, no change, no migration. Data scientists experiment with federated access to all data sets, and they can switch technologies. They'll get the same experience. So no data silos, no forced migration. Teams choose the best tools, whether it's AWS or data bricks. We're not controlling that and governance is consistent. So we talked about what AWS provides and Abix provides, but if you look at the practical example of what Intuit had to do to bring all this together. So we had to do 3 important things. Every data has to be tagged, either table, attribute, schema, before data is produced. So that we know, example is this table has tax data or this column is PII. Before a producer produces data, you have to tag it with standard tax governed by us. The second is Every principle, whether it's human or machine, has to be labeled. This human is outside the United States or works in a project outside the United States. This human belongs to a project focused on marketing purposes. So we are, we have project level labeling and that label applies to all members in that project. And then comes our legal and privacy officers who use that information to define policies in English in our policy management system. They don't know technical aspects. They define policies in English, and that policy is translated into machine readable format and stored in our authorization component, on the left side. Now take an example. A data consumer comes to our data discovery portal. Intuit has, as I said, 2 years we invested in this. All consumable data is discoverable in our data discovery portal. The consumer explores data, finds a tax table. The guy is interested in that. He requests access to it. Our data access control management will now look at the data tax for the table. Look at the principal attributes this user is trying to do what from where. Sends the request to our authorization component and that component will either approve, deny, or semi-approve, saying give access but only to consented data. And then our data lake control plane. Syndicates that policy across AWS and data bricks at the same time. So that happens during access request flow. And on the right side if you see there's a query time where the same consumer user comes and says query data, either it can go to, he can go to AWS or data bricks and he gets the same results back. So in this partnership, if you see what technologies we use from AWS and what we had to build, it's pretty straightforward. We use lake formation, which gives us production ready RLS CLS. query time enforcement is there. Of course, Iceberg Rest catalog really helps. And of all data, our data lake is in S3, which helps us with cost, intelligent hearing, scale, and reliability. And what we had to build was a policy management system. The data tagging system. It's very important to tag your data. It's very important to label your principles. And our data lack control play. And a sample use case is here. And this is all real, by the way. Uh, a data bricks, a data engineer creates a table, stores data, and if you look at it has, the user has tagged the table saying this table has machine learning opt out. So please check before giving access. And there's a column called Email which is marked as PIIS true. ML engineer is using sage maker, tries to query that table. If the table has 100,000 records, imagine if 20,000 people have opted out of machine learning. The ML engineer will only get 80,000 records, and the ML engineer has no idea what's happening. Everything is automatically happening here. And if another user opts out in the middle before the next one, the number will go down further. So this is our current state and of course with every development we have some feature requests for AWS. So one big ask we have put the request is for right now the RLS and CLS are based on a single table, but most of our opt-outs and consent tables are stored in a separate table, not in the main data table. So you need to join those two tables before returning the result back. So that's a feature request. The other one is column masking. We just don't want the catalog to, you know, deny access to a column. We want to mask the column as well. And the last one is hybrid ABAC plus TBAC policies. That's what we have implemented today, but we want 100% ABAC policies so that. Intuit is not in the business of pushing policies for access request. We can just dump all the policies, all the labels, all the tags, and everything should happen at the catalog level. So with that, and I just want to add, it was not easy to get here. It sounds very easy on the slides and the presentation, but it's not easy. So thank you, and I'll hand that back to Pavni. Thanks Achel. As we close today's presentation, I want to reinforce the benefits of lake house architecture of Amazon Sagemaker. First, we bring both the benefits of data warehouse and data lakes together. You no longer have to choose between performance and flexibility. You get both. The unified approach means that your teams can work with one consistent interface, one security model, and one governance framework for all your data. Next, the openness of this architecture. We use Apache Iceberg as the foundation. You can now connect to external catalogs and discover your data across organizations. And last but not the least, security is built into every layer from lake formation integration to fine-grained access controls. Your data is protected while remaining accessible to authorized users, engines, and agents. With that, we are ready to take some questions. Thank you.