---
video_id: Ms0lDo_19sM
video_url: https://www.youtube.com/watch?v=Ms0lDo_19sM
is_generated: False
is_translatable: True
summary: |
  Suraj Sajjad and Visa’s Hema Zuchi and Arvinder Nagala describe how Visa built Visa Protect Account-to-Account on AWS to combat surging fraud in account-to-account (A2A) real-time payments. As RTP volumes and values climb, Juniper forecasts A2A fraud losses reaching $58B by 2030, pushing Visa to deliver sub-250 ms fraud scores without breaking user experience or regulatory demands. Requirements included zero-trust security with in-memory protection, GDPR-aligned data localization, global low latency, 99.99% uptime (tier-zero), dual-region disaster recovery, and the ability to scale to 1,000 TPS bursts.
  
  Security drove the choice of AWS Nitro Enclaves to process PII inside isolated “vaults” unreachable even by root, addressing threats like core dumps or memory scraping. A zero-trust architecture terminates TLS and validates tokens inside enclaves; sensitive fields are KMS-encrypted in-memory before leaving. Data localization and latency are achieved by deploying transactional scoring and model inference in EU regions (London/Ireland) near banks, using Valkey (Redis-compatible) on Amazon MemoryDB/ElastiCache for low-latency reads with durability. TLS connection pooling, TLS 1.3, HTTP/2, and VPC links keep traffic on AWS backbone and within AZs. Resilience is provided via active-active multi-AZ in two regions, with Cloudflare/Amazon DNS steering, MSK Replicator for Kafka, S3 cross-region replication, and Aurora Global Database for database failover—meeting Visa’s ITDR rules (regions >60 miles apart) and 52-minute annual downtime budget.
  
  The real-time flow lands client requests in a DMZ VPC guarded by Network Firewall and mTLS at the load balancer, then through Transit Gateway and private API Gateway into an enclave-based gateway (via NLB). The gateway converts REST to gRPC, encrypts PII, and calls an Amazon EKS decision service that deduplicates/enriches with Valkey, then invokes Ray-based model inference on EKS. Long-term profiles live in MemoryDB; short-term aggregations and rules engines finalize the risk score, which is returned through the enclave. Requests and responses stream to MSK for billing, reporting, and near-real-time feature engineering.
  
  Offline, banks SFTP PGP-encrypted files through AWS Transfer Family into S3; GuardDuty scans for malware, and an enclave-based ingestion service decrypts, validates, encrypts PII, and publishes to Kafka. Firehose and EMR aggregate data in S3 for feature generation and model retraining; entitlement and branch data flow into Valkey and Aurora to keep real-time and offline views consistent. Nitro Enclave best practices—allocating huge pages at boot, tuned SOCAT proxies, health checks, and caching envelope keys in memory—yield 150–200 TPS per m5a.2xlarge gateway instance. Latency tuning includes AZ affinity at NLBs, cross-zone LB for HA, cached secrets, and per-AZ traffic routing.
  
  EKS is tuned for performance and HA: AWS Load Balancer Controller provisions NLBs with IP targets to avoid extra hops; node affinity pins enclave-capable nodes; pod anti-affinity and topology spread constraints keep replicas balanced across AZs; topology-aware routing keeps traffic in-AZ unless unhealthy. Combining these with Valkey/MemoryDB produced P99.5 latencies ~40% under the 250 ms objective. Visa now treats this as a blueprint to replicate via IaC into new regions (e.g., São Paulo) while preserving cybersecurity bar, latency, and uptime. They credit close AWS collaboration for translating on-prem controls to hybrid cloud and enabling rapid expansion of A2A fraud defense.
keywords: account-to-account payments, Nitro Enclaves, low latency, Valkey, active-active architecture
---

Alright everybody, thank you for joining us at Reinvent 2025. It is exciting to see a full room today. My name is Surid Sadjo. I'm an AWS customer solutions manager working with our largest global financial service customers on their enterprise transformation journey. Welcome to Secure in Milliseconds where you will learn about how Visa built an AI powered fraud defense on AWS to secure account to account payments. I'm joined today by Ja Mazucci, senior director of engineering at Visa, and Arvinda Nagala, chief architect for the Visa Protect A2A product at Visa. It's a pleasure to have them here with me. We've had a phenomenal partnership over the years, and it is a testament to their team's hard work that we're able to share key learnings from their journey. Let's go ahead and get into the agenda. First, I will talk about the growing payments landscape focusing specifically on account to account payments, it's increased relevance, and the growth in risk. Then my colleagues from Visa will talk about how they secure this risk through Visa Protect for account to account payments, the high bar of requirements that they held themselves to. And the journey in building this on AWS, we will deep dive into how Visa leverage AWS Nitro enclaves to meet stringent security requirements, achieve low latency through Amazon EKS opt optimizations and memory DB, and built for a high bar of resilience. We will then wrap up by reflecting on the key takeaways Visa had from their journey. So in today's day and age, consumers and businesses alike are looking for flexibility and convenience when it comes to paying for goods or services. For years, the swipe of the credit card had done the job. For decades, this had been the backbone of commerce, and it still continues to play a critical role. Then we saw the rise of the digital wallet. All of a sudden, with a tap of your phone, you can now pay for your goods and services in a seamless, contactless checkout manner. This was the next stage in the payments evolution. But with the rise of digital technology, consumers and businesses are looking for even more streamlined cost optimal methods of payment. This is where account to account payments or A2A payments come into play. A2A payments are a secure, reliable, streamlined instant payment method that moves payments or funds from the sender's bank account directly to the recipient's bank account without the need for additional intermediaries like credit card networks or credit card instruments. There's various use cases for A2A payments like person to person, person to business, business to person, and beyond. There are two types of A2A payments. First is the push payment. The push payment is initiated by the sender and moves funds directly from the sender's bank account to the recipient's bank account. We've all probably used this. A great example is, let's say a buddy of mine covered me for lunch yesterday and I need to pay him back. I'll go ahead and send him a push payment. The next is the poll payment. The po payment is initiated by the recipient and moves funds from the sender's bank account to the recipient's bank account. Examples of this include a business that has subscriptions or recurring payments that they would like to receive from their clients at an agreed upon time interval and so they initiate a full payment. Let's go ahead and look at how the A2A payment actually flows. First, the consumer or business requests the payment, and that request gets routed to the sending financial institution who then processes it and initiates the payment via a real-time payment network or RTP network. Few examples of RTP networks include PIS in Brazil, UPI in India, faster payments in the UK and beyond. The real-time payment network then forwards the payment to the receiving financial institution who then processes it and posts it to the receiver. This more streamlined payment method has seen increased adoption over the years, uh, that is attributed primarily due to the cost optimizations available through RTP networks. RTP networks are significantly more cost efficient than credit card networks. Juniper Research is forecasting 83% growth of the annual number of transactions in the A2A landscape by 2029, reaching 1 trillion annual transactions. By 2030, the annual transaction value is expected to grow by 113%, reaching $195 trillion of transactions annually. That is tremendous scale and tremendous growth, but why does this matter? Well, as 828 payments grow, the transaction rates grow, the transaction value grows, so does the associated fraud. Juniper Research also forecasts that the annual value of fraud related losses is expected to grow by 153%, reaching $58 billion worth of losses in the banking segment. That is not a negligible amount. And this risk and this fraud is exactly what customers like Visa are solving for by leveraging AWS while meeting stringent requirements around security, latency, resilience, and even data localization requirements while enabling tremendous scale. And so I invite my colleague Hala to tell us how Visa secured account to account payments and built this on AWS. Thank you for setting that context. My name is Hemazuchi. I'm a senior director of Engineering at Visa. Visa is synonym with trust, and to build that trust, Visa has invested over the years in core principles around resilience, high up time, high availability, stringent adherence to cybersecurity, and low latency. And based on these principles, as Suraj was talking about account to account uh payments, the growth of account to account payments, wherever there is a growth in the payments, unfortunately fraud follows. And since these account to account payments do not use the existing fraud payment solutions of the card network. Visa built a solution specifically for non-card payments called Visa Protect Account to Account, which is aimed at enhancing the security of non-card payments. It's designed to address the demand of the greater security in non-card payments, and as Suraj was talking about, it caters to all kinds of non-Card payments B2B, B2C, C2C, and C2B. To build this solution, we leveraged the AWS infrastructure. And this infrastructure allowed us to build both our scoring engine, which basically gives the fraud score back to our customers, and they can make a decision on this, as well as our model infferencing AIP platform, which is our backbone for giving the score back to our customers based off the account account models. Um, in Suraj's diagram earlier, we talked about account to account payments, how, when a sender receives, sender sends a payment through its sending FI, which is, uh, sender's Financial Institute. It goes to RTP networks. Here to avoid the fraud between the two sender and receiver, to avoid that kind of fraud, we are introducing a call made to Visa Protect account to account. Here this call lets the sending FI decide based off a score that. Protect account to account is going to return back to make a decision. If the score is high, it can stop the payment to go to the RTP network, thus preventing the fraud from happening. Now let's say for whatever reason if we are not able to make that call to visa protect account to account through the sending FIRit did not get the high enough score. There's another opportunity for us at the receiving FI Receiving Financial Institute, which is a receiver's bank, to make a call to visa Protect account to account and stop the fraud from fully happening. It can reject the payment from the RTP network. Visa builds solutions for its global audiences for all over various different regions. Within various regions like the UK, the liability of fraud lies both on sender and receiver. So to protect both senders and receivers from their liability of fraud, Visa Protect Account Account is a solution that a lot of our customers are using to prevent fraud from happening. Um, in the next few slides, I will be talking about, um, the, the requirements that we had to build this solution and how did we leverage AWS platform for building this solution. Visa has traditionally built its solutions within its on-prem system. So a lot of our requirements were catered towards our on-prem data centers. We had to rethink some of that and make sure we are able to build it as per the same guidance as we are building to protect our customer data um in the on-prem systems. So the first requirement is adherence to very stringent cybersecurity guidelines. This is not just a buzzword for visa. In Visa, this is taken very, very seriously because our customers trust us with their PII information and with their PII data. So on top of your regular DSRs, which is design security requirements and technical security requirements and everything that comes with. Data protection, we had a unique problem to solve as we were taking our solution into cloud, which was to protect against in-memory data exposure. Data is encrypted, protected when it is in transit or when it is stored. It is not protected, however, when it is getting processed in memory, and it is these times that various threat vectors and malicious agents try and steal the sensitive data. Um, threat vectors like, um, core dump and swap files, exposed data endpoints, and memory scraping hardware, this is the chance where they, they can get into the cloud network and steal customer information. To protect against that, not only did we build another layer of security, we rethought the architecture. We went, our solution against this threat was to go with zero trust architecture and use AWS Nitro enclaves. Think about nitro enclaves as a secure vault within a main server, and nobody has access to that secure vault, and all communication to that secure vault happens through the main server on a secure channel and nobody, not even AWS or Visa accounts have access to that. My colleague Arvinda is gonna deep dive on this in in the next few segments. Next very important requirement to build a solution that caters to all the regions that we support is to adhere to the local regional requirements. Since we were building this solution for our European markets, one of the very important requirements for us was to build around GDPR requirements, even though GDPR does not necessarily. Request you to have data localized, it, it's, we are able to do better with their requirements if the data is localized. We also had requirements from our banks who are our customers to keep data locally. Um, for that, for that specific reason, our solution to build the, uh, to this, um, to, to this platform was to use AWS, um, regions, uh, for both transactional and model influencing engines to be hosted in the European regions, uh, in AWS. And this not only helped us, um, with the localization requirement, this also helps us with the next requirement, which is around low latency. Again, low latency is not just, you know, we don't take it lightly. It's a very, very serious requirement in the financial sector. Transactions happen within a few seconds, and any kind of latency can break the overall user experience. Take for example, let's talk about the example that Suraj took where two friends are splitting the bill. Now that is happening over the RTP networks. If we now want to add another layer to it of security by using Visa Protect account to account, we cannot afford to extend that time of end to end transaction between these two because that will break the overall experience. To make sure that we are able to provide this fraud solution, we had to make sure the solution that we are building is able to give the end to end results in less than 250 milliseconds. That is a fraction of a second, which means that you have to accept the transaction, process it, run it through our, uh, models, and respond back within 250 milliseconds. To solution this, we looked at two different approaches. One was co-localization, which is what I talked about in the previous slide, where to avoid any kind of network latency, any time we are trying to create a TLS connection, it goes back and forth a few times. That takes time to establish. To avoid that latency, we made sure that we build our solutions within the local regions. And we also architected our solution based off Amazon Memory DB with Valy. Now this is a a very interesting solution because not only does Amazon, a memory DB gives us faster reads, but it also gives us durability of data, which is the right combination to be used in, in a financial solution, um, which, Basically gives us the faster read time of a cache and durability of a database. Arvinda will talk in detail about this. We also used a lot of TLS connection pooling mechanisms to help. Uh, be able to give us the 250 milliseconds, um, latency that Narvinda will be covering as well. Um, the last business requirement that I, I want to talk about is around resiliency and high availability. Um, one of the principles for our trust are high availability, resilience, and uptime. Within visa, there is a categorization within tiers for our applications. Anything that is transactional will be considered a tier RT or a tier zero. And to be able to consider a tier RT or a tier 01 needs to be able to fulfill at least 99.99% of uptime. Think about this uptime that within an entire year, application can only go down. It cannot go down for more than 52 minutes, 35 seconds. Beyond that, we will be breaking our SLAs. There's another important requirement that we had to cater to, um, within visa, we have our internal, Uh, availability disaster recovery requirements which we call, um, ITDR and to cater to those ITDRs we have to build redundancy in, in at least dual regions and within that redundancy, the regions have to be more than 60 miles apart. So to build our solution to cater to our tier zero uptime requirements of 99.99, our solution was to build our. Um, uh, application within two regions and give them redundancy both in London and Ireland region with a multi AZ setup. We are using 3 AZs, uh, availability zones, uh, within AWS setup. So this was all about various requirements that we had to build around. I'll spend some time talking about the functional architecture of Visa Protect account to account, and then Arvinda will go into the details um of the, Architecture So Visa Protect Account Account is a scoring engine, uh, which basically takes a request and processes it, runs it through our models and gives a score back, um, to the end customers. To enable this, we have two important components. We have the real-time component which has. scoring API as well as the model infferencing API. Here the scoring API talks to our external clients. It does all the data processing. It does all the data acceptance, security checks, everything, and then decides to through a decision service decides to call the right model for its inferencing. Within moral influencing, we have aggregation service, we have orchestration service, we have long-term profiles, um, that run and are able to provide a, a score, a risk score for a transaction. Along with the real-time system, which is the core of our platform, we also have a near real-time offline system which uses a scoring API as well as a feature engineering platform. Scoring API is an important uh offline system which accepts uh daily files where daily status files, it accepts the, it has data consumers that accepts data from our clients. It also. So anytime you're on boarding a new client, this is another system that allows us to get historical data of the clients and we are able to build our features, uh, in, um, uh, based off that, that data. Our feature engineering components has both uh long time profile generation and short time profile generation, um, features. Along with your billing, reporting, and all other, uh, uh, components that comprise, um, a system. So with that, I'll hand it over to Arvinda, who'll be going deep on the architecture. Thanks, Emma. Hello, everyone. My name is Arvinder Nagala. I'm a solution architect at Visa. Next, I'll walk through the technical architecture for the functional components that Hema just described. Later, we will dive deep into some of the AWS technologies we used in our architecture. Let's start by looking closely at our real-time scoring API workload. This is the REST API that our clients use to get their score from our model. Client requests land in our DMZ VPC. Think of this as a secure perimeter facing the internet. Where each incoming packet is inspected by Amazon Network firewall, and only the traffic from whitelisted IP addresses is allowed inside. The, then the request lands at our load balancer where we authenticate the request using mutual TLS. Successfully authenticated request travel via transit gateway, VPC endpoints, and then land at our Amazon API gateway, which is hosted in privatepo VPC. API gateway enforces usage plans, applies throttling per client, and then sends it to the AOA gateway application running inside the n enclaves via a network load balancer. A gateway application receives the request, terminates the TLS connection inside the enclave, and then performs some validations, and it also does the secondary authentication using token validation. And then it encrypts the sensitive PII data using KMS encryption. Then it converts the rest request into GRPC protocol, and then it sends it to our Downstream Decision Service application. The Decision Service runs on top of Amazon EKS. It transforms the incoming request and then it does the duplicate check by doing lookups into Amazon Elastic cash for VALLE. It also does the enrichment of the incoming payload by doing some lookups into the Elastic cash as well. Then it calls our moral influencing platform. Our model influencing platform runs on Ray luster, which is deployed on top of Amazon EKS. Model inferencing mainly consists of three different services. Um, LT profile service is fetching the long-term profile features from Amazon Memory DB. aggregation service performs real-time aggregations by fetching short-term profile features from the memory DB as well. Once the features are ready, um, AIS runs the model and generates the risk score. The score is then sent back to the decision service where we have a rule engine applying the rules and then transforming the response by decisioning. Then it sends that response back to the gateway where we convert the GRPC response to rest, and then that response is sent back to the client. So that completes the real-time flow. Both the model influencing service and the decisions and the gateway service are writing writing the requests and responses to Amazon managed streaming for Kafka. Which is fed to our downstream uh applications such as reporting, billing, and, um, near real-time feature engineering processes. So, uh, we chose Elasticash for Valy and memory DB with VALKI engine because they provide about 20%, uh, uh, they are about 20% cheaper than RADIS, and they provide the same functionality and performance as well. Um, our applications didn't need to change because, uh, the REDDI client libraries work as is with the VALKy engine. Uh, that's on the real-time side. Next, let's take a look at our offline flow pipeline. This flow enables our clients to upload daily files such as entitlements, fraud data, and um pan to bank account data and transaction status files. This is also used by our clients to upload one-time uh historical transactional data, which gets used for model training and, um, and baseline feature generation. Clients connect to AWS Transfer Family Service running again inside the DMZ VPC which is facing the internet, and um they upload the PGP encrypted files using SFTP protocol. Uh, the files land in an S3 bucket from where we have a scan process, picking up these files and running AWS guard duty, scanning for malware detection. The successfully scanned files get moved to another S3 bucket, and an SNS notification is also generated. This SMS notification is consumed by our data ingestion process, which is also running inside the nitro enclaves. It consumes the notification and then it picks up the file that was successfully scanned and it pulls that file into the nitro enclave memory and it decrypts the file inside the nitro enclave. Then it parses the file and it performs various checks, validations. And then it starts encrypting the PII data which is in each record. The encrypted records are written to Amazon managed streaming for Kafka. And the filele metadata is also stored in Amazon Aurora DB for reporting purposes. The KafkaA data is consumed by various applications. We use Amazon Data fire hose for consuming the data from Kafka and writing into an S3 bucket. From there we have EMR jobs picking up these files and then aggregating and then writing into a separate S3 bucket. This S3 data gets used by our downstream feature engineering and model validation and model retraining purposes. Uh, we also have another consumer application which, uh, reads from topics such as entitlement and branch code data, and then writing that data into Amazon Elasticash for VALLKI. This VALKI data is used by our real-time scoring API workload. Uh, the same data is, is written to Amazon Aurora DB for long-term storage. This ensures that both our real-time pipeline and offline pipeline. Use secure, validated, and enriched data, share, share that data across the both flows. Next, let's dive deeper into AWS nitro enclaves, the Security backbone for our sensitive data processing. So the nitro enclaves are the secure, hardened, and isolated environments running inside the EC2 instances. They have no external connectivity, no SSH access, and no persistent storage. Not even the root user from parent EC2 can log into the enclave. Only the parent EC2 can connect to the enclave using a secure local channel via VSOC protocol. These are ideal for processing sensitive data such as bank account numbers, credit card numbers, and customer data. Our eight-way gateway application runs inside the nitro enclave. This is the technical design for our application. During the startup, uh, uh, uh, the application connects to Secrets Manager and KMS proxy uh via proxies and fetches the envelope encrypted data encryption key into the enclave. Then it decrypts the encrypted key using KMS customer managed key, and then it caches that key in the enclave memory. When an input scoring request arrives, the NLB sends it to the gateway proxy running on the parent EC2. From the proxy converts the incoming TCP protocol request into VSOC protocol and sends it into the enclave. Inside the enclave, A2A gateway application receives the request, terminates the TLS connection, and then, um, it validates the payload, and then, um, it starts encrypting the plain text PII data. Using the cashed deck that was cached during the startup. Then it sends it converts the request from rest to GRPC and then it sends that request to downstream Decision Service application, proxying via VSA channel again. This ensures that the sensitive data processing is entirely happening inside the nitro enclave without exposing it to the memory in memory exposure threats that Hama talked about earlier. These are some of the best practices we follow for our application deployment inside the nitro enclaves. The enclave resource allocation is done using this allocator.ML file. We declare the VCPUs and memory inside this YAML configuration, and the best practice to do this is using user data scripts because nitro enclaves requires contiguous huge pages memory. If the allocation or reservation is not done during the startup, the memory can get fragmented and it may not be available for enclave to reserve. So, in order to avoid that, we we do the allocation with the user data scripts. Next is careful tuning of proxies is, is a must, uh, because this provides better throughput. Otherwise, uh, we may not get the throughput that we require. So we use SOCAT proxies for converting the TCP to VSOC, and vice versa. And SOCAT has options for concurrent connections such as reuse address and fork. So by carefully tuning them, we were able to achieve better throughput. Next is health check endpoints should check the health of the application as well as proxies. So in order to make sure that the instance is healthy before sending the traffic to the, to a particular instance. As we saw earlier, the application inside the enclave fetches the encryption keys during the startup and then caches them into the memory so that it avoids latency in getting the keys for every request. So by using all these optimizations, we were able to achieve around 150 to 200 TPS throughput with a single M5A 2X large, easy to instance. So performance is just as critical as security for our application. Next, let's take a look at how we optimize for latency. We use TLS 1.3 and HTTP 2.0 throughout our stack, um, for faster handshakes because TLS 1.3 is one less round trip and uh HTTP 2.0 provides multiplex connections. Um, both, I mean, AWS load balancers do support both of these, and even our applications support them. Uh, we recommend our clients also to use TLS 1.3 and HTTP 2.0, uh, because it provides better end to end latency and throughput. And, um, This is also highly secure as well. On the Amazon API gateway for the increased traffic, we use VPC endpoints, and for the backend integration with our 828 gateway application, we use VPC link. So this ensures that the traffic stays entirely on the Amazon backbone network, giving us consistent low latency performance. So on the NLB for client routing policy, AZ affinity setting is enabled. This means that the client, uh, the traffic stays within the AZ where client is making the request from. This provides better latency as well. And for the load balancer target selection policy, uh, cross zone load balancing is enabled in order to provide the high availability. And the application gateway which is running inside the enclave, as we saw earlier, the secrets and keys are fetched during the startup and cached in the memory. And we do have background threads refreshing these keys as and when needed. This avoids repeated fetch latencies. Next, let's take a look at how we fine tune EKS for both performance and resiliency. So we use AWS load balancer controller to provision and manage load balancers. So, uh, load balancer controller provisions ALB for Kubernetis ingress resource and NLB for Kubernetis uh load balancer resource. NLB can be provisioned with either instance or IP target type. And um so the the annotation that we see here, AWS load balancer NLB target type is set to IP, which means the pods get IP addresses directly assigned from Amazon VPC subnets, avoiding any extra network hubs and providing better performance. Uh, Next, let's look at part scheduling. There are mainly 3 ways we can control the part scheduling node affinity. This addresses whether you want to place a particular part on a node with a specific characteristic, for example, a hardware characteristic like enclave enablement. So for example, we, we are now migrating our gateway gateway application from EC2 to EKS. And this application part needs to run on the nodes where enclaves are enabled, so this is done by using the node affinity. Pod affinity and pod anti-affinity when the placement of a pod depends on the location of other pods. Pod affinity can be used to co-locate synergistic workloads. For example, if we have a web server and a corresponding caching pod, they can be co-located together using pod affinity. Pod anti-affinity, this distributes the pods across a failure domain. Providing high availability. For example, if you declare a service with 3 replicas, you don't want all of them to stay within the same AZ. So this can be controlled by using anti-affinity. Next is topology spread constraints. When you want to ensure that the replicas from a pod for a single service are evenly balanced across the different failure domains, that's when we use the topology spread constraints. Next, we will see how we can define some of these. This is a sample manifest definition for topology spread. The section topology spread constraints. This ensures that the replicas are evenly distributed across the failure domain. Max Q is set to 1. This is a crucial setting. Um, this dictates that the number of pods for a given service in a given AZ will not differ by more than 1 with the number of pods in other AZs. And the topology key is set to zone, which means that the failure domain that we talked about earlier is at availability zone level. Next is topology aware routing. This um keeps the network traffic within the AG where it originated. Uh, this helps with the reliability, performance and costs because cross-zone traffic is charged. So this is a sample definition for topology aware routing. Uh, we have the service definition under which, uh, we have the metadata section, and we are setting the topology mode annotation with auto. This auto mode tells Kubertis to keep the network traffic within the same AZ where it originated. Uh, it also has a smart default. If there are, there are no healthy parts available within that AZ, it will automatically reroute to the available parts in other AZs. This feature works by adding hints to endpoint slices. In order for the routing algorithm to work effectively, it is recommended that we have at least 3 replicas running in a given AZ. For example, if we have a service with 9 replicas defined within 3 AZs, this definition will make sure that there are 3 parts per each AZ. So combining all these optimizations together and using Amazon Elastic cash for VALLI and memory DB in our real-time scoring API workload, we were able to achieve our latency OLAF of 250 milliseconds. In fact, our P99.5 latencies are much better, about 40% lower than that. And we were able to successfully test our surveys with bursts of up to 1000 TPS. Next, let's take a look at our multi-region active active architecture, which is essential for a tier zero application such as VPA2A for visas ITDR requirements. So we have cloud flare AMa DNS routing traffic to the nearest region. And in each region we have both compute and data services deployed across 3 AZ. Uh, load balancer is distributing the traffic across these 3 AZs. We have the application layer writing to Elasticache in local as well as remote region for our real-time duplicate checks. And, um, Amazon managed streaming for Kafka. MSK data is replicated using MSK replicator. This is used by our, uh, near real-time feature engineering flows. S3 cross region replication is enabled for our offline flow pipe buckets. And we use Amazon Aurora DB in our file processing pipelines. Uh, and Aurora DB replication is enabled from primary to primary to secondary. Using AWS MSK replicator and S3 cross region replication and Aurora Global DB replication reduces the operational complexity on our side because AWS takes care of the replication overhead. So this gives us high availability, disaster resilience, and consistent performance globally. That's about as deep as we'll go on the technical side today. Next, I invite Hema back to the stage to wrap up. Thank you very much. Thank you, Arvinder, for the detailed architecture overview. Um, I would spend a few minutes just to talk about the key takeaways of our partnership with AWS team and, and what did we learn as part of building, um, the solution, uh, visa Protect account to account. Um This is for the first time actually, we took something internally and, and tried to take it to cloud. Visa has spent a lot of time and effort to build its hybrid cloud solution, and our application was one of the very first applications that decided to go and use that hybrid cloud, um, solution and, uh, host an application on that hybrid cloud solution. As part of this journey, what we built, and Arvindar did talk about that, was a solution that caters to visas, stringent operational and cybersecurity guidance is something that we follow very religiously on our on-prem system, but to see how that is going to work on a hybrid cloud and to take that with a 1000 TPS with 99.99 availability with a fraction of a second latency. This, I, we consider as a blueprint for, uh, our application, and we were able to take this, it's fully functional. Our clients are using the scoring service today. In our UK region now the next part of this uh journey was to see how do we use this blueprint and take it to various other regions, um, as I talked about Visa being a global company, a lot of our solutions are built for our global markets to cater to their local requirements we. Use our using this blueprint that we have built and kind of uh on top of that using the IAC scripts and all to go faster to other regions with slight tweaks that are needed for those regions as an example, I'll talk about. Um, visas, uh, foray into the South American markets with Brazil being one of our clients where we are taking this solution to Sao Paulo region and opening it up, uh, for our markets, uh, the visa protect account to account solution to help protect, um, and prevent fraud, um, account to account fraud there. The third important part that I want to talk about is our partnership with the AWS team, Surajan team. We've worked for the past 2.5 years in trying to build this solution with various other partners within Visa, with our operations team, with our cybersecurity team, and it has been a great partnership. We have all evolved, we have learned with experiences, and we have leveraged a lot of different AWS patterns that has helped us scale um our services and our solution. With that, I would like to end, uh, this presentation for here. I would like to thank you all for coming here, um, and listening to us. And if you guys have any questions, we are happy to have side conversations on the site, but thank you all for your time. I would love for all of you to fill in the survey in the mobile apps. Thank you.
