---
video_id: U3xO5QbzYQY
video_url: https://www.youtube.com/watch?v=U3xO5QbzYQY
title: AWS re:Invent 2025 - Build Advanced Search with Vector, Hybrid, and AI Techniques (ANT314)
author: AWS Events
published_date: 2025-12-02
length_minutes: 61.93
views: 475
description: "Amazon OpenSearch Service is at the heart of how Artificial Intelligence is transforming the way we store, search, and use data. Learn how businesses are using AI to power their search-backed applications, achieving significant improvements in search accuracy and performance with OpenSearch Service’s advanced vector capabilities. Learn how Recruit uses OpenSearch Service to find available tables for \\"Hot Pepper Gourmet\\" diners. Dive deep into OpenSearch’s vector capabilities, and its role in bui..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Um, I am John Handler. I'm a senior principal solutions architect with AWS. I'm joined with, uh, Bobby Mohammed, who's a product manager for Open Search Service, and Ryoke Sudo, who is a, uh, ML engineer at Recruit. Um, so we'll be giving you a talk today in sort of three parts. Uh, I'll take the first part, then, uh, Pseudoan will take the second part, and then Bobby will take the third part. So, who here has actually bought a product on Amazon.com? Like, everybody, right? But who here has done that without using the search engine? Yeah, like nobody, right? So when you go to Amazon.com, you have an intention, you have a goal, you want to buy something, and the search engine is how you express to Amazon.com that you want it. If you go to Google, you have something you want to know, you type some words in and Google hopefully comes back with some helpful information that solves the question that you have. If you're building an application, all kinds of different use cases across things apart from e-commerce, document stores, all of these things, we. Mediate between the our, our intentions, our desires, and what the application knows through search. And that has been traditionally until recently that has meant. That we've used words to communicate to the search engine what it is our intentions are. Now we're obviously we have huge step change in the world that has changed how we interact. Now we expect more from our, our information retrieval systems. We expect them to be able to interpret and understand what we're saying. So we're gonna talk a lot about that today. The problem with uh search engines, or one problem with search engines, and you probably all have had this experience too, you say like socks for children, and you get random stuff, some of it's socks, some of it's children's toys. And there's a mismatch between what you expressed and what you got. Really what's happening is, until these new technologies, search engines have keyed on words and have done word to word matching. So here's an example, you know, I said like, alright, I want a good TV to watch sports. Now, I know that that means fast refresh, very bright, very sharp picture, but. Those are not things that I actually typed into the search box, and even if I did, perhaps the people who were describing their TVs would not have actually used those exact words, right? And you can see here there's a bunch of search results, um, mostly what it keys on is the word TVs, right? So, with the newer technologies, we're gonna talk about how we get to a better way to do retrieval. Um, who is familiar with OpenSearch? Uh, anybody? Oh, great, a lot of familiarity in the audience. So OpenSearch, community driven, open source suite for AI powered search and log analytics. Uh, the project itself has open governance under the Linux Foundation. Uh, we have some of our premier members AWS, IBM, SAP, and Uber, as well as general members of the foundation. It is open governance, it is a vibrant community, and it is going gangbusters. And of course we have our Amazon Open Search Service, so if you want to run OpenSearch in the AWS cloud, Amazon Open Search Service, uh, first of all, makes it easy for you to deploy and operate OpenSearch in the AWS Cloud. Second of all, we have performance gains, uh, both in the open source project as well as in the managed service. Uh, 6X from 1.3, I think this slide is old, 2.19. Uh, we have more performance gains in 3, which is live now. We also provide a number of different options for cost efficiency, knobs and ways you can tune, either for uh vectors or for log analytics, and we're gonna be talking about those later. We also have a number of integrations that make it easy to move your data from other AWS services to open search service. I'm not gonna hit these too hard. Open source itself is a distributed database system. It runs on a set of nodes. Uh, data nodes provide the storage and request processing. Uh, we have our cluster manager nodes, they orchestrate the cluster. Uh, we have our coordinator nodes, they coordinate request processing, and we have ultra warm nodes that make it, uh, that let you tear out older, especially log data. We also have a serviless offering, uh, serviless offering, you just create a collection, and the collection is a collection of indices. You use REST APIs to send your data. In all cases, uh, we front the cluster with some kind of, uh, load balancer and you send your requests, uh, to OpenSearch. OpenSearch is a REST API, so it's all Jason-based, uh, very, very familiar. So as I was alluding to, uh, you know, if you're building a search application, chances are you started with lexical search. You maybe put some data into OpenSearch, and you're using the lexical query with BM 25 scoring to get results for that. But with all the new technologies we, we have all this demand and drive to include semantic search. And semantic search up levels and uh large language models, embedding models are able to pull out meaning from strings of text. So, we have a lot of this like, OK, well, I, I have this lexical search, it works pretty well. I have this semantic search, it works pretty well, um, but ultimately, you kind of, we see the world coming to this kind of hybrid model, where you wanna have the benefits of x. the benefits of semantic and blend them somehow. So I'm gonna take the first part of the talk to talk about this, uh, and then the newest thing is agentic. Uh, with Agentic you have an even better way of, uh, building search results from queries. All right, so let's talk about the journey from lexical to hybrid. Lexical search, uh, again, is very good. If I know I want the uh Samsung 64 inch OLED blah blah blah, and I type those words, I can match those words and I'll get a search result that's a good result. So it's good for very specific queries, but it don't use, or it's bad for, more abstract queries like, I want a TV for watching sports. When you use OpenSearch, we start with the search document, so the, the lexical sort of journey begins with the search document. As a design point, uh, when you're using OpenSearch, first you figure out what you want to retrieve, then that's what you put into OpenSearch, right? So this search document, it's a bunch of Jason. This is an example from the Amazon product Q&A public data set. It's a question. It has some information about the product, it has answers for the question. Uh, all of this goes to OpenSearch. OpenSearch creates indices for each of these adjacent fields, enables fielding fielded matching for that content. So, when we do lexical search, the first thing we do is we take blocks of text and process them according to language-aware analyzers, right? So, um, if you look at the number 22 over there, the tickle, uh, 98 inch Q 65 Q LED 4K blah blah blah, you can see in there, there are some kind of weird looking words like larg and Uh, Dolby with an I, all of that is language processing, and the purpose of that is to make it easier for queries to match those words, right? Runs, run, running. It's all the same word. A query that says any of those should match that. So there's a stemming that goes on. We remove common terms, those are called stop words, we, we add synonyms to enable better matching. Now the second one there is a TV frame, actually, not a TV. And this query large screen televisions ought to match the 1st 1 better than the 2nd 1, right? So we, we have to analyze the query as well. We did, we changed the text that we indexed, so we analyze uh the query as well. With an open search core data structure is an inverted index. Inverted index maps every term that's been seen in a field to the documents that contain that term. That's an efficient way to set up for retrieval, so when we have the terms large screen and television, we just go look up in the index, those terms, very fast, and we have the set of posting lists, that's the documents that contain them. Uh, we apply the query logic, in this case an OR, so we take all of the documents with large screen and television. And we or them together and we get 13, 1218, etc. The next thing we wanna do is score them. And the, the basis, one of the core features of a search engine is to provide a relevance value that helps sort the possible search results into the desired search results. And this is the, the kind of core of what search engines do. So how do we score? Well, the first thing we do is we look at all the words. How common are those words? Something like uh television, let's say, is less common across all of the documents than, say, large. Large is going to be a very common word. So, television gets a higher score than large. Um, we then multiply by how many times it matched in each of these two example documents, right? We multiply together, and that gives us a result. In this case, we see that indeed document 22, surprise, surprise, uh matches a little bit better than document 38, right? And so this is the desired result. This is what lexical search is. This is all lexical search does. Like I said, we're matching words and we're scoring based on the value of those words. With semantic search, we are using models to get at the meaning of strings of words, right? So, if you think about it, like a word itself is already carrying meaning, so matching word to word, we already have some small window of meaning. Now we've expanded that window, so that when I say large screen television for sports viewing, the. The fact that sports viewing and large screen television are close together enables us to bring in that large screenness to the the sports television into the large screenness, right? It, it lets us, it lets the model bring that together. It's not good at specific queries though. If I say like this tickle 98 inch Q65, blah blah blah, um, I don't want it to try to understand what I mean, I just wanna put those words in and get an exact match, right? But it is really good based on, for contextual kind of information, and it's good for multiple modalities, for lang multiple languages, uh, all of these cases where meaning is the, is what you want to match. So, when we start with our document, our document has a little bit of a journey, right? We have some portion of the document that is texturally representing what that document is. Uh, in this case, you can't see it, it's tiny, it's the product description. I take that out as a chunk. That chunk I then send to Bedrock, or Sagemaker or some other uh model technology to generate a vector embedding. I add that vector embedding back into the document, so you can see there, I have it in pink, uh, the vector embedding is there. I then ingest that document into OpenSearch service. Uh, in this case, I was using uh open open search ingestion. So how do we decide which is a better document? Well, vector embeddings, they're, they're just big areas of numbers, they're vectors. They create a multi-dimensional space. And when I ask a question like what TVs are good for watching sports, I create an embedding for that as well, and I, that will live in the same space as all of the products and documents that I have. So I project that into the space, I then find the nearest neighbors. Closeness is the measure of similarity for vector search. So closeness is a kind of, uh, nebulous concept, but we can put some definition around it. So let's say I have these three vectors. The first thing we can do is do the Euclidean distance. Um, the darker one on the left top is the query vector, the other two are document vectors. So I can take a Cartesian distance between those and I can say, OK, well, this one is 3 times farther away than that one, so that one's closer. Right? Uh, we also can use cosine, these are common similarity measures. We can say, OK, uh, here's my 3 vectors again, the query vectors in the middle. Uh, and we can look at the angles between the vectors to decide which one is actually closer, and the one on the bottom, twice as far away in cosine world. So I'm gonna introduce you to the neural plug-in, if you don't know about the neural plug-in. Uh, OpenSoch's neural plug-in provides a bunch of different things that make all of this semantic processing easier. The first of those is there is a connector library that enables you to connect to embedding models that are external, also large language generating models. Uh, so we have a connector framework. And then we can define, or you can define in OpenSearch, pipelines, so that as you ingest your documents into OpenSearch, through the neural plug-in, OpenSearch calls out to the embedding model, gets the embedding and puts it on the document. This simplifies the ingestion. There are lots of ways to do ingestion, we'll talk about other ones, but this is a simple way, uh, if you wanna get started quick and test. Similarly, on the search side, we provide a number of different processors that enable you to create a query embedding with a neural query and you send a text and a model ID. OpenSearch does the embedding for you. When you are, so to engage this, you're gonna use an ingest pipeline, and that's a simple put ingest pipeline blah. You can see there's a field map in there. It says, for this field, create an embedding and put it in that field. Super simple to use. You pass a model ID. Model ID is the reference to the connector that tells open source which uh external model host to call. You put that into your mapping. Now open search behind the scenes is kind of databasey in that it has a schema for every field. The schema tells it how to process that field, like if it's text, cut it up, apply language stuff, all of that. Um, you can, once you've defined the pipeline, you just set that as the pipeline for your mapping, and OpenSearch automatically, again, calls out and creates vector embeddings on the way in. And finally we have our query, so our neural query, uh, here, again, we just say which field is the embedding field, and we give it the query text and the model ID. It does all the work to uh do that query. I'll mention really quickly, uh, we do have a batch inference that we recently launched that enables you to use open search ingestion, that's a feature of the open search service, uh, to call out to Bedrock batch inference, uh, and embed all the stuff as it's going in. I'll also mention we have something called automatic semantic enrichment. This works with a different kind of uh embedding model, a sparse model, and it's completely automatic. You simply set the field type, and OpenSearch does the work of uh creating the sparse uh vectors for that field. All right, so coming to hybrid. So we saw problems with semantic, we saw problems with lexical, and so it's natural to say, sometimes I have really exact queries, sometimes I have really semantic or meaning-based queries, I wanna do the best of both of them. It's not, you know, you're not gonna look at it for everything, um, but it's really good for most search use cases, and in some ways this helps you maintain a stronger relationship between the text and the uh meaning of the text. In open search, uh, it's pretty simple to use hybrid query like this, you have the query itself, and then you have 1 to 5, uh, query clauses. Each of those clauses then runs in parallel. Uh, in this example, we have a neural query and a sparse query running both of those, that could be a lexical query as well. Running both of those, and then we need to blend the scores. So how do we blend the scores? Well, first we have to create a search pipeline. The search pipeline tells OpenSearch how to blend those scores together. This example is using a normalization processor that's doing a numeric, um, normalization of the scores, and then, uh, comparing them or blending them that way. So, how that works uh in practice is, let's say I have query one, and those are the scores on the left. And query two, query one is a lexical lexical query. So I got 412 and 396 and all of that. And then query two is a neural query, and I'm using cosine similarity, so my score range is 0 to 1. Clearly I can't just put those together. So with numerical, what I do is I put them in a normal I normalize them to the same scale, and then I average uh to get the. Some of the, to get the result, right? And so that gives me a final ranking like that. The other way that you can do this is called reciprocal rank fusion. Reciprocal rank fusion looks at the slot that each result is in, and then gives a weight for closer to the top, uh, descending. And then combines based on the weights for the query for the result position, right? So with RRF you actually get uh some benefits if you have outliers in terms of the scores, they don't affect as much, it's a much more stable way to go about this. So Kind of wrapping up this little section. We have some uh standardized cor corpora here. So NF corpus, Ar Arguana, FICA, Trek, COVID, Pydo, and Quora. You can see the lexical score. This is a NDCG at 10. So this is the normalized discounted cumulative gain, and what that means is the higher up you are in the results, uh, compared with the ideal set of results, you get more points. So it's a scale of, um, it's a scale of percentages, it's not a scale, um. You get better as you more closely match the ideal search results. OK, so we have our lexical scores there, uh, hybrid with numeric combination, hybrid with RRF, and then a bunch of percent differences. The thing about these is, it is super hard to even move 0.1% on these standard measures. So if we look at something like the um Quora or FICA. Uh, workloads, those are both question and answer workloads, by the way, where we get up to like 12 or 13% improvement. With RRF, that's a huge result. So we're seeing a lot of benefit of doing hybrid, and of course some of them are a little bit worse, most of them are better, right? Um, so that's what I got for you and just sort of seeding the thought about lexical, semantic, and hybrid, and how to use each of those and what the benefits are of picking usually hybrid. I'm gonna ask, uh, Pseudosson to come up and talk about how they do that at recruit. Thanks. OK, so hi everyone. I'm Ross Castillo. I'm a machine learning engineer at Redwood. Uh, this is my first time at Green Band, so yeah, I'm very excited and a little nervous, but I'm OK, yeah, so I will enjoy the sessions. So today, I'm gonna talk about our hybrid search systems. Yeah. So why we need hybrid search and how to build it. Let's get started. OK, and this slide is one of the most important slides today. Yeah. So our legal team asked me to include, yeah. So the key takeaway is we are good guys, yeah. No personally identifiable information was collected and stored or used for training, yeah. So our chicken teams are happy now, so let's give them next, OK? So this is our company, Recruit. We operate many kinds of fields from jobs to travel and beauty, yeah, anymore. You might know our global brand like Indeed and Glosso, yeah, they are part of our company. And today, I'll focus on one of them, restaurant searcher in Japan, Hot Pepper Agreement. This is the Hope Agreement, one of Japan's largest restaurants, search and booking platforms. We are #1 in both the numbers of reservable restaurants and online reservation users. We handle hundreds of thousands of reservations every day, and we operate also tens of millions of users, uh, monthly, yeah. So most of the users starting from search box at the top. Improving such coding is key to our success. Let me walk through our such challenges. Our original search systems relied heavily on lexical lexical search, you might call traditional keyword matching systems. For a long time, it had great demand, but it had obvious limits. For example, imagine user wants sushi restaurants. Yeah, that is great. Um, if they have it perfectly, lexical search match correctly. But what if, what if they make a typo, like sushi restaurants? Simple lexical search can't match sushi to sushi. So lexical search is fragile agent against typos. And these problems get way worse in language like Japanese, which has no spaces. To help you imagine what that's like, I'll just remove spaces from query. In quick query sushi restaurants, our segmentation process correctly splits query into sushi and restaurants. That's great. But at one typo, sushi restaurants, our segmentation process breaks, such or EA, yeah, uh, word is wrong and number of words is also wrong. So lexical search is fragile against typos, especially in language like Japanese without spaces. You might think, yeah, hey, let's use Betasearch. It's mining and it's handle tables better, yeah, you're right, partly. I will share another scenario. Here is user query Suhihiro, yeah. And there are two candidates, Sushi Hero on strip and Sushi Magic, and we are using state, state of the art MNE model. Yeah, you might know, um, starting with G, yeah, that's a very strong large language model, yeah. Uh, you might think this is a very easy problem and better to success. Let's look at the result. Number one is so magic. Why? Even we use state of the art large language model? Because vector search focus on meanings, not keywords. So, The word magic has context-less, but word on the strip has strong location context. So strongation context ironically pushes correct answer further away rather than completely completely different restaurant names, so that is the limits of vector search. This is a quick summary. Lexical search is precise, but struggle with typos and segmentation. And vector search is smiling and context-aware, but it can miss exactly match. What we want. We want the best of both worlds. That is why we need hybrid search, precision and smiling understanding. So let's get into the technical math of how we build it. First, Why did we choose OpenSearch? For 3, for 3 quick reasons. First, hybrid search capabilities, for tickets and a vector. It's easy to implement, so we can focus on improving vector search model. And second, it's fully managed on AWS. It's of development and operations, scalability and bluegreen deployments, backups and security, all are managed. It helped us. And third, it has a rich ecosystems. It has managed ETL like open source injections, and it has also language-specific plug-ins, yeah, it helps for uh Japanese people, yeah. Uh, we also considered other options. Uh, Bender E has rich features, but, uh, higher cost and less averse integrations, and the vendor B has a powerful, but yeah, it's complex to manage. And other vector DBs is great for similar search, but it's limited for text search. So for us, open search hit our open our switchboard, yeah, that's great. So let's next talk about our vector model. We use two tower model architecture, uh, one encodes for query and one fort restaurant information. Uh, it ran on our user logs and synthetic data we generated with large language model. Today, I'm not going to go deep into our architecture, but the key takeaway is fine-tuning really matters. For our cases, lightweight, uh fine-chain lightweight BT actually outperforms large language models. So I recommended, so if you're dealing with Domain-specific vocabulary like restaurant names. Don't just rely on over the shelf vector models. Fine tune them, it makes it difficult. OK, so next, after we get better model, our first step was try to use the built-in hybrid search systems. It is very simple. We just write multiple queries in crazy field. That's very easy. We don't need any changes architecture. And it's very easy to implement, so we can focus on improving vector models. And it gave us many business impacts, but we have realized that we could go even more further. To understand, uh, opportunity. Uh, let's look at our hybrid search, uh, building hybrid search systems. In building habit search systems, open search first take lexical and vector results in parallel and normalizes, and after that, merge them into single score before ranking. That is a problem. Because, yeah, when both scores are high and both score is low, uh that is OK, carry on. But uh vector score is high and less score low, in case that means typos or paraphrase, and lexical score is high and vector score is low, it might the case means their word or domain-spec vocabulary. So they represent completely different things. And which should be better more depends on userta. So to handle this problem, We realized that our ranker needed vector score and the lexical score separated. That is our key insight. To solve this, we build custom hybrid search. Uh, in our customer hybrid search, Our executor run Lexical search and vector search in parallel and get both scores separately and feed them into our ranker. So our ranker can use lexical search, lexical score, and vector scores separately and also incorporate the user logs and user intend other signals, yeah. That's great. It enables us to make way smarter decisions and adoptive ranking. It's a little bit more complex to implement, but it gave us up to twice times improvement in top one search accuracy, so totally worth it. This is our binary architecture. Our custom ranker moves us, moves out of Open search and running in HS container, and also our vector models running in HS container. This setup gave us more flexibility and easy to uh make scalability more, much easier. Yeah, that's great. Let's look at our business impacts. We tested several versions from original and hybrid and hybrid plus Ranker. Finally, after launching custom hybrid search, we achieved a 10% improvement in number of bookings via search and 90% reduction in their searches, and yeah, that's great. So our key takeaway is properly combining Leo and vector scores led to significant business impact. So to wrap up, what worked well? First, managed blue-green development is great. o-time downtime scaling gave us operational confidence, and also open source accelerate our experimentation, yeah. And finally, uh, hybrid search, seamless hybrid search in thisions is great. It enable us to launch quickly our first steps. And next, uh, what's next? First, I would love for more, I love for support, uh, building hybridstructure with, um, model-based fusion, yeah, today I described, yeah. That would depress our customer solutions entirely. And also we need multi-vector search exploration, yeah. Multi-vector, multi-vector search uh improve not only the money, but also accuracy, so that is a key feature for us. And I was going to ask for support open source Oo 3.3 support, uh, but, uh, it was released, uh, last week, so yeah, so thanks for the best, yeah, so yeah. So, uh, that's for my presentation, and thank you for listening for me. Uh, so I'll hand it over to Bobby, yes. Hm. All right. Thank you, Surasan. That was excellent demonstration of uh the value of hybrid search. Thanks for sharing with us all the way coming from Japan. OK. Thanks. Uh, before I get started, quick show of hands, how many of you are doing Asiantic search or agentic retrieval in your applications now, experimenting or trying out? Can I see a quick show of hands? That's, that's a good number. And how about people who are deploying in production? Anybody deployed in production, agent agents or agenting workloads? That's good. That's more than I thought. 2 people. That's good. So, uh, so we got, uh, we're gonna cover agent search. Uh, basically John introduced the traditional search and advanced search, how they can be used to improve the relevance. I'm gonna cover, uh, agentic search, how open search is, uh, implementing many features to help with this kind of emerging use cases in the agentic AI applications. Uh, so If you, before we talk about agentic search, I just quickly want to kind of understand how agentic workloads differ from traditional workloads. If you look at the traditional workloads, they are meant for humans and single query or short queries for millisecond kind of response times. On the other hand, the aging workloads are kind of executing multiple dense queries expecting real-time responses. And for that to happen, they have to use the Asiatic reasoning loop that is dynamic and iterative in nature. And the agentic reasoning loop has many components, as you see in the diagram there. It uses the LLMs, which is the brain behind this agentic reasoning loop that leverages the tools and the. Data sources to enrich the context and then it retrieves the information and and kind of brings some results and this this process keeps iterating until it finds the right uh it meets the goals for that particular job and while iterating it keeps updating the index so that the data is fresh in the in the index and keep updating the all the actions accordingly. So for for the agent to run these iterations and extract the right context, retrieval is the foundational layer to make that happen and for the retrieval, uh, if you don't have a good retrieval, the amount of time you spend in the iterations and the. The context that you have will be not suitable and the relevance will be very poor, so force fitting the traditional work search methods onto agency workloads will lead to poor relevance and also you're kind of wasting a lot of money running many iterations and trying to retrieve unnecessary context, so. How the industry is approaching to enrich this context, there are multiple approaches people are taking. The first one is LLMs. Why can't we just use LLMs to enrich this context and enhance the relevance uh that that you can get with the search results. So if you really look at the last 3 to 5 years or the GII or the Asian tech, the all these explosion in these use cases is primarily driven by the what's the innovations happening in the LLMs and They are getting smarter and smarter with each iteration of the LLMs, and the primary driver for them to make them smarter is in these agentic applications we want to make them model driven, and the models are smart enough to decompose the task and execute all the steps that you have seen in the gentic reasoning loop so that you don't depend on external tools or anything. But the problem is, uh, and, and, and the way they're doing all these LLM vendors is enhancing the context. If you really look at the last 1-2 years, the context windows have exploded. Uh, it used to be a couple of 100 of 1000 tokens. Context window size now you see in millions and basically you can hold a couple of pages of information with a few 1000 few 100 tokens. Now you can hold like thousands of pages, which is great. That means you can really enhance the context, hold a lot of information, but it has its own challenges, right? So, uh, if you really look at the uh agentic workloads, if you, if you have run the test cases, the. POC or the experimentation phase, mostly those are lightweight agents. You try with few agents. The context window that you see with these LLMs is sufficient to handle those kind of jobs. But if you really get into production workloads, these agents are getting into hundreds if not thousands of agents, and the context window that you have like millions is not enough. That means you need additional mechanisms to hold this information. So basically they cannot hold all the information. And the second one is the retrieval quality and the latency. That's a challenge. Just like humans, we cannot hold infinite information. After a certain point, I keep talking to you, you will stop listening to me. So, Just information warflows for the LLMs with large context window, it's the same problem. There is so much context as you try to retrieve the quality of that retrieval gets, uh, not doesn't meet the mark, and also you will have a lot of, you'll spend a lot of time to retrieve that information. So that's the second challenge. Just increasing the context window forever doesn't solve the problems. That's the point. And the third one is context management, right? So as you have this context between the. Uh, short term or the context window on the how you process this information, there has to be a mechanism to really move that information back and forth between this memory hierarchy, so LLMs cannot handle that and you need some external mechanism to do that. And that's why we have seen last few years, right? So there are like popular design patterns like rag and Asiatic search that really evolved and. Sorry, so, well, so if you really look at RAG, RAG is the most popular use case that I have seen last year, last 12 years at AWS. We get to work with hundreds of thousands of customers, and they really benefit if you have a single query, a static system, and. And we have built a rich set of capabilities within Open search vector database to enable these rag use cases and that's why we are the like a most recommended vector database for AWS and also the default vector database whenever you use Bedrock knowledge base. So because we have this rich set of capabilities and also we give the price performance to make this uh contest augmentation using rack design pattern. So, like I said, it's, it's a, it, it, the primary problem with rag is it can't reason, it's very static, and if, if it is a single query system, it's, it's works really well. So given these limitations, we kind of see. All these search use cases slowly moving towards agentic search. And agentic search is primarily a, it's a retrieval is driven by reasoning. That means it can leverage the agent reasoning loop. And whenever you do a dense or multi uh multiple vectors, it has the intelligence to decompose that complex query and kind of create subqueries if necessary DSL queries so that you can find some results that are very relevant for your search and. Another benefit with Agentic reasoning loop is it can really enrich the data by talking to a number of tools using the MCP protocol and also it can extract a lot of information from multiple data sources. So you can retrieve the enterprise data and if you need to retrieve external data, you can use the MCP to enrich this context. And as you enrich this context, you need to have the ability to store this information because the volumes will grow and you need a memory management, a memory hierarchy, and that's why for an agent reasoning loop if you have the ability to manage the short term or the working memory and bounce back and forth between the long term memory and make that judgment very smartly so that you have that information when you need it, when you don't need it, push it to the persistent long term memory. And and and that way you manage the valuable short term memory to do the job at hand. So that's why agent search has tremendous value when you have a a dense query system or when you're looking for a multi-ton system where you need to go after and think and solve these problems. That's why we kind of invested in this future, uh, with OpenSearch 3.3 and also with OpenSearch Manage service. You have the out of the box experience with agentic search and the way I just highlight some of the key capabilities here. There are more to it. The first one is a query planning. As you receive a a a agent receives a query, it could be a very complex query. So we've kind of built a query planning tool that can break down these complex queries and translate these national language queries into DSL queries and does the heavy lifting for you. And the second one is the AI powered intelligence. So the, the benefit of open search is it has a rich set of search capabilities. It has a rich set of quantization capabilities and others to save the cost and the performance for you. But it also creates burden for our customers because they have to think through which one is making sense for me, when to use what it brings some complexity with it. So with agenttic search we kind of added this layer of AI powered intelligence where based on the query type, based on the user intent, the agenttic search module can pick and choose which aspects of quantization, which aspects of search make sense so that they can apply at the right time for the right problem. So that is the intelligence layer that that's really useful even some of the things that John mentioned scoring, what kind of uh search technique you wanna use lexical, semantic, or hybrid. So underneath there are many filtering techniques. There's many things you can use with open search, but this layer does a lot of simplification for the user. And the third one is flexible and customizations, right? So with, with the ability to work with the MCP both with the internal data and also the external data. You can customize your search index with what kind of information you want to keep within your index, and what I mean by that is you can add tools, you can add data sources, you can customize the context that you want to deal with, you have that complete freedom to pick and choose, and you can go to hundreds or if not whatever makes your application useful. And the, and the last one is accuracy, especially when you have multi-ton applications. Agentic search really makes total sense and, and really gives you that additional benefits with relevance. So, To make agentic search work, there are many components that go into it, as you have seen, there is a memory. There is how do you work with MCP server, and there are like, can you deal with multimodal embeddings. So there are a bunch of capabilities we we brought with OpenSearch 3.3 and also in the manage service. Uh, that you can leverage to build this agentic search, uh, either out of the boss experience or you can put together an agenttic search solution by yourself. So I'll quickly walk through these four capabilities here and there are more, but I just want to quickly highlight so that you know what's there so that you can leverage for your application. The first one is the multi-modal embeddings. Why multi-modals? If you look at most of the organization data, we know it's, it's unstructured data. It's very fragmented, and it's very siloed. As if you take a medium sized losses organization, you'll have hundreds of developers building applications. Each application kind of brings you data in different modalities, and the ability to tap into those modalities is super critical. And if you, if the, the company or the organization cannot use these all modalities, basically there is so much hidden information that's missing out. So that's why if you see the LLM vendors, they, they invested quite a bit last one year. Most of the models that you see are multi-modal generative models or multi-modal embedding models. And to make this happen, we kind of worked with Amazon Nova model, uh. If you look at open search, we, we already kind of taken this path of multi-model last year itself. We we support text and image with the Titan embedding models. Now with Amazon Nova multi-modal embeddings, we support 6 modalities that we integrated natively with OpenSearch. It's a very intuitive, easy to easy, easy to use experience, one click. And, and, and the main thing with this NA multimodals is, is a unified embedding space. What I mean by that is When you have multiple modalities, you can create a vector embedding space for each modality, and you can store them separately. But with the NA multimodal, it unifies all these modalities. You, you kind of build one embedding space that's unified and also you bring together the semantic understanding of the relationships between these modalities. So what's happening is you have a rich context. Like humans, we kind of perceive in many ways audio, video, and also visually, and we kind of make sense of that. Now the context that you have is really enriched, and the second benefit is because you are saving in one embedding space instead of multiple embedding spaces, your vector storage is also smaller, and you're saving cost on the vector storage. So that's the benefit with this NA model and the integration. And you can quickly access to build these next-gen applications that are multi-modal. And the next one is MCP, right? So we heard this last one year, a whole industry kind of accepted or recognized this as the standard protocol to work with agenttic systems to access the external data or the tools. So to, to enable this open search also built MCP client and server that you can. Use uh based on your needs and also we built a lot of uh ready to use tools. We have about 20 tools that you can tap into based on your application needs like query planning, search index, and so forth, and we continue to build more tools so that you don't have to invest time to build these tools that you can work with MCP and the third one is authentication and credentials. The moment the MCP came into the industry, the biggest concern for many people is, hey, I'm giving access to my data uh to external system. How is my like authentication, how secure is the data, right? So we wanted to address that head on so we kind of provided a configurable credentials based on the session or the user. You can give permissions and manage these security boundary when you're using MCP server with OpenSearch. And on top of that, in the Agentic applications, it's it's a very early and there are many frameworks people are using. Everybody has their own preferences and winner is yet to be declared. So just to play with the ecosystem, we integrated our open search MCP server pretty much with all the mainstream vendors that you see here like AWS Strands, Crew and Langchain and so forth. And you can, if you go to use those frameworks, you can just quickly use the OpenSearch MCP server. So that's the MCP. And the third building block is the memory. Um, memory is very, very, uh, important. It's a backbone of intelligence. This is what it makes agents, uh, stateful. So if I want to highlight a few key benefits of having, uh, for, uh, for agencies, one is the knowledge retention and retrieval, right? So if you have, uh, a short term and long term memory, uh, hierarchy. And you have an ability to store vectors, like I mentioned before that are immediately required and process them for future usage, uh, that really gives you that flexibility to enhance the context. And the second one is the personalization. So, because you have this memory, you can store every user behavior and their reactions, and you can store per session or across sessions, and also you can store over a period of time. That really gives you the ability when I'm searching the the the system knows what are my preferences and gives me the more targeted results. So you can really hyper personalize if once you have the memory module integrated. And the third one is adaptive intelligence. What I mean by that is because you know preferences, you're very context aware. The agentic reasoning group that we discussed can deploy the right set of tools, right set of tasks to really solve the problem at hand, and based on the user information, it can change the tools as well. So you are highly context aware, uh, and you can, you can be adaptive with your actions and tasks. So we also built agentic memory with OpenSearch, uh, as a memory API. You can access that if you're using within OpenSearch, or you can access that, um, using MCP as a tool. So, the memory API has ability to store short-term memory, long-term memory, session memory. We also store all the event history that way you're not missing out any actions that were taken within the agenttic system. And the way it works is, uh, once the raw agent passes the raw messages, it captures them and it has the ability to run memory operations like add, delete, or whatnot and so that the information is up to date. So once you have the up to-date information, um, you can decide to represent either in the dense or sparse representations because it's a trade-off between quality and cost. When you go to dance, you get a lot of quality. But if you're trying to trade off to save the storage and the cost, you can go to sparse representation. And also, it's a very multi-tenant. You can configure the memory per user and really store all this information um based on your enterprise, how many users are there. You can give them certain specific access to the certain memory boundaries, OK. Uh, so, I, I strongly recommend give it a try. Uh, it really makes your Asiane application smarter and wiser. And the 4th building block we have built is the specialized agents, right? Um, so what it means is we are giving local agents within the open search environment. You don't need to go to some third party agency platform to build any applications. The benefit of giving this is you have access to open search data. You'll be within the open search cluster environment, whatever permissions you have, you, you inherit the. Uh, security, uh, and the, and the security boundaries for that user. And also, You will have the observability that comes with it. You can look at locks and traces. You can understand what's really going on. So, it's a really quick to use. Uh, you, all you have to do is a few lines of code. You pick your agent and you're good to go. And and these are production worthy as well. So basically within open search environment you can pick the local agents and deploy them to meet your needs. Just to give an example, a flow agent is very good if you are trying to do some rack pipeline orchestration. It can get you started really quickly, and the conversational agent is something if you have a chatbot uh or conversational application, it can help you to build that really quickly. And the and the third one here is the plan execute reflect agent. So if you are trying to build something like a deep research agent, which is exploratory in nature, going after multiple resources, trying to think and explore and find the best answer, this can help you to build that kind of solution. So, uh, and we're gonna build more of these specialized agents to make the user experience easier looking forward. So that concludes the Asiantic search of what we are doing there. There is a lot of interest. Some of our better customers already tried some of these features. Um, we're gonna invest, uh, we're already working on some more new features, looking forward to share those in near future. So I'm, I'm, I'm gonna change gears really quickly. Uh, we talked about different kinds of search techniques. And how open search is building these capabilities. Most of these capabilities will be running on the Open search core engine. If you don't have the performance the core engine offers, all the things that you're applying, you may not benefit that much. So we, we always invest quite a bit of time. How do we optimize the performance? How can we get the extra. Extra boost to your performance. So if you look at this chart compared to 1.13, which is released a few years ago, we already kind of boosted about 11x core search engine performance, which is significant. And, and also vector search is a very dominant workload for us, and we kind of improved that performance by 2.5x. Uh, so this is all with the latest release 3.3 and. Uh, we want to make sure the performance is what you're looking for. And the And on and on the cost side, right, so if you really look at vector search workloads, uh, the workloads are exploding in size. They used to be in the billions. Now they're kind of transferring to trillions of vectors, and the more we talk to customers every other week we hear it's becoming common to enable these such large workloads. The memory footprint really becomes a challenge. And how do you manage the memory footprint while giving the performance and the cost benefits is is a problem on the table. So we already have the exact KNN ANN techniques where it's uh, it gives you most accuracy and also high performance, right? Uh, we, we added disc mode which, which gives you that, uh, additional cost advantage when you have. A a large scale storage to make for these large workloads. Uh, what dismode does is it has the ability to manage the the vectors both in uh RAM and also disk. You store the high precision vectors in the disk and, and, and the, and the, uh, uh, you, you apply quantization and you apply and and then you store them in the. In the in the RAM, so you may you use the combination of these and kind of save the cost while maintaining the quality of those uh results and the and the last one we made uh in this tiered vector storage is S3 vector integration. As you all know, S3 vectors are the lowest cost way of storing storing information and this year they introduced vector storage capability. And we, we natively integrated S3 vectors with OpenSearch. You can work with managed service, Openear Manage service, or OpenSearch serverless, and store the information in S3 vectors while taking advantage of open search search capabilities. So it's a good, powerful complementary strengths to kind of leverage and build the applications that are at massive scale. So, that concludes all the things that we wanted to discuss today. Um, just to summarize, right, so, Open search, uh, search relevancy is front and center of search, right? So to, to give you guys, uh, best relevance, we've been investing quite a bit in all types of search techniques, and you have a choice to make, and each search technique is applicable for a specific use case and lexical search is not going away, semantic search is not going away, and, and on top of it, based on the complexity, you can apply uh agentic search. And like I shared with you, uh, Asiantic Search, we built multiple capabilities. I strongly recommend to give it a try. Um, and I'm, I'm sure you'll find it useful for your advanced applications. And the third one is the cost structure. Like I mentioned, we've been investing quite a bit to improve the performance and also to give you the best cost structure to deploy these large scale workloads. And if you upgrade to 3.3, you get all these additional features and also the performance benefits. And the last one is easy integrations. Uh, we strongly believe to work with all the services within AWS. We are natively integrated with services like Bedrock, SageMaker, or Agent Core if you're building GI or Agentic applications, and also we are doing a lot of third party integrations. If you're building any search applications. Uh, we want to make sure we are part of the ecosystem. Either you build within OpenSearch and work with those partners, or you build applications with the partners and leverage OpenSearch, um, so to build those applications, OK. So we, we try to compress a lot of information, right, with one hour. So there's more to do. OpenSearch is a very broad service. We continue to build and innovate. So for if you want to learn more, get more hands on, or dive deep into OpenSearch, I strongly recommend to read this book and it's written by our friend John and other teammates. Um, it covers a lot of things. It's a very dense book. It has a lot of details. I strongly recommend you get a copy of it. And And if, and if you wanna get lucky, you can visit the booth, give it a try. You get a free copy for perhaps we have a bunch of them at the booth. And also we talked about a lot of techniques today. If you wanna see them in action live demos, and also our partners are doing a bunch of things you can see all the things we talked in action at the booth. I, I, please go ahead and check out the booth and so that you can see them in action. And and on top of it, if you want to build skills like AWS is uh many services and uh too many choices. There's so much to learn. I strongly recommend you can check out the SkillBuilder. You can double click on any particular area of your choice, any service, open search or others to learn and develop your skills. And last, right, so, Please give us feedback. We work hard to bring together the right content and also the information that's useful for us. It will help us to iterate and make sure we bring the good content for you for the future sessions. And thank you for your attention joining us and thanks. Have a good green one.