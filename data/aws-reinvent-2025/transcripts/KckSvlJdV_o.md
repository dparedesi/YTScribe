---
video_id: KckSvlJdV_o
video_url: https://www.youtube.com/watch?v=KckSvlJdV_o
is_generated: False
is_translatable: True
---

Good afternoon everyone. Uh Today we are going to talk about innovations in data analytics. And my name is Kinhuk Pahare. I'm head of product for our analytics portfolio. That's Glue EMR, Athena, and Redshift. With me is Neil Mukherjee. And uh Anjali, uh who is uh engineering manager at Netflix. And today, the topic is gonna be around uh innovation in data processing. Uh, so let's dive right in. This is going to be our agenda. We are going to just talk about latest innovations, specifically touch on some of the agentic innovations we have done. We're gonna talk about Iceberg and ease of use, uh, innovations that we have done. And finally, Anjali is gonna share how Netflix is leveraging EMR for their data processing workloads. So within data processing services, you are familiar with Glue ETL. Which is used for surveillance data integration. Uh, you're obviously familiar with EMR. EMR is one of those services which had been around since 2009. It is used primarily for the purpose of open source data processing. Customers use EMR and multiple engines that EMR supports for their data workloads. And Athena, which is our interactive query service that is primarily used for querying your data lake in S3. Um Together, you know, millions of customers are using these services to author their, you know, data pipelines, um, querying their data for interactive workload. Um, on, you know, 300 million plus red ship surplus queries are executed per week. There's 19 years of innovation that goes behind it. Um, there are a billion plus Athena queries that are executed against the data lake every week. And then EMR Servers, one of the fastest growing service, executes about 300 million Spark jobs. Together these services query about 100 petabytes plus of data from S3 during the course of the week, and these data workloads are primarily built on S3 as the data lake. So over the years, estuary has emerged as the best place to build your data lake. Uh, there are exabytes of, you know, parquet data that is stored in S3 for the purpose of data lake and querying. Now parquet, as you are familiar, is a columnar format that is typically used for building data lakes because it is much more efficient for reads and writes, and the engines that are writing and reading from S3 are, you know, uh, behind Glue, EMR, and Athena. Um, there are 25 million+ requests per second against those data lakes. So the reason I'm sharing this, these numbers, these are exciting numbers. It's actually a testament of how customers are trusting these data processing services and our S3 for building their, you know, um, mission-critical data applications. Now, every year we gather here to talk about how we are going to make it even better. Um, this is, you know, my 4th reinvent, um, but, you know, the, the Athena service was launched 9 years ago, and one of these reinvent Glue was launched in 2017 August. So these are all the services where, you know, they have years of innovation behind, uh, uh, these. Exactly for the purpose of data processing and and querying these data, so. The first and foremost. Price performance is key for us. This is a recurring theme. Year after year, This year also, our engineering has delivered the AWS Spark performance, which exceeds open source Spark performance by 4.5x. It's 2x better for rights. And about 5.4x better if you use a Spark 4.0, which is the new innovation that we have done. The reason why price performance becomes important is that the faster the spark engine. Better you are able to meet your SLA targets. And together, you're also able to actually realize pricing and the cost gains because if a compute or if a data processing job finishes faster, you're actually paying lower price for that. So with the price performance out of the way, there are 4 innovation themes this year that the team is driving. Iceberg has emerged as the standard for building data lakes. The key advantage is that you can build a data lake in Iceberg and use any engine to query that data lake. AI agents in the context of data processing have emerged and they are solving some of the most gnarly problems of data engineering and I'm gonna talk about some of these um in my session and then Neil is gonna talk about how you, how we are using data agent for some of the ease of use use cases. And finally, governance. Governance is always a topic of interest for a lot of our customers. How do you govern data lake? How do you actually ensure fine grain access control? How do you ensure that the identity of an end user is carried forward for all your enforcement decisions when it comes to querying and writing data leaks? So with that out of the way, let's start with the AI agents and data. Now, AI agents have many applications. This had been a year of AI agent in various contexts. So, for this talk, we are gonna focus on one of the hardest problem that data engineers face, which is upgrading a spark runtime from version X.Y to version P.Q. Now the Spark version upgrade always had been a challenge. The reason it is getting exasperated is because of the fact that all the data lake innovations depend on the latest and the greatest spark run time. Now, obviously, as a practitioner, everyone should run the latest spark reversion to get the best performance, uh, out of it, uh, get the best data lake performance out of it, as well as, you know, the features out of it. But it's not usually an easier. Upgrade to a sheep. There are 2 problems that Compound the spark upgrade. Number one, there is a code upgrade part. But then there is also a data consistency part. So it's not just about, you can upgrade the Spark code, it compiles well, but is it actually the same thing with the data that your previous version was doing? So there are 3 elements. And in practice, what ended up happening is that the data engineers. Have to iterate while they are upgrading the code. So you upgrade the code, see some new error messages, edit the code, rinse and repeat. That process ends up becoming not only the time consuming, it is also not a foolproof process because it ends up happening that you miss a transform which is defined differently in the latest Spark version. To solve this, we have unleashed. The AI agent for spark upgrade. Um, the way this agent works is similar to how a data engineer works. It actually builds a plan for the upgrade. And to build the plan, it actually reads through your project structure. And it generates an upgrade plan. It tells you that, hey, this is what I'm planning to achieve. You can look at the plan, edit the plan, you can provide your own upgrade plan, depending on, you know, how your pipeline is structured. The next stage is to build and execute, and this is the stage where it will execute this plan. See the error messages. Edit the code based on the error message. Rinse and repeat. So it will do the same thing which a data engineer is expected to do, but it will do it in an automatic fashion. Finally, when the code is upgraded and it's compiling well, so all the test plans are passing, it's also going to do a data quality check. And what I mean by data quality check, it was the AI agent will actually look at your data structure before and after upgrade. Whether the schema matches, whether the column type matches, whether the data, you know, the The size of the data matches. It will do all those checks, and then it will say that, OK, this is ready for your, you know, uh uh this code is finally upgraded. Throughout the process, you are in full control. There is a guard rail which which can be configured. You can actually say that I want to manually execute every single step in the plan, or I want agent, I, I trust the agent and uh I trust it to upgrade the code properly. And During all the runs and all the build and all these, you know, the entire process, you have access to the observability logs, so you can actually see what changes it has done, and I'm going to show you a demo which actually talks about how this works in the practice. Um It also, uh, like what I didn't mention before, you can actually upgrade your entire pipeline or multiple pipelines. The task that used to take 6 months, sometimes 6 to 12 months in minutes. Um, because this agent is actually, uh, you know, automatically able to achieve some of these, uh, you know, very hard to find problems, the spark problems, and act upon it. So, in this case, here's a little demo. Help me upgrade from, you know, 2.4 to 3.5. It will say, OK, you want me to upgrade this. I read through the, the file, I read through your project structure. It will actually build a plan. It will say, hey, do you like this plan? When you say I like this plan, you can say. And it is going to actually say, here are the steps which I'm about to execute. Um, and as part of the execution, you can save, plan, and proceed. And finally it will say, OK, I found this diff between the previous version and new version. Here are all the changes which I'm about to execute. Do you approve? It's gonna hit yes. And then it's going to keep doing this for every single process. And finally, it will tell you exactly what changes it made. A D do and once you are satisfied with the subgrade, you can actually execute this into product. Um, FINRA is using this upgrade agent for the purpose of, you know, uh, upgrading the multiple data pipelines. The second innovation which I talked about is support for Iceberg V3. Iceberg V3 is the latest table format. It is based out of Iceberg version 1.10. It supports some of the new capabilities around deletion vectors, role lineage. All of these are available to you as part of EMR Spark runtime 7.12. In case you are not aware, EMR Spa run time is also shared with Glue ETL as well as Athenas. These are all the spark runtime which benefit from the same innovations that are delivered with EMR 7.12, so you get the same performance characteristics. You get the same feature set, uh, uh, as part of Iceberg V3. And what deletion vectors and roll lineage allows you to do is it actually makes it easier for you to manage your data lake. Um, it reduces your right amplification problems. It eliminates smart delete problems. And, and, you know, it is, it makes it very, very efficient for you to actually build and maintain your Italy. We are also announcing EMR Spark version 4.0. It is supporting, you know, the latest greatest iceberg version, as I talked about. It is also 5.4 times faster than the open source Spark, which means. You are able to use a sparkho and achieve your SLA or reduce your cost of operating a pipeline by an equivalent amount. Finally, there is another launch that we did this year, which is Iceberg Materialized View. Materialized view as you're familiar with uh allows you to define a view in the in the sequel, and iceberg materialized views are iceberg tables that are a result of that SQL code execution. So it automatically speeds up your You know, query from your spark engine. It is compatible with the Iceberg APIs, which means when they are executed as part of SQL queries, they manifest as a catalog object, and against that catalog object you can run your Athena Spark or EMR Spark or Glue ETL queries, and more importantly, you can actually do incremental refresh to the iceberg views. So what that means is that instead of having to configure an ETL pipeline. And configure all the triggers and all the scheduling. You can just define a materialized view, which is refreshed automatically based on the frequency that you choose, um, or based on, you know, as soon as the data is available, it automatically gets refreshed. Um, this is available as part of the Glue data catalog. Um, the view definitions are stored in the catalog. They can be, uh, you know, accessed from EMR, uh, Spark, Glue ETL Spark, or Athena Spark, um, and it is an announcement we did yesterday. So with that, I'm going to hand off to Neil, who's going to talk about ease of use. All right, everyone, let's focus on features that. That focus on ease of use, right? So the most shiny feature that we have is StageMaker notebooks. Now it is important to note that StageMaker Studio is the new UI, the new front end for all data processing as well as AIML, which means whether you're using EMR or Glue or Athena, your main UI front end for users is Sage Maker Studio, and Sage Maker Notebooks is the newest addition to Sage Maker Studio. It's uh. It's a new modern notebook experience that comes with a purpose-built AI. Agent, you can get started in seconds and you can choose the language best suited for your task, which can be SQL, which can be Python or Spark and. What is great about it is that you can start with Python and you can seamlessly scale to, you know, spark workloads to handle large data sets. Now let's see how that works. Behind the scene Sagemaker Notebook steps into Athena for Apaches Park to deliver the Spark capabilities. Now, Athena Apache Spark uses the same performance runtime engine as EMR. It actually uses AMR 5.12, which is 4.7x faster than open source Spark. Being serverless, of course, it eliminates all management overhead. And it starts up in scales in a low order of seconds, which means if you go into a notebook and type spark, within seconds you have a massive spark cluster ready for you for data processing, and what is unique about it is that it. The architecture allows a unified authoring, execution and debugging experience across Python and Spark workloads. Now let's see how that works. So Sagemaker Notebook connects to Athena Spark using Spark Connect. So Spark Connect is a new client-server architecture in Spark that separates the client application from the remote Spark driver while establishing a protocol to seamlessly exchange data between the two, facilitating remote execution, which means that really when you type Spark on a notebook cell, it appears that Spark is running locally in the notebook while it's actually running on a massive cluster behind the scene. You can debug Spark variables just as you can debug Python variables. And which means that now you can tap into Python and SQL, or maybe Python packages like, you know, Pandas and DogDB, etc. for small to medium data sets, and you can tap into Spark for large scale data. Processing at the same time. Now let's have a deeper look at the AI agent that, you know, comes with the notebook. Now, the AI agent experience is embedded in the notebook interface, right? It's on the right hand side of your notebook. It understands your data catalog, it understands your business metadata. It can generate SQL of the right dialect, which is Spark SQL or Athena SQL if you're using Athena. It can generate Python code, Spark code, which is Sy Spark code, and it can also do more than generate data, it can actually generate an entire plan, a full notebook filled up with cells to achieve a specific outcome, so you can really talk to it. So let's have a quick demo of how the data agent works. So, in this example, in my data agent panel, I asked a question, show me tables in my sample database. Now it'll go and discover the tables I have. Now I have 3 tables here, the churn table, the LTV table, and the New York taxi trips table. Now I can type SQL to actually query it myself, or I can go to the table cell and I can visualize the data I have. But what's even better, I can just talk to it, say, can you please help me visualize data in my LTV table? Right? And so It then generates the corresponding Python code to build visualizations and gives me a comprehensive visualizations of the data in my LTV table. So I hope you like the, you know, convenience and the power of it and you use it. So moving on to. A highly asked for feature, we have managed airflow now in an entirely serverless deployment, right? Of course, airflow is, you know, extremely popular in the data orchestration space, and now we have it in a serverless deployment model. The UI for this, again, is Sagemaker Unified Studio, so you can also create manage workflows in a single UI. Now it comes with an enhanced security model, where now per workflow you can specify an IM role, which means each workflow can now have a unique permission profile. And of course being thermoless, you only pay for what you use. So let's do a comparison of what we already have within a provisioned airflow and in a and in a compare it with servers. With provisioned airflow, you have a fixed infrastructure, you size it first, and you can have a micro size and so on, and it, it's always on you. Regardless of whether the workflows are running or not, there are certain capacity limits based on the size you chose, and yes, you have to manage the environment, but on the serverless side, there's of course no infrastructure anymore, it's pay per use, it's unlimited capacity, and it's entirely service managed. From the security perspective, there are some, you know, key changes in the provision mode. Your entire airflow environment has a single security model, right? So all the workflows have the same, you know, permission profiles. If you want to separate these workflows, you have to launch separate environments, which means the authorization is shared and isolation is complex. On the serverless side, you have workflow level security, so per per workflow you can define, you know, granular access control, which means you can now have the least in a. Privileged IM roles for each workflow, which really simplifies, you know, compliance. Now all of this is embedded in. The UI Workflow UI in StageMaker Unified Studio, you can easily drag and drop these activities to build a dag yourself without any prior expertise on, you know, in Python or Airflow. And, and then once you have built them, you can, you know, submit these workflows, you can monitor these jobs and so on, all from a single UI. Now what's also available is a a one click scheduler for visual ETL. This allows you to easily streamline your ETL flows and you know queries directly from the same UI in Sagemaker. Unified studio and you can create, you can modify, you can monitor these you know schedules. Behind the scenes, it's integrated with Amazon eventbridge scheduler to make these schedules happen. Now, customers have told us that it's not easy to onboard to Sagemaker Unified Studio, and that is because of certain dependencies. You have to configure IM INDD Center and so on. We, we have listened and we have made the onboarding to Sagemaker Unified Studio a one click step from existing services like, you know, Athena console, from ST Tables, from Amazon Redshift console. Now from those pages with a single click, you can go. Go to StageMaker Unified Studio, we will auto create um a default StageMaker Unified Studio workspace. The entire process takes a minute. You don't have to configure IM Identity Center anymore. It uses the IM roles you were already using in those consoles, and you'll land in a much sophisticated interface which allows you to use notebook capabilities, credit editors, and so on on the, on the same data sets. All right, so let's move into a very exciting feature. This is an industry first where. You must With serverless storage entirely eliminates local disk provisioning for Spark workloads. So with this you can achieve up to 20% reduced costs as per our benchmarks. So here's how it works. So Spark workers have have local disks, and these local discs have, you know, shuffle in them. When this disk run out of capacity, your jobs fail. If the discs are constraint, your jobs slow down, and often what happens, you have certain tasks which are stragglers, so some workers keep running for a long time and all workers have to be retained when some workers are working because the discs have shuffled data in them. This makes spark scaling inherently. Inefficient. Now with serverless storage, these problems are entirely eliminated. Serverless storage offloads shuffle to a high performance storage layer, which means you never run out of disk space. You don't face any job failures. Your jobs never slow down. And as as the shuffle is outside the cluster, Spark can be far more elastic, scaling out and scaling in as per the needs of a stage. So the combination of the. The computer efficiencies that serverless storage brings, along with the fact that they don't have to, you know, provision this anymore, is an absolute game changer in this space, and you can save up to 20% as for all benchmarks and, and maybe even more depending on the shape of your workloads. All right. So let's now move to the topic of security and, you know, governance, and a very important topic for enterprise and customers. Of course, the main concern is Is data access control, right? In the customers want, you know, consistent data access control across the various formats they want to use. Of course, Apache Iceberg is very important, but so is Delta Lake and Hoodie, and of course, Traditional formats like Park, CSV, and JSON, right? For most use cases, core gain access control is what is needed, where you want to secure certain S3 locations, you wanna secure certain tables, ensure that the users get access to them, and for some use cases you also need F access control where you have column row and and and cell level security as well. So let's see how these, uh, but we have made a lot, lot of launches this year, you know, focused on security and governance. So let's see how all of this, you know, comes together. So the first We'll focus on is coasttrain access for S3 locations. Now, For this, the recommendation is to use S3 axis grams, of course, where you can have simple SQL-like permissions, where you can have rewrite, you know, permissions to S3 buckets, prefixes, as well as objects. So how it works is that when you submit a job, Your job, the engine simply looks up S3 access grants, gets scoped down. Credentials and uses those credentials to talk to S3. Now this is, this has been in EMR for a while from, you know, 6.15 plus. It is now also Polson glue with 5.0 and it, it, it works, you know, consistently across all, all table formats. Now let's look at Co-scre access for actually tables in your catalog. This is definitely the most common, you know, scenario where what you want is you don't want to deal with in a Fegra access control overhead, but you want to grant users access to certain tables, right? So. This is, this has no performance overhead because it's just filtering of metadata and the expectation is of course full support, which means read, write, you know, support for all the use cases. It's a fairly easy implementation, so let's look how, how this is achieved. So the first step is that the admin will go to Lake formation and will grant, you know, permissions for, you know, specific users for, you know, specific tables. Now when a job is, you know, submitted, then. It talks to the glue catalog, which redirects to lake formation and gets scoped down credentials back, and those credentials are used to exercise 3. Now, this is available in EMR from 7.9 plus, as well as is available in the glue from 5.0+, and it, and it supports region right, so iceberg, Delta, as well as hoodie tables. All right, so let's now look at find and access control where you want column row as well as cell level security. Of course, enterprise, you know, customers want, you know, audit trails so that you can have, you know, compliance reporting and of course this should work typically with, you know, granular identity based in the controls as well. So let's see how this is implemented here. The admin will set up fine-grained permissions in lake formation for, you know, specific tables, rows, and, you know, column cell as, as, as needed. Here at the engine level, you have to enable the information, and now when the user submits a job. The communicates with leg formation, gets, you know, scope down credentials, and accesses S3 as well as the right, you know, catalog tables. So, so note that from 7.7 onwards on EMR we support reads on all deployments. The glue is supported 5.0 onwards. I'm very excited to announce that we support rights now as well, right? So 7.12, which is almost the most, most recent launch, we support rights, you know, consistently on all AMR deployments as well as in Glue 5.1, with, you know, Iceberg Delta, as well as HoD. Now this is a very unique architecture we have here where we separate the system and the user drivers. The system drivers is where all of the filtering happens and the user driver is where your spark code runs, and this isolation is what creates the security model that ensures that you cannot run spike spark code to bypass the controls. OK, so let's move to a very interesting topic and a very important topic for enterprise. Customers of your trusted entity. Propagation. So the expectation of of enterprise customers is that the user will authenticate against established identity providers like Active Directory, Octa, and so on, and those identity has to be pushed down to the various services, right? And of course the expectation is end to end in tracing of all, all users' actions. So now this is possible with IM INDD Center. So you will enable single sign on with IM NNDD Center, which allows end to end, you know, data access and a traceability. You have fine-grained permissions there and you'll access based on the user's identity. Now we support this from both interactive sessions and jobs from Sagemaker. Unified studio and on, on the specific EMR, you know, versions you mentioned from 7.11, it is supported on all EMR deployments and this works in in in Glue 5 as well. So let's look at how this works. So I have two users here, you know, Charlie and, you know, Elle. Now, Charlie wants to access a specific S3 table, and, and Elle wants to access another SS3 table. So, so when they log into Sagemaker Studio, they will authenticate with IM entity Center, which federates the identity from Actor Director Octa. Etc. and, and that entity and authorization is then pushed down to EMR when they submit a job or start an interactive session and that will talk to Lake Formation and can give them access. So, I hope this was useful of how, you know, trust entity. Propagation as well as finding an access control, delivers the enterprise in a security. All right. So now, I would like to invite Angelli Norbert from Netflix to share their story of evaluating EMR. Thank you, Neil. I'm here to share our Netflix story of uh POCing Spark EMR. Stick around with me to find out whether this has a happy ending. Was this a drama? Was this a story of collaboration and teamwork, or was there fighting action? Stick around and you'll find out. I'm Anjali. I lead the big data analytics platform at Netflix. My team is responsible for management of data in our warehouse. Um, it's an S3. Table maintenance, iceberg table format. Governance security around that engines to access the data. Spark being one of the big workhorses. Other engines like Trino, Druid solutions around Snowflake, new edition of LsB and orchestration technologies. This is something we have built in-house. Netflix is first and foremost an entertainment company. But almost all the decisions are data driven. So when you log on to your Netflix account, TV, cell phone, You start seeing these rows. This is a recommended content just for you. That decision is very much data driven. Which rows you see in what order you see them, which cover art you see, this is all personalized and data driven decision. These are user facing decisions, and then there are decisions we make internally, like when we run Spark on EC2, which instance type should we use? Is R7 ready for adoption or should we stay on R5 for a little while? All these decisions are also made with data. That is our streaming video on demand, our legacy business, but now we started offering ads plans in some countries. We started carrying live content. Anybody watching NFL on Christmas Day? I am. Games, Netflix house, those are consumer experiences. And also we will start carrying podcasts. This is all to say all of these new business initiatives result in more and more data in our warehouse and more and more of a need to get insight out of that data. This is a bit of a spark story. This started more than 7 years ago. Netflix decided to do storage and compute decoupling. That was a big bet we took, it paid off. Since then we have been operating Spark on Hadoop. We cut a fork from open source Spark. We hyper optimized it and customized it for Netflix scale, for performance, for a variety of use cases that emerge. We operate multiple Hadoop clusters and some stats are there just to give you an idea of scale. Compared to what Kinhuk shared, this doesn't look like a lot, but 11,000 unique workflows, 250,000 jobs, more than exabyte size data in our warehouse, that's some pretty good scale that we operate at. We have about 8000, 8500 or so R7 nodes. What do we use Spark for? It's, it's a variety of things, getting inside out of data, ETL that's a very popular engine for ETL, uh, large scale analytics, recommendations, and more. Then you would ask, if we have this nice platform and we have been operating for all this time, why would we even think of something else? Turns out things are changing a little bit for Netflix. When we started out, our warehouse was open by default. There was no security. Anybody could access any piece of data. Netflix is a very transparent culture. We still are a transparent culture. But addition of new businesses like ads means that some of the data is sensitive. It has to be protected. Our current platform. That we operate on Hadoop and Yan, it's kind of hard to get the level of security and isolation that we need. Adopting new technologies is hard because of the scale, the number of jobs and workflows we run. A Scala migration can take a long time. Python version change takes a long time, Spark migration takes a long time. It's a lot of work. There is a lot of operational overhead and cost to our teams. We call that undifferentiated heavy lifting. As in this work is important, but it's not moving the needle for business. We would like to see how we can offload this work, reduce this work for our teams. Limited support for specialized hardware with AI use cases emerging, we need GPU support, which is again harder to do with our current setup. Our environment is good, but it's a little bit difficult to use and debug. Ease of use is not where we would like it to be. And this is where we started thinking about Amazon EMR. This journey started Q4 of 2024, where we said, You know what, let's check out EMR, and it turns out it provides isolation. It provides improved security. It provides frequent releases every 90 days, every quarter. Along with that comes new iceberg, new Python, Scala, all the nice support. Reduces operational overhead. Things scale up. A scale up, down. Supports GPUs, integrates very well with the rest of the AWS services like S3, IM, and other things. So what did we test? EMR comes in various flavors. We started with testing EMR on EC2. A couple of reasons. One is it is very close to how we operate our current system. While we want managed services, uh, we also want to kind of get there slowly. We still want to be able to tune the knobs, you know, change the configs, make sure things work for our users. We tested feature compatibility. This was important because, as I mentioned, we have customized our Spark for Netflix users. We tested performance. We started with TPCDS checks out, and then we decided to bring in our user workflows. These are representative user workflows. We literally reached out and said, give us the workflows that represent your team the best. And they fall in various categories. We have SQL, we have Pio Spark, we have Scala. Java Then we tested for scale. We basically mimicked our production runs on EMR to see if EMR holds up, how is the resource consumption like. Second part of scale testing was saturation testing. Can we just, you know, overload the system, use up memory, throw all memory heavy, IO heavy, CPU heavy workloads, and see how it holds up. And then we look for operational complexity as platform providers, my team has to know how to operate this platform, and this has to be easy for them. And then cost of course is important, so we check that as well. Here are some numbers. If you look at the P90 column, you'll see that. Pie Spark by far saw the most gains, followed by Sequel and Scala. Again, I would like to remind you that our platform is hyper optimized, so seeing these kind of gains is amazing. Here is another slide. This talks about resource consumption. Again, P Spark shines with most V core gains at P90. Followed by sequel and then scala. Know that the consumption of VCPU is directly correlated to cost, so the, the less resources you use, that's that's a good thing. It's a bit of an Eiffel chart. We have a pretty long doc detailing our POC and evaluation. It's kind of hard to talk through it all, but some salient points I want to touch upon. Futureproofing We are looking to be on a platform that would just work for us and will grow with us as our business grows. And we see AWS investing heavily, have strong roadmap, and have continuous innovation in this area. This was important for us. Security, I already mentioned, security and isolation is, is really great with EMR. Cost structure we found to be competitive and as transparent. User experience is good. Notebook integrations are supported. Our operational burden will go down. This is a managed service. Performance and scale. This worked out. I shared performance results, scale results were also equivalent or better than our current platform. This platform is pretty extensible through bootstrap actions and so on. It's not going to be as flexible as do it yourself, but that is to be expected. But taking that into account, we still found this platform to be pretty nimble. Observability and debugging. We did not try cloud Watch. We are bringing our own observability. But the rest of the integrations and the overall story looks good. We have a little bit of work to do here, but the story still, still looks good. Going back to the slide that was shown earlier by Kashuk, we checked out 3 of the 4 boxes and, and find them to be, to be really great price, performance, ease of use, security. AI acceleration is something we haven't tried yet. This is something we'll try next. And this is how this story ends, but maybe the new story begins, right? So we have decided to move Netflix Spark workflows gradually to EMR. The way we plan to do that is. In the first half of next year, We have some work to do. We are paying off tech debt, building new control planes, job routing, infrastructure, and so on. In the second half, we'll be migrating our own platform workflows, dog fooding. Checking things out We'll fine tune based on our learnings and In 2027 we will be migrating user workflows. We plan to test EMR serverless during the POC. We kind of touched upon it, saw early positive results, but didn't have the time to do a deep dive there. AI acceleration, as I mentioned, adding cost attribution. I want to thank the EMR account team. Wedi Manju, Kartik, it was one team, EMR product team, June, um, Geo, of course, Pinhuk Neil, it's been. It's like an extension of Netflix team. It never felt like they're outsiders. We were working together. It was a great experience. Big thank you to Netflix Spark team, Anurag and team, DSI Security, other BA teams. It was a huge effort that went on for three quarters. At the end of, um, Q2 this year, we finished our POC and then there were more follow-ups because we have custom patches. We have customizations that we needed to talk through, and all that is coming together. I'm super excited for our journey with uh AWS AMR. The best is yet to come. Thank you.