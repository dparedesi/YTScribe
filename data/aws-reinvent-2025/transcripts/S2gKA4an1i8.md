---
video_id: S2gKA4an1i8
video_url: https://www.youtube.com/watch?v=S2gKA4an1i8
is_generated: False
is_translatable: True
---

Hello everybody. Thank you very much for coming all the way over to Mandalay Bay to hear me talk about logs. Now, this presentation. Partly is a story about what's really possible with AI. And uh this picture here was actually generated by Sara. It's actually me at the 1966 World Cup, which I am far too young to have been a part of. Uh, witnessing England's winning goal, which is the only time we've ever seen success in a sport we invented, right, so, uh. Right. So let's talk a little bit about, about the problem that people are experiencing today. Observability today has been laid down with a foundation of data collection, proprietary agents. And In a lot of ways, we've come to rely on dashboards and visualizations, but we're struggling to get answers. And we have a lot of alerts out there today, also without context, right? So, you know, I get an alert about some CPU usage issue on one of my boxes or my Kubernetes cluster, pods are running out of memory. I get the alert. I just don't have a lot of context, I don't know what to do with it. And this is a challenge that we see our customers struggle with every day. And even when, you know, you're looking at the outcomes. A lot of time is spent working on putting together the data pipelines just to get the data into the observability solution. You have to create lots of uh log pipelines, maybe you use a log stash or something like that, and it takes a lot of work, and a lot of data gets lost during this process, so I talk to customers all the time and they say to me things like, you know, oh. Half of the logs that I'm trying to get into my solution here are just being dropped on the floor, and if you don't have the information, then you spend a lot of time. Wrestling with getting answers to the problems that you have, which increases the amount of downtime, customers are experiencing slow systems with, I was trying to get onto Claude earlier today actually, and it was uh, and it was, it was currently down. So, you know, what do we do while customers are suffering with these problems, right? We need to get things up and running again. And get people back online. And part of the problem here is the Y has been missing. I, I, I was an SRE a long time ago. And uh you know, I used to work with uh logs on a regular basis to figure out what's wrong with my systems. And uh you know, back in those days, we only had 5 or 6 different servers, and I had a notepad file with a list of rep and said patterns that we'd come up to come up with together as a team. I'd copy and paste those into my terminal and try to find the logs that way. And then over time. We built systems that were, became very, very complex, right? Microservices everywhere, Kubernetes everything. We've basically built enormously complex systems. And as a result of that, the industry has been telling you to throw your logs away. Turn them into metrics, or put all that information into traces, but you can't rely on that, right, because you can't instrument everything with traces. You know, I think I looked the other day at a survey that an observability vendor had put out there. You can get to 50 or 60%. Of traces deployed out in your environment, what do you do with the rest of it? So. We shouldn't be throwing away all this log data, we should just be being more intentional and more intelligent about it. So why aren't SREs using logs more, because there's some great information in there, and like I said, every time I've solved the problem. It usually has come down to the logs. Well, they're messy and unstructured, right? So, you know, the, the information in there is sometimes a little bit, a little bit messy, um, sometimes you can't, you can't do analytics on that data because it's unstructured. You have to pause it first, which goes back to the pipeline problem I was talking about, you need to write all these grok expressions or regular expressions. I actually did a, did a search and uh and looked up like what was the most complicated Regex expression. And the first result on Google is somebody trying to use Reg X to parse a log line, right, which I thought was pretty funny. And there's a lot of logs, right? Millions, millions of logs or more. It's uh, it's volumous, the most volumist observability signal that we have. And finally, There's all sorts of logs from all over the place, different systems, and that problem has gotten a lot worse. Again, when I, when I was doing the SRE role about 1010 years ago. Things are starting to get very complicated, but. We still had a manageable number of systems. We didn't have like a million different. AWS services, for example, sending us data, all these different SAS applications that we have now, it was kind of manageable, and every single system out there. Has a, has a different format. And it's a shame that we can't rely on logs more because it's easy to collect logs. Even SAS applications typically output, output uh logs. So you can get logs from just about everywhere, even, uh, you know, mainframes put out logs, ATM machines, everything emits a log. And it has a lot of blogs typically have a lot of great context. They were written for humans to consume, typically. And so, you know, things like user IDs, explanations, you know, you can read them, they're like a novel in some ways, right, you can actually read logs if they're well written, of course, um, and you can get a lot of really good information, like you can see on the screen here, the server restarted, a host, for example. But what's needed to manage logs well? Well, first of all, You need a scalable platform because we're, we've got a lot of logs out there. And uh you need to be able to, you need a platform that can handle all of that at scale, right, so, you know, if you want to write a query, or you've got a dashboard, you wanna be able to have those things updating quickly. You don't wanna have to wait for hours and hours to get the results that you need when you're trying to solve a problem. Um, and you need to be able to store this stuff fairly cost effectively. If you wanna keep all your log data instead of throwing it away, it needs to be in a way that's cost effective. And so that's what you need on the platform side. On the other side, we can use AI driven analysis to extract incredible insights from logs. And we'll talk a little bit more about that later on. But first of all, let's talk a little bit about what elastic does well already today. We have a lot of integrations out there in Elastic, so we can, you know, consume logs from a huge variety of different sources. The platform is very, very scalable. Customers tell me all the time that they moved to elastic, or they adopted elastic, and they were very, very impressed with the performance of the platform. They can get queries back in half the time that they could before. They can get information on their dashboards, and they can actually see the information quickly without having to wait hours for the dashboards to load. Elastic does these things very, very well. And We also have good ML driven pattern analysis, classic ML algorithms, anomaly detection, pattern analysis, things like that. We can do really great search and filter things. We've had this new query language called ESQL if you haven't seen it already. Check it out, we keep adding stuff to that all the time, we just GA joins, which is great, so now you can join Data in Elastic with ESQL and we keep adding, ML functions to ESQL as well, which is great, so you can actually query a lot of data in Elastic now. And inside the query language, you can send the result to an LLM and get it to have a look at what, what the uh what the issue is from the ESQL query, directly using the piped query language, you just pipe it to the LLM have the LLM do the analysis, and it comes right back into your results. Really fantastic, so ESQL has some very, very powerful analytic capabilities. So how do we store log data efficiently? We recently released logs DB index mode. And the way it works is, is it does. One of the main ways that it works is that we do some additional compression on logs. We use index sorting. We also use a technique called synthetic source. So now, we're actually storing logs in a columnar data format. We can save a lot of disk space this way. In fact, many of our customers are seeing up to 70% of their data storage costs have gone down using LogsDB. And so, you know, that's one way we're making it so that customers can store all their logs without having to throw them away. And we're going even further than that. We haven't actually uh released this particular um part of Elastic yet, but we, we're currently working on a compressed log processor as well. So if you've got repeated log lines, like you can see in the, in the slide here. We'll template those, right, so we only store the bit of the log line that changes, so we can save you another 50% in storage, using this technique. And this is completely transparent to you, it's just happening in the background. So we have a great platform. You can store all your logs, you can, you know, manually analyze the data really nicely with um, with a lot of the analytic functions that we provide. If we take all of that power. And we bring LLMs into the picture, we can do a lot more, we can do some really great things with all this log data that you have, that means that you don't have to go hunting and pecking, you get the insights delivered to you. And if we do that And give you those actionable capabilities, rather than just having to trawl through all this data, you actually get insights that you can action on. Then we've actually created something magical for you, right? So we, we already talked about how we store the data efficiently. Once we have the data stored efficiently inelastic, the first thing we do. With this new AI powered log analytics engine that we, we're calling streams, is we identify the systems. So we send the logs into Elastic, store them efficiently, and then the first thing we do is we look at those logs. With the, with the assistance of an LLM to understand what systems they come from. And you may have tried this yourself, I've certainly tried it myself, and it works phenomenally well, which is, take a log line, chuck it into an LLM. Quite often, the LLM will be able to tell you where that log came from, what system it came from, it'll do some really good analysis on the logs, might even help you with the root cause. So we can usually tell just by looking at the logs or a sample of the logs, not just one log line, but you know, we get a sample of the logs, we can tell what system it comes from, right? So we can identify if it's Elastic Search or Engenics per this example. Once we've identified what system it comes from, we can then start to partition them automatically. So we can send Hadoop logs, for example, into a special Hadoop index. We can send Spark logs to a special Spark index, or if you have Java applications and you've got an order processing service, we identify that it's an order processing service, and we send those logs into an order processing index. So now we've got your logs organized, and this all happens automatically. You can see here. An example of how we organize that data. So it's not just going, you know, one level deep, like I described here, you can go multiple levels down, so if you've identified an order processing system, you might want to separate error logs from info logs, for example, or you may wanna go higher up, you might wanna bucket everything under Kubernetes and the things that are under so we have a hierarchical organization system here in play that enables you to organize your logs. How you see fit, and like I said. Using AI assistance to help you. So once we've had We've got the, the systems, we've identified those, we've partitioned the data into sensible streams. We then want to extract meaning from your logs, so like, at the moment, most people are still using unstructured logs, right? I would say a good 90% of the logs I'm still seeing out there are completely unstructured. And that is not good for analytics, right? So the next thing you wanna do is take that unstructured data and structure it. So that's the next thing that we want to do with our data processing pipeline. Once we've finally done, done all of that, we've actually processed your logs, structured them, made it easy for analytics. The next thing we look for is any significant events. So like I said, you know, we know if it's an order processing service, we maybe know if it's. They're hosted on Kubernetes, we might know if it's a Spark system, whatever it happens to be. What we then do is with the assistance again of an LLM we say, what kinds of problems am I likely to encounter with this particular system? And here are some samples of some logs, as well, so it gets familiar with uh what it's dealing with, and it will come back and say, You know what, this is a spark system. There, there might be out of memory errors that you're likely to encounter. And what it does then is it generates a query that runs on a regular basis to see if there are any new occurrences of, say, this out of memory. Error. Now, you don't have to do any of this manually, right? This is AI doing a lot of the work for you. And then We can generate assets, we can generate dashboards, we can generate SLOs, we can generate things like um visualizations, and we can even do things like put information about the systems that we've detected into our knowledge base. So if you don't know about this already, elastic has a vector database in it, right, it's one of the big things that elastic does. And all of our AI technology in the Elastic observability platform is built on top of Elastic search capabilities, like the vector database. And so we have this thing we talk about on a regular basis here at Elastic, called retrieval augmented generation. So, what we can do is, is if we have the AI analyze your systems, we can say, you know, this is a Spark system. These are the things you need to think about with Spark systems. We can put that into the knowledge base, and so when you're dealing with problems within the AI system, you know, having natural language conversations with your, about your problems, what is the root cause of this problem and that, it can then refer to any information that it's uh recorded about the systems that it's working with, which will help you accelerate root cause analysis, right? And this is what it looks like end to end. Now there's a couple of things that are interesting on this slide. Firstly, if you look at the bottom. The bottom left hand corner on this slide, we still support all the existing elastic things like integrations that we have, if you're using elastic agent or log stash or whatever, we still support all of those ingest mechanisms with this new streams product. You can still use those. But naturally, those uh when we send data in using an integration, we don't need to partition that data because the integration already tells it where to go in Elastic and organizes that data for you. So, it does a, it skips the partitioning step. That's kind of like the legacy approach, we'll still support that. The new approach that we're pushing over here at Elastic is to send your data in using whatever you like. It could be the vanilla Oel collector, it could be fluent, fluent B. It could be, you know, whatever it is you're using today, it doesn't have to be elastic technology. You can send it into our new logs endpoint. And we just put the logs in elastic, you don't have to do any edge processing anymore, move all of that over to elastic. Send all the data to the log stream, and then we figure it out, by starting out with partitioning. So that's elastic streams. Now. You'll see when I show you the demo, which I'm gonna do in a minute, that it is like a magical experience. So let's do that, let's switch over to the demo and I'll show you what Elastic Streams looks like. So here we go. This is Discover And I've got an ESQL query here, and this is the log data. Going back to the previous diagram that you just saw, it's coming in via the classic elastic agent route, OK? And the first thing you'll notice about this data is that it's completely unstructured. OK, so it's just raw, unstructured log data, which is what most of you are probably dealing with today. At the moment, you could probably recognize a little bit about this data, it looks like trading data. And what we'd want to do. Is we wanna, we wanna structure this data, so we're gonna click this little button here, pause content in streams. And the first thing we're gonna do is we're going to create a new processor. Now As you can see here, we have this handy little button here called generate pattern, and it has that little AI logo that you're probably seeing all over the place now. But if we click that button, You'll see that it generates a grog pattern for us, so you don't have to do it manually anymore, right? And you can see that it's like looked at the data, and it's unders it's got some understanding of the data, right, so it knows that this is a stock symbol, and a quantity and a price, and it inferred this, it, you know, we didn't tell it any of that information, we just gave it the raw logs that you see here. So if we accept that now. The data is going to be much easier to do analytics on, right? We can now start to look at how much did a particular stock trade for over a 30 day window, or something like that, which would have been much harder to do had we left this data in an unstructured format. So let's save that. One thing you'll notice here that's very interesting as well, is this little source thing here that says open telemetry. So when we do this with the generating of the patterns, we'll try to use open telemetry semantic conventions. So we're using this open standard schema. We store that in that standard schema format natively inelastic. So you can be sure that all your data is being written in a standard format. We'll confirm those changes. And now, if we go back to Discover. We can now have a look and see how easy it is to run analytics on that data, right? So now we can actually do a sum of the quantities. Over the cloud regions, we can see that our workloads are pretty evenly split between all the different cloud cloud regions that we have there. Um, and we can do, you know, other types of analytics now, because we couldn't have done that on the unstructured data, not very easily. Or you would have had to have manually created a processing pipeline to deal with it, right? So Now we look to how you do it with the uh. With the uh the, the standard processing pipeline, using elastic agent and all that sort of thing, right? Now, we're gonna look at how you do it with the, with the new, in the new way, which is just sending your data to the elastic log index. So this is a new special index called logs. And we manage all the complexity behind the scenes like the ingest pipelines and things like that. You don't have to worry about those. We manage it. So Let's take a look at what we want to do with that first. So the first thing we would do with that is partition the data. So we're going to go into partitioning here. And of course, we can still do things manually. We don't always have to use AI, right? We wanna give you that flexibility. The AI isn't always gonna get it right. And so we'll create a partition manually first. We'll always call it Spark. We're gonna use a field attribute. To uh to split off spark logs from the, from the log stream. So if we save that, what's gonna happen now is you can see this hierarchy appearing. Where we've got logs at the top level, which is where we're just sending all our data, and now we've got a new index that's being written, written to, which is gonna contain just spark logs. But we can also do this with AI so we click, click suggest partitions with AI and AI would have done a perfectly fine job. In fact, we had Hadoop and Spark logs in the log stream, and it has split those out perfectly. So we're gonna just uh save those changes. And you can see here that we have Spark and Hadoop blogs. And you can see that hierarchy that I talked about before. So the log stream has now been split into two. We've got Hadoop logs and Spark logs separate from how they were. So once they're all separated out, we can then also do the same processing that we did before on those logs. We can create a processor. We can use AI to generate those grok patterns. And we can split out the severity text using that, and again, do a better job of analytics on this data. So let's confirm those changes. And then you'll see here. Now I can run ESQL against that data. Much, much easier. Right. So what have we done so far? So we've partitioned our logs. We, uh We've structured our logs, we've ran analytics on our logs. The next thing that we wanted to do, if you go back to thinking about the diagram I showed you, is we wanna find problems in our logs, right? So let's take a look at the spark logs. And the first thing we'll do here is we'll detect. What systems are here? We know it's a spark system, but what the AI has done in the background is it's added additional information about the Spark system. So you can see that description field, it's kind of uh cut off at the moment, but it's got a, a large description about spark systems and what spark systems do and what types of problems that you might encounter with spark systems. So once we've got a uh we've identified the system, and we've described the system to Elastic. We'll add that to streams. And Will generate suggestions on the types of problems that can occur with Spark systems. So, you can see this list here. We've got um spark debug events, spark warnings, spark out of memory exceptions. These are all things that are specific, specific problems to spark systems that might occur. And by bringing them into elastic. We then keep track. To see if any of these problems occur over time. We're actually using a technique called um change point detection. Which is a machine learning technique, it'll detect whether there is a spike in the number of these, of, in the number of these logs over time, or if there's a dip in the number of these logs, or if there's a change in the distribution in these logs. It'll detect all of these things. And you can get alerted. To those problems. And I'll show you how to do that. So here what we do is, is we actually are gonna set up an alert based on the query that the AI generated. So we go in here and you can see we copied the, the query from here into the alert. And now If we save that. We will get alerted anytime there is an out of memory exception with our spark system. I didn't have to create this query manually, the LLM did it for me. Um, it actually probably knows the Spark system and the types of problems that can happen with Spark systems far better than I do, to be honest. And you can see that alert inelastic over here. So we've done Significant events now, we've found problems in our logs. We've also made a lot of other things much, much easier. It used to be kind of tricky to do in Elastic. We've made things like retention and data quality issues, much easier to see using uh the streams UI. So the next thing we're gonna dig into is I'm gonna show you how easy it is to do that now. So if you look in here, we can actually change the retention pretty easily. So, you know, if I want to change it to 30 days, for example, for these particular spark logs, I can change it very, very quickly, and here, I can adjust the ILM policies. So if I wanna just put, you know, 2 days of hot. And 30 days of warm, or maybe I wanna use the frozen tier to save even more money on storage, I can do that quickly in streams. We've just made these things a little easier to do, hide hiding a lot of the complexity behind the scenes. So we can set retention, and the other thing we can do is we can actually look at data quality. So if we look at the uh the data quality here, there's actually two types of data quality things that we could do. So if you think about data coming into Elastic, we can tell you if there is a problem with a particular field that's coming in, a specific piece of data, or we can tell you if the entire document failed to come into Elastic. And we store that information in, in our failure store, so you can dissect it. So I'll show you how that works, again, we've made it much, much easier to do all these things in streams. So the first thing I'm gonna do. Is I'm going to Deliberately break this regular expression. So you can see here I've put some nonsense in there. We're gonna ignore failures for this processor, and we're gonna update that. We're gonna save the changes, and now, what'll happen is, is. Elastic has caught. The problem. If you see here, the, the entire document has failed because that regular expression is broken. So it failed to bring that document into Elastic successfully. And we can dig into what the problem is, so you can see here, it tells us straight away that the provider Grok expression did not match the field value, which stopped the data from coming into Elastic properly. So it's much easier now to see when there are problems with the data stream. So that's one thing, that's the entire failed document. But we also can do it on a field level as well, right? So if we go back here, we'll um first of all, we'll fix this, so it's not broken anymore. And then what we'll do is we will create a new processor. We will Use the host name and we'll put a ridiculously large host name for you, that is definitely not a host name that will fit any uh any sort of validation logic that I'm aware of. So we'll do that, and then you can see what happens in the data quality pages. Now, instead of a failed document, I've got a degraded document. And that's because it's ignoring the host name field. And if you look at the host name field in here, it's because the, it's, it, the, the size of the host name is too big. So we, we basically just ignored that specific field. We're still ingesting the document and we're still ingesting all the other data, all the other attributes that are in there. We're just ignoring the host name because that field's been corrupted. It's, it's got a completely ineligible. Host name in it, right? So we'll go back and we'll fix, we'll fix these problems. We'll delete this. We'll save the changes, and then if we go back in. Everything's working again now, right? So that's a So that's streams. Those are all the, the new goodies that we've released in elastic. I'll switch back to the slides, here we go. So You can see now why we need a good, solid platform. We need to scale to billions of logs and, and do that in a way that's cost effective. We wanna simplify ingest, right? We don't want people to have to wrestle with pipelines and drop data on the floor. Um, we wanna automate as much of that as possible, and we, we now have AI that can really help us with this automation. And now you can spend more time solving problems instead of working on automa on uh you know, blog processing pipelines, which I know is a massive headache for a lot of you. And we then use AI. To do a lot of analytics work, to surface problems in your logs automatically, to give you the ability to, you know, find problems, for example, that you may not even be aware of in your spark logs, for example. And you know, at some point, we, our vision is to bring this um beyond. Locks, right, bring it to traces and metrics at some point in the future. So, thank you very much everybody, thanks for joining me on this talk about streams, and I appreciate you coming all the way down here from the Venetian. Thank you very much.