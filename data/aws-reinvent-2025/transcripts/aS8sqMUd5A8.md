---
video_id: aS8sqMUd5A8
video_url: https://www.youtube.com/watch?v=aS8sqMUd5A8
is_generated: False
is_translatable: True
---

First of all, welcome to Reinvent 2025. I hope day one is going well for you. My name is Enrique Bastante. I am a customer solutions manager with AWS Strategic Accounts, and I'm really excited about this session today for a couple of reasons. Um, first of all, as part of my role, I get to work closely with Capital One on some really cool stuff, including some of the things that you'll hear about today. But also because resilience is actually one of my favorite topics when it comes to cloud. It cuts across every industry. It's important to everyone. And you may or may not know this, but Capital One is actually really, really good at resilience. Uh, they're often regarded across AWS as a world class organization when it comes to resilience practices, so I'm really excited for you guys to hear about it today. I've got with me two leaders from their enterprise platform organization. And I'm gonna talk to you about how Capital One goes about building highly resilient platforms for business critical applications that run a massive scale. They're gonna talk a little bit about how they got to where they are today, some of the challenges they faced early on, some of the decisions that they made along the way. And some of the best practices and lessons learned. But they're not gonna stop there they're gonna talk to you also about where they're headed, so. Some of the more advanced resilience patterns that they're implementing now to continue to build more resilience into this business critical applications because as you know when it comes to the stuff that runs your business you always have to continue to raise the bar. So welcome again thank you for joining us and with that said, I'm gonna pass it over to Sovan. Thanks, thanks for setting the stage for us. Uh, first of all, good afternoon, everyone. Thanks for joining us today. I'll start with a simple question. How many of you are using any of Capital One apps, like our shopping, banking, or any of our services? Lot of you So, our, all of these services apps are built on top of our modern platforms. So today we are going to focus on our journey as a platform company. And what are the principles that we believe in building the platforms? And our focus on reliability, the architectural patterns. And the best practices that we adopted to gain our customer trust. Platforms. The platforms are nothing but a digital Lego blocks that are leveraged to build our customer capabilities, our customer. Uh, functionalities. So, which we can use to build similarly like a, a vibrant city with a Lego box. So before 2019, We used to have duplicated capabilities across our lines of business. For example, our line of business like card, bank, used to build the payment capabilities. Or like transaction processing capabilities. As per to support their business needs. So we observed in non-standardized architectural patterns. A duplication of data. For example, if I'm a customer that have both a banking account and as well as a credit card account, my customer data is duplicated across these lines of businesses. In a different format. And we spent a lot of development, developer effort and the runtime engine cost. With that, we incurred. And we have an impact on Our speed to market our business capabilities. And also scaling out platforms. And in turn, it resulted to our customer trust. So in 2019, we declared ourselves as a platform organization. So we put a heavy investment. And building the foundational structures or platforms. That are not just reusable capabilities, but also that are scalable, reliable. That untrustworthy. So some of the platforms that we built throughout this journey, there are multiple flavors, flavors of these platforms that can be leveraged by both our internal customers like our developers, our LOB partners, and as well as our business, businessholders. And also our external customers and the end users. Some of them are like the developer partners, uh, platforms. Like the CICD platform. Where we streamline our deployment processes, with a lot of controls, guardrails embedded into it. So that every engineering team or an engineer can benefit out of it, and they don't need to reinvent the wheel. Similarly, we. Resolve the arbitrary uniqueness across lines of business by building the core business platforms like a transaction processing platform. Or payment processing platform. To provide the support across a seamless experience across all lines of businesses. And we have invested building like customer interacting platforms, like identity platform or a messaging platform. That provides a seamless customer experience. Irrespective of what type of platforms that we build on. Our objective is how we can scale as a. organization. We can scale. These platforms can scale based upon the trust that we get from our users. So, to get that, secure that foundational trust, We believe Our platforms, our modern platform has to stand on 7 foundational pillars. We are going to, all of these 7 pillars are critical, but today we'll be focusing on our first pillar, reliability. I'll take an example of like, if I'm a customer and trying to make a payment. And my deadline is today. I used the Capital One app. My payment was not successful on the first attempt. I'd retried it. It's still not gone through. As a customer, I look at my other options to make that payment. So, with that, I'll, as a company, I'm losing my customer trust, and also the business. So in this like journey, the availability of your platform and the resiliency are very important. So your availability is going to be measured based upon your up time. So uptime is a classic matrix that can help to define your service is operational or not. And your success rate. Even though the service is up and running, if the customer transactions are ending up with an errors. That means they are not happy. So how can we measure The success rate. So we aggregate all the requests that we receive as a platform, and out of it, how many are actually error out. So as an industry standard, we measure using those with nines. Resilience plays a critical role. Even though we build, as you all know, we still face failures. While we are designing these platforms, how can we ensure these platforms can be recovered from those failure points, unexpected chaos? So the fault tolerance, when I'm, when we design a building a bridge, With a structure redundant pillars, we always ensure if something goes wrong with one of those structures, the, the bridge is still up. Similarly, when we are building a platform, we need to ensure if a component is failed. Is our platform still serviceable? And we also measure our mean time to recover. So when a failure happens, How quickly we are going to identify the failure? How can we isolate from that failure? How quickly we can auto resolve from that failure? These are very critical. Reliability. Reliability, I, I treat it as an engine for trust. So along with availability and as well as resiliency, it is also very important. The capabilities that are provided by our platform are functioning as intended over a period of time. That is going to ensure the trust with our platform, with our users. Our users, our customers are going to provide their data, spend their time, rely on our systems. So, with a lot of trust on these services, as a platform builders, owners, we need to ensure how we can make sure our capabilities are working as expected. And also reliable, scalable, and trustworthy. To get more details on the how part, I'm going to hand it over to Aaron, who can get into the details of our architectural practices and best, best practices that we adopted throughout this journey. Thank you, Solburn. Cool. Nice. All the good things have to start like this, OK? Cool. So I have about 47 minutes, a lot of engineering content relating to uh reliability engineering and resilience engineer and capital and approach to accomplish those. Uh, so I'm going to dive deep. So there are two focus areas in my talk. So in my first focus area, I'm going to dive deep into the architectural strategies that we follow and architectural pitfalls we had in the past, and our architectural journey. Basically, our architectural evolution towards the high availability and resiliency. And then I will talk about our surveillance adoption, like why we are doing surveillance in Capital One and how it is improving the reliability aspects of our system that we built. Um, a lot of times I tend to see when we talk about, uh, resiliency and reliability, we just stuck with the architecture, right? So there are a lot of non-architectural experts that impacts your reliability of the system. So I'm going to dive deep into the different failure modes that could impact your platform services, and then I will talk about the zero downtime deployments using AWS code deploy, uh, code deploy, excuse me. And then I will also use about, uh, I'll talk about the reliable infrastructure building using the AWS. CD cases And then I will jump on to uh resiliency testing framework that we built using the AWS FIS. And then I will finally wrap it up with the observability standards. Resilient architecture, right? So architecture is the foundational building blocks to build any system. So when I talk about architecture, I'm talking about the system architecture and as well as the deployment architecture. So these are all the building blocks and resiliency and reliability shouldn't be an afterthought. The moment we go build something into the architecture, if we add more components in the architecture, we need to bring the shift lift, uh, thought process in about the reliability into the architecture. So, let me start with some of the anti-patterns that we had in the past, in the pre-platform world, because Shobman talked about some of the non-standard architectures and the pitfalls that we had. Let's start with this. We call this as a all in one approach. Uh, you can see a lot of single point of failures in this approach, right? So we have all these platform capabilities packaged into a single monolith. It deploys into a single AWS region with a single availability zone, with no auto-scaling and no database replication. Right? This pattern is a big no in Capital One. From the day one of Capital One cloud migration journey, we made a clear decision that we are not going to deploy in a single region or with many single point of failures. So we had always deployed our services into two regions. Then you might be wondering why I'm talking about this now, right? We had a recent US East One outage in AWS in October. All of our systems were able to quickly recover from it by auto failover to the other region. But I have seen like some of the systems outside of Capital One had struggle during this outage, right, because they were heavily reliant on a single region with no other region to run their systems. This is a big no. If in 2026, if you're still thinking about having multiple regions, you're in this room because you think that resiliency is important. You cannot take your car to a road without an insurance, right? The same concept. You have to have multiple regions for higher resiliency and higher reliability. So avoid single point of failures. It's one architectural bat pattern. The next one that we have seen in our pre-platform paradigm is having this multiple uh multiple uh proliferation of microservices with zero fault tolerance. So, the moment we thought about Monolith move to a microservices based approach, we tend to create a lot of microservices with a tightly coupled nature. So if one service goes down, it impacts all the other services. So when we think about that, microservices, think about it in a way that every single service should be able to. Uh, service and capability and avoid cascading failures. So this example, if one service goes down here, the customer is impacted because all these services are tightly coupled in order to provide a capability to the customer. So this is an architectural anti-pattern for high resiliency, for a low resiliency. And then we'll talk about uh another deployment. Uh, pattern we had was in the pre-platform paradigm. We were, this is the deployment anti-pattern where we had a single cluster that was sharing the same uh traffic patterns. So we had web and mobile customers using the real-time clusters and also at the same time, the batch load was hitting our real-time clusters. So this cast pressure into our systems, a lot of database timeouts, and it impacted the real-time users' experience. So with this, you cannot reach a high availability. So this is an anti-pattern. All the systems that we are building in the modern platform world, we have routed all the batch traffic into a separate cluster by using step functions or other vendor products for a batch execution model. The other one that we had in the pre-platform paradigm was having the single database that is used to share between the analytical loads and the real-time customer loads. So whenever the analytical job runs, it goes and hacks the system resources. When it hacks the system resources, we started getting a lot of database timeouts on Throttle, and it impacted our user experience. So this is another anti-pattern that existed in our pre-platform world. Cool. So I talked about most of the anti-patterns that impacted our high reliability. Let's talk about some of the patterns that we follow and what are the enterprise architectural standards we follow. When it comes to deployment, we all the time deploy into two regions. All our CACD pipeline will deploy the same version of software into the two different regions, and all of our services are scaled out so that they are anytime capable of taking the load in one region. And when it comes to the architectural dependency, No single region should have a tight coupling or any dependency with the cross region. So all these services should have the regional affinity. Think of this, when you fail over, To one region to the other region because your region one is impaired, but if you still have a dependency with the empire region, you are not solving the problem. So you're still defeating the purpose of having two regions. So our guidance for you is all these regions services should have the regional affinity and should operate in a silo. If you have any other dependencies in our systems, we have other platform dependencies within Capital One systems, so we maintain the same level of resiliency standards across all of our systems. So if I have a top tier resiliency standards, all my dependencies should also follow the same resiliency standards. And when it comes to data consistency, we always tend to choose the databases that are auto replicable. When it comes to failure recovery management, any single region failure should not cause impact to our service capabilities, and all of our services should be equipped with auto failover, and all of our platform services have a very strict RTO and RPO defined. So the bottom line here is when we are aiming for building a high reliable system, we need to clearly define what are the goals we are trying to achieve. Without goals, you cannot track your resilience, your reliability goals, aspirations. So let's move on to the architectural patterns. Putting the words into a diagram. You can see, we follow the domain-driven design pattern. All of our modern platforms follow domain-driven different pattern because domain-driven design gives us uh high flexibility. This creates the modular services which are highly decoupled and fault tolerant. To think of this in this way, think of this uh bigger domain and divide this bigger domain into multiple subdomains. These subdomains have their own entities, aggregates, and their own database and layers, and they heavily operate on a bounded context. Now think this into a platform model. You have a platform. This platform has multiple capabilities. Each capability has its own entities, aggregate models, and value objects, and they do not depend on each other. So every capability runs in a single bounded context with no dependency with the other capability. With the banking example, not this banking domain as a larger domain. You can divide this multiple banking domain into subdomains like a transaction processing service, a reporting service, customer management, and account service. The bottom line here is these subdomains are operate independently and every capability has its own SLAs. You are not. Targeting all these services to be 59 available or 39 available, right? So you can set different uptime SLAs and response time SLAs, and domain-driven design is best for this. What are the other benefits that we have? These domain-driven designs are modular, which means those services are easy to maintain, repair, and replace. And they operate in a bounded context, in a highly decoupled manner, fault tolerant, and they are independently scalable, and these service capabilities of gives you the SLA flexibility. And not only that, our customers. Now pick and choose the capability they want to connect to. For example, our finance will use a reporting module, whereas our card and bank will use capabilities like transaction processing. So this offers the extreme flexibility. So now putting these words into a diagram. So this is the minimum requirement to deploy in Capital 12 regions, multiple availability zone, domain-driven design with bounded context services, auto scaling enabled, and you have a database that's auto replicate, replicate the data. And then we have this R53 record sets that has two types, right? One, we have a geo-based location. So every request that comes from the customer goes to the nearest data center of the customer, and after that we also have the failover record set. This failover record set is the mechanism for us to do the auto failover. This failover record set has two routes. The primary route goes to the nearest region of the customer. The secondary route always. Takes care of the uh failover mechanisms. It keeps an eye on the other region's health, and the moment we find out that your primary region is faulty, impaired, this auto failover will kick in using the R53 auto failover record set. All of that is done automatically. So this is the minimum deployment architecture requirement for us, but this is giving us guaranteed 3.9 availability up to 4.9 availability. But there are some mission critical applications that require fin and availability, and let's look into what are the challenges we have in this architecture pattern. So now imagine you have 10 customers, and you have 2 regions, and you have these bounded contact services and all the good things I talked about. Let me introduce something called poison pill request. Poison pill requests are type of requests that are capable of killing your services every single time they get processed, right? It could cause memory outage or it could cause some saturation into your resources, or it causes high database timeouts or retrace storms. But the bottom line here is every single time it causes an outage very consistently. Now think this poison pill, and apply it to our architecture. Customer C2 is sending a poison pill request, and we have the router that sends the request to the region 1. The region will have multiple tasks. So now this this possible request goes to the service task one. The service task 1 will crash. We have an ALB in front of the service tasks. So the ALB operates in a round robin manner. Every time when a customer sends a request, it goes to the service task 1, crashes, service task 2 crashes, 33 crashes. Now guess what? Now we have our auto failover. It will kick in because our auto failover will think the region one is impaired because all the services are getting crashed. We have alerts based on the unhealthy host count and HTTP error codes. So now the R53 will think that our region is down. So it will automatically flip the traffic to the region too. But now what will happen? The customer is still sending the poison pill request. Now the request will go to the region too. In the region too, it will start killing the task one after the other. So now, it is more bad than the previous situation, right? So you are not only impacting the region one, you are also now impacting the region too. Now, how are we solving this problem? We are creating a circuit breaker. Typically, the circuit breaker is used in the microservices world when we have a dependency, the dependency is struggling. We do not want to overwhelm the dependency, so we'll wait for a minute. We are shifting left that circuit breaker capability into our router. Our circuit breaker has a customer level metrics, so our circuit breakers are enabled with observability that tracks poison pull requests from a customer level. The moment we find out the customer is sending a poison pill request, we'll open the circuit and we'll stop taking the request. We also have this router enabled with the rate limiters because sometimes our customers used to overrun the systems and cause reliability issues. Even though we have these two capabilities, these are more reactive capabilities, right? Because when we talk about finance, your budget for error or a downtime in a year is 5 minutes. When these things happen, you already breached that, uh, uh, reliability, uh, aspirations. So your reliability goals are down. This is a reactive system. What are we doing now? Think of this. So we have been designing sharding techniques. We went with this edition one using the standard shing pattern where we have these 3 shots, 3 customer groups, and we have a consistent hashing algorithm that always creates a stickiness session, right? So customer group 1 always gets attached to the shard 1, Group 2 goes to shot 2, group 3 goes to the shot 3. Now, Imagine now customer C6 is sending the poison pill request. It goes to the shard one and then it completely takes down the shard one, but your other customer groups are not impacted because now your request is going to the shard one. Is this good enough? This is not good enough for us because now imagine that we did not have the circuit breaker or we did not have these uh rate limiters and things like that. We used to have the blast radius of 100%. Now after the shard technique, now our blast radius is reduced to 33%. Now out of 10 customers, you're only impacting 3 customers because the other customers are having the healthy shots. Is this good enough for us to build the finance services? Definitely not. So where we are heading towards is the suffle sharing patterns. I'm pretty sure Everyone in this room would have played playing cards, right? So you have the deck of playing cards and then you shuffle the cards and you have multiple players and then you will create the combinations and then you will deal 57, or 3 cards according to the game, and then you will try to create the combinations. The same concept is applied here. So imagine you have this deck of cards as the deck of shots. So you have now multiple shots. Now you have multiple customers. So you create the multiple combination of shots and you will, every time you onboard a customer, you will create a unique combination and you will attach a particular shot to a customer. So, in this diagram, you can take a close look into the customer 1 and customer 3. They are having, in the middle block, they're having the same shard, shard 30. Now think of this poison situation. Customer one is sending a poison request. It will only impact the shot that he belongs to shot 1, shot 30, and shot 70, even though customer 3 shot 30 is impacted, the customer 3 has other two healthy shots. So either the customer retries or internally when we retry, the customers 3's request will go to the shot 7 or shot 99. At the same time, if the shot 30 had a problem, if there is a database problem or a short problem, The customer 1 or customer 3, either of them are not impacted because they have the other healthy shots to use. This is the pattern that has been proven to be very useful for us to achieve the finance. How do we create these multiple shot combinations? So that is this mathematical formula binomial coefficient. You key in these different values. In my examples, I have these 100 shots, a million customers, and I'm trying to assign 3 shots per customer. So with this mathematical formula, I'm getting 1,061,700 unique combinations. These unique combinations of shots will be split across the different customers. In this case, a million customers. So the success of this model will depends on how do you reduce the overlapping. So with this formula, at any given point of time, 7 customers will have the same combination of shots. So now think of this. From the 100% blast radius to the 33% of blast radius, we have come down to 0.0007% of blast radius because of the sole sharing technique. So this has been proven to be very helpful for us to reach the fine end for some of the mission critical applications. Where do we put this logic? So this is a sample Python code to create the different short combinations. So you key in your number of customers, the number of nodes you want, and the chart per customers you want. Then this code will output the number of different chart combinations that you want to create. In this example, you can see customer C1 is having S2, S4, and S5, and customer 100 is having S2, S4, and That's 5. Where does this logic sit? So we have this router. This router is where this logic sits. So every single time when a customer is on boarded, this router will create the shuffle. The router will create the combinations of the shuffle, and it will attach it to the customer with a durable storage. So we have the database attached to the router, so the router will create this combination and will create the stickiness with the customer and the number of shots. And also Before the shots, we also have the load balancer. The load balancer by nature, by default is a round robin algorithm. So every single time a customer sends a request, in this example, you can take a close look into customer C1. Imagine C1 is sending a request at first with, he has a shard S1, S21 S6. The first time the router will know, so this is the first request the customer is sending, and the shard he belongs to is S1, S2 and X6, and I have the round robin algorithm. Uh, built within me. So it will set the header as S1. So now this header will send it to the uh load balancer. This load balancer based on the header-based routing, will send it to the shard. And the second time when the customer send the request, the router will attach the header as S2, and the request will go to the S2. On the third time, S6, it goes to the S6, and so on and so forth. When you think of the shard, you can think of this shard as target groups in easiest tasks, right? Every shard that I've shown is the ECS target group. So they can be independently scaled within that shard. There was another way of achieving this. We explored this way of achieving this by shift lift the skip, the sharding mechanism into an SDK. So our platforms have an SDK where this code that I've shown. lies on this uh SDK. This SDK now knows the back, backend capability, like, what is the capacity of the backend, how many shots are running. And how many customers are going to be on boarded and how many shots per customer I need to assign. So this SDK will create these combinations and create the headers, and every single time it'll send the request to the load balancer. With this approach, you avoid the router, but the biggest problem with this approach is that every single time you make a code change or if you want to make a version change on the SDK, you have to chase all your customers and have them enforce them to take the latest version of the SDK. So for this reason, maintaining this SDK will be a challenging aspect. So we are pretty much going to incline with the router approach. We talked a lot about architecture. What are the recommendations, right? So, multiple regions, multiple availability zones, and uh use the domain-driven design if applicable to you. And Use auto scaling, retrace dromes, and uh try to use fail fast timeouts because we had some reliability issues due to the high timeouts and use some of the sharding techniques that fits best for your use cases, uh, because managing the infrastructure for the sharding will be a challenging for you. And beware of different failure modes. I'm going to talk about different failure modes uh in the later part of the session. And uh when we talk about finance, we're not talking about all your platform capability must be financed, right? Because the, only the mission critical capability should be finance. It is very hard to uh manage a finance service. Good We are also a surveillance company. Why are we moving to the servers? Because we had some critical loads that were running on EC2 instances, and that caused some reliability issues. I will give you some couple of examples. So we had these docker containers running on the EC2 instances, and whenever we want to scale out, we had a few incidents like where the specific instance type was not available. The IPSR ran out, so we couldn't auto scale in a timely manner. And also whenever we want to scale, EC2 takes a lot of lead time to uh come up, right? You need to bring out uh EC2, run the user scripts, and then bring the docker up. It takes a lot of time uh for the EC2 based uh instances to come and uh uh take the request. Not only that, it also created some operational problems. So managing a fleet of thousands of nodes of EC2 instances and every single time we have to patch them, manage them, it was a big human toil for us, right? And it also created some reliability issues when we tried to update EC-2s. We, we did some manual errors and then it caused some outages. That's the reason that we moved to uh uh Savelas. So with Savelas, none of these manual operations are anymore exist, and we are able to like uh reliably scale out, and all of our operational overhead has been taken care, right? Because if you are in cloud, if you're still managing the EC2 instances, uh, you have to consider that, right? You have to reconsider that, try to adapt to a surveless technology, and especially all of our critical mission critical platforms are running with uh ECS target services. When you talk about AWS lambda, I want to make a note of here, a note, note out of it here, right? So AWS lambdas that we use on a non-critical asynchronous path, because in the past we had some reliability issues. Make sure that whenever you run the AWS lambda functions, you create bulkhead. Patterns between these functions by setting the maximum concurrency limit to the lambdas because they have the tendency to go and occupy the entire account level concurrency limits if you're not careful about that, so that will cause some reliability incidents to you. And these are all the different uh surveillance uh uh functions that we surveillance services that we use in Capital One, and we are greatly getting benefited. This has been the biggest game changer for us. It's been operationally, we are really efficient uh with this uh adoption. Let's talk about failure modes. Failure modes, we constantly review our different failure modes, right? So these are all the failure modes we look into. Your cloud provider will fail you. Your internal platforms dependency could fail you, your external vendors could fail you. Our customers can send a poison request and fail your systems, or your own platform engineers will cost some bucks and create a reliability problem. And the last one is the untrusted code. This is a very special condition where we are building a platform where we take this business logic from our internal developers from our different business units. Like for instance, we have a card business unit and bank business unit. They will pack their business logic in a code and they'll ship it to us and we will execute it in our platform. So this is not causing a security issue. This is causing a reliability issue. I want to talk about that a little bit here. We had this design in our version one, where our services and this untrusted code were sharing the same VM resources, right? So, what happens when you get a request, we'll call this untrusted uh code function. And in a separate thread, even though they run, we run a separate thread, they share the underlying JVMs and underlying VMs and things like that. Now imagine this untrusted code at something like a system.exit or it has some wild true loop. The code is constantly running nonstop or it has a recursion it causing some stack overflow errors. There are possibilities either intentionally or. Intentionally, our internal developers could bring reliability issues into these platforms like this. So for this kind of platforms, we are moving towards the sandbox safety model. So in the sandbox safety model, we'll create multiple micro JVMs. So these multiple micro JVMs are heavily built with bounded context. So this service with VMs are never shared with these micro VMs. Our untrustood code will be allocated a very fixed capacity of let's say 10 MB. And we set a very strict timeline of execution, like the timeouts for every single function execution, and we will strictly prohibit. The IO access paths. So this untrusted code can never access the critical file system path and cause an outage. So if in case if you're running a malicious code inside this untrusted block of sandbox, whatever happens between the sandbox will just stay within the sandbox. It will never impact your service because you're not sharing anything with the untrusted code. If you are building a platform similar to this, I highly recommend you to go and read about the sandbox safety approach. Cool. So that's a lot of non-archi a lot of architectural uh context. And let's move on to some of the non-architectural uh context. What are the challenges we had? So we talked about a lot of infrastructure management and things like that. One of the biggest challenges we had was infrastructure management. Why? Because our infrastructure as a Generation 1 pipeline, our generation 1 pipeline was heavily AML-based, so there are multiple values, multiple environments, multiple copy paste, and it was used to be a lot of manual errors and it caused some reliability problems. For to give you a couple of examples, we had some port misconfiguration. Instead of port 8080, someone did port 8808, they missed the last digit, right? That cost and reliability instant. And our pipeline did not have drift detection capability, so we had one set of setup is running in the cloud, but your repository is completely different. So the next time when you go and create it the infrastructure, you're creating the infrastructure with the faulty configuration, and that has caused a reliability incident. What we are doing We are digging. Quantum leap. We are trying to mitigate this with complete automation using AWS CDK. Because when we talk about reliability, architecture is one thing, and these non-architectural things that infrastructures and code releases have to support your reliability goals. So I talked about CDK. So now we are taking this quantum leap from infrastructure as configuration into infrastructure as code. We manage our complete infrastructure using code. We use the AWS CDK for that. AWCDK is the cloud. Development kit. Now think of this, you have some R53s, ALBs, and ECS containers and things like that. If I say all of this can be coded. Instead of using a vendor-specific domain-specific language or AML file. All of this can be coded in an imperative programming language way, such as TypeScripts, Java, Go, or Python, things like that, right? So, it unblocks the capability. This is an example of an AWSCDK code where I'm trying to create uh ECS Fargate service behind an ALB. And whenever you create this, this underneath creates a cloud formation template, and the infrastructure is managed using the cloud formation templates, and it unblocks the capability of writing test cases. So the moment you write code, you now can write code to test your infrastructure. So in this example, you can see, I'm trying to verify whether I'm using the ALB with the right SSL certificate. If not, I'm not going to create this. This test case will fail in my developer mission and as well as in the CICD pipeline. So this is the shift lift testing capability. It's the biggest game changer for us. We're no longer creating faulty infrastructures or causing incidents. Another example of how do you find this uh Bugs. So let's imagine that you are trying to create an ECS task with 256 CPU and some memory that is probably non-standard for you. So this can be caught much earlier because in the V1 of our CICD. Plan we used to create the resources in the cloud and then later we figured out, OK, we created with the faulty configuration and then we'll go back, destroy the resources, and then we'll rerun, fix it, correct it. So we'll do this back and forth of fixing and making sure the infrastructure we created was correct and it also caused some reliability issues and also it was too much impact to developer productivity and cloud cost. We also run infrastructure rules as code using the CDK NA, so we have multiple rules that we run, like naming standards. The ALB style port configurations, or do you have the right SSL ports open, right? Things like that are also managed using CDK NAG. So infrastructure as code offers this capability for you, and all these rules we run in the developer machine and in your pipeline before we get created in the cloud. Let's talk about some of the release techniques that we have. So, Whenever we do releases, we never take down these services, right? We should not. So we do the releases while the customers are using the services. Let me bring back this shuffle sharing diagram here. Just now imagine that we have a new version, version in the blue box, version N + 1. Now this blue box is ready to go to production. Now, instead of applying to all these different shards, I will pick and choose which shard and which particular task I should go and update, right? Now, in this example, customer group one, I will release to this customer group one, I will just drop the code into a particular task and then replace, and I will constantly monitor the behavior of the new version. And if I get confident, then I will automatically roll the new version to the other shots and the other shots and other shots and on. How are we doing this? It's completely using the code deploy. So we do not have any manual release process. So since ever since we moved to the AWS Fate services, uh, the surveillance services, we are using code deploys. So code deploy with the appspect deployment configuration files, we will just, just orchestrate our releases so we can say what is our roll forward strategy, what is our rollback strategy, and things like that, and code deploy will takes care of everything for you. The biggest force multiplier of code deploy is the deployment life cycle hoax. So I've highlighted three examples here before install, after install, and before allowed traffic. Before you install a particular version of the software, make sure that this software has all the right configurations. And after you install, Before you service the customer traffic, make sure the release is actually correct by running some synthetic functional test, right, using the synthetic data. So we run some functional tests in production using some synthetic data and ensure that the release is good and we are not going to break any reliability. And before we allow the customer traffic, because some of our systems had some cold start problem, so we use this capability, this life cycle hook, to warm up our containers. So this is a lambda example that listens to all these different life cycle events. So for every life cycle event, we will run a different pre-validation steps. Another great part is that all of this is maintained in AWSCDK, include not only creating our infrastructures, also doing the releases is maintained in AWSCDK as code so that we test out everything before we create. Like I said, we use the code deploy to do the gradual rollout, and we have this warranty period before we completely uh route the customer traffic to the new version, we will use the code deploy's linear gradual growth for traffic routing. This is in another check that we do called readiness checks. We had some reliability incidents where, to give you an example, we had introduced a new capability that required a Dynamo DB table access, and we had an IAM bug. So what happened? We opened the customer traffic and then the customer use case was failed because the IIM policy was not updated. So that incident taught us a lesson. We should shift up the capability of checking the readiness on the application side. So our applications, before they even come up, they do this, they call this custom IIM simulate simulate policy endpoint, and they will think, OK, I have, I have to access these many services. Do I have the right Access before I take the customer traffic, before I say I'm healthy and attach it to the ALB, this application itself will do several checks before it takes the customer traffic, and this is again a game changer for us with that, a lot of reliability reliability issues related to code releases have been gone. We also have this automated failover. And also we have the automated failback. So the moment we fail over, we will create an event. That event will be going to the AWS lambda, and the AWS lambda will listen to this failover event, and it will kick in, and then it will start sending a particular load to the unhealthy region, and it will ensure that the unhealthy region has come back up. Sometimes we use the synthetic load, and sometimes we'll open up a little bit of traffic, customer traffic, to check the other region is good. So all of this is automated. So none of this, either the failover or failback is manual. I wanted to spend a minute here. Some of our critical mission critical platforms require zero RTO RPO goals, and they also require cross-regional consistencies. Uh, Think of this. You are a new customer with a $0 balance. You're now trying to make a $1 deposit, and we have Dynamo DB Global table. So your request goes to the region one. Your $1 deposit goes to the region one. We successfully processed it and we persisted the data in the Dynamo DB global table. But the data, in order for the data to replicate to the other region, there is no guaranteed SLA. It will be eventually available on the other region, right? It could take a few milliseconds. A Few seconds or even a few days or hours we do not know, but it'll be even now think that you made a $1 deposit to the region one and we failed over to region two. Now you're trying to make another $1 payment or a deposit. It goes to the region two, but now after the second transaction, your balance will be still $1 because the other transaction you made on the region one have not replicated to the region two. This has been a problem for some of our mission critical platforms, which requires cross-region consistency. So what we're exploring here is the Dynamo DB multi-region strong consistency in which every single time you write it to the Dynamo DB, it will durably write it to the other region in a strongly consistent manner, so that you fail over, you go read on the other region, your data is available. Some of our learnings. Using the cold star, right, so there are a couple of things, uh, that impacted our high availability because I'm talking about this finance because when you talk about finance your downtime, uh, budget is very low, your error budget is very low. So there were two scenarios, uh, where we were impacted. One is the cold start immediately after the release, and the other one is when there is a long lull period. So customer comes, uses your platform, and there is a lull period. They'll come back. They use the platform again. So every single time. This pattern is repeated, you will see a spike in the response time. Why this is a problem? Because we have set a clear response time of 500 milliseconds, and every single time we breach that SLA, we will count that as not available, even though the request was successful. It breached the response time. Hence it counted as not available. How are we mitigating this problem? We are mitigating this problem using the code deploy deployment hooks. So the moment we deploy, the code deploy deployment hook will know the new version is deployed. Let me go and use this pre-warming script that I have. So with that we are able to eliminate the cold start problems. Real resilience testing as code is another bigger area that we have been doing. You can also call that as a chaos engineering. If you remember, I talked about the different failure modes. So, what is the outcome of the different failure modes? So, we will decide for every failure mode, we will apply a what if scenario. What if your cloud provider region one has gone down? What if your cloud providers. Couple of availabilities have gone down. What if the platform had a bug? So we will convert the different failure modes into several what if scenarios. After we convert several what if scenarios, we will try to reverse those scenarios in a very controlled environment. When we reverse those environments, it helps in two ways. One, it helps to check whether our alerts are working properly, our runbooks are up to date, and it helps tremendously our engineers to prepare and Be prepared when the real things happen, right? So this is where we are getting the biggest benefit because it constantly trains your engineers. How are we doing this? There's several different what if scenario. We are creating an AW we are creating an AWS FAS based SDK where we'll use this Gherkin format and then we'll define uh in a simple Gherkin format, the different failure scenarios, and then our SDK will convert into the failure scenario testing. We also do game days. In our game days, we will completely isolate traffic to one of the regions, and we will see whether our systems are able to independently operate in a single region. So I talked about regional affinity. So how do we test the regional affinity by doing this? So we'll completely isolate and we will ensure the systems running in the single region is able to take up the load and there is no cross-national dependency. Resilience testing is the way you test your architecture's resiliency, right? We also do several observability related standardization. We standardize all our logging, so all our loggings are based on structured logging. This helps to immediately identify the problems and reduce the mean time to identify the issues. And we also standardize the several metrics, metrics related to saturation, metrics related to availability and error budget and things like that. And all our platforms are enabled with tracing. So we have a distributed tracing, and we also have a continuous profiling. Every single time, we will run a continuous profiling on different versions of the same function, and we use the histograms to compare the performance of the same function with the different versions. And we also do the error code standardization because error code standardization is very important because some errors need a failover, some errors need minus 1. Some error codes might need you to retry. So you cannot have a single error code for all your resiliency requirements. So standardizing the error code is another important thing that we follow in Capital One. And we constantly measure these KPIs, what are our mean time between the failure, meantime to identify an issue, and meantime to recover from an issue and things like that. And another important thing that we do or we have been doing in this space is moving away from static alerts into dynamic alerts. We had this problem where if you look into the left side, we had the static alerts for every 5 minutes window. This alert Right now, uh, measures a count of error that is greater than 100 for 5 minutes. Now imagine you have for every 5 minutes, you have 99 requests and all the 99 have failed. If that happens, this alert will never go off, right? Because even though you're a 100% failure, this alert is based on a static threshold, which is the count of error. In this scenario it will never go off. So what we are doing is we are moving to a more dynamic based alerts based on the different traffic patterns, our low traffic analysis, medium traffic patterns, and then high throughput patterns. For different patterns we have different dynamic thresholds and error budgets. With this, there is no way we are missing any alerts. In the static alerts, we used to miss a lot of alerts because we are tracking a static number. And then we have this kind of a dashboard in all our platforms where we have multiple shelves of 5 minute intervals. These green boxes are where our thresholds were not breached, and white boxes is where there were no requests come to our platform, and they're considered as good, and our red boxes is where we breached the threshold, and we constantly review these dashboards every day, every week, every month. Another important aspect about running reliable systems and resilient systems is using these bulkhead patterns. Uh, we had an incident where our service task and our observability tool were both sharing the same resources, right? And our observability back end was. A problem and that was creating a back pressure to our service task and it was impacting the throughput of the service task. So that incident cost us taught us a lesson that we need to create and create a bounded context between these services and these auxiliary things like Loggings and audits, and retries and things like that. So, we are moving away from uh tightly coupled services into a bulkhead patterns for all of our auxiliary services. We use the sidecar patterns. Guess what? Because of the surveillance adoption, using this sidecar pattern has become very easy for us. Imagine if you're running on EC2 instance, if any of these auxiliary systems are having a pressure, it will completely occupy your easy to instances resources, and it will impact your services. With the surveillance adoption, we are able to create these sidecar patterns really well. Why are we doing all these things is to Uh, like we talked about in the beginning, we all wanted to create the trust between the customers, right? So customers trust, they're highly reliable, highly resilient, always on, always secure, always scalable systems. Not only that, there are other four C's customers' trust and competitive edge. The moment your systems are not reliable, your customers will start using the other systems and. For highly regulated industries as such, we have to be always adhere to our compliance and also the company's reputations. These are very important and that's why we take the resiliency very seriously. And every single time we make a code commit and we'll push it to the production, please make sure that you are responsible for the company's reputation and the customer's trust. That's pretty much what I got. Uh, thanks for tuning in. And I hope that you got something out of this, and you will bring it back and build reliable systems in your organizations. Thanks a lot.