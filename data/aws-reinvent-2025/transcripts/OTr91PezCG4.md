---
video_id: OTr91PezCG4
video_url: https://www.youtube.com/watch?v=OTr91PezCG4
summary: "In the panel session \"GenAI in Private Equity: Moving from Pilots to Performance\" (AIM276), hosted by Sanjay Subramanyan (PwC), a group of experts including Chai (Warburg Pincus), Ravi (Abrigo/Carlyle Portco), Chad (AWS), and Nate (PwC) dissects the status of Generative AI in the Private Equity (PE) sector three years after ChatGPT's debut. The conversation moves past the initial hype to the \"lived experience\" of implementation. Chai reveals that at Warburg Pincus, AI has transitioned from an \"optional curiosity\" to a mandatory component of every Investment Committee memo; deal teams must now explicitly defend their thesis against \"existential AI risk\"—essentially answering, \"Can two guys in a garage with an LLM replicate this business model?\" However, the panel stresses that \"AI for AI's sake\" is a trap. Success is found not in broad, shallow deployments (like generic corporate chatbots) but in targeting specific \"high-friction\" workflows. Ravi shares Abrigo’s strategy for their 2,500 community bank clients: the goal is not to replace the human credit analyst, but to use AI to draft credit memos, saving 10-20 minutes of \"undifferentiated grunt work\" per loan. This accumulation of small efficiencies drives margin expansion without sacrificing the high-touch relationships that community banks rely on. Conversely, Chai warns against automating high-value touchpoints—like specialized insurance underwriting—where an automated $5 million saving might cost the firm its competitive \"white glove\" differentiator. A significant portion of the discussion focuses on talent and culture. The bottleneck has shifted from Engineering (which is now faster than ever due to AI coding tools) to \"Left of Code\"—Product Management and User Experience. The scarce resource is no longer the Python developer, but the \"AI-literate business leader\" who can spot value. Chad from AWS advises against the \"paralysis of data governance,\" where firms delay AI adoption for years to \"clean their data lake.\" Instead, successful firms adopt a \"Tiger Team\" mentality, giving a small group the \"startup-like\" freedom to fail fast—often prototyping solutions in 24 hours rather than 18 months. The session concludes with a consensus on \"Defensive Diligence\" and \"Offensive Ops.\" On defense, PE firms must rigorously diligence the AI readiness of potential targets—not just their tech stack, but their \"human infrastructure.\" On offense, portfolio companies should prioritize huge, bold ideas but execute them in \"bite-sized\" experiments to build momentum and silence \"transformation antibodies\" (internal skeptics). As Chad summarizes, the winners in this new era aren't those with the cleanest data, but those with the most aligned leadership and the courage to experiment rapidly in the face of uncertainty."
keywords: Private Equity, Generative AI, Due Diligence, Value Creation, Workflow Automation, High-Friction Workflows, Tiger Teams, Investment Committee, AI Risk
is_generated: False
is_translatable: True
---

Robbie. Right. Bright lights, Bright lights, fantastic. After lunch, always a perfect time to chat. Uh, well, listen, thank you all for taking the time to spend with us today to talk about Gen AI and private equity. Um, first of all, I'd like to thank all of you for taking the time to spend with us today. Um, and it's been great catching up with a number of you, uh, that I'm seeing in the audience, uh, and meeting some of you for the first time. Uh, and also like to thank AWS for just hosting such a wonderful conference. Uh, it's been fantastic to date, uh, and look forward to what comes, what comes next. We've, you know, we've heard a lot about um. The expectations of AI, we're talking about this every moment of the day, uh, and one of the great things about being in the private equity sector is we get to see firsthand on a regular basis, across industry, what's real, what's not. And so what we wanna do today is talk a little bit, not about the hype, not about the theory of AI, but the practical lived experiences of those who are actually dealing with this on a regular basis. And and we've got leaders here today that come at it from different perspectives. Whether it's from the uh as an operator in a private equity, as a, as a, you know, uh a CTO, a CPO in a portfolio company having to actually manifest and create change. Uh, whether it be AWS, who's leading the charge in so many of these areas, um, and, or Nate Barnes, you know, who's spending a ton of his time as our lead data scientist helping to transform private equities. So before we begin, let's introduce, let's introduce us and uh think of questions as you go through, because we'll be asking some, you know, questions, uh, you know, to the panel, but we want to get questions from each of you in the audience. So some background real quickly on me, my name is Sanjay Subramanyan. I'm a partner in PWC. I lead a lot of our alliances and our cloud, uh, data and AI and engineering business, and I work alongside a number of my colleagues in the private equity space predominantly, um, and have had the pleasure of experiencing a lot of the ups and downs of that journey over the last few years. Uh, next to me, we have Chai, who's a Wahlburg Pincus, uh, operating partner who who joined a Wahlberg Pincus with deep analytical capability, deep AI experience over the years, and whose job it is to help transform the portcos under his supervision and transform how Wahlberg Pincus as an entity thinks about embedding AI into how it does, does its work day to day. We have uh Robbie, um who sits there, who's who's who's sitting next to me, um, who is a CTO and CPO of um of Ab Brigo, Carlisle Portco, who's really had to live day to day on how to manifest change within his organization. And you'll hear from him about the successes and also some of the struggles along the way. Um, and some recommendations for for the rest of us in terms of what success looks like, but also areas where we should avoid, right? We have Chad Burnick, principal solutions architect for AWS in the PE space, who's been spending a ton of time working with hundreds, if not thousands of different Uh, PE clients and portfolio companies in helping them transform. And last but not least, we've got Nate Barnes, who's the chief data scientist for PWC's deals practice. He sits in our, uh, value creation team and has really been helping both diligence um companies from a sell side and a buy side perspective and the impact of AI on, on that thesis, but also helping companies transform um throughout their portfolio. So with that said, Um, let's go on to, to this discussion. So it's, it was, what was interesting was, uh, you know, it was only a few days ago, I think it was November 30th, which was the third anniversary of the, uh, chat GBT being released, and that, and I remember that time, we all remember that time, there was a lot of excitement, um, everyone was talking about the change that could happen, would happen, um, but. The four of you have had actually lived experiences of what actually happened, but I wanna maybe go back to that moment, right, and Shai, can you just maybe start with you? What was sort of your expectations? What were you, what were, what were you thinking at that time? Yeah, no, it's, it's a good question, um. I think at least at Warb Pinkers there's a lot of attention on these technologies primarily from investing angle so didn't come out as surprise at that moment because chat GPT was always like, you know, this is a foundation models LLMs are gonna be a big thing, so we need to monitor. So there was a lot of monitoring being done, but what was surprised is that, you know, the growth and the pace at which we've seen adoption, right? So, uh, that was actually something I think we didn't really, you know, uh, grasp it, so. And as expected, a lot of questions being asked everywhere. So does it impact our portfolio? Do we need to reinvent our, uh, you know, due diligence process? Uh, does it actually change our investment team? So a lot of questions and, and maybe less clarity, uh, to be very honest. Uh, but, uh, I, I think one few things that we did right at that time was, you know, we said, you know what, let's have a targeted focus both on the, on the, you know, internal side. We set up a small teams to start really thinking, you know, should we be using that tool? What does it really mean? Uh, we started really looking at our investment thesis that we have been writing and should we start incorporating AI as one of the elements of this, uh, you know, we'll talk in a bit actually how it has changed, but. Uh, but we slowly started asking every 2-3 questions. So every time a deal gets presented, everybody's asked like, Oh, does Chachi PT has an impact? Don't know about it, but curious, right? So they're all just curious, uh, at the same time on the, on the portfolio side, right? So, uh, fortunately, you know, we didn't have any, uh, direct businesses which will directly get impacted, but, uh, we do, we did know that, you know, uh, software as a world will evolve. We didn't know what to do about it. So, you know, the first thing we did is like we looked at our software portfolio and called the, the top CEOs and said, you know what guys, uh, at least keep in mind, right? So we don't know where it goes, but it's something we should talk about it in the future, right? Um, yeah, no, at least a lot of questions, but we did like a more thoughtful approach to pick a few areas and start like having those questions answered, uh, like raised at least, um, uh, and start like thinking about where to focus on, you know. Excellent thank you. So Ravi, how about for you? I mean, you, you, you know, you had a roadmap before that moment, and then how did that change for you and what was that journey like for you? Has it been only 3 years? 3 years. It feels like 10 years, um, yeah, the, the evolution, the technologies that have come to the surface, so. Before I get to that question, just briefly, what we do, uh, in the banking space, we sell over 24 or 2500 banks and credit unions in the US, and most of them are community banks and credit unions. So think of us right as a brigo we are their innovation partner, right? So our goal is to try to stay ahead. Uh, because they don't have the budgets that, you know, the JPMorgan's and the Bank of Americas have, right? They're spending billions and billions of dollars on, uh, technology and innovation, so we act as that innovation partner for, for all of these banks, right, that are around the corner. So when Chat GBT came to the surface, uh, there was, um, a lot of initial awe and excitement, right? What does this mean, right? So we have a small advisory practice within, um, within our, our company, and part of that, um, part of the capabilities that they offer is understanding what types of updates FFIAC is making or what's happening in, in BSA space, right? Well, now you can actually upload the document and ask the questions, right within Chat GPT. So there was, you could already see the writing on the wall. There's going to be some level of disruption. But in the initial excitement and a it's more about OK what can we do with this technology, right? and um and we started deploying it internally, um, which is again for us as a technology company in a very regulated space we take a good hard look before we roll out any new technology onto our employees even right so. It took a little bit of, uh, you know, conversation and debate internally because this was, we're talking about early 2023, right? We've just seen Chat GPT hit the market and within like two or three months you have 30 million users, right, consumers, including ourselves. So that's when we, we started to see the potential of the technology and uh very quickly our teams um again we're in the workflow space, intelligent workflows, uh, whether it's uh fraud detection, anti-money laundering, loan origination workflows, so there's a lot of uh areas where you have to narrate what's happening. It could be a fraud alert or it could be a. A loan or a credit memo, right, so people have to write things word by word and our product managers started to see, well, could we actually use this technology to start writing memos in a, in a quicker fashion, right? If they spend 30 minutes today, could we give them a quick start that, you know, brings that down to 10 minutes. So those are the types of initial ideas that we went after and then it very quickly took off and we started engaging with with Nate and uh started building more um more complicated workflows um that again in the regulated space you have to think about a lot of other things that we can we can talk through as well so. Excellent. So and Chad, you were getting all these requests at that time, right? And when you look back, uh, what were sort of some of these, what were people, where do people wanna start, and if you look back, what were some of the misconceptions people had? Yeah, when we got started, um, you know, people in portfolio companies across private equity were, were very excited once chat GPT came out looking for, for ways to create value and I'll say pretty much everybody followed the same, same patterns, um, they were trying to create customer facing chatbots. Everybody wanted one since we saw what chat GPT had, uh, they quickly moved to. Let's look at, uh, doing some summarization, particularly in the pre, you know, the private equity space summarizing what's in the data room, summarizing financial statements, you know, trying to look for, you know, is this a good investment or is it not a good investment and can the tools really do that for me, uh, we saw a lot of, uh, people wanting to take their knowledge bases and extend it into that space and, you know, the, the interesting thing with all of that is. And they ran into a lot of challenges thinking that the the chat GPTs and custom GPTs that we're working on. Could understand all of their kind of less than structured less than pretty data and then it would just figure it out and they would get value with with kind of those messy scenarios and things just didn't didn't quite pan out for them as they were exploring those scenarios but. In the efforts that we were doing, we were trying to help these firms have their aha moment that this can be a really valuable tool. So one of the things that we pushed back on a little bit was rather than explore, you know, the technology for technology's sake and that, oh my gosh, we need to move fast is let's look at some real high friction points inside your business and look at how we could potentially use AI to solve some of those and Nate and I were talking, uh, before this. It's like what are some of the. The things that actually made it into production, I go, believe it or not, it was the things that didn't conform to the patterns of what everybody else was trying that we saw there was those novel use cases and one of them was it, I can't say that it went to production, but it solved a real business need. We had one customer with a, um, a catalog of images that they were looking to put on their e-commerce platform. And they wanted to enrich that data set and it was a very large data set of images and they wanted to have, you know, the, the multiple side views and back views and three dimensional images and hey, can we see that, what it would look like this couch in my house and those sort of things. And we used AI at the time to go through and take all of the, the data that they have and then have it reimagined and build the synthetic um images and pictures that they wanted to complete for their catalog and they did a fantastic job. So rather than having this super high friction uh problem where you're going through and taking new photos and you have to have a team that goes and processes the photos and then. You know, get them uploaded and make sure they were compliant and the right sizing and everything for all the different, uh, e-commerce platforms and places it was going to be listed. We used AI to do all of that and in a fraction of time it ripped through it and solved what was a real high friction problem for a customer, but it was a one-off problem. But doing those sort of things with customers really started to, to turn the light bulb on, you know, this isn't as neat as the technology is. Maybe it's not so much a technology problem, you know, because people have this technology solution trying to go out there and find things to solve with it. Maybe this is more. We should be thinking about high friction workflows and problems in the business and take a more a business lens to it and so we always were pushing back and walking back and saying let's not talk about the technology. Talk to me about your challenges and things that you want to solve for this is more of a business problem, a workflow problem than it is a neat technology solution looking for problems and taking that approach, our, uh, customer base really had a lot more success taking these AI tools into production. And moving forward with them and still working with us to accelerate what their business is doing and deliver value across the portfolio. Thank you, thank you. So Nate, you, you know, looking back, I mean there was a lot of, there was a lot of excitement around use cases and think about the number of conversations I'm sure you've been involved in. Um, when you look back, what sort of distinguishes, right, those use cases, uh, to, to follow on from Chad that sort of resulted in successful implementation, right? And those that sort of stalled at that, that sort of POC stage. Yeah, and I, I wanted to come back to a couple of things here because, uh, just the original TF was like what did every, what are all these PE portcos end up doing? And I think two things I saw, right? There's a, there's so much attractive stuff in the market around consumer AI that we see. In social media or elsewhere and it's hard to not get distracted by that, um, so there's, there's thousands of things you can do with AI, but back to Chad's point, it's like at the end of the day AI for AI's sake or AI for IT's sake isn't really gonna work. You've got to start with a business problem. Um, I think that there are some archetypes that kind of emerged and like to me the big takeaway I thought was like those people that prioritized had success, right? Like how do you prioritize what to to focus on and what is a business problem where there's a high degree of friction, there's a lot of value to extract. And the other thing is, um, just getting started with it, right? Understanding that you need to experiment and move quickly and like you, you think about what Abrigo did and, you know, you guys did some experimentation really early on, um, not all of it worked, I don't imagine, right? But, but the idea is that you get in there and you've got to experiment fast where it really matters, which was kind of your customer base and understanding, you know, how you can inject AI into credit memos or alert dispositions, wherever that might be. But in terms of like what, what drove them to suc like what, what, what was the differentiating factor that made a, a, a, a POC go to production or become realized, um, Robbie might have perspectives on it too, but like to me it was those that started with a business, uh, value prop in mind upfront, right? Like, um, too many businesses gave their AI AI problems to the IT people and said, oh, this isn't. These guys understand what that is and didn't think about transforming their business or thinking about the value in their business first. And so whenever we enter in any of these conversations. We start with value. We say, where is there complex in your business that's preventing you from either growing your business into new customer bases or expanding upon customers or new markets or taking out costs, right? Like, do you have a bunch of people that are sitting around processing documents, um, but that these are value questions and not AI questions. I think that's the biggest gating factor, I think in the successes we saw is like those that started with. AI for value as opposed to AI for AI's sake. So, so maybe just if I can ask you, Robbie on this because um I've seen multiple different approaches, right? One approach, um, is let's give everyone access to something and we, we're really proud as a company because we announced the number of people that have run a session, and that shows that we're AI enabled, right? But you took a different approach, you guys were very focused on key business issues. What, what made you sort of take. What made, what made you take that strategy? Yeah, I think to, to Nate's point and the question that you're, you're leaning into there, you should always start with the customer, right? End of the day, AI could be very, very transformative, but AI is a technology or a hammer looking for a nail that doesn't work, right? I mean, we have seen that again and again over the last 25, 30 years, uh, with some of the technology waves that we've seen. So what we've done at Abrego again, um, if you take a specific product set, right, like loan origination as an example. About 1000 customers use us for loan origination, 1000 banks and credit unions. So what that allows us to do is see how they've implemented the workflows. And, for example, if you take a community bank, around 2 to $5 billion asset bank, they would have anywhere from 5 to 15 people that touch a loan application right from inception from a relationship manager creating a lead all the way to, uh, to that loan getting approved or some decision made on it, right? So that's 5 to 15 people, right? So what the, the systems that we have and the usage analytics and the data that we have allows us to see is where exactly are they spending time within the application that is a friction point and where are they spending time outside the system, right? And how do we bring them on. To, to the, to the flow that we've created and how do we remove that friction right as you go through it like oh they spend, you know, again 15 minutes, 20 minutes creating a credit memo. Can we give them a better start so you save another 10 minutes again our mission. Rodrigo, again this sounds like a sales pitch, right, uh, to maybe there's no banks here so it's not very useful, um, we, our mission is to, is to help the community banks, right, be very efficient and yes it helps, uh, helps them increase their, uh, net interest margins and all of the goodness that comes along with it, but truly if you think about a community, right, or a community bank, it's built on relationships, right? They know what the local business coffee shop's doing, right? And how do they make them successful? Is there a capital need coming up that they need to help them bridge? So those are the unique insights that they have because of the relationships, right? So if we can save 10 minutes from them doing the undifferentiated work, the grunt work in the system, and have them spend time in building and nurturing that relationship and know their customers better, right? That's a win, right? That is our mission. So every time we, we look at these workflows and having the scale allows us to see where they're spending the. Time and how do we save a few minutes here a few minutes there? It doesn't need to when people talk about, oh, that role's going away. There is no credit analyst, there is no credit underwriter, right? Go do something else. Well, yeah, maybe there's a path in the future to that, but there's a, there's a time in between where you don't wanna completely disrupt, right? A workflow that's been around for 30 years, right? So what does that journey look like and how do we give them the power of AI without pushing them to reimagine the entire workflow, right? Is what we have done. So Johnny, a question for you then, when you look at, you look at your portfolio, you look back in time, what is, you know, just your perspective in terms of how do you know the ones that are gonna work, that don't work? What, what were those lessons that you learned? Yeah, no, it's interesting, right? So just hearing from Ravi and, uh, Nate's point of view, I think as a, as a PE owner, I think we have a little bit of a more a different angle to it, uh, you know, which is more like a financial lens to it too, because there are use skills that really work, but they don't make financial sense, right? Just to give an example, you know, I have an insurance portfolio company and underwriting is everybody talks about it in our workflow that we can automate. You know, when you look at the bottom of it, they're spending $5 million on underwriting. Great job, but like, what's the point of automating, right? So you're losing a lot more personalized hand touch white glove service to an automated tool, you know, with spending so much money, but there's no ROI to it, right? So, it's one of the things that I think we constantly debate on is, you know, what's the actual true impact here and do we need to even focus on it, right? Um, another example for us is like, oh, let's think about customer support like an automate. But if it's not core to a business function, uh, to a business, and it's not gonna be the one that's differentiating for you, so do we want our CEOs to spend time and energy on it, you know, so I think a lot more prioritization that happens in our sense to a little bit higher level on where we should actually be focusing on, uh, you know, even if AI is gonna make an impact, should be doing it, you know, both from a financial sense, from a competition lens, so I think those are the questions that boils down. And, and I know a little bit, I think, uh, you know, uh, Chad and Nate, uh, like we talked about it, we, we also begin to focus on prioritization because we're, we have a short window to influence an organization and we don't want AI to be a distraction, right, because there are only so many things that we want to change in an organization. So with all that lens, more and more, you know, what, what we have learned actually in this process is to be focused. Uh, you know, we, we have a lot many portfolio companies, uh, where we can do AI and maybe they use skills that actually work, not as a, you know, horizontal but like a very focused workforce. There are a few companies where we think actually where, you know, there's a true value, there's a true commode differentiation, there's a true timing that that matters most. And having them starting there and getting the right alignment in the CEO leadership is where I think we've seen the unlock, right? So because we don't want to embark on this journey and the, and, and the rest of leadership not getting there, so. So maybe let's, you know, one question I guess open up to anyone, any one of you, um, is we look at the, there's the business focus, you've talked about, business problem, the customer, great. But now let's, let's go to the, to the, to the engineering talent and the developer, right? Um, what was, what was the fit at that time, what was the learnings from, you had a development team, you got development teams already doing work, this drops. Any sort of perspectives in terms of how easy was it to to pick up things? Was it difficult? What any, any sort of perspectives from the audience from my, from you guys just volunteering Ravi because you know he's the chief technology officer, but happy to answer after him, you know. Thank you, Chai. Um, yeah, so there's two different perspectives here, right? So one is when the real coding assistance tools really started taking off for the first time in my career, and I've been around 25 years. Engineering is no longer the bottleneck right now you're creating a lot more code, right? And whether it's you're going in the right direction or not is is a different question, but you can certainly get to an answer pretty quickly, right? and create work products. So that was exciting, but also that exciting was met with, um, a little bit of either you, you can call it bitterness or skepticism, right, especially with, with some of our like top tier folks well. I don't like the style of uh what this coding assistance right is uh is putting out right doesn't meet some of the standards or formats, right? Like design aesthetics, you name it um so it took a little bit of uh maybe cajoling and convincing to say. Yeah, maybe we're not writing for human consumption. Maybe this is going in the direction of maybe agentic consumption where there's more agentic coding algorithms that need to understand and maybe it doesn't need to look as pretty as long as there's documentation around. So there's things like that, the bridges that we needed to cross, uh, I don't think we're ever out of it. We still have the dialogue and debate, uh, within the engineering teams. But one of the other things, the second perspective of all of this is, as I was saying earlier, for the first time in my career, right, no one can point at the engineering team, right, or half of me is not pointing at the other half and saying, right, engineering is slow. But what that meant was the bottleneck now moved to the left, right? How about UX, right? Can UX move fast now or product management, right? Uh, how do you vet an idea, right? The ways in which we've, uh, vet ideas and we are always again, as I was alluding to. Earlier we always start with the customers, so a lot of customer interviews we want to have the idea, we want to build prototypes. When I say prototypes, is wire frames put in front of a customer, and, and that process usually took, you know, 6 to 8 weeks, right, depending on the size of the idea. Well, guess what? In the new world, the experimentation, the prototyping, the cost of building something is near zero, right? So why do you have to put so many words around an idea? Just show it, right? Build it and show it. So we've been actively taking a look at. Just the same way we are talking to our customers and saying you need to start thinking about reimagining how work might flow irrespective of, uh, whichever workflow that might be, it could be an anti-money laundering workflow or a loan origination workflow. We're also drinking the same, you know, Kool-Aid, so to speak, and saying or challenging ourselves to say the way we are building products, it needs to change, right? Because what used to take, you know, 4 or 5 people in a garage, 6 months to get to an idea. Uh, or a demo idea now takes a weekend, right, or a couple weeks, so that I think is the biggest shift from, from our perspective that we've been openly challenging and changing the overall process of developing products. Yeah, we had a very similar, uh, amount of friction with the developers even internally at uh AWS with, you know, my team included, you know, very, very skeptical about the quality of things that were coming out and it wasn't meeting our patterns and whatnot and I took a different approach with our, our customers and even the team talking to them about how to think about this. I said really if we, the, the returns you're gonna get are really gonna be in the, the green field space where you're looking to build something new, build a proof of concept, let's use this as an enablement tool to understand, you know, help us understand. The customer's vision of what they're asking for and build them real rough prototypes really quickly and get buy-in that we could then down the road develop into, you know, through our normal processors and make it something that was, uh, more production ready. And in doing those exercises, we started pulling in some of our customers and doing the hackathon type programs and getting them excited about how quickly they could greenfield something. With the expectation that eventually the tooling would mature enough that we could actually build in the controls to have it follow the architectural patterns and standards that existed inside of, uh, you know, individual companies and what they wanted to see done and that's been the case it's been developing that way and and been quite good, but in doing that it got the, the engineers, especially those that are typically the ones that lean in and want to learn new technologies, usually it's your senior guys got very excited about where this could go and became ambassadors and advocates inside their own portfolio companies. And they're the ones that are driving, you know, hey, we really need to, to think about this. We need to drive it more in our business. We need to be teaching my junior guys how to use this. They understood how to communicate with the tools, how to get the value out of it, so we've seen it really accelerate as the, the space is maturing that we're seeing a lot more, you know, trust in the tools and, uh, the thing I'm running into now with the friction point, uh, is that. They're wondering about the, the architecture designs behind the code. Now they have something functioning. Is it done right? Like, and I'm saying, well, does it matter if it works? And you know, what if I get, you know, a, a bottleneck in my scaling on the cloud or whatever? Well, why aren't you just feeding that back into an agent to change either the architecture or change how it's being deployed and have it solve for it? Why do you care if you're still delivering value to the customer quickly? It is it really a problem? And so I'm challenging to think about the way they're doing things and I said, what you're gonna really run into is that you're gonna get so fast at doing things is that QA is going to become your bottleneck. Think about how you're going to start solving that problem now because you're gonna be producing so many new products, so many new revisions, so many new features that somebody's got, you know, besides having agents doing your smoke tests and verifying things, somebody, you still need to have human in the loop somewhere. To make sure that the quality is, is in place, and that is the problem you need to be thinking about solving for. That's the next challenge you're going to have as you adopt these technologies. Uh, you know, I, I think I agree with, with, with all that. I think, you know, the, the question of like talent, which I think we're trying to get to is like, like, no, no, no, that was great. I, I was just coming back to it because I think all that's relevant. You can disagree as much as you want. It's fine. It's fine. I is the question talents, I think there's still a lot of people out there who could put, you know, a prompt into whatever and create code and wouldn't know what the heck to do with it, right? We, we still have people out there that, that, that still think Python is a snake, right? Like they don't know what the heck to do with Python, right? And so I think there, there's that group of people, but there's also this group of people of technologists who are really deep in this who don't understand anything about value creation within a PE porto. So you kind of got that dichotomy at both ends of this. And so, I do think that there's a lack of, of talent that's right for making the most out of generative AI within businesses, right? Like, it's not just, oh, anybody off the street, just pull them up and we can turn them into coders and we can write a bunch of crappy code that senior devs want to look at and. Kick back to the junior devs. Like I think there, there's, there is a, a gap, I think, quite frankly in that talent and oftentimes what it is is you're seeing people that, that aren't your PhDs in data science that are the people you need and they're not necessarily like the accountants from your business that you need, but you kind of need somebody that has dual hats and they can really think about like value within a business and also can interpret. And, and, uh, um, prompt an AI model, get things done that like that's where we've got to adapt all of our, our people infrastructure needs to adapt around that type of thinking. I think one thing, if I, sorry, one thing that I, uh, I'd like to add to that, I totally agree it does start from the top, right? You need to set the right tone, right, because product managers, you need to know how to code, right? I think somebody was saying it yesterday at the, the conference that. The, the biggest lie that's being told everywhere is that there's gonna be less quarters and less coding. No, in fact, I think there's gonna be more people that'll be coding, more people that'll be prompting that have to know how to prompt. So if you as a leader of a business, right, especially in, in PE because I think a lot of value, right, is up for grab, especially if you're a workflow company. Right, and understanding and we can, we can go into all of that, but you as a leader need to start getting your toes, sweat, fingers in there, make sure that you understand the power of what, whether it's a coding assistance tool or, you know, lovable, you know, you pick your favorite tool, right? But you have to understand and appreciate and then push it down, right? And then you'll always find the grassroots champions, right? So that's when I think, uh, truly a cultural change will start to happen. And one of the things that I found is really interesting as uh people are moving forward with the tools and enabling their uh the teams by just handing out a tool they're expecting their employees to just know how to use it effectively and nobody's really investing in the workforce and doing the training to get the most out of these uh LLM models and I'd like to stop and make them think like you know. Do you ever, you know, interact with your, your junior developers and some of your IT staff and ever think, wow, that's a really great communicator? I mean, are they the best communicators out there? Probably not. We're all have a little bit of autism and don't know how to communicate with one another. Well, that's the fundamental skill you need to make these large language models do what needs to be done. You have to communicate your intent with your context to get something out of it. And we're just not teaching people that we're giving them tools and expecting them to succeed. And what we're finding is that. Those senior devs and leaders that naturally work with junior devs and know how to communicate, uh, what tasks they want them to perform, do a great job working with the LLMs and getting them to do tasks and provide good output. But those junior devs, those people who haven't been put in leadership positions, don't know how to communicate effectively. They struggle with this, and it's not just that they, you know, the IT people struggle with it. This is across the entire business. Everybody we want to enable to start doing these things needs to fundamentally have some understanding of how to communicate, how to task manage, how to work with other individuals, and we're just not doing that today. I think we're missing that as an enablement piece. So we've we've talked a little bit about, you know, the then, right, when this, when, you know, 3 years ago, which feels like 10 or more years, maybe, maybe each year has been dark years, right? Dark years. And then we talked about some of the learnings. Let's, let's go into the today, right, the now. And start, you know, I'll start with China and then get also, you know, Nate's perspective as well as we think about, you know, how private equity is thinking differently about AI and then you and I were chatting a little bit about how you're thinking about diligence and and our difference. Maybe you can sort of talk a little bit about what you're doing today from that, from that perspective. I think one big change from the last few years is there's a lot more clarity and emphasis on AI. I think it was more an optionality in discussions maybe in the last few years and the few people who used to ask, not everyone, but now it's a, it's a must, right? So just to give an idea, every time we actually go to investment committee, we need to actually give a one pager both on the risk and the opportunity on the AI side, right? It's a must. It's not a. An optional thing to have and you need to actually be able to clearly defend, you know, that you're not gonna get disrupted in the next 5 years, right, because in some way if you take an appeal lens, you know, I'm the exit is gonna happen in the next 5-10 years, it's not gonna happen in any time soon, so we're really talking about long forward risk, right? So even if there's a small risk here, it's gonna amplify the, um, well, the other thing I think interesting is, you know. AI is a value creation lever always existed, right? And I think even before Gen AI has come, like people talked about AI as a value creation lever, but it never was actually like, you know, one of the standard toolkit to be used, right? So it was always like, oh, you know what, in one of those edge cases we'll find some interesting data, we'll use them, but now it's becoming a more and more common practice to, you know, say, you know what, let's think about it. But then it comes with challenges, you know, uh. Nate and I were chatting, you know, looks like every time he does a due diligence they ask him how much is the cost and, uh, you know, how much is the impact on get, but we don't know it yet always. But, uh, uh, but that's, that's basically, you know, the gap that, you know, the folks are actually currently in, you know, it's trying to figure out how do you actually come with the right guesstimate, you know, not an estimate. Uh, because you're, you're trying to really forecast the future, uh, uh, and trying to see how to get a balanced, you know, VC view on this, um, and, uh, and try to make sure that you, you know, you set up well for success. So, so I, I totally agree with that, um, uh, on the diligence question, it's interesting to see how it's evolved because I think when we were asked. Shoot, 22 plus years ago, um, we started getting asked questions around how do we diligence AI and it was very much like a maturity and readiness question, right? How ready is this organization to adopt AI? Um, I see the question very much now evolving into, and I'm sorry, I guess I'll say the other thing is, it was how ready are they? And that was kind of like a back office question, right? It's like, do they have the right data? Do they have the right tech stack? Do they have the right people? Kind of a check the box kind of question. Um, in diligence now it is much more of an existential question. Like the primary, well, not primary, but one of the primary questions we get now is like risk is like can two guys in a garage replicate this thing? And unfortunately a lot of the time our answer is. Yeah, probably. But you still have a product mode. You might have a data mode, you have a customer mode that can't be overcome. So I think that's the big evolution I've seen is like, it's no longer an IT enterprise question of like, check the box, is this thing ready? It's a question of. Well, what does the market look like? The commercial diligence, it's an operational question of like, to your point, cost takeout of call centers or engineering teams or whatever. It's a, um, product question of like in the market, how are we relative to our customers in terms of features like this AI question has grown and grown and grown and grown to where you're exactly right. Like every IC is asking about. The disruptive risk of generative AI the same way, and I had a, another, uh, PE firm explain it this way, which is like, oddly enough, we're here at AWS but they're like, you had to answer the question of Amazon and how it would disrupt any sort of consumer packaged good. You know, 1520 years ago, whatever, you had to answer that question to an IC like, you're gonna buy a clothing brand, you're gonna buy this, you're gonna buy that. What happens if Amazon gets into this and disrupts this business? That question is now AI. It's like, what if AI can disrupt call centers or it can disrupt, you know, factory floors or, you know, the generation of manuals or whatever that might be. Like every IC is asking this question now and it's very much more of a risk question or it's evolving into a risk question and an opportunity modeling question than necessarily like a check the box maturity kind of diligence. Yes, excellent. So when we think about um. You know, you know, into, you know, Robbie and Chad, your, your, your perspectives, you guys are seeing multiple production, you had multiple successes in rolling things out, and if you look, if you look into this audience, right, you're gonna have private equity executives, you're gonna have, You know, CTOs, CPOs, you know, other executives within, within companies, and you're gonna have people in the development community, right, different levels, right? What, what sort of lessons should they take in terms of components that made, made you, yours, you know, when you look at the success, what resulted in that success, and therefore what should they be thinking about, um, to improve the chance of their success within their companies and within their careers. Yeah, I'll start. Uh, one of the things I think is key to success is really understanding, well, I guess there's more than one thing, there's several things, uh, one is understanding that, you know, the business problem you're trying to solve so you're, you're laser focused on, you know, focusing on something that's impacting the business rather than the technology and do I have the data in place today to support that initiative? So, uh, the companies I've really seen accelerate in every way are the ones that. Uh, took approaches early on to build that, that data lake lake house architecture with their data, not for the sake of Gen AI, but to enable every other part of the business to more quickly build their own dashboards, fish for what they need, make data-driven decisions at every level. The companies that lean into that data. Uh, whenever they look at generative AI for solutions, tend to, uh, have everything in place they need to accelerate the adoption of that and have really tremendous outcomes where they're able to do the complex things they want to talk about, have really low rates of hallucination, you know, if at all, keep them down to something that's really acceptable. Uh, they already understand, uh, the scaling piece a little bit so that you don't run into those type of, of challenges, and they've already got a team, a fundamental team that is, uh, developed enough, so they don't have that talent challenge. They're already thinking about data and how it's going to be used and, uh, planning for the future and so that's fantastic as, uh, as an initial thing. And if you don't have that or it may be not perfect, I think the way you can move forward without kind of clipping the wings of your teams that wanna accelerate adoption. as you analyze your, your use cases, uh, for generative AI and, and what you can do to accelerate some business outcomes, grab two or three of those, look to see if data is close to what you need to kind of go from that, uh, proof of concept where you've got, you know, small sample size of clean data to to the, the, uh, the production readiness, and if it's not there, just put a program and some governance in place just to accelerate that use case and start doing it piecemeal. That's what I see too often. Is that companies have realized they've got some, some data challenges, and they've stepped back and said, well, OK, we need to have build, you know, strong data governance. We need to go do some data cleanup, and these are business problems and it's not an IT problem. And so you've got to get all the different stakeholders involved to get your data clean, and it really slows down your rate of change in your progress. You end up spending all of your time getting mired in data and you see no value out of that because you're just basically putting governance and rigor around it and everybody's like, why are we doing this? It sounds like a lot of work. But if you can just piecemeal it and know that you know your governance programs and stuff will adopt and be, you know, it's a constantly evolving, growing thing and just focus on what needs to be done to accelerate your business, those companies tend to do quite well. I mean, in every way you're trying to enable them to not clip wings, and they'll move forward and, and produce something of value. That's, that's what I've been saying. Yeah, I, I cannot agree more with you, Chad. I think as, uh, Yogi Berra would say, it's 90% culture and the other half is technology. Right, um, look, I think, yeah, there's a lot of learnings. I think one of the things that, uh, comes along with trying to at least stay at pace with what's happening in, in, in AI, um, you tend to make a lot of, a lot of mistakes, but again, mistakes is just a perception, right? You learn from those mistakes and then you course correct. Um, one thing that I would add to everything that Chad said is hesitate. Away from big transformation projects in preparation for taking on AI, right? Just relax the rules, right? Governance is important, especially in a regulated sector, right? Make sure that you're not doing any anything stupid, but. Fast paced, right? How do you get that pace into the team so they can experiment, right? The other thing that I would say is, um. AI as a magic wand is just being waved at everything, right? We see it in the space and to Nate what you're saying, we run into prospect meetings with um with no regional banks, community banks, and. They would often talk about, you know, somebody that's been around for 6 months and they built an entire loan origination system and they're gonna make it exactly like we want, right? These are the conversations that we'll have and some of the things that get missed in the process and sometimes we lose those deals and we know that they're gonna come back because the last mile of software still exists, right? By that I mean is the services, the support, it's all the things that AI can do today anyway, right? So it's important to. To make sure that you're not overindexing, right? And the third thing I'd say is, um, I was alluding to this earlier, right, have an intricate understanding of your customers and the workforces because when anybody waves that magic wand of AI and that you can AI everything, well, which parts and why. What is the value that's being added to the point that you were raising earlier too, right? What is the value that's being created here? Make sure that you have the wisdom to differentiate between the deterministic and indeterministic workflows, right? How does it improve the customer experience? Like, for example, you know, if you get a, uh, you're applying for a loan and you submitted the documents and by mystic submitted a 2023. Let's call it a W-2, right? But you're looking for 24 W-2, you can have an AI assistant that's actually looking at it and then immediately responding back to you, so you're not waiting for the, the data analyst to come back the next day, right? That improves the customer experience. So do that, right? Because the interaction is interminiistic, right? You're learning from what's being. Uh, exchange, right, so those are the types of things that I think it's important not to, you know, wave the AI at everything and make sure that you understand where do you truly add value. The, the one thing I was gonna, yeah, that, that, and that's great, right? And I was gonna chime in with something similar to that is like I think you emphasize the data piece and like 100% like people's data is not ready and there's a bunch of problems and, and I'll say from like we've done probably 100+ diligences in market for different PEs and to the. Maybe not surprising, but maybe no one's surprised that mid-market PE portos don't have good data. That's not a shock to anyone, right? That there's a lot of poor data out there in, in companies, even, even bigger ones, even bigger ones that don't have great data. But the other gap that I think people disregard sometimes is, frankly, that human infrastructure gap, that talent gap, and whether that be, you know, we've got some people who are. Um, uh, hiring PhDs and whatever or hiring BI people and converting them into AI people is like that talent gap, I think is very real within a lot of the companies we've looked at. And like you think about it, Robbie, like the amount of talent you've, you've. Broughtten in over the last couple of years, I feel like you guys have got a fantastic set of AI people that know how to do this the right way. And as we've diligence companies, we see a lot that fall short in talent, not, not just, not just data that's a mess, but they don't have the right people who understand the true art of the possible and the art of the implemented. And, uh, you know, I was talking with a, a portco literally a couple of days ago, um, that we did diligence on, and they said, yeah, we've scrapped. Half of our engineering team because we didn't have people who could actually deploy AI. And so I think you're, that's the other thing that I don't think people are realizing is that this is a more of a people problem than they realize is integrating AI into their business doesn't require a people change too. Yeah, and just to add to that, uh, when it comes to the, the people problem, there's really, you know, some psychology to this, uh, believe it or not, and I think that exists in everywhere when it comes to transformations, but if you think of how to frame the problem, if we, we look back, I don't know if anybody's aware of this example or not where you drop off, uh, somebody in the African wilderness and they're told to survive, you drop off a lion in this African wilderness and you drop off a zebra. They both have the same problem of surviving, but the way they look at the problem is fundamentally different. And when you take that same approach to AI and you've mandated where potentially, you know, your teams can't hire anybody new, they've got to try and solve this problem in an agentic or an AI approach, that has a real negative connotation. They feel like they're, they're being mandated and under friction. But if you say, hey, I want you to pretend you're a startup, build a little tiger team and see what you can get done over here. It's the same thing you're doing to them. You're not letting them hire new staff, but you're making them, it makes people more excited to have that, that reframing of the problem that now you're in this startup mode, and we want to see what you're capable of with this little tiny team. People get energized by that and we'll go do something fun, like, hey, the rules are off. Just go play. This is now a green field for you as a new team. Go do it because this is what you'd be forced to do if it was a startup. Have at it. And people somehow the psychology thing and magically get bought in and we'll start doing this and and move forward and those people become your ambassadors and advocates as you uh move forward and we're gonna, we'll then we'll move to questions for the audience in a moment. Yeah. Do you wanna go for it? Yeah, yeah, no, I was gonna build on what you were saying, Chad. I think people do gravitate towards that, you know, silo startup go create, right? I'll give you a challenge. And just go after it like we had this product, it's a pretty squeaky product, I'm not gonna name it, um, that's been around for, uh, multiple years and every single time I've been with the company for for about 4 years, every single time a conversation came about rebuilding it, it was 12 to 18 months, so it was always in the distant future. So middle of this year or somewhere in July or August, um, we picked like a small group of people, like literally 4 or 5 people, including the product manager and the UX person and say, hey, just go. I'm gonna give you 2 weeks, right? And if it fails, falls on its face, it's totally OK. Just go try to rebuild the whole thing in 2 weeks. And everybody looked at me and saying, right, everybody that's supporting it, right, you guys are out of your mind, right? That's not gonna happen. I said, OK, no, just go try it, right? Um, go to a location, sit down and do this. Now they failed, right? We didn't rebuild the whole product. They got about 50% of the way. 50% right and we are releasing the product right in January and this was back in August, right? So the out of the pause sometimes we hold ourselves back, right? So just create the environment where people can actually go and just again it's cliched but give them the right to fail, right, and give them the right tools and they will deliver. Just, just one small thing, you know, from an investor vantage point of view, honestly we have seen it's not data. It's not actually tech team or talent. It's broad leadership alignment, right? So the most successful programs we've seen is they had a crappy data, but the leadership was aligned. They made it happen, right? So they made it to the challenges, they, they showed the perseverance. Uh, and it's, it's, it's a recurring pattern we've seen like the every time I've seen a success, honestly it falls down to leadership alignment, right? So in fact our prioritization starts with leadership alignment now. So you don't actually engage even have the best idea in the world, best product in the world, that you know, we don't wanna do it if the leader is not aligned, right? So, uh, and it sounds a little bit different because, you know, we're seeing it from a different vantage point of view, but that's the single most success factor we've seen. Excellent, excellent. So um I wanna turn it over to the audience uh for any and all questions. Uh, someone can put, if you wanna put your hand up. Hi there, um, and since you've been covering like all the implementation, I was wondering if you all could touch on how you are securing AI because that has been a challenge for our organization. Um, not just making sure people don't put it in the wrong places, like put good data out in places they shouldn't be putting it, but also if people have access to data and AI, somebody gets into their system and they get in that, that AI, they can extract a lot of data very quickly. What have you worked on anything like that with your organizations, you know. Yeah, see, I, I think it's, see, just to be very honest, I don't think people have figured it out, right? So, in fact, we just had a conversation yesterday with all our portfolio CTOs and, and we've been discussing about how they're thinking about it, honestly people, there's nobody had a clear answer. But a couple of friends, right, I think we are noticing one, I think it's Security comes in different lenses. Um, it starts even thinking about, you know, from your data, you know, you do when you have usage policies for this data. People take it for granted in many cases that we're using this, especially in the regulated industry. So before even like thinking about security of the city, how do you use this data? Do you want to use this data? Um, I think the second step that is actually gonna do is when you're offering services, right? So I'm sure like we're all creating reports, we're creating chatbots, they're gonna actually have their own. Certain set of uh quality issues and and challenges there so how are you thinking about from that angle where is the how are you trying to be thoughtful there um but to your core question on on on security I I think it's a. It's a good question, and I think people are trying to really think about from a validation, you know, guard race that you want to keep in inside your models when you're actually throwing the outputs back. How are you trying to actually have a. Uh, you know, telemetry set up to actually monitor everything that's being done. How are you trying to refresh the output? How are you, how are you trying to make sure the model refreshes are being, you know, thought through and there is a sufficient harness, test harness built into it so that you are validating everything that goes through. So there's a lot of technical problems to it, but I just want to like underline the fact that there are like fundamental questions that people have really not solved about data and providing services there. Um, I, I, I think it's an open topic and people are constantly thinking about it. Yeah, I would say it's an open topic too. I mean, for those of you that, uh, may not be aware, uh, why we care so much in some of these things, especially when we are first turning things loose like, uh, queue against your, um, your, your file servers and, and using it internally to, to source data. One of the early challenges, uh, that would come up was, you know, should everybody have access to this data? Well, it's not really that big of a deal. It seems like it's OK. They already got access to it from a file system level. We don't really have it well aggregated, so just go with it. I said, wait, step back for a second. Have you ever wondered, looking at, you know, Department of Defense and government data, why something that gets to be unclassified, why it was ever classified to begin with, it seems like a mundane piece of data. I said the reason that is if you take enough of those mundane pieces of data, somebody who's really clever could get a material view of something that's going on and that can happen with your, your file servers internal data sets as well when something we don't want. So early on it was that data segregation piece, making sure data was segregated and your, your AI tools can only access certain things or people could only go down certain uh knowledge paths with those tools. Now, not to sound like a, uh, an AWS sales pitch, but we are, you know. Uh, encouraging a defense in depth approach using the bedrock guardrails and tools and the agent core policies that that gives you a lot more granular control than you've had before and maybe have anywhere else. Uh, but yes, this is still something that's, that's developing, uh, as we move forward and it'll continue to develop and eventually we'll get there where everything feels nice and tight and tidy, but, uh, there's still work to be done for sure. So it hasn't been solved yet. Over that we got a question over here. Um, my question revolves around, um, sanctioning the AI project. Um, so in some organizations, they expect you to win, they expect you to. Um, to succeed. So, what, what are your thought processes when you're trying to sanction an AI project? Do you go for the quick wins? Do you go for the big? You know, project and you know wanna execute everything at once. I just wanna know. Yeah for me, uh, whenever I'm looking at a project this is all about building trust with your stakeholders so I'm going for those quick wins and clear paths to victory so that I can build both initiative, muscle memory and get the continued investment that I want to move forward with those bigger projects. So I'd always start with something that's achievable and something that's quick and move forward with it and and showcase it to the team because you want people to get excited you want broader adoption if you have. Uh, negative, you know, I'll call them failures, uh, which are fine, but if you have a bunch of failures or you take on something that's too big to achieve or you, you've hyped it up a lot and it doesn't work, you start creating almost like this cancer inside the business where they think it's not going to work. And so, uh, we run into that challenge with transformation, uh, initiatives in general that you've got these kind of transformation antibodies that don't want to move and if you give them, you know. Ammo or and give them data where they can say well this failed this failed this failed they'll talk amongst themselves and amongst other people and then start poisoning the well so you've really got to have a couple of those good wins up front in order to build that momentum and kind of keep the naysayers at bay so I would go that approach. Yeah, I completely agree with that and it's also, it's also a capability question, right? So you're also proving that. You can actually build it and deliver values so it just creates a nice credibility factor there and I think Robbie hit it. It's like to me this question of failure, like, you know, some people here might know about the Prompt 100 program AWBS did where you're doing POCs in like 246 weeks or something. This is even more rapid now. The way we work with Porcos now is like. A day. Like we, we've done some where it's 24 hours. Sometimes it's an afternoon. The point is like, this failure doesn't, should not be a couple of months exercise. And as Robbie described, put two guys and say, go to, Miami for a couple of days and see if it works like that's what I think we need to frame reframe the conversation is like if it's AI prototyping, it's like you should be able to fail really fast and really inexpensively. But I, I think there's, there's also, there's also a piece I think to go back to comments you guys said about having business alignment. There has to be, you can't just assume success. You've got to allow things to fail and learn from that failure, otherwise if your leadership is not. And this is where I think I, I, I, I keep reinforcing it like if your leadership is not aware of how to do AI program, then you're set up for failure, right, because they need to know that it takes time, it takes perseverance. They have to fail a few times. There's a journey to it, right? So. Uh, it sounds fascinating, but you know, the single most thing is leadership training on the business side, not on the tech side. On AI is actually the single most thing, and then getting them aligned on, on your targeted question, right? So I, I, I, I think actually it's a 9, right? I would not take an effort to take small ideas because sometimes, you know, it slowly gets to a very, like, you know, is this high value ROI? Do you wanna do this? So I would almost think about two pronged approach. Where you try to get some quick wins and get some excitement but in parallel have big bold ideas and maybe what we need to do is how do you eat your elephant, how do you bake the big bold ideas into small pieces to chat point to to show progress, I think is, is where the, uh, the unlock is. But if you don't focus on the big bold ideas, wait for it. Your CEO is gonna, uh, any leadership is gonna call in 3 months and say let's. Reprioritize this, you know, yeah, and that's one of the thing I don't want it to come across as treat it like a pass fail. Just look for the ones that you know are gonna succeed. Frame this all as experimentation. That's really what this is. You wanna encourage people to experiment. They do need to have room to fail, but don't, don't cast it in that this is a, a pass fail type scenario. So, well, we, uh, we're coming to the end of, of this session. Uh, first of all, I'd like to thank the, the panelists, Chai Robi, Chad, Nate, uh, for, for your perspectives. I'd like to thank the audience, uh, for spending time with us. We're just starting, it's been 3 years, and we're just starting to see things in production effectively, but there's so much more to go. We haven't even started talking about the impact of agents and everything else that's coming around. We're gonna be in this journey for a long time. There's a lot to do, a lot to learn collectively. Um, so look forward to going on this journey with the rest of you. Thank you for your time, thanks to the audience. Thank you.